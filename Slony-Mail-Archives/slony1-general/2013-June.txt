From anthony.vitale at sonymusic.com  Wed Jun  5 13:07:48 2013
From: anthony.vitale at sonymusic.com (Vitale, Anthony, Sony Music)
Date: Wed, 5 Jun 2013 16:07:48 -0400
Subject: [Slony1-general] Slony Versions and Postgres Versions and the
	compatibility.
Message-ID: <538A19CDA8C5CA4F87977C0FA47689F101743DC86405@NYCMNET7EV003.mnet.biz>

Hello Guys

I am new to the Slony World as well as Postgres World, And I have a question which I have tried to get info from the Web but without success. (At least maybe I did get the answer and do not like what I think I understand).

Here is my question.

At a point in Time,  I have a Master Database running Slony V 2.1.0 and a Slave Node running Slony V 2.1.0

I now need to update Slony to v2.1.2

Can I upgrade the Slave Node To V2.1.2 ahead of the Master being upgraded to V2.1.0 or vice versa

Or Does Slony require 100% version equality within the nodes at all times.
I Got this From the  Documentation Regarding Minor Slony-I version Upgrades.


*         Stop the slon<http://slony.info/documentation/2.1/slon.html> processes on all nodes. (e.g. - old version of slon<http://slony.info/documentation/2.1/slon.html>)



*         Install the new version of slon<http://slony.info/documentation/2.1/slon.html> software on all nodes.



*         Execute a slonik<http://slony.info/documentation/2.1/slonik.html> script containing the command update functions (id = [whatever]); for each node in the cluster.


Which Would Say the Answer to my question is YES.


But What Happens if Based on Geographic location of the Servers within the Clusters Not All the Nodes Can be upgraded at the Same Time,


Is there any Real Time Experience out there with anyone who  Can Tell me how they may have upgraded multiple servers in which on some nodes based on the availability of the techs overseeing the servers there was delays regarding the Install the new Software Version in the steps above.

Is there a Hard Rule that the Slony versions must be 100% exact at all times, or can a Target Node be at a newer subversion ahead of the Master and still run.

And Since It appears that the Slony C libraries Are put out there as part of the Postgres libraries it would appear that I would not be able to have the upgraded lib .so's out there on the Target server just waiting to be used, the Make needs to happen after the Stop.


Thanks



-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130605/e19b4c19/attachment.htm 

From ragavendra.dba at gmail.com  Thu Jun  6 01:16:56 2013
From: ragavendra.dba at gmail.com (Raghav)
Date: Thu, 6 Jun 2013 13:46:56 +0530
Subject: [Slony1-general] Slony Versions and Postgres Versions and the
	compatibility.
In-Reply-To: <538A19CDA8C5CA4F87977C0FA47689F101743DC86405@NYCMNET7EV003.mnet.biz>
References: <538A19CDA8C5CA4F87977C0FA47689F101743DC86405@NYCMNET7EV003.mnet.biz>
Message-ID: <CANwAqWheEi_FgBJYPQ=1K+g6Ntagy76bQ6CBPwkJGBbw13aNyw@mail.gmail.com>

>
>
> **
>
> Is there a Hard Rule that the Slony versions must be 100% exact at all
> times, or can a Target Node be at a newer subversion ahead of the Master
> and still run.
>

AFAIK, Slony-I need exact version on both the node's.
Because upgrading a node to newer version will do changes to slony_func.so
file where it should match the version with the other replicating nodes
else it will break. Below link has a comment after the upgrade steps as " If
there is any mismatch between component versions, the
slon<http://main.slony.info/documentation/2.1/slon.html> will
refuse to start up, which provides protection against corruption."

http://main.slony.info/documentation/2.1/slonyupgrade.html

For your other questions, hopefully you will see some answer's here.

-- 
Regards
Raghav
Blog: htt://raghavt.blogspot.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130606/82bee8a6/attachment.htm 

From ssinger at ca.afilias.info  Fri Jun  7 07:51:21 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 07 Jun 2013 10:51:21 -0400
Subject: [Slony1-general] Slony 2.2.0 beta 4 released
Message-ID: <51B1F369.9080801@ca.afilias.info>

The Slony team is pleased to announce the release of the fourth beta for 
Slony 2.2.0.

Slony 2.2.0 will be the next major release of the Slony-I replication 
system for PostgreSQL.

Key features of the 2.2.0 release include:

* The storage and transport and application of the slony log 
(sl_log_1/sl_log_2) has changed providing performance improvements. Data 
is now stored in a different format and the postgresql COPY protocol and 
triggers are used to replicate and apply changes to replicas.

* DDL handling with the EXECUTE SCRIPT command has changed.  The DDL is 
no longer stored as a special event in sl_event but is instead stored in 
sl_log_script and is processed as part of a SYNC event inline with data 
changes. DDL can also be specified inline

* FAILOVER has been reworked to be more reliable but not all nodes can 
be used as failover targets.

* A RESUBSCRIBE NODE command was added because the provider of a 
subscribed set can no longer be changed with the SUBSCRIBE SET command 
in some cases.  All sets from a particular origin must send data to 
receivers through the same path/forwarder nodes. This must remain  true 
during cluster reshaping.

This beta 4 adds support for PG 9.3 so you can beta test both slony 
2.2.0 and postgresql 9.3 betas at the same time.

See the release notes for details and changes from the last beta

The release notes are available at

http://git.postgresql.org/gitweb/?p=slony1-engine.git;a=blob_plain;f=RELEASE;h=be03be66d8f39ee148b50dadf71ddbe20a1e5ad8;hb=e3e785c93d14b009abc8424fa7e53e8b75c0f098

Slony-I 2.2.0 beta 4 can be downloaded from:

http://www.slony.info/downloads/2.2/source/slony1-2.2.0.b4.tar.bz2
http://www.slony.info/downloads/2.2/source/slony1-2.2.0.b4-docs.tar.bz2

This release is a beta release. Users are encouraged to try the release 
out and report bugs.

I would like people who test this beta to report both successes and bugs 
to me, either on the slony mailing lists or through private email.  Test 
this beta, report your results to me and earn Slony karma points.

Thanks


Steve


From JanWieck at Yahoo.com  Fri Jun  7 13:18:27 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 07 Jun 2013 16:18:27 -0400
Subject: [Slony1-general] Slony Versions and Postgres Versions and the
 compatibility.
In-Reply-To: <CANwAqWheEi_FgBJYPQ=1K+g6Ntagy76bQ6CBPwkJGBbw13aNyw@mail.gmail.com>
References: <538A19CDA8C5CA4F87977C0FA47689F101743DC86405@NYCMNET7EV003.mnet.biz>
	<CANwAqWheEi_FgBJYPQ=1K+g6Ntagy76bQ6CBPwkJGBbw13aNyw@mail.gmail.com>
Message-ID: <51B24013.6060704@Yahoo.com>

On 06/06/13 04:16, Raghav wrote:
> 
>     __
> 
>     Is there a Hard Rule that the Slony versions must be 100% exact at
>     all times, or can a Target Node be at a newer subversion ahead of
>     the Master and still run.
> 
>  
> AFAIK, Slony-I need exact version on both the node's.
> Because upgrading a node to newer version will do changes to
> slony_func.so file where it should match the version with the other
> replicating nodes else it will break. Below link has a comment after the
> upgrade steps as " If there is any mismatch between component versions,
> the slon <http://main.slony.info/documentation/2.1/slon.html> will
> refuse to start up, which provides protection against corruption."

That is correct. Slony requires the same Slony version on all nodes, but
supports running on top of different PostgreSQL version.


Jan


> 
> http://main.slony.info/documentation/2.1/slonyupgrade.html
> 
> For your other questions, hopefully you will see some answer's here.
> 
> -- 
> Regards
> Raghav
> Blog: htt://raghavt.blogspot.com/ <http://raghavt.blogspot.com/>
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From write2sridevi at gmail.com  Wed Jun 12 04:14:59 2013
From: write2sridevi at gmail.com (Sridevi R)
Date: Wed, 12 Jun 2013 16:44:59 +0530
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
Message-ID: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>

Hello,

The slony logs are consistently posting this error:

2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock table
"_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
minutes'::interval);commit;" - server closed the connection unexpectedly
2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock table
"_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
minutes'::interval);commit;" - server closed the connection unexpectedly

checked and found that sl_confirm table is not cleaned up. cleanup event
never succeeds.
Additionally, the child processes terminates and restarts after each such
cleanup failure.

2013-06-11 11:20:04 GMT CONFIG slon: child terminated signal: 9; pid:
20172, current worker pid: 20172
2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10 seconds

When cleanup is run manually, on the psql prompt it runs to completion
without any issues and cleans up sl_event and sl_confirm tables
"begin;lock table "_xx_cluster".sl_config_lock;select
"_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"

Soln version: 2.1.2

Any help/insight would be greatly appreciated.

Thanks,
Sridevi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130612/aedc192b/attachment.htm 

From JanWieck at Yahoo.com  Wed Jun 12 07:03:29 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 12 Jun 2013 10:03:29 -0400
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
Message-ID: <51B87FB1.4000700@Yahoo.com>

On 06/12/13 07:14, Sridevi R wrote:
> Hello,
> 
> The slony logs are consistently posting this error:
> 
> 2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock table
> "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> minutes'::interval);commit;" - server closed the connection unexpectedly
> 2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock table
> "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> minutes'::interval);commit;" - server closed the connection unexpectedly
> 
> checked and found that sl_confirm table is not cleaned up. cleanup event
> never succeeds.
> Additionally, the child processes terminates and restarts after each
> such cleanup failure.
> 
> 2013-06-11 11:20:04 GMT CONFIG slon: child terminated signal: 9; pid:
> 20172, current worker pid: 20172
> 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10 seconds
> 
> When cleanup is run manually, on the psql prompt it runs to completion
> without any issues and cleans up sl_event and sl_confirm tables
> "begin;lock table "_xx_cluster".sl_config_lock;select
> "_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"
> 
> Soln version: 2.1.2
> 
> Any help/insight would be greatly appreciated.

Slon kills its worker(s) with signal 9 (SIGKILL) when it needs to
restart, like when there are errors in event processing or if it
receives certain signals. Are there any other errors in the slon log or
is something on the machine sending signals to slon?


Jan

> 
> Thanks,
> Sridevi
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From write2sridevi at gmail.com  Wed Jun 12 07:17:23 2013
From: write2sridevi at gmail.com (Sridevi R)
Date: Wed, 12 Jun 2013 19:47:23 +0530
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <51B87FB1.4000700@Yahoo.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
	<51B87FB1.4000700@Yahoo.com>
Message-ID: <CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>

Jan,

Thanks for the reply.

The only errors in the slon log are failure of cleanupThread.
child process is restarting right after the cleanupThread Failure.
This occurs approximately every 10 minutes since cleanup_interval is set to
10 minutes.

Here is a sample from the log again:

2013-06-06 14:23:27 GMT FATAL  cleanupThread: "begin;lock table
"_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
minutes'::interval);commit;" - server closed the connection unexpectedly
    This probably means the server terminated abnormally
    before or while processing the request.
2013-06-06 14:23:27 GMT CONFIG slon: child terminated signal: 9; pid:
16135, current worker pid: 16135
2013-06-06 14:23:27 GMT CONFIG slon: restart of worker in 10 seconds

Thanks ,
Sridevi


On Wed, Jun 12, 2013 at 7:33 PM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 06/12/13 07:14, Sridevi R wrote:
> > Hello,
> >
> > The slony logs are consistently posting this error:
> >
> > 2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock table
> > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> > minutes'::interval);commit;" - server closed the connection unexpectedly
> > 2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock table
> > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> > minutes'::interval);commit;" - server closed the connection unexpectedly
> >
> > checked and found that sl_confirm table is not cleaned up. cleanup event
> > never succeeds.
> > Additionally, the child processes terminates and restarts after each
> > such cleanup failure.
> >
> > 2013-06-11 11:20:04 GMT CONFIG slon: child terminated signal: 9; pid:
> > 20172, current worker pid: 20172
> > 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10 seconds
> >
> > When cleanup is run manually, on the psql prompt it runs to completion
> > without any issues and cleans up sl_event and sl_confirm tables
> > "begin;lock table "_xx_cluster".sl_config_lock;select
> > "_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"
> >
> > Soln version: 2.1.2
> >
> > Any help/insight would be greatly appreciated.
>
> Slon kills its worker(s) with signal 9 (SIGKILL) when it needs to
> restart, like when there are errors in event processing or if it
> receives certain signals. Are there any other errors in the slon log or
> is something on the machine sending signals to slon?
>
>
> Jan
>
> >
> > Thanks,
> > Sridevi
> >
> >
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> >
>
>
> --
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130612/cb0dedc5/attachment.htm 

From JanWieck at Yahoo.com  Wed Jun 12 11:32:24 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 12 Jun 2013 14:32:24 -0400
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
	<51B87FB1.4000700@Yahoo.com>
	<CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>
Message-ID: <51B8BEB8.8070301@Yahoo.com>

On 06/12/13 10:17, Sridevi R wrote:
> Jan,
> 
> Thanks for the reply.
> 
> The only errors in the slon log are failure of cleanupThread.
> child process is restarting right after the cleanupThread Failure.
> This occurs approximately every 10 minutes since cleanup_interval is set
> to 10 minutes.
> 
> Here is a sample from the log again:
> 
> 2013-06-06 14:23:27 GMT FATAL  cleanupThread: "begin;lock table
> "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> minutes'::interval);commit;" - server closed the connection unexpectedly
>     This probably means the server terminated abnormally
>     before or while processing the request.
> 2013-06-06 14:23:27 GMT CONFIG slon: child terminated signal: 9; pid:
> 16135, current worker pid: 16135
> 2013-06-06 14:23:27 GMT CONFIG slon: restart of worker in 10 seconds

"server closed the connection unexpectedly" ...

Is this connection by any chance through some firewall or NAT gateway
that will drop idle connections?

What are the corresponding postmaster server log entries? Since slony
reports an unexpected connection drop from the server, the server must
have some message in its log too, because the client never sent the 'X'
libpq protocol message.


Jan


> 
> Thanks ,
> Sridevi
> 
> 
> On Wed, Jun 12, 2013 at 7:33 PM, Jan Wieck <JanWieck at yahoo.com
> <mailto:JanWieck at yahoo.com>> wrote:
> 
>     On 06/12/13 07:14, Sridevi R wrote:
>     > Hello,
>     >
>     > The slony logs are consistently posting this error:
>     >
>     > 2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock table
>     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
>     > minutes'::interval);commit;" - server closed the connection
>     unexpectedly
>     > 2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock table
>     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
>     > minutes'::interval);commit;" - server closed the connection
>     unexpectedly
>     >
>     > checked and found that sl_confirm table is not cleaned up. cleanup
>     event
>     > never succeeds.
>     > Additionally, the child processes terminates and restarts after each
>     > such cleanup failure.
>     >
>     > 2013-06-11 11:20:04 GMT CONFIG slon: child terminated signal: 9; pid:
>     > 20172, current worker pid: 20172
>     > 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10 seconds
>     >
>     > When cleanup is run manually, on the psql prompt it runs to completion
>     > without any issues and cleans up sl_event and sl_confirm tables
>     > "begin;lock table "_xx_cluster".sl_config_lock;select
>     > "_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"
>     >
>     > Soln version: 2.1.2
>     >
>     > Any help/insight would be greatly appreciated.
> 
>     Slon kills its worker(s) with signal 9 (SIGKILL) when it needs to
>     restart, like when there are errors in event processing or if it
>     receives certain signals. Are there any other errors in the slon log or
>     is something on the machine sending signals to slon?
> 
> 
>     Jan
> 
>     >
>     > Thanks,
>     > Sridevi
>     >
>     >
>     >
>     > _______________________________________________
>     > Slony1-general mailing list
>     > Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>     > http://lists.slony.info/mailman/listinfo/slony1-general
>     >
> 
> 
>     --
>     Anyone who trades liberty for security deserves neither
>     liberty nor security. -- Benjamin Franklin
> 
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From write2sridevi at gmail.com  Thu Jun 13 03:25:48 2013
From: write2sridevi at gmail.com (Sridevi R)
Date: Thu, 13 Jun 2013 15:55:48 +0530
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <51B8BEB8.8070301@Yahoo.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
	<51B87FB1.4000700@Yahoo.com>
	<CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>
	<51B8BEB8.8070301@Yahoo.com>
Message-ID: <CAGFfjec4xNCToJaRfWEmz91Qe+Z+WqOwX0fBFRWuo+P8MgaZsg@mail.gmail.com>

Hello Jan,

The Master and Slave DBs talk through a firewall.
VIP IPs and SNAT IPs are used in pg_hba.conf.

The corresponding messages in the postgres server log:

2013-06-13 09:46:21.224
GMT,,,6630,"10.4.2.2:42031",51b994ed.19e6,1,"",2013-06-13
09:46:21 GMT,,0,LOG,08P01,"incomplete startup packet",,,,,,,,,""
2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address printed
here>:53924",51b994f7.19ea,1,"idle",2013-06-13 09:46:31
GMT,28/0,0,LOG,08006,"could not receive data from client: Connection reset
by peer",,,,,,,,,"slon.node_1_listen"
2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address printed
here>:53924",51b994f7.19ea,2,"idle",2013-06-13 09:46:31
GMT,28/0,0,LOG,08P01,"unexpected EOF on client
connection",,,,,,,,,"slon.node_1_listen"
2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address printed
here>:53926",51b994f9.19ed,1,"idle",2013-06-13 09:46:33
GMT,32/0,0,LOG,08006,"could not receive data from client: Connection reset
by peer",,,,,,,,,"slon.subscriber_1_provider_1"
2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address printed
here>:53926",51b994f9.19ed,2,"idle",2013-06-13 09:46:33
GMT,32/0,0,LOG,08P01,"unexpected EOF on client
connection",,,,,,,,,"slon.subscriber_1_provider_1"
2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address printed
here>:53925",51b994f7.19eb,1,"idle",2013-06-13 09:46:31
GMT,31/0,0,LOG,08006,"could not receive data from client: Connection reset
by peer",,,,,,,,,"slon.node_1_listen"
2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address printed
here>:53925",51b994f7.19eb,2,"idle",2013-06-13 09:46:31
GMT,31/0,0,LOG,08P01,"unexpected EOF on client
connection",,,,,,,,,"slon.node_1_listen"

The client slon log contains:
2013-06-13 09:57:38 GMT FATAL  cleanupThread: "begin;lock table
"_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
minutes'::interval);commit;" - server closed the connection unexpectedly
        This probably means the server terminated abnormally
        before or while processing the request.


Thanks,
Sridevi





On Thu, Jun 13, 2013 at 12:02 AM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 06/12/13 10:17, Sridevi R wrote:
> > Jan,
> >
> > Thanks for the reply.
> >
> > The only errors in the slon log are failure of cleanupThread.
> > child process is restarting right after the cleanupThread Failure.
> > This occurs approximately every 10 minutes since cleanup_interval is set
> > to 10 minutes.
> >
> > Here is a sample from the log again:
> >
> > 2013-06-06 14:23:27 GMT FATAL  cleanupThread: "begin;lock table
> > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> > minutes'::interval);commit;" - server closed the connection unexpectedly
> >     This probably means the server terminated abnormally
> >     before or while processing the request.
> > 2013-06-06 14:23:27 GMT CONFIG slon: child terminated signal: 9; pid:
> > 16135, current worker pid: 16135
> > 2013-06-06 14:23:27 GMT CONFIG slon: restart of worker in 10 seconds
>
> "server closed the connection unexpectedly" ...
>
> Is this connection by any chance through some firewall or NAT gateway
> that will drop idle connections?
>
> What are the corresponding postmaster server log entries? Since slony
> reports an unexpected connection drop from the server, the server must
> have some message in its log too, because the client never sent the 'X'
> libpq protocol message.
>
>
> Jan
>
>
> >
> > Thanks ,
> > Sridevi
> >
> >
> > On Wed, Jun 12, 2013 at 7:33 PM, Jan Wieck <JanWieck at yahoo.com
> > <mailto:JanWieck at yahoo.com>> wrote:
> >
> >     On 06/12/13 07:14, Sridevi R wrote:
> >     > Hello,
> >     >
> >     > The slony logs are consistently posting this error:
> >     >
> >     > 2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock table
> >     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> >     > minutes'::interval);commit;" - server closed the connection
> >     unexpectedly
> >     > 2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock table
> >     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> >     > minutes'::interval);commit;" - server closed the connection
> >     unexpectedly
> >     >
> >     > checked and found that sl_confirm table is not cleaned up. cleanup
> >     event
> >     > never succeeds.
> >     > Additionally, the child processes terminates and restarts after
> each
> >     > such cleanup failure.
> >     >
> >     > 2013-06-11 11:20:04 GMT CONFIG slon: child terminated signal: 9;
> pid:
> >     > 20172, current worker pid: 20172
> >     > 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10
> seconds
> >     >
> >     > When cleanup is run manually, on the psql prompt it runs to
> completion
> >     > without any issues and cleans up sl_event and sl_confirm tables
> >     > "begin;lock table "_xx_cluster".sl_config_lock;select
> >     > "_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"
> >     >
> >     > Soln version: 2.1.2
> >     >
> >     > Any help/insight would be greatly appreciated.
> >
> >     Slon kills its worker(s) with signal 9 (SIGKILL) when it needs to
> >     restart, like when there are errors in event processing or if it
> >     receives certain signals. Are there any other errors in the slon log
> or
> >     is something on the machine sending signals to slon?
> >
> >
> >     Jan
> >
> >     >
> >     > Thanks,
> >     > Sridevi
> >     >
> >     >
> >     >
> >     > _______________________________________________
> >     > Slony1-general mailing list
> >     > Slony1-general at lists.slony.info
> >     <mailto:Slony1-general at lists.slony.info>
> >     > http://lists.slony.info/mailman/listinfo/slony1-general
> >     >
> >
> >
> >     --
> >     Anyone who trades liberty for security deserves neither
> >     liberty nor security. -- Benjamin Franklin
> >
> >
>
>
> --
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130613/0932e707/attachment.htm 

From JanWieck at Yahoo.com  Thu Jun 13 11:16:40 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 13 Jun 2013 14:16:40 -0400
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <CAGFfjec4xNCToJaRfWEmz91Qe+Z+WqOwX0fBFRWuo+P8MgaZsg@mail.gmail.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
	<51B87FB1.4000700@Yahoo.com>
	<CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>
	<51B8BEB8.8070301@Yahoo.com>
	<CAGFfjec4xNCToJaRfWEmz91Qe+Z+WqOwX0fBFRWuo+P8MgaZsg@mail.gmail.com>
Message-ID: <51BA0C88.1010706@Yahoo.com>

On 06/13/13 06:25, Sridevi R wrote:
> Hello Jan,
> 
> The Master and Slave DBs talk through a firewall.
> VIP IPs and SNAT IPs are used in pg_hba.conf.
> 
> The corresponding messages in the postgres server log:
> 
> 2013-06-13 09:46:21.224 GMT,,,6630,"10.4.2.2:42031
> <http://10.4.2.2:42031>",51b994ed.19e6,1,"",2013-06-13 09:46:21
> GMT,,0,LOG,08P01,"incomplete startup packet",,,,,,,,,""
> 2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address printed
> here>:53924",51b994f7.19ea,1,"idle",2013-06-13 09:46:31
> GMT,28/0,0,LOG,08006,"could not receive data from client: Connection
> reset by peer",,,,,,,,,"slon.node_1_listen"
> 2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address printed
> here>:53924",51b994f7.19ea,2,"idle",2013-06-13 09:46:31
> GMT,28/0,0,LOG,08P01,"unexpected EOF on client
> connection",,,,,,,,,"slon.node_1_listen"
> 2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address printed
> here>:53926",51b994f9.19ed,1,"idle",2013-06-13 09:46:33
> GMT,32/0,0,LOG,08006,"could not receive data from client: Connection
> reset by peer",,,,,,,,,"slon.subscriber_1_provider_1"
> 2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address printed
> here>:53926",51b994f9.19ed,2,"idle",2013-06-13 09:46:33
> GMT,32/0,0,LOG,08P01,"unexpected EOF on client
> connection",,,,,,,,,"slon.subscriber_1_provider_1"
> 2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address printed
> here>:53925",51b994f7.19eb,1,"idle",2013-06-13 09:46:31
> GMT,31/0,0,LOG,08006,"could not receive data from client: Connection
> reset by peer",,,,,,,,,"slon.node_1_listen"
> 2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address printed
> here>:53925",51b994f7.19eb,2,"idle",2013-06-13 09:46:31
> GMT,31/0,0,LOG,08P01,"unexpected EOF on client
> connection",,,,,,,,,"slon.node_1_listen"
> 
> The client slon log contains:
> 2013-06-13 09:57:38 GMT FATAL  cleanupThread: "begin;lock table
> "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> minutes'::interval);commit;" - server closed the connection unexpectedly
>         This probably means the server terminated abnormally
>         before or while processing the request.

This all can very well be a slightly too eager firewall dropping idle
connections. Have you tried to enable TCP keep alive options that kick
in after something like 30 seconds? If not, enable them on both, the PG
server and the Slony side. That usually prevents those firewall issues.


Jan


> 
> 
> Thanks,
> Sridevi
> 
> 
> 
> 
> 
> On Thu, Jun 13, 2013 at 12:02 AM, Jan Wieck <JanWieck at yahoo.com
> <mailto:JanWieck at yahoo.com>> wrote:
> 
>     On 06/12/13 10:17, Sridevi R wrote:
>     > Jan,
>     >
>     > Thanks for the reply.
>     >
>     > The only errors in the slon log are failure of cleanupThread.
>     > child process is restarting right after the cleanupThread Failure.
>     > This occurs approximately every 10 minutes since cleanup_interval
>     is set
>     > to 10 minutes.
>     >
>     > Here is a sample from the log again:
>     >
>     > 2013-06-06 14:23:27 GMT FATAL  cleanupThread: "begin;lock table
>     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
>     > minutes'::interval);commit;" - server closed the connection
>     unexpectedly
>     >     This probably means the server terminated abnormally
>     >     before or while processing the request.
>     > 2013-06-06 14:23:27 GMT CONFIG slon: child terminated signal: 9; pid:
>     > 16135, current worker pid: 16135
>     > 2013-06-06 14:23:27 GMT CONFIG slon: restart of worker in 10 seconds
> 
>     "server closed the connection unexpectedly" ...
> 
>     Is this connection by any chance through some firewall or NAT gateway
>     that will drop idle connections?
> 
>     What are the corresponding postmaster server log entries? Since slony
>     reports an unexpected connection drop from the server, the server must
>     have some message in its log too, because the client never sent the 'X'
>     libpq protocol message.
> 
> 
>     Jan
> 
> 
>     >
>     > Thanks ,
>     > Sridevi
>     >
>     >
>     > On Wed, Jun 12, 2013 at 7:33 PM, Jan Wieck <JanWieck at yahoo.com
>     <mailto:JanWieck at yahoo.com>
>     > <mailto:JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>>> wrote:
>     >
>     >     On 06/12/13 07:14, Sridevi R wrote:
>     >     > Hello,
>     >     >
>     >     > The slony logs are consistently posting this error:
>     >     >
>     >     > 2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock table
>     >     > "_xx_cluster".sl_config_lock;select
>     "_xx_cluster".cleanupEvent('10
>     >     > minutes'::interval);commit;" - server closed the connection
>     >     unexpectedly
>     >     > 2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock table
>     >     > "_xx_cluster".sl_config_lock;select
>     "_xx_cluster".cleanupEvent('10
>     >     > minutes'::interval);commit;" - server closed the connection
>     >     unexpectedly
>     >     >
>     >     > checked and found that sl_confirm table is not cleaned up.
>     cleanup
>     >     event
>     >     > never succeeds.
>     >     > Additionally, the child processes terminates and restarts
>     after each
>     >     > such cleanup failure.
>     >     >
>     >     > 2013-06-11 11:20:04 GMT CONFIG slon: child terminated
>     signal: 9; pid:
>     >     > 20172, current worker pid: 20172
>     >     > 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10
>     seconds
>     >     >
>     >     > When cleanup is run manually, on the psql prompt it runs to
>     completion
>     >     > without any issues and cleans up sl_event and sl_confirm tables
>     >     > "begin;lock table "_xx_cluster".sl_config_lock;select
>     >     > "_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"
>     >     >
>     >     > Soln version: 2.1.2
>     >     >
>     >     > Any help/insight would be greatly appreciated.
>     >
>     >     Slon kills its worker(s) with signal 9 (SIGKILL) when it needs to
>     >     restart, like when there are errors in event processing or if it
>     >     receives certain signals. Are there any other errors in the
>     slon log or
>     >     is something on the machine sending signals to slon?
>     >
>     >
>     >     Jan
>     >
>     >     >
>     >     > Thanks,
>     >     > Sridevi
>     >     >
>     >     >
>     >     >
>     >     > _______________________________________________
>     >     > Slony1-general mailing list
>     >     > Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>     >     <mailto:Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>>
>     >     > http://lists.slony.info/mailman/listinfo/slony1-general
>     >     >
>     >
>     >
>     >     --
>     >     Anyone who trades liberty for security deserves neither
>     >     liberty nor security. -- Benjamin Franklin
>     >
>     >
> 
> 
>     --
>     Anyone who trades liberty for security deserves neither
>     liberty nor security. -- Benjamin Franklin
> 
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From write2sridevi at gmail.com  Sat Jun 15 01:46:03 2013
From: write2sridevi at gmail.com (Sridevi R)
Date: Sat, 15 Jun 2013 14:16:03 +0530
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <51BA0C88.1010706@Yahoo.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
	<51B87FB1.4000700@Yahoo.com>
	<CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>
	<51B8BEB8.8070301@Yahoo.com>
	<CAGFfjec4xNCToJaRfWEmz91Qe+Z+WqOwX0fBFRWuo+P8MgaZsg@mail.gmail.com>
	<51BA0C88.1010706@Yahoo.com>
Message-ID: <CAGFfjefvXM9FLGXPk1GnP5SHa0VhZRFxrbhy6VApqpZinO7Rig@mail.gmail.com>

Jan,

You are right. Tweaking the tcp keep alive parameters helped.

Now slon.conf contains:

tcp_keepalive=true
tcp_keepalive_interval=45
tcp_keepalive_count=20
tcp_keepalive_idle=30
cleanup_interval=30

Thanks a lot for the timely response.

- Sridevi



On Thu, Jun 13, 2013 at 11:46 PM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 06/13/13 06:25, Sridevi R wrote:
> > Hello Jan,
> >
> > The Master and Slave DBs talk through a firewall.
> > VIP IPs and SNAT IPs are used in pg_hba.conf.
> >
> > The corresponding messages in the postgres server log:
> >
> > 2013-06-13 09:46:21.224 GMT,,,6630,"10.4.2.2:42031
> > <http://10.4.2.2:42031>",51b994ed.19e6,1,"",2013-06-13 09:46:21
> > GMT,,0,LOG,08P01,"incomplete startup packet",,,,,,,,,""
> > 2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address printed
> > here>:53924",51b994f7.19ea,1,"idle",2013-06-13 09:46:31
> > GMT,28/0,0,LOG,08006,"could not receive data from client: Connection
> > reset by peer",,,,,,,,,"slon.node_1_listen"
> > 2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address printed
> > here>:53924",51b994f7.19ea,2,"idle",2013-06-13 09:46:31
> > GMT,28/0,0,LOG,08P01,"unexpected EOF on client
> > connection",,,,,,,,,"slon.node_1_listen"
> > 2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address printed
> > here>:53926",51b994f9.19ed,1,"idle",2013-06-13 09:46:33
> > GMT,32/0,0,LOG,08006,"could not receive data from client: Connection
> > reset by peer",,,,,,,,,"slon.subscriber_1_provider_1"
> > 2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address printed
> > here>:53926",51b994f9.19ed,2,"idle",2013-06-13 09:46:33
> > GMT,32/0,0,LOG,08P01,"unexpected EOF on client
> > connection",,,,,,,,,"slon.subscriber_1_provider_1"
> > 2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address printed
> > here>:53925",51b994f7.19eb,1,"idle",2013-06-13 09:46:31
> > GMT,31/0,0,LOG,08006,"could not receive data from client: Connection
> > reset by peer",,,,,,,,,"slon.node_1_listen"
> > 2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address printed
> > here>:53925",51b994f7.19eb,2,"idle",2013-06-13 09:46:31
> > GMT,31/0,0,LOG,08P01,"unexpected EOF on client
> > connection",,,,,,,,,"slon.node_1_listen"
> >
> > The client slon log contains:
> > 2013-06-13 09:57:38 GMT FATAL  cleanupThread: "begin;lock table
> > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> > minutes'::interval);commit;" - server closed the connection unexpectedly
> >         This probably means the server terminated abnormally
> >         before or while processing the request.
>
> This all can very well be a slightly too eager firewall dropping idle
> connections. Have you tried to enable TCP keep alive options that kick
> in after something like 30 seconds? If not, enable them on both, the PG
> server and the Slony side. That usually prevents those firewall issues.
>
>
> Jan
>
>
> >
> >
> > Thanks,
> > Sridevi
> >
> >
> >
> >
> >
> > On Thu, Jun 13, 2013 at 12:02 AM, Jan Wieck <JanWieck at yahoo.com
> > <mailto:JanWieck at yahoo.com>> wrote:
> >
> >     On 06/12/13 10:17, Sridevi R wrote:
> >     > Jan,
> >     >
> >     > Thanks for the reply.
> >     >
> >     > The only errors in the slon log are failure of cleanupThread.
> >     > child process is restarting right after the cleanupThread Failure.
> >     > This occurs approximately every 10 minutes since cleanup_interval
> >     is set
> >     > to 10 minutes.
> >     >
> >     > Here is a sample from the log again:
> >     >
> >     > 2013-06-06 14:23:27 GMT FATAL  cleanupThread: "begin;lock table
> >     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
> >     > minutes'::interval);commit;" - server closed the connection
> >     unexpectedly
> >     >     This probably means the server terminated abnormally
> >     >     before or while processing the request.
> >     > 2013-06-06 14:23:27 GMT CONFIG slon: child terminated signal: 9;
> pid:
> >     > 16135, current worker pid: 16135
> >     > 2013-06-06 14:23:27 GMT CONFIG slon: restart of worker in 10
> seconds
> >
> >     "server closed the connection unexpectedly" ...
> >
> >     Is this connection by any chance through some firewall or NAT gateway
> >     that will drop idle connections?
> >
> >     What are the corresponding postmaster server log entries? Since slony
> >     reports an unexpected connection drop from the server, the server
> must
> >     have some message in its log too, because the client never sent the
> 'X'
> >     libpq protocol message.
> >
> >
> >     Jan
> >
> >
> >     >
> >     > Thanks ,
> >     > Sridevi
> >     >
> >     >
> >     > On Wed, Jun 12, 2013 at 7:33 PM, Jan Wieck <JanWieck at yahoo.com
> >     <mailto:JanWieck at yahoo.com>
> >     > <mailto:JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>>> wrote:
> >     >
> >     >     On 06/12/13 07:14, Sridevi R wrote:
> >     >     > Hello,
> >     >     >
> >     >     > The slony logs are consistently posting this error:
> >     >     >
> >     >     > 2013-06-12 10:01:05 GMT FATAL  cleanupThread: "begin;lock
> table
> >     >     > "_xx_cluster".sl_config_lock;select
> >     "_xx_cluster".cleanupEvent('10
> >     >     > minutes'::interval);commit;" - server closed the connection
> >     >     unexpectedly
> >     >     > 2013-06-12 10:12:24 GMT FATAL  cleanupThread: "begin;lock
> table
> >     >     > "_xx_cluster".sl_config_lock;select
> >     "_xx_cluster".cleanupEvent('10
> >     >     > minutes'::interval);commit;" - server closed the connection
> >     >     unexpectedly
> >     >     >
> >     >     > checked and found that sl_confirm table is not cleaned up.
> >     cleanup
> >     >     event
> >     >     > never succeeds.
> >     >     > Additionally, the child processes terminates and restarts
> >     after each
> >     >     > such cleanup failure.
> >     >     >
> >     >     > 2013-06-11 11:20:04 GMT CONFIG slon: child terminated
> >     signal: 9; pid:
> >     >     > 20172, current worker pid: 20172
> >     >     > 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker in 10
> >     seconds
> >     >     >
> >     >     > When cleanup is run manually, on the psql prompt it runs to
> >     completion
> >     >     > without any issues and cleans up sl_event and sl_confirm
> tables
> >     >     > "begin;lock table "_xx_cluster".sl_config_lock;select
> >     >     > "_xx_cluster".cleanupEvent('10 minutes'::interval);commit;"
> >     >     >
> >     >     > Soln version: 2.1.2
> >     >     >
> >     >     > Any help/insight would be greatly appreciated.
> >     >
> >     >     Slon kills its worker(s) with signal 9 (SIGKILL) when it needs
> to
> >     >     restart, like when there are errors in event processing or if
> it
> >     >     receives certain signals. Are there any other errors in the
> >     slon log or
> >     >     is something on the machine sending signals to slon?
> >     >
> >     >
> >     >     Jan
> >     >
> >     >     >
> >     >     > Thanks,
> >     >     > Sridevi
> >     >     >
> >     >     >
> >     >     >
> >     >     > _______________________________________________
> >     >     > Slony1-general mailing list
> >     >     > Slony1-general at lists.slony.info
> >     <mailto:Slony1-general at lists.slony.info>
> >     >     <mailto:Slony1-general at lists.slony.info
> >     <mailto:Slony1-general at lists.slony.info>>
> >     >     > http://lists.slony.info/mailman/listinfo/slony1-general
> >     >     >
> >     >
> >     >
> >     >     --
> >     >     Anyone who trades liberty for security deserves neither
> >     >     liberty nor security. -- Benjamin Franklin
> >     >
> >     >
> >
> >
> >     --
> >     Anyone who trades liberty for security deserves neither
> >     liberty nor security. -- Benjamin Franklin
> >
> >
> >
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> >
>
>
> --
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130615/cab5c0d1/attachment.htm 

From JanWieck at Yahoo.com  Sat Jun 15 06:24:27 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat, 15 Jun 2013 09:24:27 -0400
Subject: [Slony1-general] Slony cleanupEvent erroring out with "server
 closed the connection unexpectedly" - Soln version: 2.1.2
In-Reply-To: <CAGFfjefvXM9FLGXPk1GnP5SHa0VhZRFxrbhy6VApqpZinO7Rig@mail.gmail.com>
References: <CAGFfjeeTqnyaP_6bqZyo0xYeRzmoKKFcGbxjyBbSfycGXqphJA@mail.gmail.com>
	<51B87FB1.4000700@Yahoo.com>
	<CAGFfjedhpi0r0E0Kat20X2rU5bNoDUJ0prHJwQCJuKGop-7Y2w@mail.gmail.com>
	<51B8BEB8.8070301@Yahoo.com>
	<CAGFfjec4xNCToJaRfWEmz91Qe+Z+WqOwX0fBFRWuo+P8MgaZsg@mail.gmail.com>
	<51BA0C88.1010706@Yahoo.com>
	<CAGFfjefvXM9FLGXPk1GnP5SHa0VhZRFxrbhy6VApqpZinO7Rig@mail.gmail.com>
Message-ID: <51BC6B0B.8090606@Yahoo.com>

On 06/15/13 04:46, Sridevi R wrote:
> Jan,
> 
> You are right. Tweaking the tcp keep alive parameters helped.
> 
> Now slon.conf contains:
> 
> tcp_keepalive=true
> tcp_keepalive_interval=45
> tcp_keepalive_count=20
> tcp_keepalive_idle=30
> cleanup_interval=30
> 
> Thanks a lot for the timely response.

You're welcome.


Jan


> 
> - Sridevi
> 
> 
> 
> On Thu, Jun 13, 2013 at 11:46 PM, Jan Wieck <JanWieck at yahoo.com
> <mailto:JanWieck at yahoo.com>> wrote:
> 
>     On 06/13/13 06:25, Sridevi R wrote:
>     > Hello Jan,
>     >
>     > The Master and Slave DBs talk through a firewall.
>     > VIP IPs and SNAT IPs are used in pg_hba.conf.
>     >
>     > The corresponding messages in the postgres server log:
>     >
>     > 2013-06-13 09:46:21.224 GMT,,,6630,"10.4.2.2:42031
>     <http://10.4.2.2:42031>
>     > <http://10.4.2.2:42031>",51b994ed.19e6,1,"",2013-06-13 09:46:21
>     > GMT,,0,LOG,08P01,"incomplete startup packet",,,,,,,,,""
>     > 2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address
>     printed
>     > here>:53924",51b994f7.19ea,1,"idle",2013-06-13 09:46:31
>     > GMT,28/0,0,LOG,08006,"could not receive data from client: Connection
>     > reset by peer",,,,,,,,,"slon.node_1_listen"
>     > 2013-06-13 09:57:38.596 GMT,"postgres","db01",6634,"<ip address
>     printed
>     > here>:53924",51b994f7.19ea,2,"idle",2013-06-13 09:46:31
>     > GMT,28/0,0,LOG,08P01,"unexpected EOF on client
>     > connection",,,,,,,,,"slon.node_1_listen"
>     > 2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address
>     printed
>     > here>:53926",51b994f9.19ed,1,"idle",2013-06-13 09:46:33
>     > GMT,32/0,0,LOG,08006,"could not receive data from client: Connection
>     > reset by peer",,,,,,,,,"slon.subscriber_1_provider_1"
>     > 2013-06-13 09:57:38.607 GMT,"postgres","db01",6637,"<ip address
>     printed
>     > here>:53926",51b994f9.19ed,2,"idle",2013-06-13 09:46:33
>     > GMT,32/0,0,LOG,08P01,"unexpected EOF on client
>     > connection",,,,,,,,,"slon.subscriber_1_provider_1"
>     > 2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address
>     printed
>     > here>:53925",51b994f7.19eb,1,"idle",2013-06-13 09:46:31
>     > GMT,31/0,0,LOG,08006,"could not receive data from client: Connection
>     > reset by peer",,,,,,,,,"slon.node_1_listen"
>     > 2013-06-13 09:57:38.608 GMT,"postgres","db01",6635,"<ip address
>     printed
>     > here>:53925",51b994f7.19eb,2,"idle",2013-06-13 09:46:31
>     > GMT,31/0,0,LOG,08P01,"unexpected EOF on client
>     > connection",,,,,,,,,"slon.node_1_listen"
>     >
>     > The client slon log contains:
>     > 2013-06-13 09:57:38 GMT FATAL  cleanupThread: "begin;lock table
>     > "_xx_cluster".sl_config_lock;select "_xx_cluster".cleanupEvent('10
>     > minutes'::interval);commit;" - server closed the connection
>     unexpectedly
>     >         This probably means the server terminated abnormally
>     >         before or while processing the request.
> 
>     This all can very well be a slightly too eager firewall dropping idle
>     connections. Have you tried to enable TCP keep alive options that kick
>     in after something like 30 seconds? If not, enable them on both, the PG
>     server and the Slony side. That usually prevents those firewall issues.
> 
> 
>     Jan
> 
> 
>     >
>     >
>     > Thanks,
>     > Sridevi
>     >
>     >
>     >
>     >
>     >
>     > On Thu, Jun 13, 2013 at 12:02 AM, Jan Wieck <JanWieck at yahoo.com
>     <mailto:JanWieck at yahoo.com>
>     > <mailto:JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>>> wrote:
>     >
>     >     On 06/12/13 10:17, Sridevi R wrote:
>     >     > Jan,
>     >     >
>     >     > Thanks for the reply.
>     >     >
>     >     > The only errors in the slon log are failure of cleanupThread.
>     >     > child process is restarting right after the cleanupThread
>     Failure.
>     >     > This occurs approximately every 10 minutes since
>     cleanup_interval
>     >     is set
>     >     > to 10 minutes.
>     >     >
>     >     > Here is a sample from the log again:
>     >     >
>     >     > 2013-06-06 14:23:27 GMT FATAL  cleanupThread: "begin;lock table
>     >     > "_xx_cluster".sl_config_lock;select
>     "_xx_cluster".cleanupEvent('10
>     >     > minutes'::interval);commit;" - server closed the connection
>     >     unexpectedly
>     >     >     This probably means the server terminated abnormally
>     >     >     before or while processing the request.
>     >     > 2013-06-06 14:23:27 GMT CONFIG slon: child terminated
>     signal: 9; pid:
>     >     > 16135, current worker pid: 16135
>     >     > 2013-06-06 14:23:27 GMT CONFIG slon: restart of worker in 10
>     seconds
>     >
>     >     "server closed the connection unexpectedly" ...
>     >
>     >     Is this connection by any chance through some firewall or NAT
>     gateway
>     >     that will drop idle connections?
>     >
>     >     What are the corresponding postmaster server log entries?
>     Since slony
>     >     reports an unexpected connection drop from the server, the
>     server must
>     >     have some message in its log too, because the client never
>     sent the 'X'
>     >     libpq protocol message.
>     >
>     >
>     >     Jan
>     >
>     >
>     >     >
>     >     > Thanks ,
>     >     > Sridevi
>     >     >
>     >     >
>     >     > On Wed, Jun 12, 2013 at 7:33 PM, Jan Wieck
>     <JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>
>     >     <mailto:JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>>
>     >     > <mailto:JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>
>     <mailto:JanWieck at yahoo.com <mailto:JanWieck at yahoo.com>>>> wrote:
>     >     >
>     >     >     On 06/12/13 07:14, Sridevi R wrote:
>     >     >     > Hello,
>     >     >     >
>     >     >     > The slony logs are consistently posting this error:
>     >     >     >
>     >     >     > 2013-06-12 10:01:05 GMT FATAL  cleanupThread:
>     "begin;lock table
>     >     >     > "_xx_cluster".sl_config_lock;select
>     >     "_xx_cluster".cleanupEvent('10
>     >     >     > minutes'::interval);commit;" - server closed the
>     connection
>     >     >     unexpectedly
>     >     >     > 2013-06-12 10:12:24 GMT FATAL  cleanupThread:
>     "begin;lock table
>     >     >     > "_xx_cluster".sl_config_lock;select
>     >     "_xx_cluster".cleanupEvent('10
>     >     >     > minutes'::interval);commit;" - server closed the
>     connection
>     >     >     unexpectedly
>     >     >     >
>     >     >     > checked and found that sl_confirm table is not cleaned up.
>     >     cleanup
>     >     >     event
>     >     >     > never succeeds.
>     >     >     > Additionally, the child processes terminates and restarts
>     >     after each
>     >     >     > such cleanup failure.
>     >     >     >
>     >     >     > 2013-06-11 11:20:04 GMT CONFIG slon: child terminated
>     >     signal: 9; pid:
>     >     >     > 20172, current worker pid: 20172
>     >     >     > 2013-06-11 11:20:04 GMT CONFIG slon: restart of worker
>     in 10
>     >     seconds
>     >     >     >
>     >     >     > When cleanup is run manually, on the psql prompt it
>     runs to
>     >     completion
>     >     >     > without any issues and cleans up sl_event and
>     sl_confirm tables
>     >     >     > "begin;lock table "_xx_cluster".sl_config_lock;select
>     >     >     > "_xx_cluster".cleanupEvent('10
>     minutes'::interval);commit;"
>     >     >     >
>     >     >     > Soln version: 2.1.2
>     >     >     >
>     >     >     > Any help/insight would be greatly appreciated.
>     >     >
>     >     >     Slon kills its worker(s) with signal 9 (SIGKILL) when it
>     needs to
>     >     >     restart, like when there are errors in event processing
>     or if it
>     >     >     receives certain signals. Are there any other errors in the
>     >     slon log or
>     >     >     is something on the machine sending signals to slon?
>     >     >
>     >     >
>     >     >     Jan
>     >     >
>     >     >     >
>     >     >     > Thanks,
>     >     >     > Sridevi
>     >     >     >
>     >     >     >
>     >     >     >
>     >     >     > _______________________________________________
>     >     >     > Slony1-general mailing list
>     >     >     > Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>     >     <mailto:Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>>
>     >     >     <mailto:Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>     >     <mailto:Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>>>
>     >     >     > http://lists.slony.info/mailman/listinfo/slony1-general
>     >     >     >
>     >     >
>     >     >
>     >     >     --
>     >     >     Anyone who trades liberty for security deserves neither
>     >     >     liberty nor security. -- Benjamin Franklin
>     >     >
>     >     >
>     >
>     >
>     >     --
>     >     Anyone who trades liberty for security deserves neither
>     >     liberty nor security. -- Benjamin Franklin
>     >
>     >
>     >
>     >
>     > _______________________________________________
>     > Slony1-general mailing list
>     > Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>     > http://lists.slony.info/mailman/listinfo/slony1-general
>     >
> 
> 
>     --
>     Anyone who trades liberty for security deserves neither
>     liberty nor security. -- Benjamin Franklin
> 
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From bbellows at dotomi.com  Sat Jun 22 11:48:57 2013
From: bbellows at dotomi.com (Bambi Bellows)
Date: Sat, 22 Jun 2013 18:48:57 +0000
Subject: [Slony1-general] Slony replication has stopped replicating
Message-ID: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>

Hi Gang!

Slony replication has stopped replicating, and I'm not sure why, or how to get it replicating again.  I have bounced Slony on the master and all the remote nodes, but no luck.  Is there a Slony expert out there who can help point me in the right direction?

Version info:
slonik version 2.1.0.b2
psql (9.0.4)

Thanks!
Bambi.





This email and any files included with it may contain privileged,
proprietary and/or confidential information that is for the sole use
of the intended recipient(s).  Any disclosure, copying, distribution,
posting, or use of the information contained in or attached to this
email is prohibited unless permitted by the sender.  If you have
received this email in error, please immediately notify the sender
via return email, telephone, or fax and destroy this original transmission
and its included files without reading or saving it in any manner.
Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130622/bb464c33/attachment.htm 

From steve at ssinger.info  Sat Jun 22 12:45:52 2013
From: steve at ssinger.info (Steve Singer)
Date: Sat, 22 Jun 2013 15:45:52 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
Message-ID: <BLU0-SMTP412C6E05045BE5AE842E12DC880@phx.gbl>

On 06/22/2013 02:48 PM, Bambi Bellows wrote:
>
> Slony replication has stopped replicating, and I'm not sure why, or 
> how to get it replicating again.  I have bounced Slony on the master 
> and all the remote nodes, but no luck.  Is there a Slony expert out 
> there who can help point me in the right direction?

Have you used the 'grep' command to grep ALL of your slon logs for the 
word 'ERROR' ?


From JanWieck at Yahoo.com  Sat Jun 22 13:53:17 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat, 22 Jun 2013 16:53:17 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
Message-ID: <51C60EBD.1000500@Yahoo.com>

On 06/22/13 14:48, Bambi Bellows wrote:
> Hi Gang!
> 
> Slony replication has stopped replicating, and I'm not sure why, or how
> to get it replicating again.  I have bounced Slony on the master and all
> the remote nodes, but no luck.  Is there a Slony expert out there who
> can help point me in the right direction?
> 
> Version info:
> 
> slonik version 2.1.0.b2
> 
> psql (9.0.4)

Slony 2.1 is on 2.1.3 and PostgreSQL 9.0 is on 9.0.13. Is there a
particular reason why you keep running software that is known to have
plenty of bugs?


Jan


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Sat Jun 22 14:27:27 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat, 22 Jun 2013 17:27:27 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
Message-ID: <51C616BF.2000508@Yahoo.com>

On 06/22/13 16:56, Bambi Bellows wrote:
> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.

Slony related?

I would say the whole thing you inherited is a complete mess and Slony
is a casualty of that. There is nothing so far that I can see could be
blamed on Slony. Only a grossly outdated and unmanaged server setup.

Sorry to be so blunt, but I just don't tolerate it when Slony or
PostgreSQL are blamed for server admin negligence and incompetence.

This is apparently not your fault, but your predecessor's fault. So
don't take it personal.


Jan

> 
> -----Original Message-----
> From: Jan Wieck [mailto:JanWieck at Yahoo.com] 
> Sent: Saturday, June 22, 2013 3:53 PM
> To: Bambi Bellows
> Cc: slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Slony replication has stopped replicating
> 
> On 06/22/13 14:48, Bambi Bellows wrote:
>> Hi Gang!
>> 
>> Slony replication has stopped replicating, and I'm not sure why, or how
>> to get it replicating again.  I have bounced Slony on the master and all
>> the remote nodes, but no luck.  Is there a Slony expert out there who
>> can help point me in the right direction?
>> 
>> Version info:
>> 
>> slonik version 2.1.0.b2
>> 
>> psql (9.0.4)
> 
> Slony 2.1 is on 2.1.3 and PostgreSQL 9.0 is on 9.0.13. Is there a
> particular reason why you keep running software that is known to have
> plenty of bugs?
> 
> 
> Jan
> 
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From scott.marlowe at gmail.com  Sat Jun 22 15:10:39 2013
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat, 22 Jun 2013 15:10:39 -0700
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <51C616BF.2000508@Yahoo.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
Message-ID: <CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>

On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
> On 06/22/13 16:56, Bambi Bellows wrote:
>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>
> Slony related?
>
> I would say the whole thing you inherited is a complete mess and Slony
> is a casualty of that. There is nothing so far that I can see could be
> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>
> Sorry to be so blunt, but I just don't tolerate it when Slony or
> PostgreSQL are blamed for server admin negligence and incompetence.
>
> This is apparently not your fault, but your predecessor's fault. So
> don't take it personal.

Yeah when I'm handed a grand mess like this I just tell the bosses you
can either let me fix it the way it needs to be fixed, or watch it
break over and over. Your choice, but I'm not getting up at 2am to fix
things you didn't let me fix right.

From bbellows at dotomi.com  Sat Jun 22 13:56:01 2013
From: bbellows at dotomi.com (Bambi Bellows)
Date: Sat, 22 Jun 2013 20:56:01 +0000
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <51C60EBD.1000500@Yahoo.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
Message-ID: <476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>

LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.

-----Original Message-----
From: Jan Wieck [mailto:JanWieck at Yahoo.com] 
Sent: Saturday, June 22, 2013 3:53 PM
To: Bambi Bellows
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Slony replication has stopped replicating

On 06/22/13 14:48, Bambi Bellows wrote:
> Hi Gang!
> 
> Slony replication has stopped replicating, and I'm not sure why, or how
> to get it replicating again.  I have bounced Slony on the master and all
> the remote nodes, but no luck.  Is there a Slony expert out there who
> can help point me in the right direction?
> 
> Version info:
> 
> slonik version 2.1.0.b2
> 
> psql (9.0.4)

Slony 2.1 is on 2.1.3 and PostgreSQL 9.0 is on 9.0.13. Is there a
particular reason why you keep running software that is known to have
plenty of bugs?


Jan


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin




This email and any files included with it may contain privileged,
proprietary and/or confidential information that is for the sole use
of the intended recipient(s).  Any disclosure, copying, distribution,
posting, or use of the information contained in or attached to this
email is prohibited unless permitted by the sender.  If you have
received this email in error, please immediately notify the sender
via return email, telephone, or fax and destroy this original transmission
and its included files without reading or saving it in any manner.
Thank you.


From dan at randomdan.homeip.net  Sat Jun 22 15:15:36 2013
From: dan at randomdan.homeip.net (Dan Goodliffe)
Date: Sat, 22 Jun 2013 23:15:36 +0100
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
Message-ID: <51C62208.1060908@randomdan.homeip.net>

Haha, yes, we've all been there. I like a good rant as much as the next 
person, but it has to be at the right thing.

That aside, you might find lsof helpful...

akira ~ # lsof -c slon

If it's got a log file open somewhere, that should show it up.

Dan

On 06/22/13 23:10, Scott Marlowe wrote:
> On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
>> On 06/22/13 16:56, Bambi Bellows wrote:
>>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>> Slony related?
>>
>> I would say the whole thing you inherited is a complete mess and Slony
>> is a casualty of that. There is nothing so far that I can see could be
>> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>>
>> Sorry to be so blunt, but I just don't tolerate it when Slony or
>> PostgreSQL are blamed for server admin negligence and incompetence.
>>
>> This is apparently not your fault, but your predecessor's fault. So
>> don't take it personal.
> Yeah when I'm handed a grand mess like this I just tell the bosses you
> can either let me fix it the way it needs to be fixed, or watch it
> break over and over. Your choice, but I'm not getting up at 2am to fix
> things you didn't let me fix right.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
'rascal' Dan Goodliffe
dan.goodliffe at randomdan.homeip.net
07970 523693


From bbellows at dotomi.com  Sat Jun 22 15:16:04 2013
From: bbellows at dotomi.com (Bambi Bellows)
Date: Sat, 22 Jun 2013 22:16:04 +0000
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
Message-ID: <476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>

LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.

-----Original Message-----
From: Scott Marlowe [mailto:scott.marlowe at gmail.com] 
Sent: Saturday, June 22, 2013 5:11 PM
To: Jan Wieck
Cc: Bambi Bellows; slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Slony replication has stopped replicating

On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
> On 06/22/13 16:56, Bambi Bellows wrote:
>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>
> Slony related?
>
> I would say the whole thing you inherited is a complete mess and Slony
> is a casualty of that. There is nothing so far that I can see could be
> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>
> Sorry to be so blunt, but I just don't tolerate it when Slony or
> PostgreSQL are blamed for server admin negligence and incompetence.
>
> This is apparently not your fault, but your predecessor's fault. So
> don't take it personal.

Yeah when I'm handed a grand mess like this I just tell the bosses you
can either let me fix it the way it needs to be fixed, or watch it
break over and over. Your choice, but I'm not getting up at 2am to fix
things you didn't let me fix right.




This email and any files included with it may contain privileged,
proprietary and/or confidential information that is for the sole use
of the intended recipient(s).  Any disclosure, copying, distribution,
posting, or use of the information contained in or attached to this
email is prohibited unless permitted by the sender.  If you have
received this email in error, please immediately notify the sender
via return email, telephone, or fax and destroy this original transmission
and its included files without reading or saving it in any manner.
Thank you.


From scott.marlowe at gmail.com  Sat Jun 22 15:19:16 2013
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat, 22 Jun 2013 15:19:16 -0700
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
Message-ID: <CAOR=d=2kaeMw2jxURXbwFTqbbpOeCoqEMKQwREAVzA3OKRZ0Jw@mail.gmail.com>

On Sat, Jun 22, 2013 at 3:16 PM, Bambi Bellows <bbellows at dotomi.com> wrote:
> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.
>

I'm not telling you to tell them to suck it, I'm telling you to tell
them you need to fix it right. Take the system offline for as long as
it takes to upgrade postgresql (a few minutes typically). Drop the
current slony, install the latest 2.1.x, and restart replication from
scratch. Running around with your hair on fire at this point isn't
likely to result in a long term sustainable fix, but a slowly
repeating cycle of endless horror as the system gets fixed, breaks,
and gets fixed over and over.

From steve at ssinger.info  Sat Jun 22 16:00:17 2013
From: steve at ssinger.info (Steve Singer)
Date: Sat, 22 Jun 2013 19:00:17 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
Message-ID: <BLU0-SMTP24619070DBAC83D8DFF03B6DC880@phx.gbl>

On 06/22/2013 06:16 PM, Bambi Bellows wrote:
> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.

How is slony being started?
Is the startup script redirecting the output of slon somewhere (to a 
logfile, or /dev/null ?)
One option might be to restart slon so that it redirects its output to a 
logfile that you can then view.
Are you using a slon.conf ? If so does it specify a destination for logs?


> -----Original Message-----
> From: Scott Marlowe [mailto:scott.marlowe at gmail.com]
> Sent: Saturday, June 22, 2013 5:11 PM
> To: Jan Wieck
> Cc: Bambi Bellows; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Slony replication has stopped replicating
>
> On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck<JanWieck at yahoo.com>  wrote:
>> On 06/22/13 16:56, Bambi Bellows wrote:
>>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>> Slony related?
>>
>> I would say the whole thing you inherited is a complete mess and Slony
>> is a casualty of that. There is nothing so far that I can see could be
>> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>>
>> Sorry to be so blunt, but I just don't tolerate it when Slony or
>> PostgreSQL are blamed for server admin negligence and incompetence.
>>
>> This is apparently not your fault, but your predecessor's fault. So
>> don't take it personal.
> Yeah when I'm handed a grand mess like this I just tell the bosses you
> can either let me fix it the way it needs to be fixed, or watch it
> break over and over. Your choice, but I'm not getting up at 2am to fix
> things you didn't let me fix right.
>
>
>
>
> This email and any files included with it may contain privileged,
> proprietary and/or confidential information that is for the sole use
> of the intended recipient(s).  Any disclosure, copying, distribution,
> posting, or use of the information contained in or attached to this
> email is prohibited unless permitted by the sender.  If you have
> received this email in error, please immediately notify the sender
> via return email, telephone, or fax and destroy this original transmission
> and its included files without reading or saving it in any manner.
> Thank you.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>


From steve at ssinger.info  Sat Jun 22 16:09:21 2013
From: steve at ssinger.info (Steve Singer)
Date: Sat, 22 Jun 2013 19:09:21 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <476A789550F3704390F8B2B95D2464022135B3@LA-EXDB101.corp.valueclick.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
	<BLU0-SMTP24619070DBAC83D8DFF03B6DC880@phx.gbl>
	<476A789550F3704390F8B2B95D2464022135B3@LA-EXDB101.corp.valueclick.com>
Message-ID: <BLU0-SMTP1140DF71982B1BB369AD5FFDC880@phx.gbl>

On 06/22/2013 07:02 PM, Bambi Bellows wrote:
> postgres  2841     1  0 Jun21 ?        00:00:00 /opt/postgresql/bin/slon -f /opt/db/scripts/slony/mp/lane1.conf
> $ cat /opt/db/scripts/slony/mp/lane1.conf
> cluster_name='mpcluster'
> conn_info='service=prodlane1-slonik'

This tells me that slon isn't logging to syslog (otherwise that would be 
specified in your config file).
You might want to restart slon and redirect standard error and standard 
output to a log file that you can then examine.



> -----Original Message-----
> From: Steve Singer [mailto:steve at ssinger.info]
> Sent: Saturday, June 22, 2013 6:00 PM
> To: Bambi Bellows
> Cc: Scott Marlowe; Jan Wieck; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Slony replication has stopped replicating
>
> On 06/22/2013 06:16 PM, Bambi Bellows wrote:
>> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.
> How is slony being started?
> Is the startup script redirecting the output of slon somewhere (to a
> logfile, or /dev/null ?)
> One option might be to restart slon so that it redirects its output to a
> logfile that you can then view.
> Are you using a slon.conf ? If so does it specify a destination for logs?
>
>
>> -----Original Message-----
>> From: Scott Marlowe [mailto:scott.marlowe at gmail.com]
>> Sent: Saturday, June 22, 2013 5:11 PM
>> To: Jan Wieck
>> Cc: Bambi Bellows; slony1-general at lists.slony.info
>> Subject: Re: [Slony1-general] Slony replication has stopped replicating
>>
>> On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck<JanWieck at yahoo.com>   wrote:
>>> On 06/22/13 16:56, Bambi Bellows wrote:
>>>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>>> Slony related?
>>>
>>> I would say the whole thing you inherited is a complete mess and Slony
>>> is a casualty of that. There is nothing so far that I can see could be
>>> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>>>
>>> Sorry to be so blunt, but I just don't tolerate it when Slony or
>>> PostgreSQL are blamed for server admin negligence and incompetence.
>>>
>>> This is apparently not your fault, but your predecessor's fault. So
>>> don't take it personal.
>> Yeah when I'm handed a grand mess like this I just tell the bosses you
>> can either let me fix it the way it needs to be fixed, or watch it
>> break over and over. Your choice, but I'm not getting up at 2am to fix
>> things you didn't let me fix right.
>>
>>
>>
>>
>> This email and any files included with it may contain privileged,
>> proprietary and/or confidential information that is for the sole use
>> of the intended recipient(s).  Any disclosure, copying, distribution,
>> posting, or use of the information contained in or attached to this
>> email is prohibited unless permitted by the sender.  If you have
>> received this email in error, please immediately notify the sender
>> via return email, telephone, or fax and destroy this original transmission
>> and its included files without reading or saving it in any manner.
>> Thank you.
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>
>
>
>
> This email and any files included with it may contain privileged,
> proprietary and/or confidential information that is for the sole use
> of the intended recipient(s).  Any disclosure, copying, distribution,
> posting, or use of the information contained in or attached to this
> email is prohibited unless permitted by the sender.  If you have
> received this email in error, please immediately notify the sender
> via return email, telephone, or fax and destroy this original transmission
> and its included files without reading or saving it in any manner.
> Thank you.
>
>
>


From janwieck at yahoo.com  Sat Jun 22 16:45:25 2013
From: janwieck at yahoo.com (Jan Wieck)
Date: Sat, 22 Jun 2013 19:45:25 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
Message-ID: <6523io3m0oul6yla2cgus4sj.1371944278507@email.android.com>

There is nothing laughable about your situation, so I don't know what you LOL all the time.

Whoever installed that database and Slony system determined where the logs go. Check the system startup scripts ans syslog files/configuration for any hints. Unfortunately every Linux packager has his/her own mind where things are supposed to go. Not like in half organized FreeBSD territory, where you at least can guess where the logs may be.


Jan

--
Anyone who trades liberty for security deserves neither 
liberty nor security. -- Benjamin Franklin

Bambi Bellows <bbellows at dotomi.com> wrote:

>LOL.  Thank you for your advice.  I will talk to my boss about this on Monday.
>
>-----Original Message-----
>From: Scott Marlowe [mailto:scott.marlowe at gmail.com] 
>Sent: Saturday, June 22, 2013 5:19 PM
>To: Bambi Bellows
>Cc: Jan Wieck; slony1-general at lists.slony.info
>Subject: Re: [Slony1-general] Slony replication has stopped replicating
>
>On Sat, Jun 22, 2013 at 3:16 PM, Bambi Bellows <bbellows at dotomi.com> wrote:
>> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.
>>
>
>I'm not telling you to tell them to suck it, I'm telling you to tell
>them you need to fix it right. Take the system offline for as long as
>it takes to upgrade postgresql (a few minutes typically). Drop the
>current slony, install the latest 2.1.x, and restart replication from
>scratch. Running around with your hair on fire at this point isn't
>likely to result in a long term sustainable fix, but a slowly
>repeating cycle of endless horror as the system gets fixed, breaks,
>and gets fixed over and over.
>
>
>
>
>This email and any files included with it may contain privileged,
>proprietary and/or confidential information that is for the sole use
>of the intended recipient(s).  Any disclosure, copying, distribution,
>posting, or use of the information contained in or attached to this
>email is prohibited unless permitted by the sender.  If you have
>received this email in error, please immediately notify the sender
>via return email, telephone, or fax and destroy this original transmission
>and its included files without reading or saving it in any manner.
>Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130622/bd5861cb/attachment.htm 

From jeff at pgexperts.com  Sat Jun 22 16:45:49 2013
From: jeff at pgexperts.com (Jeff Frost)
Date: Sat, 22 Jun 2013 16:45:49 -0700
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <WM!c39bf9746199a4455bf903c4ad8dba98958edbabfe9a3c04f44bd1850a54b4e432bdb086408afd1f7f0229f9ba9e47b4!@asav-2.01.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
	<WM!c39bf9746199a4455bf903c4ad8dba98958edbabfe9a3c04f44bd1850a54b4e432bdb086408afd1f7f0229f9ba9e47b4!@asav-2.01.com>
Message-ID: <ADE29F7A-A90F-4BA6-BCE7-076613656436@pgexperts.com>

Do you think you could subscribe to the mailing list so we don't have to moderate every single one of your emails to the list?

On Jun 22, 2013, at 3:16 PM, Bambi Bellows <bbellows at dotomi.com> wrote:

> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.
> 
> -----Original Message-----
> From: Scott Marlowe [mailto:scott.marlowe at gmail.com] 
> Sent: Saturday, June 22, 2013 5:11 PM
> To: Jan Wieck
> Cc: Bambi Bellows; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Slony replication has stopped replicating
> 
> On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
>> On 06/22/13 16:56, Bambi Bellows wrote:
>>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>> 
>> Slony related?
>> 
>> I would say the whole thing you inherited is a complete mess and Slony
>> is a casualty of that. There is nothing so far that I can see could be
>> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>> 
>> Sorry to be so blunt, but I just don't tolerate it when Slony or
>> PostgreSQL are blamed for server admin negligence and incompetence.
>> 
>> This is apparently not your fault, but your predecessor's fault. So
>> don't take it personal.
> 
> Yeah when I'm handed a grand mess like this I just tell the bosses you
> can either let me fix it the way it needs to be fixed, or watch it
> break over and over. Your choice, but I'm not getting up at 2am to fix
> things you didn't let me fix right.
> 
> 
> 
> 
> This email and any files included with it may contain privileged,
> proprietary and/or confidential information that is for the sole use
> of the intended recipient(s).  Any disclosure, copying, distribution,
> posting, or use of the information contained in or attached to this
> email is prohibited unless permitted by the sender.  If you have
> received this email in error, please immediately notify the sender
> via return email, telephone, or fax and destroy this original transmission
> and its included files without reading or saving it in any manner.
> Thank you.
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

---
Jeff Frost <jeff at pgexperts.com>
CTO, PostgreSQL Experts, Inc.
Phone: 1-888-PG-EXPRT x506
FAX: 415-762-5122
http://www.pgexperts.com/ 







From bbellows at dotomi.com  Sat Jun 22 15:21:20 2013
From: bbellows at dotomi.com (Bambi Bellows)
Date: Sat, 22 Jun 2013 22:21:20 +0000
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <CAOR=d=2kaeMw2jxURXbwFTqbbpOeCoqEMKQwREAVzA3OKRZ0Jw@mail.gmail.com>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
	<CAOR=d=2kaeMw2jxURXbwFTqbbpOeCoqEMKQwREAVzA3OKRZ0Jw@mail.gmail.com>
Message-ID: <476A789550F3704390F8B2B95D2464022134F0@LA-EXDB101.corp.valueclick.com>

LOL.  Thank you for your advice.  I will talk to my boss about this on Monday.

-----Original Message-----
From: Scott Marlowe [mailto:scott.marlowe at gmail.com] 
Sent: Saturday, June 22, 2013 5:19 PM
To: Bambi Bellows
Cc: Jan Wieck; slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Slony replication has stopped replicating

On Sat, Jun 22, 2013 at 3:16 PM, Bambi Bellows <bbellows at dotomi.com> wrote:
> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.
>

I'm not telling you to tell them to suck it, I'm telling you to tell
them you need to fix it right. Take the system offline for as long as
it takes to upgrade postgresql (a few minutes typically). Drop the
current slony, install the latest 2.1.x, and restart replication from
scratch. Running around with your hair on fire at this point isn't
likely to result in a long term sustainable fix, but a slowly
repeating cycle of endless horror as the system gets fixed, breaks,
and gets fixed over and over.




This email and any files included with it may contain privileged,
proprietary and/or confidential information that is for the sole use
of the intended recipient(s).  Any disclosure, copying, distribution,
posting, or use of the information contained in or attached to this
email is prohibited unless permitted by the sender.  If you have
received this email in error, please immediately notify the sender
via return email, telephone, or fax and destroy this original transmission
and its included files without reading or saving it in any manner.
Thank you.


From bbellows at dotomi.com  Sat Jun 22 16:02:10 2013
From: bbellows at dotomi.com (Bambi Bellows)
Date: Sat, 22 Jun 2013 23:02:10 +0000
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <BLU0-SMTP24619070DBAC83D8DFF03B6DC880@phx.gbl>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
	<BLU0-SMTP24619070DBAC83D8DFF03B6DC880@phx.gbl>
Message-ID: <476A789550F3704390F8B2B95D2464022135B3@LA-EXDB101.corp.valueclick.com>

postgres  2841     1  0 Jun21 ?        00:00:00 /opt/postgresql/bin/slon -f /opt/db/scripts/slony/mp/lane1.conf
$ cat /opt/db/scripts/slony/mp/lane1.conf
cluster_name='mpcluster'
conn_info='service=prodlane1-slonik'

-----Original Message-----
From: Steve Singer [mailto:steve at ssinger.info] 
Sent: Saturday, June 22, 2013 6:00 PM
To: Bambi Bellows
Cc: Scott Marlowe; Jan Wieck; slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Slony replication has stopped replicating

On 06/22/2013 06:16 PM, Bambi Bellows wrote:
> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.

How is slony being started?
Is the startup script redirecting the output of slon somewhere (to a 
logfile, or /dev/null ?)
One option might be to restart slon so that it redirects its output to a 
logfile that you can then view.
Are you using a slon.conf ? If so does it specify a destination for logs?


> -----Original Message-----
> From: Scott Marlowe [mailto:scott.marlowe at gmail.com]
> Sent: Saturday, June 22, 2013 5:11 PM
> To: Jan Wieck
> Cc: Bambi Bellows; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Slony replication has stopped replicating
>
> On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck<JanWieck at yahoo.com>  wrote:
>> On 06/22/13 16:56, Bambi Bellows wrote:
>>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>> Slony related?
>>
>> I would say the whole thing you inherited is a complete mess and Slony
>> is a casualty of that. There is nothing so far that I can see could be
>> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>>
>> Sorry to be so blunt, but I just don't tolerate it when Slony or
>> PostgreSQL are blamed for server admin negligence and incompetence.
>>
>> This is apparently not your fault, but your predecessor's fault. So
>> don't take it personal.
> Yeah when I'm handed a grand mess like this I just tell the bosses you
> can either let me fix it the way it needs to be fixed, or watch it
> break over and over. Your choice, but I'm not getting up at 2am to fix
> things you didn't let me fix right.
>
>
>
>
> This email and any files included with it may contain privileged,
> proprietary and/or confidential information that is for the sole use
> of the intended recipient(s).  Any disclosure, copying, distribution,
> posting, or use of the information contained in or attached to this
> email is prohibited unless permitted by the sender.  If you have
> received this email in error, please immediately notify the sender
> via return email, telephone, or fax and destroy this original transmission
> and its included files without reading or saving it in any manner.
> Thank you.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>





This email and any files included with it may contain privileged,
proprietary and/or confidential information that is for the sole use
of the intended recipient(s).  Any disclosure, copying, distribution,
posting, or use of the information contained in or attached to this
email is prohibited unless permitted by the sender.  If you have
received this email in error, please immediately notify the sender
via return email, telephone, or fax and destroy this original transmission
and its included files without reading or saving it in any manner.
Thank you.


From bbellows at dotomi.com  Sat Jun 22 16:19:43 2013
From: bbellows at dotomi.com (Bambi Bellows)
Date: Sat, 22 Jun 2013 23:19:43 +0000
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <BLU0-SMTP1140DF71982B1BB369AD5FFDC880@phx.gbl>
References: <476A789550F3704390F8B2B95D2464022122A6@LA-EXDB101.corp.valueclick.com>
	<51C60EBD.1000500@Yahoo.com>
	<476A789550F3704390F8B2B95D2464022123EB@LA-EXDB101.corp.valueclick.com>
	<51C616BF.2000508@Yahoo.com>
	<CAOR=d=0o_=V6AtnsFJ=b=Zce1ywpo7YHHZgLnYjo2NniFT7ZEQ@mail.gmail.com>
	<476A789550F3704390F8B2B95D2464022124AE@LA-EXDB101.corp.valueclick.com>
	<BLU0-SMTP24619070DBAC83D8DFF03B6DC880@phx.gbl>
	<476A789550F3704390F8B2B95D2464022135B3@LA-EXDB101.corp.valueclick.com>
	<BLU0-SMTP1140DF71982B1BB369AD5FFDC880@phx.gbl>
Message-ID: <476A789550F3704390F8B2B95D246402213605@LA-EXDB101.corp.valueclick.com>

This explains why I can't find the logs.  LOL.  

-----Original Message-----
From: Steve Singer [mailto:steve at ssinger.info] 
Sent: Saturday, June 22, 2013 6:09 PM
To: Bambi Bellows
Cc: Scott Marlowe; Jan Wieck; slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Slony replication has stopped replicating

On 06/22/2013 07:02 PM, Bambi Bellows wrote:
> postgres  2841     1  0 Jun21 ?        00:00:00 /opt/postgresql/bin/slon -f /opt/db/scripts/slony/mp/lane1.conf
> $ cat /opt/db/scripts/slony/mp/lane1.conf
> cluster_name='mpcluster'
> conn_info='service=prodlane1-slonik'

This tells me that slon isn't logging to syslog (otherwise that would be 
specified in your config file).
You might want to restart slon and redirect standard error and standard 
output to a log file that you can then examine.



> -----Original Message-----
> From: Steve Singer [mailto:steve at ssinger.info]
> Sent: Saturday, June 22, 2013 6:00 PM
> To: Bambi Bellows
> Cc: Scott Marlowe; Jan Wieck; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Slony replication has stopped replicating
>
> On 06/22/2013 06:16 PM, Bambi Bellows wrote:
>> LOL.  You guys are killin me.  Meantime, can someone point me to where the log file destination might be hidden?  Because I can't even find out what broke and why.  And as much as it's great to blame my predecessors, they aren't here anymore, and I'm out of my depth on Slony... ob.vi.ous.ly.  And telling my boss to suck it is not really an option for me.
> How is slony being started?
> Is the startup script redirecting the output of slon somewhere (to a
> logfile, or /dev/null ?)
> One option might be to restart slon so that it redirects its output to a
> logfile that you can then view.
> Are you using a slon.conf ? If so does it specify a destination for logs?
>
>
>> -----Original Message-----
>> From: Scott Marlowe [mailto:scott.marlowe at gmail.com]
>> Sent: Saturday, June 22, 2013 5:11 PM
>> To: Jan Wieck
>> Cc: Bambi Bellows; slony1-general at lists.slony.info
>> Subject: Re: [Slony1-general] Slony replication has stopped replicating
>>
>> On Sat, Jun 22, 2013 at 2:27 PM, Jan Wieck<JanWieck at yahoo.com>   wrote:
>>> On 06/22/13 16:56, Bambi Bellows wrote:
>>>> LOL.  Thanks Jan.  No, I inherited these systems, and upgrading is not really an option for the database...  I am, however, trying to find where the log files are.  They aren't in the /opt/logs/postgres directory, and they aren't in the postgres logs directory.  I have two sudo finds running, but, so far, no luck.  Gotta say, haven't been having the best of luck so far on anything slony-related.
>>> Slony related?
>>>
>>> I would say the whole thing you inherited is a complete mess and Slony
>>> is a casualty of that. There is nothing so far that I can see could be
>>> blamed on Slony. Only a grossly outdated and unmanaged server setup.
>>>
>>> Sorry to be so blunt, but I just don't tolerate it when Slony or
>>> PostgreSQL are blamed for server admin negligence and incompetence.
>>>
>>> This is apparently not your fault, but your predecessor's fault. So
>>> don't take it personal.
>> Yeah when I'm handed a grand mess like this I just tell the bosses you
>> can either let me fix it the way it needs to be fixed, or watch it
>> break over and over. Your choice, but I'm not getting up at 2am to fix
>> things you didn't let me fix right.
>>
>>
>>
>>
>> This email and any files included with it may contain privileged,
>> proprietary and/or confidential information that is for the sole use
>> of the intended recipient(s).  Any disclosure, copying, distribution,
>> posting, or use of the information contained in or attached to this
>> email is prohibited unless permitted by the sender.  If you have
>> received this email in error, please immediately notify the sender
>> via return email, telephone, or fax and destroy this original transmission
>> and its included files without reading or saving it in any manner.
>> Thank you.
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>
>
>
>
> This email and any files included with it may contain privileged,
> proprietary and/or confidential information that is for the sole use
> of the intended recipient(s).  Any disclosure, copying, distribution,
> posting, or use of the information contained in or attached to this
> email is prohibited unless permitted by the sender.  If you have
> received this email in error, please immediately notify the sender
> via return email, telephone, or fax and destroy this original transmission
> and its included files without reading or saving it in any manner.
> Thank you.
>
>
>





This email and any files included with it may contain privileged,
proprietary and/or confidential information that is for the sole use
of the intended recipient(s).  Any disclosure, copying, distribution,
posting, or use of the information contained in or attached to this
email is prohibited unless permitted by the sender.  If you have
received this email in error, please immediately notify the sender
via return email, telephone, or fax and destroy this original transmission
and its included files without reading or saving it in any manner.
Thank you.


From scott.marlowe at gmail.com  Sat Jun 22 16:48:45 2013
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat, 22 Jun 2013 16:48:45 -0700
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <6523io3m0oul6yla2cgus4sj.1371944278507@email.android.com>
References: <6523io3m0oul6yla2cgus4sj.1371944278507@email.android.com>
Message-ID: <CAOR=d=0U6cTZOAsoqzmBD8ePuPRD6B_wP_hb4OXCBvkA=njBGg@mail.gmail.com>

On Sat, Jun 22, 2013 at 4:45 PM, Jan Wieck <janwieck at yahoo.com> wrote:
> There is nothing laughable about your situation, so I don't know what you
> LOL all the time.

It might be one of those laugh or cry things.

> Whoever installed that database and Slony system determined where the logs
> go. Check the system startup scripts ans syslog files/configuration for any
> hints. Unfortunately every Linux packager has his/her own mind where things
> are supposed to go. Not like in half organized FreeBSD territory, where you
> at least can guess where the logs may be.

To be fair, most distros put it all in a few common places. I.e.
Redhat / rpm based or Debian / deb based.  And there's nothing
stopping a BSD sysadmin from building postgresql ala ./configure
--prefix=/home/mydir/are/you/kidding/me

From JanWieck at Yahoo.com  Sun Jun 23 13:47:17 2013
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sun, 23 Jun 2013 16:47:17 -0400
Subject: [Slony1-general] Slony replication has stopped replicating
In-Reply-To: <CAOR=d=0U6cTZOAsoqzmBD8ePuPRD6B_wP_hb4OXCBvkA=njBGg@mail.gmail.com>
References: <6523io3m0oul6yla2cgus4sj.1371944278507@email.android.com>
	<CAOR=d=0U6cTZOAsoqzmBD8ePuPRD6B_wP_hb4OXCBvkA=njBGg@mail.gmail.com>
Message-ID: <51C75ED5.106@Yahoo.com>

On 06/22/13 19:48, Scott Marlowe wrote:
> On Sat, Jun 22, 2013 at 4:45 PM, Jan Wieck <janwieck at yahoo.com> wrote:
>> There is nothing laughable about your situation, so I don't know what you
>> LOL all the time.
> 
> It might be one of those laugh or cry things.
> 
>> Whoever installed that database and Slony system determined where the logs
>> go. Check the system startup scripts ans syslog files/configuration for any
>> hints. Unfortunately every Linux packager has his/her own mind where things
>> are supposed to go. Not like in half organized FreeBSD territory, where you
>> at least can guess where the logs may be.
> 
> To be fair, most distros put it all in a few common places. I.e.
> Redhat / rpm based or Debian / deb based.  And there's nothing
> stopping a BSD sysadmin from building postgresql ala ./configure
> --prefix=/home/mydir/are/you/kidding/me
> 

Fair enough. But then again, everyone can do that compiling from sources
(unless you try to imply that Linux sysadmins don't know any more how to
do that).

The points several people have made remain. Bambi has to follow the
system startup breadcrumbs to figure out where the log is, if he/she
intends to find out what is currently broken in order to fix the current
system

On the other hand, fixing that system "properly" means to upgrade
everything to the latest versions. Which should start with something
like "apt-get update" or "yum update" instead of attempting to just get
past the current "problem". I still have the feeling that Slony stopped
working because of a far greater underlying problem.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

