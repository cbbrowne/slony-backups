From h.mia at hawarIT.com  Mon Feb  7 22:40:51 2011
From: h.mia at hawarIT.com (Hanif)
Date: Tue, 8 Feb 2011 06:40:51 +0000 (UTC)
Subject: [Slony1-general]
	=?utf-8?q?How_sensible_is_turning_off_synchronou?=
	=?utf-8?q?s=5Fcommiton=09a_Slony_slave_while_leaving_it_on_on_the_?=
	=?utf-8?q?master=3F?=
References: <db471ace1003250837h6ce5c87av6920220cb112f4d3@mail.gmail.com>
Message-ID: <loom.20110208T073301-488@post.gmane.org>

Peter Geoghegan <peter.geoghegan86 at ...> writes:

> 
> Hello,
> 
> I run a Slony cluster (2.0.2, PG 8.4 on master, 8.3 on 5 slaves) where
> an application that runs on each slave should commit transactions
> quickly - it's supposed to be a realtime system. I have decided that
> the trade-off between performance and data integrity offered by
> turning off synchronous_commit in postgresql.conf on each slave (but
> not the master) is acceptable - the window for data loss is very
> small, as all the activity is small transactions. Obviously, they
> aren't touching replicated tables, but their own tables that aren't in
> any replication set. We don't perform cascading or failover - it's a
> rather simple set-up, as Slony clusters go.
> 
> My concern with turning off synchronous_commit is the potential that
> it might break replication, if a slave and the master were left in an
> inconsistent state. Breaking replication is far more expensive to me
> than losing one of my small transactions.
> 
> I am aware that it's possible to specify whether or not
> synchronous_commit is used on a transaction by transaction basis, but
> it isn't apparent how I can do this with the Qt database driver that I
> use, that wraps libpq. I'm using implicit transactions by calling
> pl/pgSQL functions on the slaves (every modifying operation is a
> function call). Perhaps that should be the next thing I investigate if
> turning synchronous_commit off server wide in postgresql.conf on
> slaves turns out to be a bad idea.
> 
> What sort of risk am I assuming specifically to replication by turning
> off synchronous_commit on the slaves but not on the master?
> 
> Thanks,
> Peter Geoghegan
> 

Dear Peter,

Basically I'm new in Slony replication. I have to setup a replication between an
8.4 master and a 8.3 slave in windows environment. From documentation I noticed
master and slave should be same version. But searching the web on "slony 8.4
master 8.3 slave" I run into your post. From this post it seems it is perfectly
normal you can run slony on 8.4 master and 8.3 slave. 

Could you tell me in may be max 5 minutes if I really should upgrade now or just
go to get things working. Any url on this issue already would be very helpful. 

kind regards

Hanif
h.mia at hawarit.com




From peter.geoghegan86 at gmail.com  Tue Feb  8 05:01:23 2011
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Tue, 8 Feb 2011 13:01:23 +0000
Subject: [Slony1-general] How sensible is turning off
 synchronous_commiton a Slony slave while leaving it on on the master?
In-Reply-To: <loom.20110208T073301-488@post.gmane.org>
References: <db471ace1003250837h6ce5c87av6920220cb112f4d3@mail.gmail.com>
	<loom.20110208T073301-488@post.gmane.org>
Message-ID: <AANLkTinOi2BtgtUSn9ah+=iBhPAzF9ApkygME1PaJU3N@mail.gmail.com>

Hanif,

Slony versions must be *identical*. However, postgres versions can be
different major releases. Before pg_upgrade, slony was a popular way
of upgrading large databases.


-- 
Regards,
Peter Geoghegan

From guillaume at lelarge.info  Tue Feb  8 14:52:52 2011
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Tue, 08 Feb 2011 23:52:52 +0100
Subject: [Slony1-general] How sensible is turning off
 synchronous_commiton a Slony slave while leaving it on on the master?
In-Reply-To: <AANLkTinOi2BtgtUSn9ah+=iBhPAzF9ApkygME1PaJU3N@mail.gmail.com>
References: <db471ace1003250837h6ce5c87av6920220cb112f4d3@mail.gmail.com>	<loom.20110208T073301-488@post.gmane.org>
	<AANLkTinOi2BtgtUSn9ah+=iBhPAzF9ApkygME1PaJU3N@mail.gmail.com>
Message-ID: <4D51C944.4020609@lelarge.info>

Le 08/02/2011 14:01, Peter Geoghegan a ?crit :
> [...]
> Slony versions must be *identical*. However, postgres versions can be
> different major releases. Before pg_upgrade, slony was a popular way
> of upgrading large databases.
> 

Still is :)

Sadly, the number of users with <8.4 are really much bigger than the
number of users with 8.4.


-- 
Guillaume
 http://www.postgresql.fr
 http://dalibo.com

From ajs at crankycanuck.ca  Tue Feb  8 15:34:02 2011
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue, 8 Feb 2011 18:34:02 -0500
Subject: [Slony1-general] How sensible is turning
	off	synchronous_commiton a Slony slave while leaving it on on
	the	master?
In-Reply-To: <4D51C944.4020609@lelarge.info>
References: <db471ace1003250837h6ce5c87av6920220cb112f4d3@mail.gmail.com>
	<loom.20110208T073301-488@post.gmane.org>
	<AANLkTinOi2BtgtUSn9ah+=iBhPAzF9ApkygME1PaJU3N@mail.gmail.com>
	<4D51C944.4020609@lelarge.info>
Message-ID: <20110208233401.GG27544@shinkuro.com>

On Tue, Feb 08, 2011 at 11:52:52PM +0100, Guillaume Lelarge wrote:
> Sadly, the number of users with <8.4 are really much bigger than the
> number of users with 8.4.

I don't find this sad.  I find this an indication that people have
things important to them in Postgres databases, and that they're
naturally conservative about touching those things.  It's a kind of
victory when you have an installed base.

(It's also a scourge, of course, but let's celebrate!)

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From JanWieck at Yahoo.com  Tue Feb  8 19:27:06 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 08 Feb 2011 22:27:06 -0500
Subject: [Slony1-general] How sensible is
 turning	off	synchronous_commiton a Slony slave while leaving it on
 on	the	master?
In-Reply-To: <20110208233401.GG27544@shinkuro.com>
References: <db471ace1003250837h6ce5c87av6920220cb112f4d3@mail.gmail.com>	<loom.20110208T073301-488@post.gmane.org>	<AANLkTinOi2BtgtUSn9ah+=iBhPAzF9ApkygME1PaJU3N@mail.gmail.com>	<4D51C944.4020609@lelarge.info>
	<20110208233401.GG27544@shinkuro.com>
Message-ID: <4D52098A.6070308@Yahoo.com>

On 2/8/2011 6:34 PM, Andrew Sullivan wrote:
> On Tue, Feb 08, 2011 at 11:52:52PM +0100, Guillaume Lelarge wrote:
>>  Sadly, the number of users with<8.4 are really much bigger than the
>>  number of users with 8.4.
>
> I don't find this sad.  I find this an indication that people have
> things important to them in Postgres databases, and that they're
> naturally conservative about touching those things.  It's a kind of
> victory when you have an installed base.
>
> (It's also a scourge, of course, but let's celebrate!)

I also am not convinced yet that the time of dump+restore upgrades is 
over for good. I may change my mind about that in 5 years or so, but 
right now, I'd say let's be prepared.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From h.mia at hawarit.com  Tue Feb  8 23:20:31 2011
From: h.mia at hawarit.com (hanif)
Date: Wed, 9 Feb 2011 07:20:31 +0000 (UTC)
Subject: [Slony1-general]
	=?utf-8?q?How_sensible_is_turning_off_synchronou?=
	=?utf-8?q?s=5Fcommiton_a_Slony_slave_while_leaving_it_on_on_the_ma?=
	=?utf-8?q?ster=3F?=
References: <db471ace1003250837h6ce5c87av6920220cb112f4d3@mail.gmail.com>
	<loom.20110208T073301-488@post.gmane.org>
	<AANLkTinOi2BtgtUSn9ah+=iBhPAzF9ApkygME1PaJU3N@mail.gmail.com>
Message-ID: <loom.20110209T080426-378@post.gmane.org>

Peter Geoghegan <peter.geoghegan86 at ...> writes:

> 
> Hanif,
> 
> Slony versions must be *identical*. However, postgres versions can be
> different major releases. Before pg_upgrade, slony was a popular way
> of upgrading large databases.
> 


Dear Peter,
As i mentioned earlier, i am a new at postgreSQL.Could you please clarify me 
about REPLICATION of PostgreSQL 8.4.6(master) database with PostgreSQL 8.3.6
(slave) database using slony-I?
When i excute slonik.exe command on master node it seems listening but on the 
slave node it shows error!


From ulas.albayrak at gmail.com  Fri Feb 11 07:57:27 2011
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Fri, 11 Feb 2011 16:57:27 +0100
Subject: [Slony1-general] Installing test_slony_state script
Message-ID: <AANLkTikE__sURgL3wY0iS9kMqxsENG_zQWytMyb+ieQb@mail.gmail.com>

Hi,

I've just set up a new Slony cluster and after having completed the
whole installation I discovered that the "test_slony_state" script is
not installed. From what I've gathered on the internet the script is
installed by setting the proper options when initially
installing/compiling Slony-I. Is there any way of getting the script
onto my machine in this stage, when everything's already setup and
running, without uninstalling Slony-I?

-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From cbbrowne at ca.afilias.info  Fri Feb 11 08:46:02 2011
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri, 11 Feb 2011 11:46:02 -0500
Subject: [Slony1-general] Installing test_slony_state script
In-Reply-To: <AANLkTikE__sURgL3wY0iS9kMqxsENG_zQWytMyb+ieQb@mail.gmail.com>
	(Ulas Albayrak's message of "Fri, 11 Feb 2011 16:57:27 +0100")
References: <AANLkTikE__sURgL3wY0iS9kMqxsENG_zQWytMyb+ieQb@mail.gmail.com>
Message-ID: <87ei7ey0xh.fsf@cbbrowne.afilias-int.info>

Ulas Albayrak <ulas.albayrak at gmail.com> writes:
> I've just set up a new Slony cluster and after having completed the
> whole installation I discovered that the "test_slony_state" script is
> not installed. From what I've gathered on the internet the script is
> installed by setting the proper options when initially
> installing/compiling Slony-I. Is there any way of getting the script
> onto my machine in this stage, when everything's already setup and
> running, without uninstalling Slony-I?

It's simply a Perl script; you can copy it to wherever you'd like to
have it be.  No terribly special installation process.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From atsaloli.tech at gmail.com  Sun Feb 13 10:41:05 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sun, 13 Feb 2011 10:41:05 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21: sync
 of large table (45 GB) fails: sequence maxed out
Message-ID: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>

Problem:  I can't setup replication, copy of a large table (45 GB)
does not complete.


Data:  Master has a 58 GB database (according to psql "\l") and over
45 GB is in a single table (including TOAST and indexes).

Slony logs show that the COPY of other tables start and complete, but
this 45 GB table, the COPY starts, and is still running after 11
hours.

(When the database was half the size it is now, it'd take about an
hour or 90 minutes to copy to our DR using Slony.)

tcpdump shows Slony is still trying to copy this table.   (I see what
looks like the contents of this table going from Master to DR)

Other indicators of trouble:


/usr/bin/test_slony_state-dbi.pl tells me:

Summary of sl_confirm aging
   Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
        1          2        156        156      11:43:00      11:43:00    1
        2          1       4141       4205      00:00:00      00:10:00    0


Node: 1 sl_seqlog tuples = 225368 > 200000
================================================
Number of tuples in Slony-I table sl_seqlog is 225368 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.


Node: 2 sl_seqlog tuples = 225368 > 200000
================================================
Number of tuples in Slony-I table sl_seqlog is 225368 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.

--- end of output from test_slony_state-dbi.pl ---

Bucardo's check_postgres.pl reports:

	POSTGRES_SEQUENCE CRITICAL: [master database] sl_log_status=100% (calls left=0)


The master's Slony log looks like this:
2011-02-13 10:35:53 PSTDEBUG2 localListenThread: Received event 1,11085 SYNC
2011-02-13 10:35:55 PSTDEBUG2 syncThread: new sl_action_seq 64567 - SYNC 11086
2011-02-13 10:35:55 PSTDEBUG2 localListenThread: Received event 1,11086 SYNC
2011-02-13 10:35:57 PSTDEBUG2 syncThread: new sl_action_seq 64578 - SYNC 11087
2011-02-13 10:35:57 PSTDEBUG2 localListenThread: Received event 1,11087 SYNC
2011-02-13 10:35:58 PSTDEBUG2 remoteListenThread_2: LISTEN
2011-02-13 10:35:59 PSTDEBUG2 syncThread: new sl_action_seq 64634 - SYNC 11088
2011-02-13 10:35:59 PSTDEBUG2 localListenThread: Received event 1,11088 SYNC
2011-02-13 10:35:59 PSTDEBUG2 remoteListenThread_2: queue event 2,4320 SYNC
2011-02-13 10:35:59 PSTDEBUG2 remoteListenThread_2: UNLISTEN
2011-02-13 10:35:59 PSTDEBUG2 remoteWorkerThread_2: Received event 2,4320 SYNC
2011-02-13 10:35:59 PSTDEBUG2 calc sync size - last time: 1 last
length: 9995 ideal: 6 proposed size: 3
2011-02-13 10:35:59 PSTDEBUG2 remoteWorkerThread_2: SYNC 4320 processing
2011-02-13 10:35:59 PSTDEBUG2 remoteWorkerThread_2: no sets need
syncing for this event
2011-02-13 10:36:01 PSTDEBUG2 syncThread: new sl_action_seq 64638 - SYNC 11089
2011-02-13 10:36:01 PSTDEBUG2 localListenThread: Received event 1,11089



The Slave's Slony log shows:

2011-02-13 10:37:02 PSTDEBUG2 remoteListenThread_1: queue event 1,11116 SYNC
2011-02-13 10:37:04 PSTDEBUG2 remoteListenThread_1: queue event 1,11117 SYNC
2011-02-13 10:37:06 PSTDEBUG2 remoteListenThread_1: queue event 1,11118 SYNC
2011-02-13 10:37:08 PSTDEBUG2 remoteListenThread_1: queue event 1,11119 SYNC
2011-02-13 10:37:10 PSTDEBUG2 syncThread: new sl_action_seq 1 - SYNC 4327
2011-02-13 10:37:12 PSTDEBUG2 localListenThread: Received event 2,4327 SYNC
2011-02-13 10:37:16 PSTDEBUG2 remoteListenThread_1: queue event 1,11120 SYNC
2011-02-13 10:37:18 PSTDEBUG2 remoteListenThread_1: queue event 1,11121 SYNC
2011-02-13 10:37:20 PSTDEBUG2 syncThread: new sl_action_seq 1 - SYNC 4328
2011-02-13 10:37:24 PSTDEBUG2 localListenThread: Received event 2,4328 SYNC


The Slony server is not breaking a sweat (according to vmstat) - CPU
is 95% idle.

Any suggestions?

Best,
-at

From atsaloli.tech at gmail.com  Sun Feb 13 11:09:16 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sun, 13 Feb 2011 11:09:16 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>
References: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>
Message-ID: <AANLkTi=sQaYeowL-q4wPKeWOz5bWh29Vp4MmsqqcqeQ0@mail.gmail.com>

The slave disk is 100% full.  I don't understand what's using the
space.  It's growing before my eyes, could Slony be filling up the
disk?

I get different reports from "du" and "df":


# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda3             117G  110G  1.3G  99% /
/dev/sda1              99M   24M   71M  26% /boot
tmpfs                 3.9G     0  3.9G   0% /dev/shm
]#


# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda3             117G  110G  512M 100% /
/dev/sda1              99M   24M   71M  26% /boot
tmpfs                 3.9G     0  3.9G   0% /dev/shm
# du -sh /
20G     /
#

From ssinger_pg at sympatico.ca  Sun Feb 13 11:16:48 2011
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Sun, 13 Feb 2011 14:16:48 -0500
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>
References: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>
Message-ID: <BLU0-SMTP786A031DBCBAFA4C0D4DABACD10@phx.gbl>

On Sun, 13 Feb 2011, Aleksey Tsalolikhin wrote:

> Problem:  I can't setup replication, copy of a large table (45 GB)
> does not complete.
>
>
> Data:  Master has a 58 GB database (according to psql "\l") and over
> 45 GB is in a single table (including TOAST and indexes).
>
> Slony logs show that the COPY of other tables start and complete, but
> this 45 GB table, the COPY starts, and is still running after 11
> hours.
>

> The Slave's Slony log shows:
>
> 2011-02-13 10:37:02 PSTDEBUG2 remoteListenThread_1: queue event 1,11116 SYNC
> 2011-02-13 10:37:04 PSTDEBUG2 remoteListenThread_1: queue event 1,11117 SYNC
> 2011-02-13 10:37:06 PSTDEBUG2 remoteListenThread_1: queue event 1,11118 SYNC
> 2011-02-13 10:37:08 PSTDEBUG2 remoteListenThread_1: queue event 1,11119 SYNC
> 2011-02-13 10:37:10 PSTDEBUG2 syncThread: new sl_action_seq 1 - SYNC 4327
> 2011-02-13 10:37:12 PSTDEBUG2 localListenThread: Received event 2,4327 SYNC
> 2011-02-13 10:37:16 PSTDEBUG2 remoteListenThread_1: queue event 1,11120 SYNC
> 2011-02-13 10:37:18 PSTDEBUG2 remoteListenThread_1: queue event 1,11121 SYNC
> 2011-02-13 10:37:20 PSTDEBUG2 syncThread: new sl_action_seq 1 - SYNC 4328
> 2011-02-13 10:37:24 PSTDEBUG2 localListenThread: Received event 2,4328 SYNC

I suspect the COPY is failing at some point and then restarting.  Can you 
find the error in your slony log on the slave?  The snippet above is too 
small, it doesn't show the remoteWorker doing anything.  The 
remoteWorkerThread_1 is what is doing the copy.

As to your disk space issue, do you know which tables are using the space?

> The Slony server is not breaking a sweat (according to vmstat) - CPU
> is 95% idle.
>
> Any suggestions?
>
> Best,
> -at
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From atsaloli.tech at gmail.com  Sun Feb 13 12:25:05 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sun, 13 Feb 2011 12:25:05 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <BLU0-SMTP786A031DBCBAFA4C0D4DABACD10@phx.gbl>
References: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>
	<BLU0-SMTP786A031DBCBAFA4C0D4DABACD10@phx.gbl>
Message-ID: <AANLkTimfE314NcQoObeGxY4H4FoLprLqCjjEvvLcyxmn@mail.gmail.com>

Dear Steve,

  Thank you for your quick response.

> I suspect the COPY is failing at some point and then restarting.

  You are quite right!   COPY is failing our large table and then restarting:

2011-02-13 03:23:38 PSTERROR  remoteWorkerThread_1: copy from stdin on
local node - PGRES_FATAL_ERROR ERROR:  could not extend relation
base/25219965/26699570: No space left on device

2011-02-13 03:23:38 PSTWARN   remoteWorkerThread_1: data copy for set
1 failed 1 times - sleep 15 seconds

...

2011-02-13 11:01:47 PSTERROR  remoteWorkerThread_1: copy from stdin on
local node - PGRES_FATAL_ERROR ERROR:  could not extend relation
base/25219965/27037006: wrote only 4096 of 8192 bytes at block 136871

2011-02-13 11:01:47 PSTWARN   remoteWorkerThread_1: data copy for set
1 failed 6 times - sleep 60 seconds


> As to your disk space issue, do you know which tables are using the space?
>

On the master database, yes.  But on the slave my db size is reported
(by "psql -c '\l+'  ") as 19 MB!


I tried this:
Shut down the Slon slave.  (Disk usage still near 100%. )
Shut down Postgres server.  (Disk usage dropped to 7%. )
Started Postgres.  Started Slon Slave.  The copy of the replication
set resumed.
Let's see how it goes now.


Best,
Aleksey

From atsaloli.tech at gmail.com  Sun Feb 13 14:04:22 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sun, 13 Feb 2011 14:04:22 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTimfE314NcQoObeGxY4H4FoLprLqCjjEvvLcyxmn@mail.gmail.com>
References: <AANLkTik6=x-ZO-GpKiLETBhm=bseJXox2-TiMnDZFuEG@mail.gmail.com>
	<BLU0-SMTP786A031DBCBAFA4C0D4DABACD10@phx.gbl>
	<AANLkTimfE314NcQoObeGxY4H4FoLprLqCjjEvvLcyxmn@mail.gmail.com>
Message-ID: <AANLkTindob_BJEyEuAtTHbZCFBJ6yDD-xTK03sk-tfVV@mail.gmail.com>

OK, same thing happened: disk usage on the Slony slave is now 96%, and
this is while COPY of our large table (over 45 GB in size) is running.
  106 GB is used on disk.

Any suggestions, please?

Best,
-at

From melvin6925 at yahoo.com  Sun Feb 13 14:17:54 2011
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Sun, 13 Feb 2011 14:17:54 -0800 (PST)
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
	sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTindob_BJEyEuAtTHbZCFBJ6yDD-xTK03sk-tfVV@mail.gmail.com>
Message-ID: <50828.17909.qm@web121801.mail.ne1.yahoo.com>

>OK, same thing happened: disk usage on the Slony slave is now 96%, and

>this is while COPY of our large table (over 45 GB in size) is running.

?> 106 GB is used on disk.



>Any suggestions, please?



Is WAL archiving enabled on the slave? If so, turn it off, restart postgresql on the slave, then try replicating.

Melvin Davidson 
  



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110213/490cfec3/attachment.html 

From atsaloli.tech at gmail.com  Sun Feb 13 17:45:46 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sun, 13 Feb 2011 17:45:46 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <50828.17909.qm@web121801.mail.ne1.yahoo.com>
References: <AANLkTindob_BJEyEuAtTHbZCFBJ6yDD-xTK03sk-tfVV@mail.gmail.com>
	<50828.17909.qm@web121801.mail.ne1.yahoo.com>
Message-ID: <AANLkTin_F_hoDzHv8YZFC8uMMT2Spwg+1tzrVg9fGoC_@mail.gmail.com>

On Sun, Feb 13, 2011 at 2:17 PM, Melvin Davidson <melvin6925 at yahoo.com>wrote:

> >OK, same thing happened: disk usage on the Slony slave is now 96%, and
> >this is while COPY of our large table (over 45 GB in size) is running.
>  > 106 GB is used on disk.
>
> >Any suggestions, please?
>
> Is WAL archiving enabled on the slave? If so, turn it off, restart
> postgresql on the slave, then try replicating.
>

Thanks, Melvin.  WAL archiving is disabled on both servers.  (archive_mode =
off).

How much disk space should I expect Postgres (8.4.4) to use when receiving a
45 GB table via Slony?

Best,
Aleksey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110213/fc885960/attachment.htm 

From atsaloli.tech at gmail.com  Sun Feb 13 21:20:06 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sun, 13 Feb 2011 21:20:06 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <437671.84302.qm@web121810.mail.ne1.yahoo.com>
References: <AANLkTin_F_hoDzHv8YZFC8uMMT2Spwg+1tzrVg9fGoC_@mail.gmail.com>
	<437671.84302.qm@web121810.mail.ne1.yahoo.com>
Message-ID: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>

On Sun, Feb 13, 2011 at 8:08 PM, Melvin Davidson <melvin6925 at yahoo.com>wrote:

> Aleksey,
>
> I can't really say how much disk space PostgreSQL 8.4 will use when
> processing the 45GB table. However, I did go back and look at your initial
> report and disk configuration.
>
> # df -h
> Filesystem            Size  Used Avail Use% Mounted on
> /dev/sda3             117G  110G  1.3G  99% /
> /dev/sda1              99M   24M   71M  26% /boot
> tmpfs                 3.9G     0  3.9G   0% /dev/shm
>
> You indicated your master has a 58GB database. Right at the start it means
> you will be using about half the available space. Add to that WAL files,
> system files, PostgreSQL binaries, etc, and it's easy to see that you will
> be using a large percentage of the only disk available.
>
> The best I can recommend would be to try increasing checkpoint_segments (
> try 20) temporarily. However, you might also look into just adding a larger
> disk (250GB?) or at least replacing the 117GB disk with a larger one.
>

Dear Melvin,

  Point taken.  Thanks for your time to look at my issue.

  This is how my master's disk free looks:

Filesystem            Size  Used Avail Use% Mounted on
/dev/sda3             124G   87G   32G  74% /

  So does the database get bigger as it is transfered by Slony? In other
words, does the replication have a disk utilization overhead?   If so, how
large should the slave's disk be compared to the master's disk?

  I had already increased checkpoint_segments from the default 3 to 24.  I
tried doubling it to 48.  Now apparently my replication is back up.
(Bucardo check_postgres script runs OK, and I am seeing updates come
across.)

  However my database size on the slave is 100 GB, and on the master is 58
GB.  What happened? How do I get it back down?

/var/lib/pgsql/data/base/25219965 on the Slave contains 48 files, each 1.1
GB in size,
and an 8.0KB PGVERSION file.  but "du -sh 25219965" says 101GB.

I'm definitely going to upgrade the disk subsystem on both servers as soon
as possible (I've already requested the funding).  Thanks for the hint re
checkpoint_segments.

I don't understand how I went so quickly from the COPY failing to
replication being up to date.

I will have to check tomorrow to make sure everything is there (on the
slave) that should be there.

Thanks!!  I really appreciate the help.
Aleksey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110213/abf552e5/attachment.htm 

From melvin6925 at yahoo.com  Mon Feb 14 11:39:32 2011
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon, 14 Feb 2011 11:39:32 -0800 (PST)
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
	sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>
Message-ID: <462413.8235.qm@web121808.mail.ne1.yahoo.com>

Aleksey,

To answer your questions:

1. So does the database get bigger as it is transfered by Slony?

It doesn't get bigger, but PostgreSQL will write WAL files for all the inserts.
I believe it is the WAL files (16MB each) that is starting to eat up you disk space.

2. how large should the slave's disk be compared to the master's disk?

At least as big as the master. If you intend to use the slave as a failover, it is essential
that the hardware and O/S be as identical as possible.

3. However my database size on the slave is 100 GB, and on the master is 58 GB.

How are you determining this? Are you using df -h?
Or are you doing:

SELECT datname,
?????? rolname as owner,
?????? pg_size_pretty(pg_database_size(datname))as size_pretty,
?????? pg_database_size(datname) as size,
?????? (SELECT pg_size_pretty (SUM( pg_database_size(datname))::bigint)
????????? FROM pg_database)? AS total,
?????? ((pg_database_size(datname) / (SELECT SUM( pg_database_size(datname))
?????????????????????????????????????? FROM pg_database) ) * 100)::numeric(6,3) AS pct
? FROM pg_database d
? JOIN pg_authid a ON a.oid = datdba
ORDER BY datname;

Also, make sure autovacuum is enabled on the slave?

Melvin Davidson 
  
www.folkalley.com





 
____________________________________________________________________________________
It's here! Your new message!  
Get new email alerts with the free Yahoo! Toolbar.
http://tools.search.yahoo.com/toolbar/features/mail/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110214/027b10ef/attachment.htm 

From atsaloli.tech at gmail.com  Mon Feb 14 14:07:05 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Mon, 14 Feb 2011 14:07:05 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <462413.8235.qm@web121808.mail.ne1.yahoo.com>
References: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>
	<462413.8235.qm@web121808.mail.ne1.yahoo.com>
Message-ID: <AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>

Dear Melvin,

  Thanks for answering my questions.

  My slave has same hardware configuration as the master.

  I ran the SQL command you sent, and it reports database size
100 GB on the slave.   Same SQL command reports 58 GB on
the master.

  I tried VACUUM FULL on the database, but the size remains
100 GB.

  The following command tells me that 97 GB is used by my large
table and its TOAST and index:

   SELECT relname as "Table", pg_size_pretty(pg_total_relation_size(relid))
       As "Size" from pg_catalog.pg_statio_user_tables
   ORDER BY pg_total_relation_size(relid) DESC;


  My disk on the slave is nearly full (only a couple of GB free) so
when I get a maintenance window, I will try dropping my large table
(which is 45 GB on production) from replication, and then add it back,
maybe it will come back smaller?

  I would like to understand why it's larger on the slave.

Aleksey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110214/4b2c76d9/attachment.htm 

From cbbrowne at ca.afilias.info  Mon Feb 14 14:27:52 2011
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon, 14 Feb 2011 17:27:52 -0500
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
	sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>
	(Aleksey Tsalolikhin's message of "Mon, 14 Feb 2011 14:07:05 -0800")
References: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>
	<462413.8235.qm@web121808.mail.ne1.yahoo.com>
	<AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>
Message-ID: <87mxly6yl3.fsf@cbbrowne.afilias-int.info>

Aleksey Tsalolikhin <atsaloli.tech at gmail.com> writes:
> Dear Melvin,
>
> ? Thanks for answering my questions.? 
>
> ? My slave has same hardware configuration as the master.
>
> ? I ran the SQL command you sent, and it reports database size 
> 100 GB on the slave.?? Same SQL command reports 58 GB on 
> the master.
>
> ? I tried VACUUM FULL on the database, but the size remains
> 100 GB.
>
> ? The following command tells me that 97 GB is used by my large
> table and its TOAST and index:
>
> ?? SELECT relname as "Table", pg_size_pretty(pg_total_relation_size
> (relid)) 
> ?????? As "Size" from pg_catalog.pg_statio_user_tables 
> ?? ORDER BY pg_total_relation_size(relid) DESC;
>
>
> ? My disk on the slave is nearly full (only a couple of GB free) so
> when I get a maintenance window, I will try dropping my large table 
> (which is 45 GB on production) from replication, and then add it back,
> maybe it will come back smaller?? 
>
> ? I would like to understand why it's larger on the slave.

If anything, I'd expect it to be smaller on the replica, because the
replica won't get any of the
   BEGIN; UPDATE; oops, something conflicted...  ROLLBACK;
traffic that would be expected to generate dead space on the "master"
node.

What I expect is that there was some other problem that caused the
attempt to populate the big table to fail on the replica, so it has
filled up with dead tuples.

I'd suggest that you run TRUNCATE against that table, on the replica,
which should not have any bad side, because that's exactly what Slony is
going to do as part of the subscription process.

If you clear it out yourself, via TRUNCATE, you can verify that things
are in good order, and have greater confidence that the subscription
process will succeed.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From JanWieck at Yahoo.com  Mon Feb 14 15:07:03 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon, 14 Feb 2011 18:07:03 -0500
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <462413.8235.qm@web121808.mail.ne1.yahoo.com>
References: <462413.8235.qm@web121808.mail.ne1.yahoo.com>
Message-ID: <4D59B597.1000508@Yahoo.com>

On 2/14/2011 2:39 PM, Melvin Davidson wrote:
> Aleksey,
>
> To answer your questions:
>
> 1. So does the database get bigger as it is transfered by Slony?
>
> It doesn't get bigger, but PostgreSQL will write WAL files for all the
> inserts.
> I believe it is the WAL files (16MB each) that is starting to eat up you
> disk space.

I find this unlikely. Slony does nothing but COPY the table content from 
the data provider to the new subscriber. That should not increase the 
size of the pg_xlog directory beyond what is allowed by 
checkpoint_segments and checkpoing_timeout anyway. A COPY that fills 
those too fast should only increase the checkpoint frequency, eventually 
to the point where Postgres is reporting too frequent checkpoints.

>
> 2. how large should the slave's disk be compared to the master's disk?
>
> At least as big as the master. If you intend to use the slave as a
> failover, it is essential
> that the hardware and O/S be as identical as possible.

Only as powerful. Slony was specifically designed to allow cross 
platform and even cross Postgres version replication.


>
> 3. However my database size on the slave is 100 GB, and on the master is
> 58 GB.
>
> How are you determining this? Are you using df -h?
> Or are you doing:
>
> SELECT datname,
> rolname as owner,
> pg_size_pretty(pg_database_size(datname))as size_pretty,
> pg_database_size(datname) as size,
> (SELECT pg_size_pretty (SUM( pg_database_size(datname))::bigint)
> FROM pg_database) AS total,
> ((pg_database_size(datname) / (SELECT SUM( pg_database_size(datname))
> FROM pg_database) ) * 100)::numeric(6,3) AS pct
> FROM pg_database d
> JOIN pg_authid a ON a.oid = datdba
> ORDER BY datname;
>
> Also, make sure autovacuum is enabled on the slave?

It would be interesting to see the part of the subscribers log where it 
starts preparing to copy. Especially if it succeeds in truncating the 
target table or not. A once failed attempt to replicate (for whatever 
reason) could well trigger this odd behavior.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From atsaloli.tech at gmail.com  Mon Feb 14 16:12:04 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Mon, 14 Feb 2011 16:12:04 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <87mxly6yl3.fsf@cbbrowne.afilias-int.info>
References: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>
	<462413.8235.qm@web121808.mail.ne1.yahoo.com>
	<AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>
	<87mxly6yl3.fsf@cbbrowne.afilias-int.info>
Message-ID: <AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>

On Mon, Feb 14, 2011 at 2:27 PM, Christopher Browne
<cbbrowne at ca.afilias.info> wrote:
> Aleksey Tsalolikhin <atsaloli.tech at gmail.com> writes:
>> Dear Melvin,
>>
>> ? Thanks for answering my questions.
>>
>> ? My slave has same hardware configuration as the master.
>>
>> ? I ran the SQL command you sent, and it reports database size
>> 100 GB on the slave.?? Same SQL command reports 58 GB on
>> the master.
>>
>> ? I tried VACUUM FULL on the database, but the size remains
>> 100 GB.
>>
>> ? The following command tells me that 97 GB is used by my large
>> table and its TOAST and index:
>>
>> ?? SELECT relname as "Table", pg_size_pretty(pg_total_relation_size
>> (relid))
>> ?????? As "Size" from pg_catalog.pg_statio_user_tables
>> ?? ORDER BY pg_total_relation_size(relid) DESC;
>>
>>
>> ? My disk on the slave is nearly full (only a couple of GB free) so
>> when I get a maintenance window, I will try dropping my large table
>> (which is 45 GB on production) from replication, and then add it back,
>> maybe it will come back smaller?
>>
>> ? I would like to understand why it's larger on the slave.
>
> If anything, I'd expect it to be smaller on the replica, because the
> replica won't get any of the
> ? BEGIN; UPDATE; oops, something conflicted... ?ROLLBACK;
> traffic that would be expected to generate dead space on the "master"
> node.
>
> What I expect is that there was some other problem that caused the
> attempt to populate the big table to fail on the replica, so it has
> filled up with dead tuples.
>
> I'd suggest that you run TRUNCATE against that table, on the replica,
> which should not have any bad side, because that's exactly what Slony is
> going to do as part of the subscription process.
>
> If you clear it out yourself, via TRUNCATE, you can verify that things
> are in good order, and have greater confidence that the subscription
> process will succeed.


Hi.  I dropped my large table from the replication set and then ran TRUNCATE
against that table on the replica.  Disk usage according to "df" dropped from
98% to 96%.  I ran VACUUM FULL on that table but that did not help.
I then shut down PostgreSQL and then started it again, and usage dropped
to 6%.

Then I set up a 2nd replication set consisting of just that one table (since one
can't add a table to an existing replication set) and started
replication.  Let's
see how this goes!

Jan:  I did get notices

   NOTICE:  truncate of "public"."my_big_table" succeeded

on prior replication attempts.

Thanks very much!
Aleksey

From melvin6925 at yahoo.com  Mon Feb 14 16:28:48 2011
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon, 14 Feb 2011 16:28:48 -0800 (PST)
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
	sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
Message-ID: <528880.19050.qm@web121809.mail.ne1.yahoo.com>

>(since one can't add a table to an existing replication set)

That is not true at all.

The following slony commands allow you to add a table to an existing set;

slonik <<_EOF_

CLUSTER NAME = $YOUR_CLUSTER;

node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST port=$PGPORT user=$REPLICATIONUSER';
node 101 admin conninfo = 'dbname=$MASTERDBNAME host=$SLAVEHOST1 port=$PGPORT user=$REPLICATIONUSER';

#CREATE NEW TEMP SET
CREATE SET ( ID=999, ORIGIN = 1, COMMENT = 'Temporary new set' );

#Add the table(s) and/or sequence(s) to the new temp set

SET ADD TABLE (SET ID=999, ORIGIN=1, ID=107, FULLY QUALIFIED NAME='you_schema.you_table', comment='your_table comment') ;
SET ADD TABLE (SET ID=999, ORIGIN=1, ID=110, FULLY QUALIFIED NAME='pts_meta.master_project', comment='master_project') ;


#Subscribe the slave(s) to the new set
SUBSCRIBE SET ( ID = 999, PROVIDER = 1, RECEIVER = 101, FORWARD = NO );

#Need to wait for 101 to confirm before continuing
WAIT FOR EVENT (ORIGIN=1, CONFIRMED=101);

MERGE SET( ID=1, ADD ID=999, ORIGIN=1 );

_EOF_

Melvin Davidson 
  


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110214/7ece2420/attachment.htm 

From atsaloli.tech at gmail.com  Mon Feb 14 16:40:53 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Mon, 14 Feb 2011 16:40:53 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <528880.19050.qm@web121809.mail.ne1.yahoo.com>
References: <AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
	<528880.19050.qm@web121809.mail.ne1.yahoo.com>
Message-ID: <AANLkTinMQr+1HmKjWwst1do2cismOjF2U9xpVbuJUqhH@mail.gmail.com>

On Mon, Feb 14, 2011 at 4:28 PM, Melvin Davidson <melvin6925 at yahoo.com>wrote:

> >(since one can't add a table to an existing replication set)
>
> That is not true at all.
>
>
Truth is relative.  =)  I am doing what you describe, creating a temp set,
and then
merging it into the existing set.

Thank you!

Aleksey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110214/67a53688/attachment.htm 

From JanWieck at Yahoo.com  Mon Feb 14 18:54:40 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon, 14 Feb 2011 21:54:40 -0500
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
References: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>	<462413.8235.qm@web121808.mail.ne1.yahoo.com>	<AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>	<87mxly6yl3.fsf@cbbrowne.afilias-int.info>
	<AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
Message-ID: <4D59EAF0.9010703@Yahoo.com>

On 2/14/2011 7:12 PM, Aleksey Tsalolikhin wrote:
> I then shut down PostgreSQL and then started it again, and usage dropped
> to 6%.

Since a successful TRUNCATE does actually truncate(2) the heap and 
reinitializes all related files (indexes, toast heap and index), the 
disk space could not have been held hostage by anything related to that 
table itself.

Is it possible that there was some stale connection that held temp 
files? PostgreSQL keeps temp files like sort sets in 
$PGDATA/base/pgsql_tmp and temp tables inside the database directory. 
They do normally go away when they are no longer needed. Sort sets when 
the statement ends, temp tables when the session ends. Since a 
postmaster restart did free all that space, I suspect a temp table.

You mentioned using some Bucardo tool to verify things. Does that tool 
eventually create temp tables and could it be, that it left a stale 
connection behind keeping those temp tables around?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cbbrowne at ca.afilias.info  Tue Feb 15 07:59:26 2011
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue, 15 Feb 2011 10:59:26 -0500
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
	sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
	(Aleksey Tsalolikhin's message of "Mon, 14 Feb 2011 16:12:04 -0800")
References: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>
	<462413.8235.qm@web121808.mail.ne1.yahoo.com>
	<AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>
	<87mxly6yl3.fsf@cbbrowne.afilias-int.info>
	<AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
Message-ID: <87ipwl70gx.fsf@cbbrowne.afilias-int.info>

Aleksey Tsalolikhin <atsaloli.tech at gmail.com> writes:
> Hi.  I dropped my large table from the replication set and then ran TRUNCATE
> against that table on the replica.  Disk usage according to "df" dropped from
> 98% to 96%.  I ran VACUUM FULL on that table but that did not help.
> I then shut down PostgreSQL and then started it again, and usage dropped
> to 6%.

Hah.  Almost certainly some file got unlinked, but some backend process
remained holding onto the file descriptor, so the space did not get
returned to the filesystem until that backend process terminated.

It's pretty likely that you could have done something less dramatic than
restarting PostgreSQL, but the restart was certainly an easy way to
flush out any such hanging FDs.

I'd anticipate that you'll find things work fine, now.

There's a mystery there, still, in terms of "what was that lingering
file descriptor???", but it's entirely possible that nothing similar
will ever recur.
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From technimadhu at gmail.com  Tue Feb 15 10:32:35 2011
From: technimadhu at gmail.com (Tech Madhu)
Date: Tue, 15 Feb 2011 13:32:35 -0500
Subject: [Slony1-general] switchover slony question
Message-ID: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>

hi all,

Iam new to slony with postgres. Using Slony 2.x with postgres 8.4

I used this link: http://www.slony.info/documentation/failover.html

In my setup, there is only one master (node:1) and one slave (node:2)

to do some switchover testing.. The controlled switchover works fine.

The test i wanted to do was 'what if master server goes down - like a
restart'. I want the 'slave' to become master and accept write requests. and
when the original master server comes back up, i want it to become the
'slave'.

I tried this
        1) on Master, did shutdown -r
        2) login to slave and run
            lock set (id = 1, origin = 1);
            move set (id = 1, old origin = 1, new origin = 2);

The above does not work, as it is trying to talk with original master.

Should i run the 'failover' command?  failover (id = 1, backup node = 2); there
are some big warnings against doing this in the slony link above

If so, how do i make the old master when it restarts, run as slave.

Thanks in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110215/d11fa4b2/attachment.htm 

From msteben at autorevenue.com  Tue Feb 15 11:36:54 2011
From: msteben at autorevenue.com (Mark Steben)
Date: Tue, 15 Feb 2011 14:36:54 -0500
Subject: [Slony1-general] change the listening port on master node
Message-ID: <20110215193708.B819A2900C1@main.slony.info>

 

Hi,

 

I would like to change the port on node1 from 5432 to 5433.  Do I need to
'start from scratch'

And recreate the node and the entire set or is there an 'update' function I
can use?

 

I'm running postgres 8.3.11 and slony1 2.0.5

 

 

 

I'm running all my administration scripts from the slonik library. 

./slonik_update_nodes and ./slonik_restart_nodes don't work.  Think they
update the internal functions.

 

 Here is a little bit of my slon_tools.conf file:

 

&add_node(node=>1, host => '192.168.1.222', dbname => 'mavmail', port
=>5433, user=>'slony', password=>'elephant1234');

&add_node(node=>2, host =>  '192.168.1.221', dbname => 'mavmail', port
=>5432, user=>'slony', password=>'elephant1234');

 

Obviously would like to change the port on server 192.168.1.222 to 5433
without dropping and redefining everything.

 

Any help appreciated.  Thanks.

 

 

 

 

 

 

 

 

Mark Steben
Database Administrator
@utoRevenue  |  Autobase  |  AVV
The CRM division of Dominion Dealer Solutions
95D Ashley Avenue
West Springfield,  MA  01089
t: 413.327.3045
f: 413.732.1824
w: www.autorevenue.com <http://www.autorevenue.com/> 



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110215/404f9c63/attachment.htm 

From JanWieck at Yahoo.com  Tue Feb 15 11:44:56 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 15 Feb 2011 14:44:56 -0500
Subject: [Slony1-general] switchover slony question
In-Reply-To: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>
References: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>
Message-ID: <4D5AD7B8.5070300@Yahoo.com>

On 2/15/2011 1:32 PM, Tech Madhu wrote:
> hi all,
>
> Iam new to slony with postgres. Using Slony 2.x with postgres 8.4
>
> I used this link: http://www.slony.info/documentation/failover.html
>
> In my setup, there is only one master (node:1) and one slave (node:2)
>
> to do some switchover testing.. The controlled switchover works fine.
>
> The test i wanted to do was 'what if master server goes down - like a
> restart'. I want the 'slave' to become master and accept write requests.
> and when the original master server comes back up, i want it to become
> the 'slave'.

This is NOT possible given the Slony-I design.

Slony-I is an asynchronous replication system. That means that changes 
to the origin are replicated some time AFTER they have been committed. 
That means that if the origin goes down unexpectedly, you have no chance 
of knowing what changes did not propagate to the replica before it crashed.

The only way to solve this situation is to actually do a hard FAILOVER, 
abandoning the old origin and rebuilding it from scratch.

To illustrate, think about a simple foreign key constraint, t2.fk 
references t1.pk. There currently are no rows in t2 referencing a 
certain t1.pk, so node:1 will allow to DELETE it. Node:1 crashes before 
the DELETE can propagate to node:2. You failover to node:2 and since it 
still has the t1 row, it will happily allow you to INSERT references to 
it into t1. Now you bring back node:1 and ... how exactly do you get the 
two to agree what is right? Will you forcefully remove the rows, node:2 
inserted into t2 or will you recreate the t1 row in node:1 so that the 
INSERT's can propagate from node:2 to node:1?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Tue Feb 15 11:52:15 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 15 Feb 2011 14:52:15 -0500
Subject: [Slony1-general] switchover slony question
In-Reply-To: <4D5AD7B8.5070300@Yahoo.com>
References: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>
	<4D5AD7B8.5070300@Yahoo.com>
Message-ID: <4D5AD96F.3010008@Yahoo.com>

On 2/15/2011 2:44 PM, Jan Wieck wrote:
> This is NOT possible given the Slony-I design.
>
> Slony-I is an asynchronous replication system. That means that changes
> to the origin are replicated some time AFTER they have been committed.
> That means that if the origin goes down unexpectedly, you have no chance
> of knowing what changes did not propagate to the replica before it crashed.
>
> The only way to solve this situation is to actually do a hard FAILOVER,
> abandoning the old origin and rebuilding it from scratch.
>
> To illustrate, think about a simple foreign key constraint, t2.fk
> references t1.pk. There currently are no rows in t2 referencing a
> certain t1.pk, so node:1 will allow to DELETE it. Node:1 crashes before
> the DELETE can propagate to node:2. You failover to node:2 and since it
> still has the t1 row, it will happily allow you to INSERT references to
> it into t1. Now you bring back node:1 and ... how exactly do you get the

into t2, of course.

> two to agree what is right? Will you forcefully remove the rows, node:2
> inserted into t2 or will you recreate the t1 row in node:1 so that the
> INSERT's can propagate from node:2 to node:1?
>
>
> Jan
>


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cbbrowne at ca.afilias.info  Tue Feb 15 12:07:16 2011
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue, 15 Feb 2011 15:07:16 -0500
Subject: [Slony1-general] change the listening port on master node
In-Reply-To: <20110215193708.B819A2900C1@main.slony.info> (Mark Steben's
	message of "Tue, 15 Feb 2011 14:36:54 -0500")
References: <20110215193708.B819A2900C1@main.slony.info>
Message-ID: <8739np6ozv.fsf@cbbrowne.afilias-int.info>

"Mark Steben" <msteben at autorevenue.com> writes:
> I would like to change the port on node1 from 5432 to 5433.? Do I need
> to ?start from scratch?
>
> And recreate the node and the entire set or is there an ?update?
> function I can use?

There is no need to recreate nodes or such.

You should only need to do two sets of changes:

 a) Reconfigure the slons to connect to port 5433.

    This is controlled where ever you store the configuration used to
    start up slon processes.

 b) Change all STORE PATH configuration to connect to port 5433.

    This will involve running a slonik script consisting of a series of
    STORE PATH commands, one for each path you have in your cluster.

    See table "_MySlonySchema".sl_path on any of your nodes for a census
    of all the STORE PATH requests that need to be submitted.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From msteben at autorevenue.com  Tue Feb 15 13:46:43 2011
From: msteben at autorevenue.com (Mark Steben)
Date: Tue, 15 Feb 2011 16:46:43 -0500
Subject: [Slony1-general] change the listening port on master node
In-Reply-To: <8739np6ozv.fsf@cbbrowne.afilias-int.info>
Message-ID: <20110215214701.5768C290244@main.slony.info>

Hi Chris

I'm familiar with the perl slonik scripts (slonik_init_cluster,
slonik_create_set, etc) but not the native action commands like STORE PATH.
Reading the doc I know I need to preface any action command like 
  STORE PATH with a preamble but am getting syntax errors when I try to
  Run it.  This is what I've coded (dataset called store_path):
  These are existing nodes,dbs, users, etc.
  
#!/bin/sh
cluster name = replication;
 node 1 admin conninfo='host=192.168.1.222 dbname=mavmail user=slony
port=5432 password=elephant1234';
 node 2 admin conninfo='host=192.168.1.221 dbname=mavmail user=slony
port=5432 password=elephant1234';
store path ( server = 1, client = 2, conninfo = 'host=192.168.1.222
dbname=mavmail user=slony port=5433 password=elephant1234' );
 

and I get the following syntax errors:


./store_path | ./bin/slonik
./store_path: line 4: cluster: command not found
./store_path: line 5: node: command not found
./store_path: line 6: node: command not found
./store_path: line 9: syntax error near unexpected token `('
./store_path: line 9: `store path ( server = 1, client = 2, conninfo =
'host=192.168.1.222 dbname=mavmail user=slony port=5433
password=elephant1234' );'

Something very obvious I'm sure - just not familiar enough with the slony
Language.  Any help appreciated.  Thank you

Mark Steben
Database Administrator
@utoRevenue  |  Autobase  |  AVV
The CRM division of Dominion Dealer Solutions
95D Ashley Avenue
West Springfield,  MA  01089
t: 413.327.3045
f: 413.732.1824
w: www.autorevenue.com


-----Original Message-----
From: Christopher Browne [mailto:cbbrowne at ca.afilias.info] 
Sent: Tuesday, February 15, 2011 3:07 PM
To: Mark Steben
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] change the listening port on master node

"Mark Steben" <msteben at autorevenue.com> writes:
> I would like to change the port on node1 from 5432 to 5433.? Do I need
> to ?start from scratch?
>
> And recreate the node and the entire set or is there an ?update?
> function I can use?

There is no need to recreate nodes or such.

You should only need to do two sets of changes:

 a) Reconfigure the slons to connect to port 5433.

    This is controlled where ever you store the configuration used to
    start up slon processes.

 b) Change all STORE PATH configuration to connect to port 5433.

    This will involve running a slonik script consisting of a series of
    STORE PATH commands, one for each path you have in your cluster.

    See table "_MySlonySchema".sl_path on any of your nodes for a census
    of all the STORE PATH requests that need to be submitted.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@"
[name;tld];;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"




From technimadhu at gmail.com  Tue Feb 15 18:52:03 2011
From: technimadhu at gmail.com (Tech Madhu)
Date: Tue, 15 Feb 2011 21:52:03 -0500
Subject: [Slony1-general] switchover slony question
In-Reply-To: <4D5AD96F.3010008@Yahoo.com>
References: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>
	<4D5AD7B8.5070300@Yahoo.com> <4D5AD96F.3010008@Yahoo.com>
Message-ID: <AANLkTimpS_j5s6nYnt=qz4_YTk5ojW+oYBafDw=6rDpO@mail.gmail.com>

Thanks for your reply.

if the orig master is down say due to some hardware issue (for few
hours/days say), we have to get the system on the slave up (we accept the
loss of some N txn)

In this case, Is the following pseudcode correct. Assuming before crash of
master, my master was node:1 and slave node:2
   On the slave (node 2),
          a) i run failover command (failover (id=1, backup node = 2)
          b) Run drop node command of node (1)
   When the orig master is ready to be brought back in service can i re-use
the node (1) for it?
   if so , is it enough to run just the following 2 commands on the original
master
       store node (id = 1, event node = 2);
       store path (server=2, client=1, conninfo='connection info to node2')

On Tue, Feb 15, 2011 at 2:52 PM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 2/15/2011 2:44 PM, Jan Wieck wrote:
>
>> This is NOT possible given the Slony-I design.
>>
>> Slony-I is an asynchronous replication system. That means that changes
>> to the origin are replicated some time AFTER they have been committed.
>> That means that if the origin goes down unexpectedly, you have no chance
>> of knowing what changes did not propagate to the replica before it
>> crashed.
>>
>> The only way to solve this situation is to actually do a hard FAILOVER,
>> abandoning the old origin and rebuilding it from scratch.
>>
>> To illustrate, think about a simple foreign key constraint, t2.fk
>> references t1.pk. There currently are no rows in t2 referencing a
>> certain t1.pk, so node:1 will allow to DELETE it. Node:1 crashes before
>> the DELETE can propagate to node:2. You failover to node:2 and since it
>> still has the t1 row, it will happily allow you to INSERT references to
>> it into t1. Now you bring back node:1 and ... how exactly do you get the
>>
>
> into t2, of course.
>
>
>  two to agree what is right? Will you forcefully remove the rows, node:2
>> inserted into t2 or will you recreate the t1 row in node:1 so that the
>> INSERT's can propagate from node:2 to node:1?
>>
>>
>> Jan
>>
>>
>
> --
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110215/6412cf50/attachment.htm 

From msteben at autorevenue.com  Wed Feb 16 07:44:09 2011
From: msteben at autorevenue.com (Mark Steben)
Date: Wed, 16 Feb 2011 10:44:09 -0500
Subject: [Slony1-general] change the listening port on master node
Message-ID: <20110216154417.43473290BD8@main.slony.info>

Got this to work - just had to put fully qualified slonik program
As the first line in the shell script, then preamble, then STORE PATH
Command, etc, etc, etc

Mark Steben
Database Administrator
@utoRevenue  |  Autobase  |  AVV
The CRM division of Dominion Dealer Solutions
95D Ashley Avenue
West Springfield,  MA  01089
t: 413.327.3045
f: 413.732.1824
w: www.autorevenue.com



-----Original Message-----
From: Mark Steben [mailto:msteben at autorevenue.com] 
Sent: Tuesday, February 15, 2011 4:47 PM
To: 'Christopher Browne'
Cc: 'slony1-general at lists.slony.info'
Subject: RE: [Slony1-general] change the listening port on master node

Hi Chris

I'm familiar with the perl slonik scripts (slonik_init_cluster,
slonik_create_set, etc) but not the native action commands like STORE PATH.
Reading the doc I know I need to preface any action command like 
  STORE PATH with a preamble but am getting syntax errors when I try to
  Run it.  This is what I've coded (dataset called store_path):
  These are existing nodes,dbs, users, etc.
  
#!/bin/sh
cluster name = replication;
 node 1 admin conninfo='host=192.168.1.222 dbname=mavmail user=slony
port=5432 password=elephant1234';
 node 2 admin conninfo='host=192.168.1.221 dbname=mavmail user=slony
port=5432 password=elephant1234';
store path ( server = 1, client = 2, conninfo = 'host=192.168.1.222
dbname=mavmail user=slony port=5433 password=elephant1234' );
 

and I get the following syntax errors:


./store_path | ./bin/slonik
./store_path: line 4: cluster: command not found
./store_path: line 5: node: command not found
./store_path: line 6: node: command not found
./store_path: line 9: syntax error near unexpected token `('
./store_path: line 9: `store path ( server = 1, client = 2, conninfo =
'host=192.168.1.222 dbname=mavmail user=slony port=5433
password=elephant1234' );'

Something very obvious I'm sure - just not familiar enough with the slony
Language.  Any help appreciated.  Thank you

Mark Steben
Database Administrator
@utoRevenue  |  Autobase  |  AVV
The CRM division of Dominion Dealer Solutions
95D Ashley Avenue
West Springfield,  MA  01089
t: 413.327.3045
f: 413.732.1824
w: www.autorevenue.com


-----Original Message-----
From: Christopher Browne [mailto:cbbrowne at ca.afilias.info] 
Sent: Tuesday, February 15, 2011 3:07 PM
To: Mark Steben
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] change the listening port on master node

"Mark Steben" <msteben at autorevenue.com> writes:
> I would like to change the port on node1 from 5432 to 5433.? Do I need
> to ?start from scratch?
>
> And recreate the node and the entire set or is there an ?update?
> function I can use?

There is no need to recreate nodes or such.

You should only need to do two sets of changes:

 a) Reconfigure the slons to connect to port 5433.

    This is controlled where ever you store the configuration used to
    start up slon processes.

 b) Change all STORE PATH configuration to connect to port 5433.

    This will involve running a slonik script consisting of a series of
    STORE PATH commands, one for each path you have in your cluster.

    See table "_MySlonySchema".sl_path on any of your nodes for a census
    of all the STORE PATH requests that need to be submitted.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@"
[name;tld];;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"




From technimadhu at gmail.com  Thu Feb 17 07:03:00 2011
From: technimadhu at gmail.com (Tech Madhu)
Date: Thu, 17 Feb 2011 10:03:00 -0500
Subject: [Slony1-general] switchover slony question
In-Reply-To: <AANLkTimpS_j5s6nYnt=qz4_YTk5ojW+oYBafDw=6rDpO@mail.gmail.com>
References: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>
	<4D5AD7B8.5070300@Yahoo.com> <4D5AD96F.3010008@Yahoo.com>
	<AANLkTimpS_j5s6nYnt=qz4_YTk5ojW+oYBafDw=6rDpO@mail.gmail.com>
Message-ID: <AANLkTi=_9Ov0dFfT01A-O_Bjc-92G_CPt45pU9Nno-UJ@mail.gmail.com>

all,
i got the failover to work.. listing the steps which might be useful for
other beginners like me

node1 = master, node2=slave

node1 goes down (crash or power failure). while node1 is being recovered, if
your apps have to continue writing to DB, only way i found is that we have
to do the failover

on node 2, run the following slonik commands
     1) failover (id = 1, backup node = 2); (this works only if you
subscribed your set originally with forward=yes)
     2) ensure DB can be written into (not readonly anymore)
     3) drop node (id = 1, event node = 2);
on node 1, run the following commands once it comes back in service
      1) dropdb <dbname>
       2) createdb -O <dbuser> <dbname>
       3) psql -U user -d DB < backup.sql
      //Note : this backup.sql should not have slony tables, so its backup
taken before slony was setup using pg_dump -c -s -U <user> <db> > backup.sql
        4) Run slonik commands to store node (1) and store path
              cluster name = clus_name;
              node 1 admin conninfo = 'dbname=yourdb host=yournode1host
user=dbuser password=pass';
              node 2 admin conninfo = 'dbname=yourdb
host=yournode2host user=dbuser
password=pass';
              store node (id = 1, comment = 'Node 1 slave', event node=2);
              store path (server = 1, client = 2,
                  conninfo = 'dbname=yourdb host=node1host user=repusr
password=pass');
              store path (server = 2, client = 1,
                 conninfo = 'dbname=yourdb host=node2host user=dbuser
password=pass');
         5)Start slony daemon on node 1
         6) subscribe to your sets
             subscribe set (id = 1, provider = 2, receiver = 1, forward =
yes);
             Note above, i am showing the provider is node 2 and receiver is
node 1 (opposite of my initial subscribe)
        7) your replication should be working..


On Tue, Feb 15, 2011 at 9:52 PM, Tech Madhu <technimadhu at gmail.com> wrote:

> Thanks for your reply.
>
> if the orig master is down say due to some hardware issue (for few
> hours/days say), we have to get the system on the slave up (we accept the
> loss of some N txn)
>
> In this case, Is the following pseudcode correct. Assuming before crash of
> master, my master was node:1 and slave node:2
>    On the slave (node 2),
>           a) i run failover command (failover (id=1, backup node = 2)
>           b) Run drop node command of node (1)
>    When the orig master is ready to be brought back in service can i re-use
> the node (1) for it?
>    if so , is it enough to run just the following 2 commands on the
> original master
>        store node (id = 1, event node = 2);
>        store path (server=2, client=1, conninfo='connection info to node2')
>
>
> On Tue, Feb 15, 2011 at 2:52 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
>
>> On 2/15/2011 2:44 PM, Jan Wieck wrote:
>>
>>> This is NOT possible given the Slony-I design.
>>>
>>> Slony-I is an asynchronous replication system. That means that changes
>>> to the origin are replicated some time AFTER they have been committed.
>>> That means that if the origin goes down unexpectedly, you have no chance
>>> of knowing what changes did not propagate to the replica before it
>>> crashed.
>>>
>>> The only way to solve this situation is to actually do a hard FAILOVER,
>>> abandoning the old origin and rebuilding it from scratch.
>>>
>>> To illustrate, think about a simple foreign key constraint, t2.fk
>>> references t1.pk. There currently are no rows in t2 referencing a
>>> certain t1.pk, so node:1 will allow to DELETE it. Node:1 crashes before
>>> the DELETE can propagate to node:2. You failover to node:2 and since it
>>> still has the t1 row, it will happily allow you to INSERT references to
>>> it into t1. Now you bring back node:1 and ... how exactly do you get the
>>>
>>
>> into t2, of course.
>>
>>
>>  two to agree what is right? Will you forcefully remove the rows, node:2
>>> inserted into t2 or will you recreate the t1 row in node:1 so that the
>>> INSERT's can propagate from node:2 to node:1?
>>>
>>>
>>> Jan
>>>
>>>
>>
>> --
>> Anyone who trades liberty for security deserves neither
>> liberty nor security. -- Benjamin Franklin
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110217/a1aeaf2b/attachment.htm 

From JanWieck at Yahoo.com  Thu Feb 17 07:34:26 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 17 Feb 2011 10:34:26 -0500
Subject: [Slony1-general] switchover slony question
In-Reply-To: <AANLkTi=_9Ov0dFfT01A-O_Bjc-92G_CPt45pU9Nno-UJ@mail.gmail.com>
References: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>	<4D5AD7B8.5070300@Yahoo.com>
	<4D5AD96F.3010008@Yahoo.com>	<AANLkTimpS_j5s6nYnt=qz4_YTk5ojW+oYBafDw=6rDpO@mail.gmail.com>
	<AANLkTi=_9Ov0dFfT01A-O_Bjc-92G_CPt45pU9Nno-UJ@mail.gmail.com>
Message-ID: <4D5D4002.4000204@Yahoo.com>

On 2/17/2011 10:03 AM, Tech Madhu wrote:
> all,
> i got the failover to work.. listing the steps which might be useful for
> other beginners like me
>
> node1 = master, node2=slave
>
> node1 goes down (crash or power failure). while node1 is being
> recovered, if your apps have to continue writing to DB, only way i found
> is that we have to do the failover
>
> on node 2, run the following slonik commands
>       1) failover (id = 1, backup node = 2); (this works only if you
> subscribed your set originally with forward=yes)
>       2) ensure DB can be written into (not readonly anymore)
>       3) drop node (id = 1, event node = 2);
> on node 1, run the following commands once it comes back in service
>        1) dropdb <dbname>
>         2) createdb -O <dbuser> <dbname>
>         3) psql -U user -d DB < backup.sql
>        //Note : this backup.sql should not have slony tables, so its
> backup taken before slony was setup using pg_dump -c -s -U <user> <db> >
> backup.sql
>          4) Run slonik commands to store node (1) and store path
>                cluster name = clus_name;
>                node 1 admin conninfo = 'dbname=yourdb host=yournode1host
> user=dbuser password=pass';
>                node 2 admin conninfo = 'dbname=yourdb host=yournode2host
> user=dbuser  password=pass';
>                store node (id = 1, comment = 'Node 1 slave', event node=2);

Please note that we may make node ids non-reusable in a future Slony 
version. Creating a node with the same id that a formerly dropped node 
had caused some adverse side effects in the past, in case the Slony 
configuration had not gotten rid of any traces for the old node 
everywhere else.


Jan

>                store path (server = 1, client = 2,
>                    conninfo = 'dbname=yourdb host=node1host user=repusr
> password=pass');
>                store path (server = 2, client = 1,
>                   conninfo = 'dbname=yourdb host=node2host user=dbuser
> password=pass');
>           5)Start slony daemon on node 1
>           6) subscribe to your sets
>               subscribe set (id = 1, provider = 2, receiver = 1, forward
> = yes);
>               Note above, i am showing the provider is node 2 and
> receiver is node 1 (opposite of my initial subscribe)
>          7) your replication should be working..
>
>
> On Tue, Feb 15, 2011 at 9:52 PM, Tech Madhu <technimadhu at gmail.com
> <mailto:technimadhu at gmail.com>> wrote:
>
>     Thanks for your reply.
>
>     if the orig master is down say due to some hardware issue (for few
>     hours/days say), we have to get the system on the slave up (we
>     accept the loss of some N txn)
>
>     In this case, Is the following pseudcode correct. Assuming before
>     crash of master, my master was node:1 and slave node:2
>         On the slave (node 2),
>                a) i run failover command (failover (id=1, backup node = 2)
>                b) Run drop node command of node (1)
>         When the orig master is ready to be brought back in service can
>     i re-use the node (1) for it?
>         if so , is it enough to run just the following 2 commands on the
>     original master
>             store node (id = 1, event node = 2);
>             store path (server=2, client=1, conninfo='connection info to
>     node2')
>
>
>     On Tue, Feb 15, 2011 at 2:52 PM, Jan Wieck <JanWieck at yahoo.com
>     <mailto:JanWieck at yahoo.com>> wrote:
>
>         On 2/15/2011 2:44 PM, Jan Wieck wrote:
>
>             This is NOT possible given the Slony-I design.
>
>             Slony-I is an asynchronous replication system. That means
>             that changes
>             to the origin are replicated some time AFTER they have been
>             committed.
>             That means that if the origin goes down unexpectedly, you
>             have no chance
>             of knowing what changes did not propagate to the replica
>             before it crashed.
>
>             The only way to solve this situation is to actually do a
>             hard FAILOVER,
>             abandoning the old origin and rebuilding it from scratch.
>
>             To illustrate, think about a simple foreign key constraint,
>             t2.fk <http://t2.fk>
>             references t1.pk <http://t1.pk>. There currently are no rows
>             in t2 referencing a
>             certain t1.pk <http://t1.pk>, so node:1 will allow to DELETE
>             it. Node:1 crashes before
>             the DELETE can propagate to node:2. You failover to node:2
>             and since it
>             still has the t1 row, it will happily allow you to INSERT
>             references to
>             it into t1. Now you bring back node:1 and ... how exactly do
>             you get the
>
>
>         into t2, of course.
>
>
>             two to agree what is right? Will you forcefully remove the
>             rows, node:2
>             inserted into t2 or will you recreate the t1 row in node:1
>             so that the
>             INSERT's can propagate from node:2 to node:1?
>
>
>             Jan
>
>
>
>         --
>         Anyone who trades liberty for security deserves neither
>         liberty nor security. -- Benjamin Franklin
>
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From rod at iol.ie  Thu Feb 17 07:43:16 2011
From: rod at iol.ie (Raymond O'Donnell)
Date: Thu, 17 Feb 2011 15:43:16 +0000
Subject: [Slony1-general] switchover slony question
In-Reply-To: <AANLkTi=_9Ov0dFfT01A-O_Bjc-92G_CPt45pU9Nno-UJ@mail.gmail.com>
References: <AANLkTi=WYeCgUqeP77YXLQkyyw0=W5a7DisF8JupvS7x@mail.gmail.com>	<4D5AD7B8.5070300@Yahoo.com>
	<4D5AD96F.3010008@Yahoo.com>	<AANLkTimpS_j5s6nYnt=qz4_YTk5ojW+oYBafDw=6rDpO@mail.gmail.com>
	<AANLkTi=_9Ov0dFfT01A-O_Bjc-92G_CPt45pU9Nno-UJ@mail.gmail.com>
Message-ID: <4D5D4214.9050800@iol.ie>

On 17/02/2011 15:03, Tech Madhu wrote:
> all,
> i got the failover to work.. listing the steps which might be useful for
> other beginners like me
>
> node1 = master, node2=slave
>
> node1 goes down (crash or power failure). while node1 is being
> recovered, if your apps have to continue writing to DB, only way i found
> is that we have to do the failover
>
> on node 2, run the following slonik commands
>       1) failover (id = 1, backup node = 2); (this works only if you
> subscribed your set originally with forward=yes)
>       2) ensure DB can be written into (not readonly anymore)
>       3) drop node (id = 1, event node = 2);
> on node 1, run the following commands once it comes back in service
>        1) dropdb <dbname>
>         2) createdb -O <dbuser> <dbname>
>         3) psql -U user -d DB < backup.sql
>        //Note : this backup.sql should not have slony tables, so its
> backup taken before slony was setup using pg_dump -c -s -U <user> <db> >
> backup.sql

It's worth adding here that all you're restoring at this point in the 
procedure is the database schema, not the data. I know it's implicit in 
the pg_dump command above, but I reckon it's worth saying it clearly.

Ray.


-- 
Raymond O'Donnell :: Galway :: Ireland
rod at iol.ie

From atsaloli.tech at gmail.com  Thu Feb 17 15:47:34 2011
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Thu, 17 Feb 2011 15:47:34 -0800
Subject: [Slony1-general] unable to set up replication: slony1-1.2.21:
 sync of large table (45 GB) fails: sequence maxed out
In-Reply-To: <87ipwl70gx.fsf@cbbrowne.afilias-int.info>
References: <AANLkTi=VBrjRo8Y5CWLcEG3TKeEx3tiJ0HBt_HDaE1sh@mail.gmail.com>
	<462413.8235.qm@web121808.mail.ne1.yahoo.com>
	<AANLkTi=mUL5tPDgGrOJnAr7Tm1vDA7tQBsX=FNDdU=Wo@mail.gmail.com>
	<87mxly6yl3.fsf@cbbrowne.afilias-int.info>
	<AANLkTimKNcvp52G0g121vHED5FLc=WOYzZJSeZQAC24_@mail.gmail.com>
	<87ipwl70gx.fsf@cbbrowne.afilias-int.info>
Message-ID: <AANLkTikOBRN0ye_s=s3BmvMVPXfw3bwMAOWnEEXmnVu2@mail.gmail.com>

On Tue, Feb 15, 2011 at 7:59 AM, Christopher Browne
<cbbrowne at ca.afilias.info> wrote:
> Hah. ?Almost certainly some file got unlinked, but some backend process
> remained holding onto the file descriptor, so the space did not get
> returned to the filesystem until that backend process terminated.
>
> It's pretty likely that you could have done something less dramatic than
> restarting PostgreSQL, but the restart was certainly an easy way to
> flush out any such hanging FDs.

Yup!  :)


> I'd anticipate that you'll find things work fine, now.

I expect so, I'll know for sure Saturday, when we're upgrading the master
(finally the database upgrade purchase request got approved so I have
a shiny new server) and then we'll turn replication back on.


> There's a mystery there, still, in terms of "what was that lingering
> file descriptor???", but it's entirely possible that nothing similar
> will ever recur.

I would be entirely satisfied if this problem never came back.

Thanks!!
Aleksey

From alessandro.sironi at loginet.it  Fri Feb 18 08:52:08 2011
From: alessandro.sironi at loginet.it (Alessandro Sironi)
Date: Fri, 18 Feb 2011 17:52:08 +0100
Subject: [Slony1-general] Blocking situation with "MOVE SET" command
Message-ID: <4D5EA3B8.2030700@loginet.it>

Hi, I'm new to Slony and I'm testing it for replicate some table between 
2 Postgres DBs... today I had try the "MOVE SET" command executing this 
script on the slave:

#!/usr/bin/slonik
define CLUSTER clusterone;
define PRIMARY 1;
define SLAVE 10;

cluster name = @CLUSTER;
node @PRIMARY admin conninfo = 'dbname=NameDB host=IPMaster 
user=test-slony';
node @SLAVE admin conninfo = 'dbname=NameDB host=IPSlave user=test-slony';

LOCK SET(ID=1, ORIGIN=@PRIMARY);
MOVE SET(ID=1, OLD ORIGIN=@PRIMARY, NEW ORIGIN=@SLAVE);

...after this I execute the same script (with @PRIMARY and @SLAVE 
inverted)on the PRIMARY node (to get back to the previous situation) but 
I obtain thefollowing error:

./script4.sk:10: PGRES_FATAL_ERROR select "_cname".lockSet(1); select 
"_cname".getMaxXid();  - ERROR:  Slony-I: set 1 does not originate on 
local node

...now @SLAVE and @PRIMARY node result both as subscriber to the set 
with ID 1 and I can't delete/drop the set.
What did I do wrong? There is a way to get all back to normal?


Thanks in advance.
Alex
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110218/87c07a5f/attachment.htm 

From sachin.srivastava at enterprisedb.com  Tue Feb 22 00:47:55 2011
From: sachin.srivastava at enterprisedb.com (Sachin Srivastava)
Date: Tue, 22 Feb 2011 14:17:55 +0530
Subject: [Slony1-general] Slony1-2.0.6 with PostgreSQL 9.0.3 on windows
Message-ID: <17170B89-A5F5-4249-B98B-88F91A73C2FB@enterprisedb.com>

Hello All,

I have built PostgreSQL 9.0.3 on MinGW and then using the same I was building slony1-2.0.6.
The first error i got was 
i) error: storage size of 'act' isn't known
I got this around by changing the ifdef on line 1065 in slon.c from "#ifndef CYGWIN to #ifndef WIN32".

ii) Now I am getting :

gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations  -I../.. -I../../src/slon -o slon.exe slon.o runtime_config.o local_listen.o remote_listen.o remote_worker.o sync_thread.o cleanup_thread.o scheduler.o dbutils.o conf-file.o confoptions.o misc.o ../parsestatements/scanner.o port/pipe.o port/win32service.o win32ver.o  -lpgport -L/c/pgsql-9.0.3/lib -L/c/pgsql-9.0.3/lib -lpq  -lpthread -lwsock32
c:/mingw/bin/../lib/gcc/mingw32/4.5.2/../../../libmingwex.a(getopt.o):getopt.c:(.text+0xb40): multiple definition of `getopt'
c:/pgsql-9.0.3/lib/libpgport.a(getopt.o):getopt.c:(.text+0x0): first defined here
remote_worker.o: In function `copy_set':
c:\slony1-2.0.6\src\slon/remote_worker.c:2553: undefined reference to `slon_terminate_worker'
c:\slony1-2.0.6\src\slon/remote_worker.c:2542: undefined reference to `slon_terminate_worker'
collect2: ld returned 1 exit status
make[2]: *** [slon.exe] Error 1
make[2]: Leaving directory `/c/slony1-2.0.6/src/slon'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/c/slony1-2.0.6/src'
make: *** [all] Error 2


In PostgreSQL 9.0.3 they have included (which was not there in 9.0.2 and thus slony builds fine there.):
> #mingw has adopted a GNU-centric interpretation of optind/optreset,
> # so always use our version on Windows.
> if test "$PORTNAME" = "win32"; then
>   case " $LIBOBJS " in
>   *" getopt.$ac_objext "* ) ;;
>   *) LIBOBJS="$LIBOBJS getopt.$ac_objext"
>  ;;
> esac

Any help to make me go forward with this would be highly appreciated.

Thanks a lot.






--Regards,
Sachin Srivastava
EnterpriseDB, the Enterprise PostgreSQL company.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110222/99c9dc0c/attachment.htm 

From ssinger at afilias.info  Tue Feb 22 12:27:43 2011
From: ssinger at afilias.info (Steve Singer)
Date: Tue, 22 Feb 2011 15:27:43 -0500
Subject: [Slony1-general] Blocking situation with "MOVE SET" command
In-Reply-To: <4D5EA3B8.2030700@loginet.it>
References: <4D5EA3B8.2030700@loginet.it>
Message-ID: <4D641C3F.2030307@ca.afilias.info>

On 11-02-18 11:52 AM, Alessandro Sironi wrote:
> Hi, I'm new to Slony and I'm testing it for replicate some table between
> 2 Postgres DBs... today I had try the "MOVE SET" command executing this
> script on the slave:
>
> #!/usr/bin/slonik
> define CLUSTER clusterone;
> define PRIMARY 1;
> define SLAVE 10;
>
> cluster name = @CLUSTER;
> node @PRIMARY admin conninfo = 'dbname=NameDB host=IPMaster
> user=test-slony';
> node @SLAVE admin conninfo = 'dbname=NameDB host=IPSlave user=test-slony';
>
> LOCK SET(ID=1, ORIGIN=@PRIMARY);
> MOVE SET(ID=1, OLD ORIGIN=@PRIMARY, NEW ORIGIN=@SLAVE);
>
> ...after this I execute the same script (with @PRIMARY and @SLAVE
> inverted)on the PRIMARY node (to get back to the previous situation) but
> I obtain thefollowing error:
>
> ./script4.sk:10: PGRES_FATAL_ERROR select "_cname".lockSet(1); select
> "_cname".getMaxXid(); - ERROR: Slony-I: set 1 does not originate on
> local node

It sounds like the move set didn't get picked up by the other node.

If you do

select * FROM _cname.sl_set;

on both nodes, do they disagree about which node is the origin of the set?

Are your slons running? Reporting any errors?






>
> ...now @SLAVE and @PRIMARY node result both as subscriber to the set
> with ID 1 and I can't delete/drop the set.
> What did I do wrong? There is a way to get all back to normal?
>



>
> Thanks in advance.
> Alex
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From technimadhu at gmail.com  Wed Feb 23 08:19:48 2011
From: technimadhu at gmail.com (Tech Madhu)
Date: Wed, 23 Feb 2011 11:19:48 -0500
Subject: [Slony1-general] controlled switchover
Message-ID: <AANLkTik1TBDfdU0rqwV5c+TtUc0y0972S+65O=Y0zq3V@mail.gmail.com>

hello all,

Iam somewhat new to Slony. Have a question on controlled switchover.

I have 1 master/ 1slave setup . When i want to do a switchover, using lock
set, move set, how do i know the 'slave' has caught up with the master? Say
the slave is behind the master by N transactions, and i issue lock set, move
set, will slony automatically ensure the slave gets in sync before doing the
switchover? or is it the 'applications' responsibility.. If its the latter,
how can i tell when the slave has caught up? I looked at:
http://www.slony.info/documentation/monitoring.html
I can't easily tell looking at contents of the sl_status, sl_confirm,
sl_event if the slave has caught up.. The sl_setsync table shows 0 rows,
(sl_setsync : Contains information about the state of synchronization of
each replication set, including transaction snapshot data. ). if its 0, does
it mean everything in sync?

Thanks in advance.
*
<http://www.slony.info/documentation/table.sl-setsync.html>*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110223/15765b5c/attachment.htm 

From JanWieck at Yahoo.com  Wed Feb 23 08:52:14 2011
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 23 Feb 2011 11:52:14 -0500
Subject: [Slony1-general] controlled switchover
In-Reply-To: <AANLkTik1TBDfdU0rqwV5c+TtUc0y0972S+65O=Y0zq3V@mail.gmail.com>
References: <AANLkTik1TBDfdU0rqwV5c+TtUc0y0972S+65O=Y0zq3V@mail.gmail.com>
Message-ID: <4D653B3E.5070209@Yahoo.com>

On 2/23/2011 11:19 AM, Tech Madhu wrote:
> hello all,
>
> Iam somewhat new to Slony. Have a question on controlled switchover.
>
> I have 1 master/ 1slave setup . When i want to do a switchover, using lock
> set, move set, how do i know the 'slave' has caught up with the master? Say
> the slave is behind the master by N transactions, and i issue lock set, move
> set, will slony automatically ensure the slave gets in sync before doing the
> switchover? or is it the 'applications' responsibility.. If its the latter,
> how can i tell when the slave has caught up? I looked at:
> http://www.slony.info/documentation/monitoring.html
> I can't easily tell looking at contents of the sl_status, sl_confirm,
> sl_event if the slave has caught up.. The sl_setsync table shows 0 rows,
> (sl_setsync : Contains information about the state of synchronization of
> each replication set, including transaction snapshot data. ). if its 0, does
> it mean everything in sync?

The MOVE_SET event, generated on the old origin, will cause it to become 
a subscriber. After that, the event is propagated to the new origin, 
which on processing it generates the ACCEPT_SET event, which in turn 
travels back. So if you do

LOCK SET ...
MOVE SET ...
WAIT FOR EVENT (ORIGIN = <old_origin>, CONFIRMED = <new_origin>);
SYNC (ID = <new_origin>);
WAIT FOR EVENT (ORIGIN = <new_origin>, CONFIRMED = <old_origin>);

you are guaranteed that the switchover is complete and everything is 
caught up.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From technimadhu at gmail.com  Wed Feb 23 11:45:00 2011
From: technimadhu at gmail.com (Tech Madhu)
Date: Wed, 23 Feb 2011 14:45:00 -0500
Subject: [Slony1-general] controlled switchover
In-Reply-To: <4D653B3E.5070209@Yahoo.com>
References: <AANLkTik1TBDfdU0rqwV5c+TtUc0y0972S+65O=Y0zq3V@mail.gmail.com>
	<4D653B3E.5070209@Yahoo.com>
Message-ID: <AANLkTimRFzzwH=ZxvfM+ssji6wq60sEMY8AoiGk-bL4_@mail.gmail.com>

thanks a bunch for the information.

On Wed, Feb 23, 2011 at 11:52 AM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 2/23/2011 11:19 AM, Tech Madhu wrote:
>
>> hello all,
>>
>> Iam somewhat new to Slony. Have a question on controlled switchover.
>>
>> I have 1 master/ 1slave setup . When i want to do a switchover, using lock
>> set, move set, how do i know the 'slave' has caught up with the master?
>> Say
>> the slave is behind the master by N transactions, and i issue lock set,
>> move
>> set, will slony automatically ensure the slave gets in sync before doing
>> the
>> switchover? or is it the 'applications' responsibility.. If its the
>> latter,
>> how can i tell when the slave has caught up? I looked at:
>> http://www.slony.info/documentation/monitoring.html
>> I can't easily tell looking at contents of the sl_status, sl_confirm,
>> sl_event if the slave has caught up.. The sl_setsync table shows 0 rows,
>> (sl_setsync : Contains information about the state of synchronization of
>> each replication set, including transaction snapshot data. ). if its 0,
>> does
>> it mean everything in sync?
>>
>
> The MOVE_SET event, generated on the old origin, will cause it to become a
> subscriber. After that, the event is propagated to the new origin, which on
> processing it generates the ACCEPT_SET event, which in turn travels back. So
> if you do
>
> LOCK SET ...
> MOVE SET ...
> WAIT FOR EVENT (ORIGIN = <old_origin>, CONFIRMED = <new_origin>);
> SYNC (ID = <new_origin>);
> WAIT FOR EVENT (ORIGIN = <new_origin>, CONFIRMED = <old_origin>);
>
> you are guaranteed that the switchover is complete and everything is caught
> up.
>
>
> Jan
>
> --
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110223/066f82b9/attachment.htm 

From ssinger at ca.afilias.info  Thu Feb 24 06:56:19 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 24 Feb 2011 09:56:19 -0500
Subject: [Slony1-general] Slony1-2.0.6 with PostgreSQL 9.0.3 on windows
In-Reply-To: <17170B89-A5F5-4249-B98B-88F91A73C2FB@enterprisedb.com>
References: <17170B89-A5F5-4249-B98B-88F91A73C2FB@enterprisedb.com>
Message-ID: <4D667193.6030605@ca.afilias.info>

On 11-02-22 03:47 AM, Sachin Srivastava wrote:
> Hello All,
>
> I have built PostgreSQL 9.0.3 on MinGW and then using the same I was
> building slony1-2.0.6.
> The first error i got was
> i) error: storage size of 'act' isn't known
> I got this around by changing the ifdef on line 1065 in slon.c from
> "#ifndef CYGWIN to #ifndef WIN32".
>
> ii) Now I am getting :
>
> /gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> -I../../sr//c/slon -o slon.exe slon.o runtime_config.o local_listen.o
> remote_listen.o remote//_worker.o sync_thread.o cleanup_thread.o
> scheduler.o dbutils.o conf-file.o confo//ptions.o misc.o
> ../parsestatements/scanner.o port/pipe.o port/win32service.o
> win//32ver.o -lpgport -L/c/pgsql-9.0.3/lib -L/c/pgsql-9.0.3/lib -lpq
> -lpthread -lws//ock32/
> /c:/mingw/bin/../lib/gcc/mingw32/4.5.2/../../../libmingwex.a(getopt.o):getopt.c:(//.text+0xb40):
> multiple definition of `getopt'/
> /c:/pgsql-9.0.3/lib/libpgport.a(getopt.o):getopt.c:(.text+0x0): first
> defined her//e/
> /remote_worker.o: In function `copy_set':/
> /c:\slony1-2.0.6\src\slon/remote_worker.c:2553: undefined reference to
> `slon_term//inate_worker'/
> /c:\slony1-2.0.6\src\slon/remote_worker.c:2542: undefined reference to
> `slon_term//inate_worker'/


It seems I broke this with 
http://git.postgresql.org/gitweb?p=slony1-engine.git;a=commit;h=6c7c91252f437e9dd5c86b6f2a0f92840c135b6d

but I am not sure what the correct fix is.

a) Do we include that slon_terminate_worker function - as written for 
WIN32 ? Would that even compile?
b) Do we change the calls in copy_set that call slon_terminate_worker to 
slon_retry() calls ?

I'm leaning towards (b) with the attached patch (untested)

Sachin, the slony binaries enterprise db distributes with the one-click 
installers, are they built with visual studio or MinGW

I should spend some time getting a proper Win32 slony built environment 
setup (but I won't get to that in the next few weeks)





> /collect2: ld returned 1 exit status/
> /make[2]: *** [slon.exe] Error 1/
> /make[2]: Leaving directory `/c/slony1-2.0.6/src/slon'/
> /make[1]: *** [all] Error 2/
> /make[1]: Leaving directory `/c/slony1-2.0.6/src'/
> /make: *** [all] Error 2/
> /
> /
> /
> /
> In PostgreSQL 9.0.3 they have included (which was not there in 9.0.2 and
> thus slony builds fine there.):
>>  #mingw has adopted a GNU-centric interpretation of optind/optreset,
>>  # so always use our version on Windows.
>>  if test "$PORTNAME" = "win32"; then
>>  case " $LIBOBJS " in
>>  *" getopt.$ac_objext "* ) ;;
>>  *) LIBOBJS="$LIBOBJS getopt.$ac_objext"
>>  ;;
>>  esac
>
> Any help to make me go forward with this would be highly appreciated.
>
> Thanks a lot.
>
>
> /
> /
> /
> /
> /
> /
> /
> /
> --Regards,
> Sachin Srivastava
> EnterpriseDB <http://www.enterprisedb.com>, the Enterprise PostgreSQL
> <http://www.enterprisedb.com> company.
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-------------- next part --------------
A non-text attachment was scrubbed...
Name: slon_terminate_worker.diff
Type: text/x-patch
Size: 1569 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20110224/adb357bc/attachment.bin 

From sachin.srivastava at enterprisedb.com  Fri Feb 25 01:56:26 2011
From: sachin.srivastava at enterprisedb.com (Sachin Srivastava)
Date: Fri, 25 Feb 2011 15:26:26 +0530
Subject: [Slony1-general] Slony1-2.0.6 with PostgreSQL 9.0.3 on windows
In-Reply-To: <4D667193.6030605@ca.afilias.info>
References: <17170B89-A5F5-4249-B98B-88F91A73C2FB@enterprisedb.com>
	<4D667193.6030605@ca.afilias.info>
Message-ID: <29E8B653-2871-49F8-A0C9-C06C2BE3083B@enterprisedb.com>


On Feb 24, 2011, at 8:26 PM, Steve Singer wrote:

> On 11-02-22 03:47 AM, Sachin Srivastava wrote:
>> Hello All,
>> 
>> I have built PostgreSQL 9.0.3 on MinGW and then using the same I was
>> building slony1-2.0.6.
>> The first error i got was
>> i) error: storage size of 'act' isn't known
>> I got this around by changing the ifdef on line 1065 in slon.c from
>> "#ifndef CYGWIN to #ifndef WIN32".
>> 
>> ii) Now I am getting :
>> 
>> /gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
>> -I../../sr//c/slon -o slon.exe slon.o runtime_config.o local_listen.o
>> remote_listen.o remote//_worker.o sync_thread.o cleanup_thread.o
>> scheduler.o dbutils.o conf-file.o confo//ptions.o misc.o
>> ../parsestatements/scanner.o port/pipe.o port/win32service.o
>> win//32ver.o -lpgport -L/c/pgsql-9.0.3/lib -L/c/pgsql-9.0.3/lib -lpq
>> -lpthread -lws//ock32/
>> /c:/mingw/bin/../lib/gcc/mingw32/4.5.2/../../../libmingwex.a(getopt.o):getopt.c:(//.text+0xb40):
>> multiple definition of `getopt'/
>> /c:/pgsql-9.0.3/lib/libpgport.a(getopt.o):getopt.c:(.text+0x0): first
>> defined her//e/
>> /remote_worker.o: In function `copy_set':/
>> /c:\slony1-2.0.6\src\slon/remote_worker.c:2553: undefined reference to
>> `slon_term//inate_worker'/
>> /c:\slony1-2.0.6\src\slon/remote_worker.c:2542: undefined reference to
>> `slon_term//inate_worker'/
> 
> 
> It seems I broke this with http://git.postgresql.org/gitweb?p=slony1-engine.git;a=commit;h=6c7c91252f437e9dd5c86b6f2a0f92840c135b6d
> 
> but I am not sure what the correct fix is.
> 
> a) Do we include that slon_terminate_worker function - as written for WIN32 ? Would that even compile?
> b) Do we change the calls in copy_set that call slon_terminate_worker to slon_retry() calls ?
> 
> I'm leaning towards (b) with the attached patch (untested)
> 
> Sachin, the slony binaries enterprise db distributes with the one-click installers, are they built with visual studio or MinGW
MinGW, I believe there isnt a VC port available for slony.

> 
> I should spend some time getting a proper Win32 slony built environment setup (but I won't get to that in the next few weeks)
> 
> 
> 
> 
> 
>> /collect2: ld returned 1 exit status/
>> /make[2]: *** [slon.exe] Error 1/
>> /make[2]: Leaving directory `/c/slony1-2.0.6/src/slon'/
>> /make[1]: *** [all] Error 2/
>> /make[1]: Leaving directory `/c/slony1-2.0.6/src'/
>> /make: *** [all] Error 2/
>> /
>> /
>> /
>> /
>> In PostgreSQL 9.0.3 they have included (which was not there in 9.0.2 and
>> thus slony builds fine there.):
>>> #mingw has adopted a GNU-centric interpretation of optind/optreset,
>>> # so always use our version on Windows.
>>> if test "$PORTNAME" = "win32"; then
>>> case " $LIBOBJS " in
>>> *" getopt.$ac_objext "* ) ;;
>>> *) LIBOBJS="$LIBOBJS getopt.$ac_objext"
>>> ;;
>>> esac
>> 
>> Any help to make me go forward with this would be highly appreciated.
>> 
>> Thanks a lot.
>> 
>> 
>> /
>> /
>> /
>> /
>> /
>> /
>> /
>> /
>> --Regards,
>> Sachin Srivastava
>> EnterpriseDB <http://www.enterprisedb.com>, the Enterprise PostgreSQL
>> <http://www.enterprisedb.com> company.
>> 
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> <slon_terminate_worker.diff>

--
Regards,
Sachin Srivastava
EnterpriseDB, the Enterprise PostgreSQL company.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110225/8698e460/attachment.htm 

From hperez at linux-si.com  Fri Feb 25 16:12:54 2011
From: hperez at linux-si.com (Hugo =?utf-8?q?P=C3=A9rez_Casanova?=)
Date: Fri, 25 Feb 2011 18:12:54 -0600
Subject: [Slony1-general] Replication problem
Message-ID: <201102251812.54172.hperez@linux-si.com>

Hi Everybody.

I'm not sure if this is the right list, correct me if I'm wrong.

This is the first time I've worked with slony.

I made a test environment with two virtual servers working in CentOS 5.5, 
Postgresql 8.4 and Slony 2.0.6. Test environment works fine.

When moving to production I have cloned a mid size database, when replication 
starts I see at the slon log that it founds all tables to replicate. The 
problem appears when writing to a tmp archive. Following is a snippet of the 
log:

2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: prepare to copy 
table "public"."model_devices"
2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: prepare to copy 
table "public"."array_name"
2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: all tables for set 1 found 
on subscriber
2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: copy 
table "public"."provider"
2011-02-25 18:07:33 CSTCONFIG remoteWorkerThread_1: Begin COPY of 
table "public"."provider"
NOTICE:  truncate of "public"."provider" failed - doing delete
2011-02-25 18:07:33 CSTERROR  remoteWorkerThread_1: Cannot write to archive 
file /tmp/slony1_log_2_00000000000000000366.sql.tmp - not open
2011-02-25 18:07:33 CSTWARN   remoteWorkerThread_1: data copy for set 1 failed 
19 times - sleep 60 seconds

What am I doing wrong?

Regards.

-- 
IEE Jos? Hugo P?rez Casanova

-- 
Este mensaje ha sido analizado por MailScanner
en busca de virus y otros contenidos peligrosos,
y se considera que est? limpio.



From stephane.schildknecht at postgresql.fr  Sat Feb 26 05:58:57 2011
From: stephane.schildknecht at postgresql.fr (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Sat, 26 Feb 2011 14:58:57 +0100
Subject: [Slony1-general] Replication problem
In-Reply-To: <201102251812.54172.hperez@linux-si.com>
References: <201102251812.54172.hperez@linux-si.com>
Message-ID: <4D690721.5060705@postgresql.fr>

Le 26/02/2011 01:12, Hugo P?rez Casanova a ?crit :
> Hi Everybody.
> 
> I'm not sure if this is the right list, correct me if I'm wrong.

Yes it is. There is no better place than slony mailing-lists to talk about
slony :-)

> 
> This is the first time I've worked with slony.
> 
> I made a test environment with two virtual servers working in CentOS 5.5, 
> Postgresql 8.4 and Slony 2.0.6. Test environment works fine.
> 
> When moving to production I have cloned a mid size database, when replication 
> starts I see at the slon log that it founds all tables to replicate. The 
> problem appears when writing to a tmp archive. Following is a snippet of the 
> log:
> 
> 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: prepare to copy 
> table "public"."model_devices"
> 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: prepare to copy 
> table "public"."array_name"
> 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: all tables for set 1 found 
> on subscriber
> 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: copy 
> table "public"."provider"
> 2011-02-25 18:07:33 CSTCONFIG remoteWorkerThread_1: Begin COPY of 
> table "public"."provider"
> NOTICE:  truncate of "public"."provider" failed - doing delete
> 2011-02-25 18:07:33 CSTERROR  remoteWorkerThread_1: Cannot write to archive 
> file /tmp/slony1_log_2_00000000000000000366.sql.tmp - not open
> 2011-02-25 18:07:33 CSTWARN   remoteWorkerThread_1: data copy for set 1 failed 
> 19 times - sleep 60 seconds
> 
> What am I doing wrong?

Could you give us more information about the way you settled the replication ?
Did you set the archive logging mode ?
Seems like you can't write in /tmp either bacause you don't have the right to,
or because there's no place left.

Regards,

-- 
St?phane Schildknecht
http://www.loxodata.com

From hperez at linux-si.com  Sat Feb 26 07:11:42 2011
From: hperez at linux-si.com (Hugo =?iso-8859-15?q?P=E9rez_Casanova?=)
Date: Sat, 26 Feb 2011 09:11:42 -0600
Subject: [Slony1-general] Replication problem
In-Reply-To: <4D690721.5060705@postgresql.fr>
References: <201102251812.54172.hperez@linux-si.com>
	<4D690721.5060705@postgresql.fr>
Message-ID: <201102260911.42839.hperez@linux-si.com>

Hi St?phane.

Don't know how to set the archive logging mode. I followed the tutorial and 
made myself a setup script as follows:

-------------------------------------------------------------------------------
#!/bin/sh

CLUSTERNAME=plasma_cluster
MASTERDBNAME=plasma
SLAVEDBNAME=plasma
MASTERHOST=192.168.100.1
SLAVEHOST=192.168.100.2
REPLICATIONUSER=plasma
DBUSER=plasma

slonik <<_EOF_
    #--
    # define the namespace the replication system uses in our example it is
    # slony_example
    #--
    cluster name = $CLUSTERNAME;

    #--
    # admin conninfo's are used by slonik to connect to the nodes one for each
    # node on each side of the cluster, the syntax is that of PQconnectdb in
    # the C-API
    # --
    node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST 
user=$REPLICATIONUSER password=XXXXXXXX';
    node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST 
user=$REPLICATIONUSER password=XXXXXXXX';

    #--
    # init the first node.  Its id MUST be 1.  This creates the schema
    # _$CLUSTERNAME containing all replication system specific database
    # objects.

    #--
    init cluster ( id=1, comment = 'Master Node');

    #--
    # Slony-I organizes tables into sets.  The smallest unit a node can
    # subscribe is a set.  The following commands create one set containing
    # all 4 pgbench tables.  The master or origin of the set is node 1.
    #--
    create set (id=1, origin=1, comment='Products handling');
    set add table (set id=1, origin=1, id=1, fully qualified name 
= 'public.product', comment='');
    set add table (set id=1, origin=1, id=2, fully qualified name 
= 'public.provider', comment='');
    set add table (set id=1, origin=1, id=3, fully qualified name 
= 'public.product_type', comment='');
    set add table (set id=1, origin=1, id=4, fully qualified name 
= 'public.content_type', comment='');
    set add table (set id=1, origin=1, id=5, fully qualified name 
= 'public.dimension', comment='');
    set add table (set id=1, origin=1, id=6, fully qualified name 
= 'public.dimension_type', comment='');
    set add table (set id=1, origin=1, id=7, fully qualified name 
= 'public.censure', comment='');
    set add table (set id=1, origin=1, id=8, fully qualified name 
= 'public.users', comment='');
    set add table (set id=1, origin=1, id=9, fully qualified name 
= 'public.category', comment='');
    set add table (set id=1, origin=1, id=10,fully qualified name 
= 'public.subcategory', comment='');
    set add table (set id=1, origin=1, id=11,fully qualified name 
= 'public.singer', comment='');
    set add table (set id=1, origin=1, id=12,fully qualified name 
= 'public.author', comment='');
    set add table (set id=1, origin=1, id=13,fully qualified name 
= 'public.publishing_house', comment='');
    set add table (set id=1, origin=1, id=14,fully qualified name 
= 'public.instance', comment='');
    set add table (set id=1, origin=1, id=15,fully qualified name 
= 'public.mime_types', comment='');
    set add table (set id=1, origin=1, id=16,fully qualified name 
= 'public.compatibility_groups', comment='');
    set add table (set id=1, origin=1, id=17,fully qualified name 
= 'public.model_devices', comment='');
    set add table (set id=1, origin=1, id=18,fully qualified name 
= 'public.array_name', comment='');

    #--
    # Create the second node (the slave) tell the 2 nodes how to connect to
    # each other and how they should listen for events.
    #--

    store node (id=2, comment = 'Slave node', event node=1);
    store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME 
host=$MASTERHOST user=$REPLICATIONUSER password=XXXXXXXX');
    store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME 
host=$SLAVEHOST user=$REPLICATIONUSER password=XXXXXXXX');
_EOF_
-------------------------------------------------------------------------------------------

I started the slon processes with "service slony1-II start" at both ends and 
then started replication at the master node with:

-------------------------------------------------------------------------------------------

#!/bin/sh

CLUSTERNAME=plasma_cluster
MASTERDBNAME=plasma
MASTERHOST=192.168.100.1
SLAVEDBNAME=plasma
SLAVEHOST=192.168.100.2
REPLICATIONUSER=plasma

slonik <<_EOF_
     # ----
     # This defines which namespace the replication system uses
     # ----
     cluster name = $CLUSTERNAME;

     # ----
     # Admin conninfo's are used by the slonik program to connect
     # to the node databases.  So these are the PQconnectdb arguments
     # that connect from the administrators workstation (where
     # slonik is executed).
     # ----
     node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST 
user=$REPLICATIONUSER password=XXXXXXXX';
     node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST 
user=$REPLICATIONUSER password=XXXXXXXX';

     # ----
     # Node 2 subscribes set 1
     # ----
     subscribe set ( id = 2, provider = 1, receiver = 2, forward = no);
_EOF_

----------------------------------------------------------------------------------------------

Regards.

On Saturday 26 February 2011 07:58, St?phane A. Schildknecht wrote:
> Le 26/02/2011 01:12, Hugo P?rez Casanova a ?crit :
> > Hi Everybody.
> >
> > I'm not sure if this is the right list, correct me if I'm wrong.
>
> Yes it is. There is no better place than slony mailing-lists to talk about
> slony :-)
>
> > This is the first time I've worked with slony.
> >
> > I made a test environment with two virtual servers working in CentOS 5.5,
> > Postgresql 8.4 and Slony 2.0.6. Test environment works fine.
> >
> > When moving to production I have cloned a mid size database, when
> > replication starts I see at the slon log that it founds all tables to
> > replicate. The problem appears when writing to a tmp archive. Following
> > is a snippet of the log:
> >
> > 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: prepare to copy
> > table "public"."model_devices"
> > 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: prepare to copy
> > table "public"."array_name"
> > 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: all tables for set 1
> > found on subscriber
> > 2011-02-25 18:07:32 CSTCONFIG remoteWorkerThread_1: copy
> > table "public"."provider"
> > 2011-02-25 18:07:33 CSTCONFIG remoteWorkerThread_1: Begin COPY of
> > table "public"."provider"
> > NOTICE:  truncate of "public"."provider" failed - doing delete
> > 2011-02-25 18:07:33 CSTERROR  remoteWorkerThread_1: Cannot write to
> > archive file /tmp/slony1_log_2_00000000000000000366.sql.tmp - not open
> > 2011-02-25 18:07:33 CSTWARN   remoteWorkerThread_1: data copy for set 1
> > failed 19 times - sleep 60 seconds
> >
> > What am I doing wrong?
>
> Could you give us more information about the way you settled the
> replication ? Did you set the archive logging mode ?
> Seems like you can't write in /tmp either bacause you don't have the right
> to, or because there's no place left.
>
> Regards,
>
> --
> St?phane Schildknecht
> http://www.loxodata.com
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
IEE Jos? Hugo P?rez Casanova
Gerente de Sistemas

Linux Soluciones Integrales
Ant?n de Alaminos #256-B, Fracc. Virginia,
Boca del R?o Veracruz,
M?xico, 94294
Tel/Fax (52) 229-937-7777
www.linux-si.com

-- 
Este mensaje ha sido analizado por MailScanner
en busca de virus y otros contenidos peligrosos,
y se considera que est??? limpio.



From ssinger at ca.afilias.info  Mon Feb 28 06:27:37 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 28 Feb 2011 09:27:37 -0500
Subject: [Slony1-general] Replication problem
In-Reply-To: <201102260911.42839.hperez@linux-si.com>
References: <201102251812.54172.hperez@linux-si.com>	<4D690721.5060705@postgresql.fr>
	<201102260911.42839.hperez@linux-si.com>
Message-ID: <4D6BB0D9.7040403@ca.afilias.info>

On 11-02-26 10:11 AM, Hugo P?rez Casanova wrote:
>
> I started the slon processes with "service slony1-II start" at both ends and
> then started replication at the master node with:
>

It sounds like you have archive_mode set in your slon.conf file.

From hperez at linux-si.com  Mon Feb 28 07:30:19 2011
From: hperez at linux-si.com (Hugo =?iso-8859-15?q?P=E9rez_Casanova?=)
Date: Mon, 28 Feb 2011 09:30:19 -0600
Subject: [Slony1-general] Replication problem
In-Reply-To: <4D6BB0D9.7040403@ca.afilias.info>
References: <201102251812.54172.hperez@linux-si.com>
	<201102260911.42839.hperez@linux-si.com>
	<4D6BB0D9.7040403@ca.afilias.info>
Message-ID: <201102280930.19914.hperez@linux-si.com>

Hi Steve.

I will check. Late yesterday I discovered that a table wich had no primary key 
was causing the problem. After I added it, replication worked fine.

Regards to everybody.

On Monday 28 February 2011 08:27, Steve Singer wrote:
> On 11-02-26 10:11 AM, Hugo P?rez Casanova wrote:
> > I started the slon processes with "service slony1-II start" at both ends
> > and then started replication at the master node with:
>
> It sounds like you have archive_mode set in your slon.conf file.

-- 
IEE Jos? Hugo P?rez Casanova
Gerente de Sistemas

Linux Soluciones Integrales
Ant?n de Alaminos #256-B, Fracc. Virginia,
Boca del R?o Veracruz,
M?xico, 94294
Tel/Fax (52) 229-937-7777
www.linux-si.com

-- 
Este mensaje ha sido analizado por MailScanner
en busca de virus y otros contenidos peligrosos,
y se considera que est??? limpio.



From ssinger at ca.afilias.info  Mon Feb 28 08:40:22 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 28 Feb 2011 11:40:22 -0500
Subject: [Slony1-general] Slony1-2.0.6 with PostgreSQL 9.0.3 on windows
In-Reply-To: <29E8B653-2871-49F8-A0C9-C06C2BE3083B@enterprisedb.com>
References: <17170B89-A5F5-4249-B98B-88F91A73C2FB@enterprisedb.com>
	<4D667193.6030605@ca.afilias.info>
	<29E8B653-2871-49F8-A0C9-C06C2BE3083B@enterprisedb.com>
Message-ID: <4D6BCFF6.6080905@ca.afilias.info>

On 11-02-25 04:56 AM, Sachin Srivastava wrote:
>

>>
>> I'm leaning towards (b) with the attached patch (untested)
>>
>> Sachin, the slony binaries enterprise db distributes with the
>> one-click installers, are they built with visual studio or MinGW
> MinGW, I believe there isnt a VC port available for slony.
>
>>

Everytime I try to do a MinGW build I wonder if I shouldn't just bite 
the bullet and write a parallel set of nmake files so we can do a VC 
build.  Does anyone else think this would be a good idea?


>> I should spend some time getting a proper Win32 slony built
>> environment setup (but I won't get to that in the next few weeks)

When I apply that patch I sent  + modify remoteWorkerThread_main to 
'return 0;' + munge the Makefile.port that gets generated I am able to 
get to the linking stage against 8.4 but I get all these undefined 
symbols complaining about stuff pgport wants being undefined.

When I try to build against 9.0.3 I can't get configure to finish.  It 
complains on "#error must have a 64-bit integer datatype" from postgres.h

Do I need a specific version of mingw?

When you applied the patch I sent did you have better luck?




>>
>>
>>
>>
>>
>>> /collect2: ld returned 1 exit status/
>>> /make[2]: *** [slon.exe] Error 1/
>>> /make[2]: Leaving directory `/c/slony1-2.0.6/src/slon'/
>>> /make[1]: *** [all] Error 2/
>>> /make[1]: Leaving directory `/c/slony1-2.0.6/src'/
>>> /make: *** [all] Error 2/
>>> /
>>> /
>>> /
>>> /
>>> In PostgreSQL 9.0.3 they have included (which was not there in 9.0.2 and
>>> thus slony builds fine there.):
>>>> #mingw has adopted a GNU-centric interpretation of optind/optreset,
>>>> # so always use our version on Windows.
>>>> if test "$PORTNAME" = "win32"; then
>>>> case " $LIBOBJS " in
>>>> *" getopt.$ac_objext "* ) ;;
>>>> *) LIBOBJS="$LIBOBJS getopt.$ac_objext"
>>>> ;;
>>>> esac
>>>
>>> Any help to make me go forward with this would be highly appreciated.
>>>
>>> Thanks a lot.
>>>
>>>
>>> /
>>> /
>>> /
>>> /
>>> /
>>> /
>>> /
>>> /
>>> --Regards,
>>> Sachin Srivastava
>>> EnterpriseDB <http://www.enterprisedb.com>, the Enterprise PostgreSQL
>>> <http://www.enterprisedb.com> company.
>>>
>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info <mailto:Slony1-general at lists.slony.info>
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>> <slon_terminate_worker.diff>
>
> --
> Regards,
> Sachin Srivastava
> EnterpriseDB <http://www.enterprisedb.com>, the Enterprise PostgreSQL
> <http://www.enterprisedb.com> company.
>


