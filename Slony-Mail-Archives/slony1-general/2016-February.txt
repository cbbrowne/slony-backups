From ttignor at akamai.com  Mon Feb  1 06:06:56 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 1 Feb 2016 14:06:56 +0000
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <56AA595E.9000706@wi3ck.info>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com> <56AA595E.9000706@wi3ck.info>
Message-ID: <D2D4CCA1.4C458%ttignor@akamai.com>


	Jan,
	Thanks much for all the help. I?ve been looking over logswitch finish and
the event and changelog tables. Selecting logswitch_finish() on one of my
replicas simply returned ""Slony-I: log switch to sl_log_2 still in
progress - sl_log_1 not truncated? All three seem to be in that state with
either sl_log_1 or sl_log_2. Looking closer at sl_event and sl_log_2 I?m
seeing some strangeness.

ams=# select min(pg_catalog.txid_snapshot_xmin(ev_snapshot)),
max(pg_catalog.txid_snapshot_xmin(ev_snapshot)) from _ams_cluster.sl_event
where ev_origin = 1;
    min    |    max
-----------+-----------
 139136948 | 139204299
(1 row)


ams=# select min(log_txid), max(log_txid) from _ams_cluster.sl_log_2 where
log_origin = 1;
    min    |    max
-----------+-----------
 631532717 | 631661386
(1 row)


ams=#


	So I understand all the txids referenced in sl_event are in the 139M
range while all those in sl_log_2 are in the 631M range. Normally, the
sl_log txids should be older, shouldn?t they? Do you think I?ve hit a
txid-wraparound problem?

	Tom    :-)



On 1/28/16, 1:09 PM, "Jan Wieck" <jan at wi3ck.info> wrote:

>On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>>
>> 	Output below. They seem to be replicating normally, except for the
>>sl_log
>> growth.
>
>Indeed. Is there anything in the slon logs for those nodes that says why
>it doesn't finish the log switch?
>
>Connect to the database as a the slony user.
>
>To check if a log switch is indeed in progress, do
>
>     SELECT last_value FROM _ams_cluster.sl_log_status;
>
>It should be either 2 or 3. If it is 0 or 1, no log switch is in
>progress and you can start one with
>
>     SELECCT _ams_cluster.logswitch_start();
>
>If it is 2 or 3, then you can do
>
>     SELECT _ams_cluster.logswitch_finish();
>
>All these operations are harmless and will only do what is safely
>possible. Look at the code of logswitch_finish() to find out how it
>determines if the current log switch can be finished. In short, the
>cleanup thread is removing events from sl_event that have been confirmed
>by all nodes in the cluster. The function logswitch_finish() looks if
>there is anything left in sl_event, that belonged to that old log. If so
>it will not finish. Running those queries manually you can find out what
>that event is that is preventing the switch to finish.
>
>
>
>>
>>
>> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>  |
>> st_last_received |      st_last_received_ts      |
>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>> 
>>-----------+-------------+---------------+-------------------------------
>>+-
>> 
>>-----------------+-------------------------------+-----------------------
>>--
>> ------+-------------------+-----------------
>>           2 |           1 |    5000611610 | 2016-01-28
>>16:06:37.343826+00 |
>>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>           2 |           3 |    5000611610 | 2016-01-28
>>16:06:37.343826+00 |
>>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
>> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>>           2 |           4 |    5000611610 | 2016-01-28
>>16:06:37.343826+00 |
>>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>> (3 rows)
>>
>>
>> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>  |
>> st_last_received |      st_last_received_ts      |
>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>> 
>>-----------+-------------+---------------+-------------------------------
>>+-
>> 
>>-----------------+-------------------------------+-----------------------
>>--
>> ------+-------------------+-----------------
>>           3 |           4 |    5000654642 | 2016-01-28
>>16:07:05.493455+00 |
>>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>           3 |           1 |    5000654642 | 2016-01-28
>>16:07:05.493455+00 |
>>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>           3 |           2 |    5000654642 | 2016-01-28
>>16:07:05.493455+00 |
>>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>> (3 rows)
>>
>>
>> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>  |
>> st_last_received |      st_last_received_ts      |
>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>> 
>>-----------+-------------+---------------+-------------------------------
>>+-
>> 
>>-----------------+-------------------------------+-----------------------
>>--
>> ------+-------------------+-----------------
>>           4 |           3 |    5000637483 | 2016-01-28
>>16:07:32.698809+00 |
>>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>           4 |           1 |    5000637483 | 2016-01-28
>>16:07:32.698809+00 |
>>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>           4 |           2 |    5000637483 | 2016-01-28
>>16:07:32.698809+00 |
>>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>> (3 rows)
>>
>>
>>
>> 	Tom    :-)
>>
>>
>>
>> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>
>>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>>
>>>> Hello slony folks,
>>>>  From my reading I?m guessing (hoping) this isn?t a new problem. I
>>>>have
>>>> a simple cluster with one provider replicating to three subscribers.
>>>>The
>>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the
>>>>subscribers
>>>> (with forwarding enabled) are all showing runaway growth. Looked
>>>>through
>>>> the FAQ and I don?t see the node I dropped or any idle transactions as
>>>> viable culprits. Are there other thoughts on the cause? Can I safely
>>>> manually delete/truncate some/all of the changelog tables? These
>>>> replicas are all leaf nodes. I only have forwarding turned on to allow
>>>> for failover, and my replication rate is the 2 sec default.
>>>> Thanks in advance for any insights.
>>>
>>>What is the output of the sl_status view "on those leaf nodes"?
>>>
>>>
>>>>
>>>> ams=# select
>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>>
>>>>   pg_size_pretty
>>>>
>>>> ----------------
>>>>
>>>>   75 MB
>>>>
>>>> (1 row)
>>>>
>>>>
>>>> ams=# select
>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>>
>>>>   pg_size_pretty
>>>>
>>>> ----------------
>>>>
>>>>   34 GB
>>>>
>>>> (1 row)
>>>>
>>>>
>>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>>>> (select no_id from _ams_cluster.sl_node) or con_received not in
>>>>(select
>>>> no_id from _ams_cluster.sl_node);
>>>>
>>>>   con_origin | con_received | con_seqno | con_timestamp
>>>>
>>>> ------------+--------------+-----------+---------------
>>>>
>>>> (0 rows)
>>>>
>>>>
>>>> ams=# select * from pg_stat_activity where current_query like
>>>>'%IDLE%';
>>>>
>>>>   datid | datname | procpid | usesysid |  usename   |
>>>> application_name      |  client_addr   | client_hostname |
>>>>client_port |
>>>>          backend_start         |          xact_start           |
>>>>    query_start          | waiting |
>>>>
>>>>                          current_query
>>>>
>>>>
>>>>-------+---------+---------+----------+------------+-------------------
>>>>--
>>>>------+----------------+-----------------+-------------+---------------
>>>>--
>>>>--------------+-------------------------------+------------------------
>>>>--
>>>>-----+---------+---
>>>>
>>>> ----------------------------------------------------------------
>>>>
>>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>>          | 88.221.209.10  |                 |       43328 | 2016-01-28
>>>> 12:24:49.706389+00 |                               | 2016-01-28
>>>> 13:18:02.427848+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>>          | 88.221.209.10  |                 |       60112 | 2016-01-28
>>>> 12:47:26.230681+00 |                               | 2016-01-28
>>>> 13:15:27.744242+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>>          | 88.221.209.10  |                 |       44302 | 2016-01-28
>>>> 12:47:25.100006+00 |                               | 2016-01-28
>>>> 13:15:27.936059+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>>          |                |                 |          -1 | 2016-01-28
>>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>>> 13:18:37.283992+00 | f       | se
>>>>
>>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>>
>>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>>> 61806 | 2016-01-22 01:59:14.800129+00 |
>>>>|
>>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>>> 61805 | 2016-01-22 01:59:14.797655+00 |
>>>>|
>>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>>> 36477 | 2016-01-22 01:56:25.637046+00 |
>>>>|
>>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>>> 51813 | 2016-01-22 01:56:25.240798+00 |
>>>>|
>>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>>> 51803 | 2016-01-22 01:56:22.896388+00 |
>>>>|
>>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>>> 51564 | 2016-01-22 01:55:23.600296+00 |
>>>>|
>>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>>> 36402 | 2016-01-22 01:55:22.964462+00 |
>>>>|
>>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>>> 61795 | 2016-01-22 01:59:12.095052+00 |
>>>>|
>>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>>> 51238 | 2016-01-22 01:54:21.481355+00 |
>>>>|
>>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>>> 36333 | 2016-01-22 01:54:21.500456+00 |
>>>>|
>>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>>> slon.local_monitor        |                |                 |
>>>> -1 | 2016-01-22 01:54:18.977015+00 |                               |
>>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4427 |   213867 | ams_slony  | slon.local_sync
>>>>          |                |                 |          -1 | 2016-01-22
>>>> 01:54:18.976932+00 |                               | 2016-01-28
>>>> 13:18:36.151998+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>>> slon.local_cleanup        |                |                 |
>>>> -1 | 2016-01-22 01:54:18.976842+00 |                               |
>>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>>> slon.remoteWorkerThread_4 |                |                 |
>>>> -1 | 2016-01-22 01:54:18.976783+00 |                               |
>>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>>> slon.remoteWorkerThread_1 |                |                 |
>>>> -1 | 2016-01-22 01:54:18.976548+00 |                               |
>>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>>> slon.remoteWorkerThread_3 |                |                 |
>>>> -1 | 2016-01-22 01:54:18.97647+00  |                               |
>>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>>   16393 | ams     |    4413 |   213867 | ams_slony  |
>>>>slon.local_listen
>>>>          |                |                 |          -1 | 2016-01-22
>>>> 01:54:18.965568+00 |                               | 2016-01-28
>>>> 13:18:37.096159+00 | f       | <I
>>>>
>>>> DLE>
>>>>
>>>> (21 rows)
>>>>
>>>>
>>>> ams=#
>>>>
>>>>
>>>>
>>>> Tom    :-)
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>
>>>
>>>
>>>--
>>>Jan Wieck
>>>Senior Software Engineer
>>>http://slony.info
>>
>
>
>-- 
>Jan Wieck
>Senior Software Engineer
>http://slony.info


From ttignor at akamai.com  Mon Feb  1 10:24:24 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 1 Feb 2016 18:24:24 +0000
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <D2D4CCA1.4C458%ttignor@akamai.com>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com> <56AA595E.9000706@wi3ck.info>
	<D2D4CCA1.4C458%ttignor@akamai.com>
Message-ID: <D2D50C91.4C7BE%ttignor@akamai.com>


	Quick update: a couple hours after deleting entries from both sl_log
tables with txids > 630M, it appears the cleanup thread has taken care of
business. sl_log_1 is down from 54GB to 24KB.

	Tom    :-)


On 2/1/16, 9:06 AM, "Tignor, Tom" <ttignor at akamai.com> wrote:

>
>	Jan,
>	Thanks much for all the help. I?ve been looking over logswitch finish and
>the event and changelog tables. Selecting logswitch_finish() on one of my
>replicas simply returned ""Slony-I: log switch to sl_log_2 still in
>progress - sl_log_1 not truncated? All three seem to be in that state with
>either sl_log_1 or sl_log_2. Looking closer at sl_event and sl_log_2 I?m
>seeing some strangeness.
>
>ams=# select min(pg_catalog.txid_snapshot_xmin(ev_snapshot)),
>max(pg_catalog.txid_snapshot_xmin(ev_snapshot)) from _ams_cluster.sl_event
>where ev_origin = 1;
>    min    |    max
>-----------+-----------
> 139136948 | 139204299
>(1 row)
>
>
>ams=# select min(log_txid), max(log_txid) from _ams_cluster.sl_log_2 where
>log_origin = 1;
>    min    |    max
>-----------+-----------
> 631532717 | 631661386
>(1 row)
>
>
>ams=#
>
>
>	So I understand all the txids referenced in sl_event are in the 139M
>range while all those in sl_log_2 are in the 631M range. Normally, the
>sl_log txids should be older, shouldn?t they? Do you think I?ve hit a
>txid-wraparound problem?
>
>	Tom    :-)
>
>
>
>On 1/28/16, 1:09 PM, "Jan Wieck" <jan at wi3ck.info> wrote:
>
>>On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>>>
>>> 	Output below. They seem to be replicating normally, except for the
>>>sl_log
>>> growth.
>>
>>Indeed. Is there anything in the slon logs for those nodes that says why
>>it doesn't finish the log switch?
>>
>>Connect to the database as a the slony user.
>>
>>To check if a log switch is indeed in progress, do
>>
>>     SELECT last_value FROM _ams_cluster.sl_log_status;
>>
>>It should be either 2 or 3. If it is 0 or 1, no log switch is in
>>progress and you can start one with
>>
>>     SELECCT _ams_cluster.logswitch_start();
>>
>>If it is 2 or 3, then you can do
>>
>>     SELECT _ams_cluster.logswitch_finish();
>>
>>All these operations are harmless and will only do what is safely
>>possible. Look at the code of logswitch_finish() to find out how it
>>determines if the current log switch can be finished. In short, the
>>cleanup thread is removing events from sl_event that have been confirmed
>>by all nodes in the cluster. The function logswitch_finish() looks if
>>there is anything left in sl_event, that belonged to that old log. If so
>>it will not finish. Running those queries manually you can find out what
>>that event is that is preventing the switch to finish.
>>
>>
>>
>>>
>>>
>>> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>  |
>>> st_last_received |      st_last_received_ts      |
>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>> 
>>>-----------+-------------+---------------+------------------------------
>>>-
>>>+-
>>> 
>>>-----------------+-------------------------------+----------------------
>>>-
>>>--
>>> ------+-------------------+-----------------
>>>           2 |           1 |    5000611610 | 2016-01-28
>>>16:06:37.343826+00 |
>>>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>           2 |           3 |    5000611610 | 2016-01-28
>>>16:06:37.343826+00 |
>>>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
>>> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>>>           2 |           4 |    5000611610 | 2016-01-28
>>>16:06:37.343826+00 |
>>>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>> (3 rows)
>>>
>>>
>>> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>  |
>>> st_last_received |      st_last_received_ts      |
>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>> 
>>>-----------+-------------+---------------+------------------------------
>>>-
>>>+-
>>> 
>>>-----------------+-------------------------------+----------------------
>>>-
>>>--
>>> ------+-------------------+-----------------
>>>           3 |           4 |    5000654642 | 2016-01-28
>>>16:07:05.493455+00 |
>>>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>           3 |           1 |    5000654642 | 2016-01-28
>>>16:07:05.493455+00 |
>>>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>           3 |           2 |    5000654642 | 2016-01-28
>>>16:07:05.493455+00 |
>>>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>> (3 rows)
>>>
>>>
>>> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>  |
>>> st_last_received |      st_last_received_ts      |
>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>> 
>>>-----------+-------------+---------------+------------------------------
>>>-
>>>+-
>>> 
>>>-----------------+-------------------------------+----------------------
>>>-
>>>--
>>> ------+-------------------+-----------------
>>>           4 |           3 |    5000637483 | 2016-01-28
>>>16:07:32.698809+00 |
>>>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>           4 |           1 |    5000637483 | 2016-01-28
>>>16:07:32.698809+00 |
>>>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>           4 |           2 |    5000637483 | 2016-01-28
>>>16:07:32.698809+00 |
>>>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>> (3 rows)
>>>
>>>
>>>
>>> 	Tom    :-)
>>>
>>>
>>>
>>> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>
>>>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>>>
>>>>> Hello slony folks,
>>>>>  From my reading I?m guessing (hoping) this isn?t a new problem. I
>>>>>have
>>>>> a simple cluster with one provider replicating to three subscribers.
>>>>>The
>>>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the
>>>>>subscribers
>>>>> (with forwarding enabled) are all showing runaway growth. Looked
>>>>>through
>>>>> the FAQ and I don?t see the node I dropped or any idle transactions
>>>>>as
>>>>> viable culprits. Are there other thoughts on the cause? Can I safely
>>>>> manually delete/truncate some/all of the changelog tables? These
>>>>> replicas are all leaf nodes. I only have forwarding turned on to
>>>>>allow
>>>>> for failover, and my replication rate is the 2 sec default.
>>>>> Thanks in advance for any insights.
>>>>
>>>>What is the output of the sl_status view "on those leaf nodes"?
>>>>
>>>>
>>>>>
>>>>> ams=# select
>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>>>
>>>>>   pg_size_pretty
>>>>>
>>>>> ----------------
>>>>>
>>>>>   75 MB
>>>>>
>>>>> (1 row)
>>>>>
>>>>>
>>>>> ams=# select
>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>>>
>>>>>   pg_size_pretty
>>>>>
>>>>> ----------------
>>>>>
>>>>>   34 GB
>>>>>
>>>>> (1 row)
>>>>>
>>>>>
>>>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>>>>> (select no_id from _ams_cluster.sl_node) or con_received not in
>>>>>(select
>>>>> no_id from _ams_cluster.sl_node);
>>>>>
>>>>>   con_origin | con_received | con_seqno | con_timestamp
>>>>>
>>>>> ------------+--------------+-----------+---------------
>>>>>
>>>>> (0 rows)
>>>>>
>>>>>
>>>>> ams=# select * from pg_stat_activity where current_query like
>>>>>'%IDLE%';
>>>>>
>>>>>   datid | datname | procpid | usesysid |  usename   |
>>>>> application_name      |  client_addr   | client_hostname |
>>>>>client_port |
>>>>>          backend_start         |          xact_start           |
>>>>>    query_start          | waiting |
>>>>>
>>>>>                          current_query
>>>>>
>>>>>
>>>>>-------+---------+---------+----------+------------+------------------
>>>>>-
>>>>>--
>>>>>------+----------------+-----------------+-------------+--------------
>>>>>-
>>>>>--
>>>>>--------------+-------------------------------+-----------------------
>>>>>-
>>>>>--
>>>>>-----+---------+---
>>>>>
>>>>> ----------------------------------------------------------------
>>>>>
>>>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>>>          | 88.221.209.10  |                 |       43328 |
>>>>>2016-01-28
>>>>> 12:24:49.706389+00 |                               | 2016-01-28
>>>>> 13:18:02.427848+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>>>          | 88.221.209.10  |                 |       60112 |
>>>>>2016-01-28
>>>>> 12:47:26.230681+00 |                               | 2016-01-28
>>>>> 13:15:27.744242+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>>>          | 88.221.209.10  |                 |       44302 |
>>>>>2016-01-28
>>>>> 12:47:25.100006+00 |                               | 2016-01-28
>>>>> 13:15:27.936059+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>>>          |                |                 |          -1 |
>>>>>2016-01-28
>>>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>>>> 13:18:37.283992+00 | f       | se
>>>>>
>>>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>>>
>>>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>>>> 61806 | 2016-01-22 01:59:14.800129+00 |
>>>>>|
>>>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>>>> 61805 | 2016-01-22 01:59:14.797655+00 |
>>>>>|
>>>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>>>> 36477 | 2016-01-22 01:56:25.637046+00 |
>>>>>|
>>>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>>>> 51813 | 2016-01-22 01:56:25.240798+00 |
>>>>>|
>>>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>>>> 51803 | 2016-01-22 01:56:22.896388+00 |
>>>>>|
>>>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>>>> 51564 | 2016-01-22 01:55:23.600296+00 |
>>>>>|
>>>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>>>> 36402 | 2016-01-22 01:55:22.964462+00 |
>>>>>|
>>>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>>>> 61795 | 2016-01-22 01:59:12.095052+00 |
>>>>>|
>>>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>>>> 51238 | 2016-01-22 01:54:21.481355+00 |
>>>>>|
>>>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>>>> 36333 | 2016-01-22 01:54:21.500456+00 |
>>>>>|
>>>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>>>> slon.local_monitor        |                |                 |
>>>>> -1 | 2016-01-22 01:54:18.977015+00 |                               |
>>>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4427 |   213867 | ams_slony  | slon.local_sync
>>>>>          |                |                 |          -1 |
>>>>>2016-01-22
>>>>> 01:54:18.976932+00 |                               | 2016-01-28
>>>>> 13:18:36.151998+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>>>> slon.local_cleanup        |                |                 |
>>>>> -1 | 2016-01-22 01:54:18.976842+00 |                               |
>>>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>>>> slon.remoteWorkerThread_4 |                |                 |
>>>>> -1 | 2016-01-22 01:54:18.976783+00 |                               |
>>>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>>>> slon.remoteWorkerThread_1 |                |                 |
>>>>> -1 | 2016-01-22 01:54:18.976548+00 |                               |
>>>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>>>> slon.remoteWorkerThread_3 |                |                 |
>>>>> -1 | 2016-01-22 01:54:18.97647+00  |                               |
>>>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>>   16393 | ams     |    4413 |   213867 | ams_slony  |
>>>>>slon.local_listen
>>>>>          |                |                 |          -1 |
>>>>>2016-01-22
>>>>> 01:54:18.965568+00 |                               | 2016-01-28
>>>>> 13:18:37.096159+00 | f       | <I
>>>>>
>>>>> DLE>
>>>>>
>>>>> (21 rows)
>>>>>
>>>>>
>>>>> ams=#
>>>>>
>>>>>
>>>>>
>>>>> Tom    :-)
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Slony1-general mailing list
>>>>> Slony1-general at lists.slony.info
>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>
>>>>
>>>>
>>>>--
>>>>Jan Wieck
>>>>Senior Software Engineer
>>>>http://slony.info
>>>
>>
>>
>>-- 
>>Jan Wieck
>>Senior Software Engineer
>>http://slony.info
>


From jan at wi3ck.info  Mon Feb  1 21:06:50 2016
From: jan at wi3ck.info (Jan Wieck)
Date: Tue, 2 Feb 2016 00:06:50 -0500
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <D2D50C91.4C7BE%ttignor@akamai.com>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com> <56AA595E.9000706@wi3ck.info>
	<D2D4CCA1.4C458%ttignor@akamai.com>
	<D2D50C91.4C7BE%ttignor@akamai.com>
Message-ID: <56B0396A.7090705@wi3ck.info>

On 02/01/2016 01:24 PM, Tignor, Tom wrote:
> 
> 	Quick update: a couple hours after deleting entries from both sl_log
> tables with txids > 630M, it appears the cleanup thread has taken care of
> business. sl_log_1 is down from 54GB to 24KB.

I still wonder how that happened in the first place.

Was there a sequence of dropping and re-creating a node recently? I
think I have seen cases like this where if a node is re-created with the
same node ID before the cleanup of data, belonging to the dropped node,
had happened everywhere. The "cleanup" I am talking about is basically 2
complete log switches on all nodes after the DROP NODE had replicated
everywhere. That takes at least 20-30 minutes and can in some cases take
hours.


Regards, Jan


> 
> 	Tom    :-)
> 
> 
> On 2/1/16, 9:06 AM, "Tignor, Tom" <ttignor at akamai.com> wrote:
> 
>>
>>	Jan,
>>	Thanks much for all the help. I?ve been looking over logswitch finish and
>>the event and changelog tables. Selecting logswitch_finish() on one of my
>>replicas simply returned ""Slony-I: log switch to sl_log_2 still in
>>progress - sl_log_1 not truncated? All three seem to be in that state with
>>either sl_log_1 or sl_log_2. Looking closer at sl_event and sl_log_2 I?m
>>seeing some strangeness.
>>
>>ams=# select min(pg_catalog.txid_snapshot_xmin(ev_snapshot)),
>>max(pg_catalog.txid_snapshot_xmin(ev_snapshot)) from _ams_cluster.sl_event
>>where ev_origin = 1;
>>    min    |    max
>>-----------+-----------
>> 139136948 | 139204299
>>(1 row)
>>
>>
>>ams=# select min(log_txid), max(log_txid) from _ams_cluster.sl_log_2 where
>>log_origin = 1;
>>    min    |    max
>>-----------+-----------
>> 631532717 | 631661386
>>(1 row)
>>
>>
>>ams=#
>>
>>
>>	So I understand all the txids referenced in sl_event are in the 139M
>>range while all those in sl_log_2 are in the 631M range. Normally, the
>>sl_log txids should be older, shouldn?t they? Do you think I?ve hit a
>>txid-wraparound problem?
>>
>>	Tom    :-)
>>
>>
>>
>>On 1/28/16, 1:09 PM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>
>>>On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>>>>
>>>> 	Output below. They seem to be replicating normally, except for the
>>>>sl_log
>>>> growth.
>>>
>>>Indeed. Is there anything in the slon logs for those nodes that says why
>>>it doesn't finish the log switch?
>>>
>>>Connect to the database as a the slony user.
>>>
>>>To check if a log switch is indeed in progress, do
>>>
>>>     SELECT last_value FROM _ams_cluster.sl_log_status;
>>>
>>>It should be either 2 or 3. If it is 0 or 1, no log switch is in
>>>progress and you can start one with
>>>
>>>     SELECCT _ams_cluster.logswitch_start();
>>>
>>>If it is 2 or 3, then you can do
>>>
>>>     SELECT _ams_cluster.logswitch_finish();
>>>
>>>All these operations are harmless and will only do what is safely
>>>possible. Look at the code of logswitch_finish() to find out how it
>>>determines if the current log switch can be finished. In short, the
>>>cleanup thread is removing events from sl_event that have been confirmed
>>>by all nodes in the cluster. The function logswitch_finish() looks if
>>>there is anything left in sl_event, that belonged to that old log. If so
>>>it will not finish. Running those queries manually you can find out what
>>>that event is that is preventing the switch to finish.
>>>
>>>
>>>
>>>>
>>>>
>>>> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>  |
>>>> st_last_received |      st_last_received_ts      |
>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>
>>>>-----------+-------------+---------------+------------------------------
>>>>-
>>>>+-
>>>>
>>>>-----------------+-------------------------------+----------------------
>>>>-
>>>>--
>>>> ------+-------------------+-----------------
>>>>           2 |           1 |    5000611610 | 2016-01-28
>>>>16:06:37.343826+00 |
>>>>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>           2 |           3 |    5000611610 | 2016-01-28
>>>>16:06:37.343826+00 |
>>>>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
>>>> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>>>>           2 |           4 |    5000611610 | 2016-01-28
>>>>16:06:37.343826+00 |
>>>>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>> (3 rows)
>>>>
>>>>
>>>> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>  |
>>>> st_last_received |      st_last_received_ts      |
>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>
>>>>-----------+-------------+---------------+------------------------------
>>>>-
>>>>+-
>>>>
>>>>-----------------+-------------------------------+----------------------
>>>>-
>>>>--
>>>> ------+-------------------+-----------------
>>>>           3 |           4 |    5000654642 | 2016-01-28
>>>>16:07:05.493455+00 |
>>>>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>           3 |           1 |    5000654642 | 2016-01-28
>>>>16:07:05.493455+00 |
>>>>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>           3 |           2 |    5000654642 | 2016-01-28
>>>>16:07:05.493455+00 |
>>>>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>> (3 rows)
>>>>
>>>>
>>>> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>  |
>>>> st_last_received |      st_last_received_ts      |
>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>
>>>>-----------+-------------+---------------+------------------------------
>>>>-
>>>>+-
>>>>
>>>>-----------------+-------------------------------+----------------------
>>>>-
>>>>--
>>>> ------+-------------------+-----------------
>>>>           4 |           3 |    5000637483 | 2016-01-28
>>>>16:07:32.698809+00 |
>>>>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>           4 |           1 |    5000637483 | 2016-01-28
>>>>16:07:32.698809+00 |
>>>>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>           4 |           2 |    5000637483 | 2016-01-28
>>>>16:07:32.698809+00 |
>>>>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>> (3 rows)
>>>>
>>>>
>>>>
>>>> 	Tom    :-)
>>>>
>>>>
>>>>
>>>> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>>
>>>>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>>>>
>>>>>> Hello slony folks,
>>>>>>  From my reading I?m guessing (hoping) this isn?t a new problem. I
>>>>>>have
>>>>>> a simple cluster with one provider replicating to three subscribers.
>>>>>>The
>>>>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the
>>>>>>subscribers
>>>>>> (with forwarding enabled) are all showing runaway growth. Looked
>>>>>>through
>>>>>> the FAQ and I don?t see the node I dropped or any idle transactions
>>>>>>as
>>>>>> viable culprits. Are there other thoughts on the cause? Can I safely
>>>>>> manually delete/truncate some/all of the changelog tables? These
>>>>>> replicas are all leaf nodes. I only have forwarding turned on to
>>>>>>allow
>>>>>> for failover, and my replication rate is the 2 sec default.
>>>>>> Thanks in advance for any insights.
>>>>>
>>>>>What is the output of the sl_status view "on those leaf nodes"?
>>>>>
>>>>>
>>>>>>
>>>>>> ams=# select
>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>>>>
>>>>>>   pg_size_pretty
>>>>>>
>>>>>> ----------------
>>>>>>
>>>>>>   75 MB
>>>>>>
>>>>>> (1 row)
>>>>>>
>>>>>>
>>>>>> ams=# select
>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>>>>
>>>>>>   pg_size_pretty
>>>>>>
>>>>>> ----------------
>>>>>>
>>>>>>   34 GB
>>>>>>
>>>>>> (1 row)
>>>>>>
>>>>>>
>>>>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>>>>>> (select no_id from _ams_cluster.sl_node) or con_received not in
>>>>>>(select
>>>>>> no_id from _ams_cluster.sl_node);
>>>>>>
>>>>>>   con_origin | con_received | con_seqno | con_timestamp
>>>>>>
>>>>>> ------------+--------------+-----------+---------------
>>>>>>
>>>>>> (0 rows)
>>>>>>
>>>>>>
>>>>>> ams=# select * from pg_stat_activity where current_query like
>>>>>>'%IDLE%';
>>>>>>
>>>>>>   datid | datname | procpid | usesysid |  usename   |
>>>>>> application_name      |  client_addr   | client_hostname |
>>>>>>client_port |
>>>>>>          backend_start         |          xact_start           |
>>>>>>    query_start          | waiting |
>>>>>>
>>>>>>                          current_query
>>>>>>
>>>>>>
>>>>>>-------+---------+---------+----------+------------+------------------
>>>>>>-
>>>>>>--
>>>>>>------+----------------+-----------------+-------------+--------------
>>>>>>-
>>>>>>--
>>>>>>--------------+-------------------------------+-----------------------
>>>>>>-
>>>>>>--
>>>>>>-----+---------+---
>>>>>>
>>>>>> ----------------------------------------------------------------
>>>>>>
>>>>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>>>>          | 88.221.209.10  |                 |       43328 |
>>>>>>2016-01-28
>>>>>> 12:24:49.706389+00 |                               | 2016-01-28
>>>>>> 13:18:02.427848+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>>>>          | 88.221.209.10  |                 |       60112 |
>>>>>>2016-01-28
>>>>>> 12:47:26.230681+00 |                               | 2016-01-28
>>>>>> 13:15:27.744242+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>>>>          | 88.221.209.10  |                 |       44302 |
>>>>>>2016-01-28
>>>>>> 12:47:25.100006+00 |                               | 2016-01-28
>>>>>> 13:15:27.936059+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>>>>          |                |                 |          -1 |
>>>>>>2016-01-28
>>>>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>>>>> 13:18:37.283992+00 | f       | se
>>>>>>
>>>>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>>>>
>>>>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>>>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>>>>> 61806 | 2016-01-22 01:59:14.800129+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>>>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>>>>> 61805 | 2016-01-22 01:59:14.797655+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>>>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>>>>> 36477 | 2016-01-22 01:56:25.637046+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>>>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>>>>> 51813 | 2016-01-22 01:56:25.240798+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>>>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>>>>> 51803 | 2016-01-22 01:56:22.896388+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>>>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>>>>> 51564 | 2016-01-22 01:55:23.600296+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>>>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>>>>> 36402 | 2016-01-22 01:55:22.964462+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>>>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>>>>> 61795 | 2016-01-22 01:59:12.095052+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>>>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>>>>> 51238 | 2016-01-22 01:54:21.481355+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>>>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>>>>> 36333 | 2016-01-22 01:54:21.500456+00 |
>>>>>>|
>>>>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>>>>> slon.local_monitor        |                |                 |
>>>>>> -1 | 2016-01-22 01:54:18.977015+00 |                               |
>>>>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4427 |   213867 | ams_slony  | slon.local_sync
>>>>>>          |                |                 |          -1 |
>>>>>>2016-01-22
>>>>>> 01:54:18.976932+00 |                               | 2016-01-28
>>>>>> 13:18:36.151998+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>>>>> slon.local_cleanup        |                |                 |
>>>>>> -1 | 2016-01-22 01:54:18.976842+00 |                               |
>>>>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>>>>> slon.remoteWorkerThread_4 |                |                 |
>>>>>> -1 | 2016-01-22 01:54:18.976783+00 |                               |
>>>>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>>>>> slon.remoteWorkerThread_1 |                |                 |
>>>>>> -1 | 2016-01-22 01:54:18.976548+00 |                               |
>>>>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>>>>> slon.remoteWorkerThread_3 |                |                 |
>>>>>> -1 | 2016-01-22 01:54:18.97647+00  |                               |
>>>>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>>   16393 | ams     |    4413 |   213867 | ams_slony  |
>>>>>>slon.local_listen
>>>>>>          |                |                 |          -1 |
>>>>>>2016-01-22
>>>>>> 01:54:18.965568+00 |                               | 2016-01-28
>>>>>> 13:18:37.096159+00 | f       | <I
>>>>>>
>>>>>> DLE>
>>>>>>
>>>>>> (21 rows)
>>>>>>
>>>>>>
>>>>>> ams=#
>>>>>>
>>>>>>
>>>>>>
>>>>>> Tom    :-)
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Slony1-general mailing list
>>>>>> Slony1-general at lists.slony.info
>>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>>
>>>>>
>>>>>
>>>>>--
>>>>>Jan Wieck
>>>>>Senior Software Engineer
>>>>>http://slony.info
>>>>
>>>
>>>
>>>--
>>>Jan Wieck
>>>Senior Software Engineer
>>>http://slony.info
>>
> 


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From ttignor at akamai.com  Tue Feb  2 05:06:23 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Tue, 2 Feb 2016 13:06:23 +0000
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <56B0396A.7090705@wi3ck.info>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com> <56AA595E.9000706@wi3ck.info>
	<D2D4CCA1.4C458%ttignor@akamai.com>
	<D2D50C91.4C7BE%ttignor@akamai.com> <56B0396A.7090705@wi3ck.info>
Message-ID: <D2D6122A.4C8E9%ttignor@akamai.com>


	I did drop one of my replicas several weeks ago, though I didn?t recreate
the node. I do have automation to do exactly that, however, if a replica
becomes defective somehow. Seems I?ll need to consider the point of the
two log switches. Is that important even if the dropped node isn?t a
provider for anybody?

	Tom    :-)


On 2/2/16, 12:06 AM, "Jan Wieck" <jan at wi3ck.info> wrote:

>On 02/01/2016 01:24 PM, Tignor, Tom wrote:
>> 
>> 	Quick update: a couple hours after deleting entries from both sl_log
>> tables with txids > 630M, it appears the cleanup thread has taken care
>>of
>> business. sl_log_1 is down from 54GB to 24KB.
>
>I still wonder how that happened in the first place.
>
>Was there a sequence of dropping and re-creating a node recently? I
>think I have seen cases like this where if a node is re-created with the
>same node ID before the cleanup of data, belonging to the dropped node,
>had happened everywhere. The "cleanup" I am talking about is basically 2
>complete log switches on all nodes after the DROP NODE had replicated
>everywhere. That takes at least 20-30 minutes and can in some cases take
>hours.
>
>
>Regards, Jan
>
>
>> 
>> 	Tom    :-)
>> 
>> 
>> On 2/1/16, 9:06 AM, "Tignor, Tom" <ttignor at akamai.com> wrote:
>> 
>>>
>>>	Jan,
>>>	Thanks much for all the help. I?ve been looking over logswitch finish
>>>and
>>>the event and changelog tables. Selecting logswitch_finish() on one of
>>>my
>>>replicas simply returned ""Slony-I: log switch to sl_log_2 still in
>>>progress - sl_log_1 not truncated? All three seem to be in that state
>>>with
>>>either sl_log_1 or sl_log_2. Looking closer at sl_event and sl_log_2 I?m
>>>seeing some strangeness.
>>>
>>>ams=# select min(pg_catalog.txid_snapshot_xmin(ev_snapshot)),
>>>max(pg_catalog.txid_snapshot_xmin(ev_snapshot)) from
>>>_ams_cluster.sl_event
>>>where ev_origin = 1;
>>>    min    |    max
>>>-----------+-----------
>>> 139136948 | 139204299
>>>(1 row)
>>>
>>>
>>>ams=# select min(log_txid), max(log_txid) from _ams_cluster.sl_log_2
>>>where
>>>log_origin = 1;
>>>    min    |    max
>>>-----------+-----------
>>> 631532717 | 631661386
>>>(1 row)
>>>
>>>
>>>ams=#
>>>
>>>
>>>	So I understand all the txids referenced in sl_event are in the 139M
>>>range while all those in sl_log_2 are in the 631M range. Normally, the
>>>sl_log txids should be older, shouldn?t they? Do you think I?ve hit a
>>>txid-wraparound problem?
>>>
>>>	Tom    :-)
>>>
>>>
>>>
>>>On 1/28/16, 1:09 PM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>
>>>>On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>>>>>
>>>>> 	Output below. They seem to be replicating normally, except for the
>>>>>sl_log
>>>>> growth.
>>>>
>>>>Indeed. Is there anything in the slon logs for those nodes that says
>>>>why
>>>>it doesn't finish the log switch?
>>>>
>>>>Connect to the database as a the slony user.
>>>>
>>>>To check if a log switch is indeed in progress, do
>>>>
>>>>     SELECT last_value FROM _ams_cluster.sl_log_status;
>>>>
>>>>It should be either 2 or 3. If it is 0 or 1, no log switch is in
>>>>progress and you can start one with
>>>>
>>>>     SELECCT _ams_cluster.logswitch_start();
>>>>
>>>>If it is 2 or 3, then you can do
>>>>
>>>>     SELECT _ams_cluster.logswitch_finish();
>>>>
>>>>All these operations are harmless and will only do what is safely
>>>>possible. Look at the code of logswitch_finish() to find out how it
>>>>determines if the current log switch can be finished. In short, the
>>>>cleanup thread is removing events from sl_event that have been
>>>>confirmed
>>>>by all nodes in the cluster. The function logswitch_finish() looks if
>>>>there is anything left in sl_event, that belonged to that old log. If
>>>>so
>>>>it will not finish. Running those queries manually you can find out
>>>>what
>>>>that event is that is preventing the switch to finish.
>>>>
>>>>
>>>>
>>>>>
>>>>>
>>>>> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>  |
>>>>> st_last_received |      st_last_received_ts      |
>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>
>>>>>-----------+-------------+---------------+----------------------------
>>>>>--
>>>>>-
>>>>>+-
>>>>>
>>>>>-----------------+-------------------------------+--------------------
>>>>>--
>>>>>-
>>>>>--
>>>>> ------+-------------------+-----------------
>>>>>           2 |           1 |    5000611610 | 2016-01-28
>>>>>16:06:37.343826+00 |
>>>>>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
>>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>>           2 |           3 |    5000611610 | 2016-01-28
>>>>>16:06:37.343826+00 |
>>>>>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
>>>>> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>>>>>           2 |           4 |    5000611610 | 2016-01-28
>>>>>16:06:37.343826+00 |
>>>>>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
>>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>> (3 rows)
>>>>>
>>>>>
>>>>> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>  |
>>>>> st_last_received |      st_last_received_ts      |
>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>
>>>>>-----------+-------------+---------------+----------------------------
>>>>>--
>>>>>-
>>>>>+-
>>>>>
>>>>>-----------------+-------------------------------+--------------------
>>>>>--
>>>>>-
>>>>>--
>>>>> ------+-------------------+-----------------
>>>>>           3 |           4 |    5000654642 | 2016-01-28
>>>>>16:07:05.493455+00 |
>>>>>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>           3 |           1 |    5000654642 | 2016-01-28
>>>>>16:07:05.493455+00 |
>>>>>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>           3 |           2 |    5000654642 | 2016-01-28
>>>>>16:07:05.493455+00 |
>>>>>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>> (3 rows)
>>>>>
>>>>>
>>>>> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>  |
>>>>> st_last_received |      st_last_received_ts      |
>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>
>>>>>-----------+-------------+---------------+----------------------------
>>>>>--
>>>>>-
>>>>>+-
>>>>>
>>>>>-----------------+-------------------------------+--------------------
>>>>>--
>>>>>-
>>>>>--
>>>>> ------+-------------------+-----------------
>>>>>           4 |           3 |    5000637483 | 2016-01-28
>>>>>16:07:32.698809+00 |
>>>>>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>           4 |           1 |    5000637483 | 2016-01-28
>>>>>16:07:32.698809+00 |
>>>>>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>           4 |           2 |    5000637483 | 2016-01-28
>>>>>16:07:32.698809+00 |
>>>>>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>> (3 rows)
>>>>>
>>>>>
>>>>>
>>>>> 	Tom    :-)
>>>>>
>>>>>
>>>>>
>>>>> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>>>
>>>>>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>>>>>
>>>>>>> Hello slony folks,
>>>>>>>  From my reading I?m guessing (hoping) this isn?t a new problem. I
>>>>>>>have
>>>>>>> a simple cluster with one provider replicating to three
>>>>>>>subscribers.
>>>>>>>The
>>>>>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the
>>>>>>>subscribers
>>>>>>> (with forwarding enabled) are all showing runaway growth. Looked
>>>>>>>through
>>>>>>> the FAQ and I don?t see the node I dropped or any idle transactions
>>>>>>>as
>>>>>>> viable culprits. Are there other thoughts on the cause? Can I
>>>>>>>safely
>>>>>>> manually delete/truncate some/all of the changelog tables? These
>>>>>>> replicas are all leaf nodes. I only have forwarding turned on to
>>>>>>>allow
>>>>>>> for failover, and my replication rate is the 2 sec default.
>>>>>>> Thanks in advance for any insights.
>>>>>>
>>>>>>What is the output of the sl_status view "on those leaf nodes"?
>>>>>>
>>>>>>
>>>>>>>
>>>>>>> ams=# select
>>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>>>>>
>>>>>>>   pg_size_pretty
>>>>>>>
>>>>>>> ----------------
>>>>>>>
>>>>>>>   75 MB
>>>>>>>
>>>>>>> (1 row)
>>>>>>>
>>>>>>>
>>>>>>> ams=# select
>>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>>>>>
>>>>>>>   pg_size_pretty
>>>>>>>
>>>>>>> ----------------
>>>>>>>
>>>>>>>   34 GB
>>>>>>>
>>>>>>> (1 row)
>>>>>>>
>>>>>>>
>>>>>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>>>>>>> (select no_id from _ams_cluster.sl_node) or con_received not in
>>>>>>>(select
>>>>>>> no_id from _ams_cluster.sl_node);
>>>>>>>
>>>>>>>   con_origin | con_received | con_seqno | con_timestamp
>>>>>>>
>>>>>>> ------------+--------------+-----------+---------------
>>>>>>>
>>>>>>> (0 rows)
>>>>>>>
>>>>>>>
>>>>>>> ams=# select * from pg_stat_activity where current_query like
>>>>>>>'%IDLE%';
>>>>>>>
>>>>>>>   datid | datname | procpid | usesysid |  usename   |
>>>>>>> application_name      |  client_addr   | client_hostname |
>>>>>>>client_port |
>>>>>>>          backend_start         |          xact_start           |
>>>>>>>    query_start          | waiting |
>>>>>>>
>>>>>>>                          current_query
>>>>>>>
>>>>>>>
>>>>>>>-------+---------+---------+----------+------------+----------------
>>>>>>>--
>>>>>>>-
>>>>>>>--
>>>>>>>------+----------------+-----------------+-------------+------------
>>>>>>>--
>>>>>>>-
>>>>>>>--
>>>>>>>--------------+-------------------------------+---------------------
>>>>>>>--
>>>>>>>-
>>>>>>>--
>>>>>>>-----+---------+---
>>>>>>>
>>>>>>> ----------------------------------------------------------------
>>>>>>>
>>>>>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>>>>>          | 88.221.209.10  |                 |       43328 |
>>>>>>>2016-01-28
>>>>>>> 12:24:49.706389+00 |                               | 2016-01-28
>>>>>>> 13:18:02.427848+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>>>>>          | 88.221.209.10  |                 |       60112 |
>>>>>>>2016-01-28
>>>>>>> 12:47:26.230681+00 |                               | 2016-01-28
>>>>>>> 13:15:27.744242+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>>>>>          | 88.221.209.10  |                 |       44302 |
>>>>>>>2016-01-28
>>>>>>> 12:47:25.100006+00 |                               | 2016-01-28
>>>>>>> 13:15:27.936059+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>>>>>          |                |                 |          -1 |
>>>>>>>2016-01-28
>>>>>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>>>>>> 13:18:37.283992+00 | f       | se
>>>>>>>
>>>>>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>>>>>
>>>>>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>>>>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>>>>>> 61806 | 2016-01-22 01:59:14.800129+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>>>>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>>>>>> 61805 | 2016-01-22 01:59:14.797655+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>>>>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>>>>>> 36477 | 2016-01-22 01:56:25.637046+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>>>>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>>>>>> 51813 | 2016-01-22 01:56:25.240798+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>>>>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>>>>>> 51803 | 2016-01-22 01:56:22.896388+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>>>>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>>>>>> 51564 | 2016-01-22 01:55:23.600296+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>>>>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>>>>>> 36402 | 2016-01-22 01:55:22.964462+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>>>>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>>>>>> 61795 | 2016-01-22 01:59:12.095052+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>>>>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>>>>>> 51238 | 2016-01-22 01:54:21.481355+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>>>>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>>>>>> 36333 | 2016-01-22 01:54:21.500456+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>>>>>> slon.local_monitor        |                |                 |
>>>>>>> -1 | 2016-01-22 01:54:18.977015+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4427 |   213867 | ams_slony  |
>>>>>>>slon.local_sync
>>>>>>>          |                |                 |          -1 |
>>>>>>>2016-01-22
>>>>>>> 01:54:18.976932+00 |                               | 2016-01-28
>>>>>>> 13:18:36.151998+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>>>>>> slon.local_cleanup        |                |                 |
>>>>>>> -1 | 2016-01-22 01:54:18.976842+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>>>>>> slon.remoteWorkerThread_4 |                |                 |
>>>>>>> -1 | 2016-01-22 01:54:18.976783+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>>>>>> slon.remoteWorkerThread_1 |                |                 |
>>>>>>> -1 | 2016-01-22 01:54:18.976548+00 |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>>>>>> slon.remoteWorkerThread_3 |                |                 |
>>>>>>> -1 | 2016-01-22 01:54:18.97647+00  |
>>>>>>>|
>>>>>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>>   16393 | ams     |    4413 |   213867 | ams_slony  |
>>>>>>>slon.local_listen
>>>>>>>          |                |                 |          -1 |
>>>>>>>2016-01-22
>>>>>>> 01:54:18.965568+00 |                               | 2016-01-28
>>>>>>> 13:18:37.096159+00 | f       | <I
>>>>>>>
>>>>>>> DLE>
>>>>>>>
>>>>>>> (21 rows)
>>>>>>>
>>>>>>>
>>>>>>> ams=#
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Tom    :-)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Slony1-general mailing list
>>>>>>> Slony1-general at lists.slony.info
>>>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>>>
>>>>>>
>>>>>>
>>>>>>--
>>>>>>Jan Wieck
>>>>>>Senior Software Engineer
>>>>>>http://slony.info
>>>>>
>>>>
>>>>
>>>>--
>>>>Jan Wieck
>>>>Senior Software Engineer
>>>>http://slony.info
>>>
>> 
>
>
>-- 
>Jan Wieck
>Senior Software Engineer
>http://slony.info


From jan at wi3ck.info  Tue Feb  2 06:25:46 2016
From: jan at wi3ck.info (Jan Wieck)
Date: Tue, 2 Feb 2016 09:25:46 -0500
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <D2D6122A.4C8E9%ttignor@akamai.com>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com> <56AA595E.9000706@wi3ck.info>
	<D2D4CCA1.4C458%ttignor@akamai.com>
	<D2D50C91.4C7BE%ttignor@akamai.com>
	<56B0396A.7090705@wi3ck.info> <D2D6122A.4C8E9%ttignor@akamai.com>
Message-ID: <56B0BC6A.9010303@wi3ck.info>

On 02/02/2016 08:06 AM, Tignor, Tom wrote:
> 
> 	I did drop one of my replicas several weeks ago, though I didn?t recreate
> the node. I do have automation to do exactly that, however, if a replica
> becomes defective somehow. Seems I?ll need to consider the point of the
> two log switches. Is that important even if the dropped node isn?t a
> provider for anybody?

Yes, it is important because even a non-forwarding leaf node is still
producing events that propagate to all other nodes.

You might want to check sl_even on all nodes if there are remnants of
dropped and re-created nodes there. This would be sl_event rows with an
ev_seqno in the future of what that node is currently producing.


Regards, Jan

> 
> 	Tom    :-)
> 
> 
> On 2/2/16, 12:06 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
> 
>>On 02/01/2016 01:24 PM, Tignor, Tom wrote:
>>>
>>> 	Quick update: a couple hours after deleting entries from both sl_log
>>> tables with txids > 630M, it appears the cleanup thread has taken care
>>>of
>>> business. sl_log_1 is down from 54GB to 24KB.
>>
>>I still wonder how that happened in the first place.
>>
>>Was there a sequence of dropping and re-creating a node recently? I
>>think I have seen cases like this where if a node is re-created with the
>>same node ID before the cleanup of data, belonging to the dropped node,
>>had happened everywhere. The "cleanup" I am talking about is basically 2
>>complete log switches on all nodes after the DROP NODE had replicated
>>everywhere. That takes at least 20-30 minutes and can in some cases take
>>hours.
>>
>>
>>Regards, Jan
>>
>>
>>>
>>> 	Tom    :-)
>>>
>>>
>>> On 2/1/16, 9:06 AM, "Tignor, Tom" <ttignor at akamai.com> wrote:
>>>
>>>>
>>>>	Jan,
>>>>	Thanks much for all the help. I?ve been looking over logswitch finish
>>>>and
>>>>the event and changelog tables. Selecting logswitch_finish() on one of
>>>>my
>>>>replicas simply returned ""Slony-I: log switch to sl_log_2 still in
>>>>progress - sl_log_1 not truncated? All three seem to be in that state
>>>>with
>>>>either sl_log_1 or sl_log_2. Looking closer at sl_event and sl_log_2 I?m
>>>>seeing some strangeness.
>>>>
>>>>ams=# select min(pg_catalog.txid_snapshot_xmin(ev_snapshot)),
>>>>max(pg_catalog.txid_snapshot_xmin(ev_snapshot)) from
>>>>_ams_cluster.sl_event
>>>>where ev_origin = 1;
>>>>    min    |    max
>>>>-----------+-----------
>>>> 139136948 | 139204299
>>>>(1 row)
>>>>
>>>>
>>>>ams=# select min(log_txid), max(log_txid) from _ams_cluster.sl_log_2
>>>>where
>>>>log_origin = 1;
>>>>    min    |    max
>>>>-----------+-----------
>>>> 631532717 | 631661386
>>>>(1 row)
>>>>
>>>>
>>>>ams=#
>>>>
>>>>
>>>>	So I understand all the txids referenced in sl_event are in the 139M
>>>>range while all those in sl_log_2 are in the 631M range. Normally, the
>>>>sl_log txids should be older, shouldn?t they? Do you think I?ve hit a
>>>>txid-wraparound problem?
>>>>
>>>>	Tom    :-)
>>>>
>>>>
>>>>
>>>>On 1/28/16, 1:09 PM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>>
>>>>>On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>>>>>>
>>>>>> 	Output below. They seem to be replicating normally, except for the
>>>>>>sl_log
>>>>>> growth.
>>>>>
>>>>>Indeed. Is there anything in the slon logs for those nodes that says
>>>>>why
>>>>>it doesn't finish the log switch?
>>>>>
>>>>>Connect to the database as a the slony user.
>>>>>
>>>>>To check if a log switch is indeed in progress, do
>>>>>
>>>>>     SELECT last_value FROM _ams_cluster.sl_log_status;
>>>>>
>>>>>It should be either 2 or 3. If it is 0 or 1, no log switch is in
>>>>>progress and you can start one with
>>>>>
>>>>>     SELECCT _ams_cluster.logswitch_start();
>>>>>
>>>>>If it is 2 or 3, then you can do
>>>>>
>>>>>     SELECT _ams_cluster.logswitch_finish();
>>>>>
>>>>>All these operations are harmless and will only do what is safely
>>>>>possible. Look at the code of logswitch_finish() to find out how it
>>>>>determines if the current log switch can be finished. In short, the
>>>>>cleanup thread is removing events from sl_event that have been
>>>>>confirmed
>>>>>by all nodes in the cluster. The function logswitch_finish() looks if
>>>>>there is anything left in sl_event, that belonged to that old log. If
>>>>>so
>>>>>it will not finish. Running those queries manually you can find out
>>>>>what
>>>>>that event is that is preventing the switch to finish.
>>>>>
>>>>>
>>>>>
>>>>>>
>>>>>>
>>>>>> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
>>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>>  |
>>>>>> st_last_received |      st_last_received_ts      |
>>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>>
>>>>>>-----------+-------------+---------------+----------------------------
>>>>>>--
>>>>>>-
>>>>>>+-
>>>>>>
>>>>>>-----------------+-------------------------------+--------------------
>>>>>>--
>>>>>>-
>>>>>>--
>>>>>> ------+-------------------+-----------------
>>>>>>           2 |           1 |    5000611610 | 2016-01-28
>>>>>>16:06:37.343826+00 |
>>>>>>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
>>>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>>>           2 |           3 |    5000611610 | 2016-01-28
>>>>>>16:06:37.343826+00 |
>>>>>>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
>>>>>> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>>>>>>           2 |           4 |    5000611610 | 2016-01-28
>>>>>>16:06:37.343826+00 |
>>>>>>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
>>>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>>> (3 rows)
>>>>>>
>>>>>>
>>>>>> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>>  |
>>>>>> st_last_received |      st_last_received_ts      |
>>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>>
>>>>>>-----------+-------------+---------------+----------------------------
>>>>>>--
>>>>>>-
>>>>>>+-
>>>>>>
>>>>>>-----------------+-------------------------------+--------------------
>>>>>>--
>>>>>>-
>>>>>>--
>>>>>> ------+-------------------+-----------------
>>>>>>           3 |           4 |    5000654642 | 2016-01-28
>>>>>>16:07:05.493455+00 |
>>>>>>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
>>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>>           3 |           1 |    5000654642 | 2016-01-28
>>>>>>16:07:05.493455+00 |
>>>>>>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
>>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>>           3 |           2 |    5000654642 | 2016-01-28
>>>>>>16:07:05.493455+00 |
>>>>>>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
>>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>> (3 rows)
>>>>>>
>>>>>>
>>>>>> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>>  |
>>>>>> st_last_received |      st_last_received_ts      |
>>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>>
>>>>>>-----------+-------------+---------------+----------------------------
>>>>>>--
>>>>>>-
>>>>>>+-
>>>>>>
>>>>>>-----------------+-------------------------------+--------------------
>>>>>>--
>>>>>>-
>>>>>>--
>>>>>> ------+-------------------+-----------------
>>>>>>           4 |           3 |    5000637483 | 2016-01-28
>>>>>>16:07:32.698809+00 |
>>>>>>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
>>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>>           4 |           1 |    5000637483 | 2016-01-28
>>>>>>16:07:32.698809+00 |
>>>>>>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
>>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>>           4 |           2 |    5000637483 | 2016-01-28
>>>>>>16:07:32.698809+00 |
>>>>>>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
>>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>> (3 rows)
>>>>>>
>>>>>>
>>>>>>
>>>>>> 	Tom    :-)
>>>>>>
>>>>>>
>>>>>>
>>>>>> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>>>>
>>>>>>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>>>>>>
>>>>>>>> Hello slony folks,
>>>>>>>>  From my reading I?m guessing (hoping) this isn?t a new problem. I
>>>>>>>>have
>>>>>>>> a simple cluster with one provider replicating to three
>>>>>>>>subscribers.
>>>>>>>>The
>>>>>>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the
>>>>>>>>subscribers
>>>>>>>> (with forwarding enabled) are all showing runaway growth. Looked
>>>>>>>>through
>>>>>>>> the FAQ and I don?t see the node I dropped or any idle transactions
>>>>>>>>as
>>>>>>>> viable culprits. Are there other thoughts on the cause? Can I
>>>>>>>>safely
>>>>>>>> manually delete/truncate some/all of the changelog tables? These
>>>>>>>> replicas are all leaf nodes. I only have forwarding turned on to
>>>>>>>>allow
>>>>>>>> for failover, and my replication rate is the 2 sec default.
>>>>>>>> Thanks in advance for any insights.
>>>>>>>
>>>>>>>What is the output of the sl_status view "on those leaf nodes"?
>>>>>>>
>>>>>>>
>>>>>>>>
>>>>>>>> ams=# select
>>>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>>>>>>
>>>>>>>>   pg_size_pretty
>>>>>>>>
>>>>>>>> ----------------
>>>>>>>>
>>>>>>>>   75 MB
>>>>>>>>
>>>>>>>> (1 row)
>>>>>>>>
>>>>>>>>
>>>>>>>> ams=# select
>>>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>>>>>>
>>>>>>>>   pg_size_pretty
>>>>>>>>
>>>>>>>> ----------------
>>>>>>>>
>>>>>>>>   34 GB
>>>>>>>>
>>>>>>>> (1 row)
>>>>>>>>
>>>>>>>>
>>>>>>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>>>>>>>> (select no_id from _ams_cluster.sl_node) or con_received not in
>>>>>>>>(select
>>>>>>>> no_id from _ams_cluster.sl_node);
>>>>>>>>
>>>>>>>>   con_origin | con_received | con_seqno | con_timestamp
>>>>>>>>
>>>>>>>> ------------+--------------+-----------+---------------
>>>>>>>>
>>>>>>>> (0 rows)
>>>>>>>>
>>>>>>>>
>>>>>>>> ams=# select * from pg_stat_activity where current_query like
>>>>>>>>'%IDLE%';
>>>>>>>>
>>>>>>>>   datid | datname | procpid | usesysid |  usename   |
>>>>>>>> application_name      |  client_addr   | client_hostname |
>>>>>>>>client_port |
>>>>>>>>          backend_start         |          xact_start           |
>>>>>>>>    query_start          | waiting |
>>>>>>>>
>>>>>>>>                          current_query
>>>>>>>>
>>>>>>>>
>>>>>>>>-------+---------+---------+----------+------------+----------------
>>>>>>>>--
>>>>>>>>-
>>>>>>>>--
>>>>>>>>------+----------------+-----------------+-------------+------------
>>>>>>>>--
>>>>>>>>-
>>>>>>>>--
>>>>>>>>--------------+-------------------------------+---------------------
>>>>>>>>--
>>>>>>>>-
>>>>>>>>--
>>>>>>>>-----+---------+---
>>>>>>>>
>>>>>>>> ----------------------------------------------------------------
>>>>>>>>
>>>>>>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>>>>>>          | 88.221.209.10  |                 |       43328 |
>>>>>>>>2016-01-28
>>>>>>>> 12:24:49.706389+00 |                               | 2016-01-28
>>>>>>>> 13:18:02.427848+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>>>>>>          | 88.221.209.10  |                 |       60112 |
>>>>>>>>2016-01-28
>>>>>>>> 12:47:26.230681+00 |                               | 2016-01-28
>>>>>>>> 13:15:27.744242+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>>>>>>          | 88.221.209.10  |                 |       44302 |
>>>>>>>>2016-01-28
>>>>>>>> 12:47:25.100006+00 |                               | 2016-01-28
>>>>>>>> 13:15:27.936059+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>>>>>>          |                |                 |          -1 |
>>>>>>>>2016-01-28
>>>>>>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>>>>>>> 13:18:37.283992+00 | f       | se
>>>>>>>>
>>>>>>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>>>>>>
>>>>>>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>>>>>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>>>>>>> 61806 | 2016-01-22 01:59:14.800129+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>>>>>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>>>>>>> 61805 | 2016-01-22 01:59:14.797655+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>>>>>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>>>>>>> 36477 | 2016-01-22 01:56:25.637046+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>>>>>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>>>>>>> 51813 | 2016-01-22 01:56:25.240798+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>>>>>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>>>>>>> 51803 | 2016-01-22 01:56:22.896388+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>>>>>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>>>>>>> 51564 | 2016-01-22 01:55:23.600296+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>>>>>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>>>>>>> 36402 | 2016-01-22 01:55:22.964462+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>>>>>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>>>>>>> 61795 | 2016-01-22 01:59:12.095052+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>>>>>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>>>>>>> 51238 | 2016-01-22 01:54:21.481355+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>>>>>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>>>>>>> 36333 | 2016-01-22 01:54:21.500456+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>>>>>>> slon.local_monitor        |                |                 |
>>>>>>>> -1 | 2016-01-22 01:54:18.977015+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4427 |   213867 | ams_slony  |
>>>>>>>>slon.local_sync
>>>>>>>>          |                |                 |          -1 |
>>>>>>>>2016-01-22
>>>>>>>> 01:54:18.976932+00 |                               | 2016-01-28
>>>>>>>> 13:18:36.151998+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>>>>>>> slon.local_cleanup        |                |                 |
>>>>>>>> -1 | 2016-01-22 01:54:18.976842+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>>>>>>> slon.remoteWorkerThread_4 |                |                 |
>>>>>>>> -1 | 2016-01-22 01:54:18.976783+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>>>>>>> slon.remoteWorkerThread_1 |                |                 |
>>>>>>>> -1 | 2016-01-22 01:54:18.976548+00 |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>>>>>>> slon.remoteWorkerThread_3 |                |                 |
>>>>>>>> -1 | 2016-01-22 01:54:18.97647+00  |
>>>>>>>>|
>>>>>>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>>   16393 | ams     |    4413 |   213867 | ams_slony  |
>>>>>>>>slon.local_listen
>>>>>>>>          |                |                 |          -1 |
>>>>>>>>2016-01-22
>>>>>>>> 01:54:18.965568+00 |                               | 2016-01-28
>>>>>>>> 13:18:37.096159+00 | f       | <I
>>>>>>>>
>>>>>>>> DLE>
>>>>>>>>
>>>>>>>> (21 rows)
>>>>>>>>
>>>>>>>>
>>>>>>>> ams=#
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Tom    :-)
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Slony1-general mailing list
>>>>>>>> Slony1-general at lists.slony.info
>>>>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>--
>>>>>>>Jan Wieck
>>>>>>>Senior Software Engineer
>>>>>>>http://slony.info
>>>>>>
>>>>>
>>>>>
>>>>>--
>>>>>Jan Wieck
>>>>>Senior Software Engineer
>>>>>http://slony.info
>>>>
>>>
>>
>>
>>--
>>Jan Wieck
>>Senior Software Engineer
>>http://slony.info
> 


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From ttignor at akamai.com  Tue Feb  2 06:32:53 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Tue, 2 Feb 2016 14:32:53 +0000
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <56B0BC6A.9010303@wi3ck.info>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com> <56AA595E.9000706@wi3ck.info>
	<D2D4CCA1.4C458%ttignor@akamai.com>
	<D2D50C91.4C7BE%ttignor@akamai.com>
	<56B0396A.7090705@wi3ck.info> <D2D6122A.4C8E9%ttignor@akamai.com>
	<56B0BC6A.9010303@wi3ck.info>
Message-ID: <D2D6281A.4C91B%ttignor@akamai.com>


	Thanks Jan. Nothing in evidence now, but I can check for that in the
future.

	Tom    :-)


On 2/2/16, 9:25 AM, "Jan Wieck" <jan at wi3ck.info> wrote:

>On 02/02/2016 08:06 AM, Tignor, Tom wrote:
>> 
>> 	I did drop one of my replicas several weeks ago, though I didn?t
>>recreate
>> the node. I do have automation to do exactly that, however, if a replica
>> becomes defective somehow. Seems I?ll need to consider the point of the
>> two log switches. Is that important even if the dropped node isn?t a
>> provider for anybody?
>
>Yes, it is important because even a non-forwarding leaf node is still
>producing events that propagate to all other nodes.
>
>You might want to check sl_even on all nodes if there are remnants of
>dropped and re-created nodes there. This would be sl_event rows with an
>ev_seqno in the future of what that node is currently producing.
>
>
>Regards, Jan
>
>> 
>> 	Tom    :-)
>> 
>> 
>> On 2/2/16, 12:06 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>> 
>>>On 02/01/2016 01:24 PM, Tignor, Tom wrote:
>>>>
>>>> 	Quick update: a couple hours after deleting entries from both sl_log
>>>> tables with txids > 630M, it appears the cleanup thread has taken care
>>>>of
>>>> business. sl_log_1 is down from 54GB to 24KB.
>>>
>>>I still wonder how that happened in the first place.
>>>
>>>Was there a sequence of dropping and re-creating a node recently? I
>>>think I have seen cases like this where if a node is re-created with the
>>>same node ID before the cleanup of data, belonging to the dropped node,
>>>had happened everywhere. The "cleanup" I am talking about is basically 2
>>>complete log switches on all nodes after the DROP NODE had replicated
>>>everywhere. That takes at least 20-30 minutes and can in some cases take
>>>hours.
>>>
>>>
>>>Regards, Jan
>>>
>>>
>>>>
>>>> 	Tom    :-)
>>>>
>>>>
>>>> On 2/1/16, 9:06 AM, "Tignor, Tom" <ttignor at akamai.com> wrote:
>>>>
>>>>>
>>>>>	Jan,
>>>>>	Thanks much for all the help. I?ve been looking over logswitch finish
>>>>>and
>>>>>the event and changelog tables. Selecting logswitch_finish() on one of
>>>>>my
>>>>>replicas simply returned ""Slony-I: log switch to sl_log_2 still in
>>>>>progress - sl_log_1 not truncated? All three seem to be in that state
>>>>>with
>>>>>either sl_log_1 or sl_log_2. Looking closer at sl_event and sl_log_2
>>>>>I?m
>>>>>seeing some strangeness.
>>>>>
>>>>>ams=# select min(pg_catalog.txid_snapshot_xmin(ev_snapshot)),
>>>>>max(pg_catalog.txid_snapshot_xmin(ev_snapshot)) from
>>>>>_ams_cluster.sl_event
>>>>>where ev_origin = 1;
>>>>>    min    |    max
>>>>>-----------+-----------
>>>>> 139136948 | 139204299
>>>>>(1 row)
>>>>>
>>>>>
>>>>>ams=# select min(log_txid), max(log_txid) from _ams_cluster.sl_log_2
>>>>>where
>>>>>log_origin = 1;
>>>>>    min    |    max
>>>>>-----------+-----------
>>>>> 631532717 | 631661386
>>>>>(1 row)
>>>>>
>>>>>
>>>>>ams=#
>>>>>
>>>>>
>>>>>	So I understand all the txids referenced in sl_event are in the 139M
>>>>>range while all those in sl_log_2 are in the 631M range. Normally, the
>>>>>sl_log txids should be older, shouldn?t they? Do you think I?ve hit a
>>>>>txid-wraparound problem?
>>>>>
>>>>>	Tom    :-)
>>>>>
>>>>>
>>>>>
>>>>>On 1/28/16, 1:09 PM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>>>
>>>>>>On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>>>>>>>
>>>>>>> 	Output below. They seem to be replicating normally, except for the
>>>>>>>sl_log
>>>>>>> growth.
>>>>>>
>>>>>>Indeed. Is there anything in the slon logs for those nodes that says
>>>>>>why
>>>>>>it doesn't finish the log switch?
>>>>>>
>>>>>>Connect to the database as a the slony user.
>>>>>>
>>>>>>To check if a log switch is indeed in progress, do
>>>>>>
>>>>>>     SELECT last_value FROM _ams_cluster.sl_log_status;
>>>>>>
>>>>>>It should be either 2 or 3. If it is 0 or 1, no log switch is in
>>>>>>progress and you can start one with
>>>>>>
>>>>>>     SELECCT _ams_cluster.logswitch_start();
>>>>>>
>>>>>>If it is 2 or 3, then you can do
>>>>>>
>>>>>>     SELECT _ams_cluster.logswitch_finish();
>>>>>>
>>>>>>All these operations are harmless and will only do what is safely
>>>>>>possible. Look at the code of logswitch_finish() to find out how it
>>>>>>determines if the current log switch can be finished. In short, the
>>>>>>cleanup thread is removing events from sl_event that have been
>>>>>>confirmed
>>>>>>by all nodes in the cluster. The function logswitch_finish() looks if
>>>>>>there is anything left in sl_event, that belonged to that old log. If
>>>>>>so
>>>>>>it will not finish. Running those queries manually you can find out
>>>>>>what
>>>>>>that event is that is preventing the switch to finish.
>>>>>>
>>>>>>
>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
>>>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>>>  |
>>>>>>> st_last_received |      st_last_received_ts      |
>>>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>>>
>>>>>>>-----------+-------------+---------------+--------------------------
>>>>>>>--
>>>>>>>--
>>>>>>>-
>>>>>>>+-
>>>>>>>
>>>>>>>-----------------+-------------------------------+------------------
>>>>>>>--
>>>>>>>--
>>>>>>>-
>>>>>>>--
>>>>>>> ------+-------------------+-----------------
>>>>>>>           2 |           1 |    5000611610 | 2016-01-28
>>>>>>>16:06:37.343826+00 |
>>>>>>>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
>>>>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>>>>           2 |           3 |    5000611610 | 2016-01-28
>>>>>>>16:06:37.343826+00 |
>>>>>>>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
>>>>>>> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>>>>>>>           2 |           4 |    5000611610 | 2016-01-28
>>>>>>>16:06:37.343826+00 |
>>>>>>>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
>>>>>>> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>>>>>>> (3 rows)
>>>>>>>
>>>>>>>
>>>>>>> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>>>  |
>>>>>>> st_last_received |      st_last_received_ts      |
>>>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>>>
>>>>>>>-----------+-------------+---------------+--------------------------
>>>>>>>--
>>>>>>>--
>>>>>>>-
>>>>>>>+-
>>>>>>>
>>>>>>>-----------------+-------------------------------+------------------
>>>>>>>--
>>>>>>>--
>>>>>>>-
>>>>>>>--
>>>>>>> ------+-------------------+-----------------
>>>>>>>           3 |           4 |    5000654642 | 2016-01-28
>>>>>>>16:07:05.493455+00 |
>>>>>>>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
>>>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>>>           3 |           1 |    5000654642 | 2016-01-28
>>>>>>>16:07:05.493455+00 |
>>>>>>>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
>>>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>>>           3 |           2 |    5000654642 | 2016-01-28
>>>>>>>16:07:05.493455+00 |
>>>>>>>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
>>>>>>> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>>>>>>> (3 rows)
>>>>>>>
>>>>>>>
>>>>>>> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
>>>>>>> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>>>>>>>   st_origin | st_received | st_last_event |       st_last_event_ts
>>>>>>>  |
>>>>>>> st_last_received |      st_last_received_ts      |
>>>>>>> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
>>>>>>>
>>>>>>>-----------+-------------+---------------+--------------------------
>>>>>>>--
>>>>>>>--
>>>>>>>-
>>>>>>>+-
>>>>>>>
>>>>>>>-----------------+-------------------------------+------------------
>>>>>>>--
>>>>>>>--
>>>>>>>-
>>>>>>>--
>>>>>>> ------+-------------------+-----------------
>>>>>>>           4 |           3 |    5000637483 | 2016-01-28
>>>>>>>16:07:32.698809+00 |
>>>>>>>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
>>>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>>>           4 |           1 |    5000637483 | 2016-01-28
>>>>>>>16:07:32.698809+00 |
>>>>>>>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
>>>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>>>           4 |           2 |    5000637483 | 2016-01-28
>>>>>>>16:07:32.698809+00 |
>>>>>>>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
>>>>>>> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>>>>>>> (3 rows)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> 	Tom    :-)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>>>>>>>
>>>>>>>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>>>>>>>
>>>>>>>>> Hello slony folks,
>>>>>>>>>  From my reading I?m guessing (hoping) this isn?t a new problem.
>>>>>>>>>I
>>>>>>>>>have
>>>>>>>>> a simple cluster with one provider replicating to three
>>>>>>>>>subscribers.
>>>>>>>>>The
>>>>>>>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the
>>>>>>>>>subscribers
>>>>>>>>> (with forwarding enabled) are all showing runaway growth. Looked
>>>>>>>>>through
>>>>>>>>> the FAQ and I don?t see the node I dropped or any idle
>>>>>>>>>transactions
>>>>>>>>>as
>>>>>>>>> viable culprits. Are there other thoughts on the cause? Can I
>>>>>>>>>safely
>>>>>>>>> manually delete/truncate some/all of the changelog tables? These
>>>>>>>>> replicas are all leaf nodes. I only have forwarding turned on to
>>>>>>>>>allow
>>>>>>>>> for failover, and my replication rate is the 2 sec default.
>>>>>>>>> Thanks in advance for any insights.
>>>>>>>>
>>>>>>>>What is the output of the sl_status view "on those leaf nodes"?
>>>>>>>>
>>>>>>>>
>>>>>>>>>
>>>>>>>>> ams=# select
>>>>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>>>>>>>
>>>>>>>>>   pg_size_pretty
>>>>>>>>>
>>>>>>>>> ----------------
>>>>>>>>>
>>>>>>>>>   75 MB
>>>>>>>>>
>>>>>>>>> (1 row)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ams=# select
>>>>>>>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>>>>>>>
>>>>>>>>>   pg_size_pretty
>>>>>>>>>
>>>>>>>>> ----------------
>>>>>>>>>
>>>>>>>>>   34 GB
>>>>>>>>>
>>>>>>>>> (1 row)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not
>>>>>>>>>in
>>>>>>>>> (select no_id from _ams_cluster.sl_node) or con_received not in
>>>>>>>>>(select
>>>>>>>>> no_id from _ams_cluster.sl_node);
>>>>>>>>>
>>>>>>>>>   con_origin | con_received | con_seqno | con_timestamp
>>>>>>>>>
>>>>>>>>> ------------+--------------+-----------+---------------
>>>>>>>>>
>>>>>>>>> (0 rows)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ams=# select * from pg_stat_activity where current_query like
>>>>>>>>>'%IDLE%';
>>>>>>>>>
>>>>>>>>>   datid | datname | procpid | usesysid |  usename   |
>>>>>>>>> application_name      |  client_addr   | client_hostname |
>>>>>>>>>client_port |
>>>>>>>>>          backend_start         |          xact_start           |
>>>>>>>>>    query_start          | waiting |
>>>>>>>>>
>>>>>>>>>                          current_query
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>-------+---------+---------+----------+------------+--------------
>>>>>>>>>--
>>>>>>>>>--
>>>>>>>>>-
>>>>>>>>>--
>>>>>>>>>------+----------------+-----------------+-------------+----------
>>>>>>>>>--
>>>>>>>>>--
>>>>>>>>>-
>>>>>>>>>--
>>>>>>>>>--------------+-------------------------------+-------------------
>>>>>>>>>--
>>>>>>>>>--
>>>>>>>>>-
>>>>>>>>>--
>>>>>>>>>-----+---------+---
>>>>>>>>>
>>>>>>>>> ----------------------------------------------------------------
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>>>>>>>          | 88.221.209.10  |                 |       43328 |
>>>>>>>>>2016-01-28
>>>>>>>>> 12:24:49.706389+00 |                               | 2016-01-28
>>>>>>>>> 13:18:02.427848+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>>>>>>>          | 88.221.209.10  |                 |       60112 |
>>>>>>>>>2016-01-28
>>>>>>>>> 12:47:26.230681+00 |                               | 2016-01-28
>>>>>>>>> 13:15:27.744242+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>>>>>>>          | 88.221.209.10  |                 |       44302 |
>>>>>>>>>2016-01-28
>>>>>>>>> 12:47:25.100006+00 |                               | 2016-01-28
>>>>>>>>> 13:15:27.936059+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>>>>>>>          |                |                 |          -1 |
>>>>>>>>>2016-01-28
>>>>>>>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>>>>>>>> 13:18:37.283992+00 | f       | se
>>>>>>>>>
>>>>>>>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>>>>>>>> 61806 | 2016-01-22 01:59:14.800129+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>>>>>>>> 61805 | 2016-01-22 01:59:14.797655+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>>>>>>>> 36477 | 2016-01-22 01:56:25.637046+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>>>>>>>> 51813 | 2016-01-22 01:56:25.240798+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>>>>>>>> 51803 | 2016-01-22 01:56:22.896388+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>>>>>>>> 51564 | 2016-01-22 01:55:23.600296+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>>>>>>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>>>>>>>> 36402 | 2016-01-22 01:55:22.964462+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>>>>>>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>>>>>>>> 61795 | 2016-01-22 01:59:12.095052+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>>>>>>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>>>>>>>> 51238 | 2016-01-22 01:54:21.481355+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>>>>>>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>>>>>>>> 36333 | 2016-01-22 01:54:21.500456+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>>>>>>>> slon.local_monitor        |                |                 |
>>>>>>>>> -1 | 2016-01-22 01:54:18.977015+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4427 |   213867 | ams_slony  |
>>>>>>>>>slon.local_sync
>>>>>>>>>          |                |                 |          -1 |
>>>>>>>>>2016-01-22
>>>>>>>>> 01:54:18.976932+00 |                               | 2016-01-28
>>>>>>>>> 13:18:36.151998+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>>>>>>>> slon.local_cleanup        |                |                 |
>>>>>>>>> -1 | 2016-01-22 01:54:18.976842+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>>>>>>>> slon.remoteWorkerThread_4 |                |                 |
>>>>>>>>> -1 | 2016-01-22 01:54:18.976783+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>>>>>>>> slon.remoteWorkerThread_1 |                |                 |
>>>>>>>>> -1 | 2016-01-22 01:54:18.976548+00 |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>>>>>>>> slon.remoteWorkerThread_3 |                |                 |
>>>>>>>>> -1 | 2016-01-22 01:54:18.97647+00  |
>>>>>>>>>|
>>>>>>>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>>   16393 | ams     |    4413 |   213867 | ams_slony  |
>>>>>>>>>slon.local_listen
>>>>>>>>>          |                |                 |          -1 |
>>>>>>>>>2016-01-22
>>>>>>>>> 01:54:18.965568+00 |                               | 2016-01-28
>>>>>>>>> 13:18:37.096159+00 | f       | <I
>>>>>>>>>
>>>>>>>>> DLE>
>>>>>>>>>
>>>>>>>>> (21 rows)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ams=#
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Tom    :-)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Slony1-general mailing list
>>>>>>>>> Slony1-general at lists.slony.info
>>>>>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>--
>>>>>>>>Jan Wieck
>>>>>>>>Senior Software Engineer
>>>>>>>>http://slony.info
>>>>>>>
>>>>>>
>>>>>>
>>>>>>--
>>>>>>Jan Wieck
>>>>>>Senior Software Engineer
>>>>>>http://slony.info
>>>>>
>>>>
>>>
>>>
>>>--
>>>Jan Wieck
>>>Senior Software Engineer
>>>http://slony.info
>> 
>
>
>-- 
>Jan Wieck
>Senior Software Engineer
>http://slony.info


From sungh.lei at gmail.com  Wed Feb  3 20:37:18 2016
From: sungh.lei at gmail.com (Sung Hsin Lei)
Date: Wed, 3 Feb 2016 23:37:18 -0500
Subject: [Slony1-general] Cannot fully drop slony node
Message-ID: <CAHD_kvkQaS+3M-OAaAuPJJEVkQK2D=ooKs+3WXaW2563Tv=4bg@mail.gmail.com>

Hey guys,

I have a cluster with 3 nodes. On the main db, I run the following script:


cluster name = slony_cluster;

node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser
password = slonPASS port = 5432';
node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user =
slonyuser password = slonPASS port = 5432';

DROP NODE ( ID = 3, EVENT NODE = 1 );



I open pdadmin on the main db and I don't see node 3 anymore. However, when
I open pgadmin on the replicated db, I still see node 3. The replicated db
is the one associated with node 3. I run the above script again on the
replicated db but get the following error:


C:\Program Files\PostgreSQL\9.3\bin>slonik drop.txt
debug: waiting for 3,5000000004 on 1
drop.txt:4: PGRES_FATAL_ERROR lock table
"_slony_securithor2".sl_event_lock, "_s
lony_cluster".sl_config_lock;select "_slony_securithor2".dropNode(ARRAY[3]);
  - ERROR:  Slony-I: DROP_NODE cannot initiate on the dropped node


Now I need to setup another node which must have id=3. I run a script on
the main db(the one pgadmin does not show a node 3). The following is the
script that I used to setup the node and the error that I get:


cluster name = slony_cluster;

node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser
password = slonPASS port = 5432';
node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user =
slonyuser password = slonPASS port = 5432';

store node (id=3, comment = 'Slave node 3', event node=1);
store path (server = 1, client = 3,
conninfo='dbname=dbNAME host=172.16.10.3 user=slonyuser password = slonPASS
port = 5432');
store path (server = 3, client = 1,
conninfo='dbname=dbNAME host=172.16.10.4 user=slonyuser password = slonPASS
port = 5432');

subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);





C:\Program Files\PostgreSQL\9.3\bin>slonik create.txt
drop.txt:6: Error: namespace "_slony_cluster" already exists in database of
node 3



Is there another way to drop nodes? Can I recover from this without
dropping the cluster and restarting from scratch?


Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160203/3dd6bbea/attachment.htm 

From ttignor at akamai.com  Thu Feb  4 05:58:06 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 4 Feb 2016 13:58:06 +0000
Subject: [Slony1-general] Cannot fully drop slony node
In-Reply-To: <CAHD_kvkQaS+3M-OAaAuPJJEVkQK2D=ooKs+3WXaW2563Tv=4bg@mail.gmail.com>
References: <CAHD_kvkQaS+3M-OAaAuPJJEVkQK2D=ooKs+3WXaW2563Tv=4bg@mail.gmail.com>
Message-ID: <D2D8C231.4CC58%ttignor@akamai.com>


If I'm reading right, did you run the drop node op at some point on node 1 and see it succeed? If it did, the sl_node table on each other node in the cluster (save perhaps node 3) should show it gone.
If that's the case, your cluster is fine and you can just run 'DROP SCHEMA mycluster CASCADE' on node 3 and then retry your store node script.

Tom    :-)


From: Sung Hsin Lei <sungh.lei at gmail.com<mailto:sungh.lei at gmail.com>>
Date: Wednesday, February 3, 2016 at 11:37 PM
To: slony <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Subject: [Slony1-general] Cannot fully drop slony node

Hey guys,

I have a cluster with 3 nodes. On the main db, I run the following script:


cluster name = slony_cluster;

node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser password = slonPASS port = 5432';
node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user = slonyuser password = slonPASS port = 5432';

DROP NODE ( ID = 3, EVENT NODE = 1 );



I open pdadmin on the main db and I don't see node 3 anymore. However, when I open pgadmin on the replicated db, I still see node 3. The replicated db is the one associated with node 3. I run the above script again on the replicated db but get the following error:


C:\Program Files\PostgreSQL\9.3\bin>slonik drop.txt
debug: waiting for 3,5000000004 on 1
drop.txt:4: PGRES_FATAL_ERROR lock table "_slony_securithor2".sl_event_lock, "_s
lony_cluster".sl_config_lock;select "_slony_securithor2".dropNode(ARRAY[3]);
  - ERROR:  Slony-I: DROP_NODE cannot initiate on the dropped node


Now I need to setup another node which must have id=3. I run a script on the main db(the one pgadmin does not show a node 3). The following is the script that I used to setup the node and the error that I get:


cluster name = slony_cluster;

node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser password = slonPASS port = 5432';
node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user = slonyuser password = slonPASS port = 5432';

store node (id=3, comment = 'Slave node 3', event node=1);
store path (server = 1, client = 3, conninfo='dbname=dbNAME host=172.16.10.3 user=slonyuser password = slonPASS port = 5432');
store path (server = 3, client = 1, conninfo='dbname=dbNAME host=172.16.10.4 user=slonyuser password = slonPASS port = 5432');

subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);





C:\Program Files\PostgreSQL\9.3\bin>slonik create.txt
drop.txt:6: Error: namespace "_slony_cluster" already exists in database of
node 3



Is there another way to drop nodes? Can I recover from this without dropping the cluster and restarting from scratch?


Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160204/80f3a20b/attachment.htm 

From sungh.lei at gmail.com  Thu Feb  4 06:48:14 2016
From: sungh.lei at gmail.com (Sung Hsin Lei)
Date: Thu, 4 Feb 2016 09:48:14 -0500
Subject: [Slony1-general] Cannot fully drop slony node
In-Reply-To: <D2D8C231.4CC58%ttignor@akamai.com>
References: <CAHD_kvkQaS+3M-OAaAuPJJEVkQK2D=ooKs+3WXaW2563Tv=4bg@mail.gmail.com>
	<D2D8C231.4CC58%ttignor@akamai.com>
Message-ID: <CAHD_kvmaefZz=pd1ogsSdtJnZza-7Da7JvUSfsEuNkZ4zpiLGQ@mail.gmail.com>

yes... that's it!!

On Thu, Feb 4, 2016 at 8:58 AM, Tignor, Tom <ttignor at akamai.com> wrote:

>
> If I?m reading right, did you run the drop node op at some point on node 1
> and see it succeed? If it did, the sl_node table on each other node in the
> cluster (save perhaps node 3) should show it gone.
> If that?s the case, your cluster is fine and you can just run ?DROP SCHEMA
> mycluster CASCADE? on node 3 and then retry your store node script.
>
> Tom    :-)
>
>
> From: Sung Hsin Lei <sungh.lei at gmail.com>
> Date: Wednesday, February 3, 2016 at 11:37 PM
> To: slony <slony1-general at lists.slony.info>
> Subject: [Slony1-general] Cannot fully drop slony node
>
> Hey guys,
>
> I have a cluster with 3 nodes. On the main db, I run the following script:
>
>
> cluster name = slony_cluster;
>
> node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser
> password = slonPASS port = 5432';
> node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user =
> slonyuser password = slonPASS port = 5432';
>
> DROP NODE ( ID = 3, EVENT NODE = 1 );
>
>
>
> I open pdadmin on the main db and I don't see node 3 anymore. However,
> when I open pgadmin on the replicated db, I still see node 3. The
> replicated db is the one associated with node 3. I run the above script
> again on the replicated db but get the following error:
>
>
> C:\Program Files\PostgreSQL\9.3\bin>slonik drop.txt
> debug: waiting for 3,5000000004 on 1
> drop.txt:4: PGRES_FATAL_ERROR lock table
> "_slony_securithor2".sl_event_lock, "_s
> lony_cluster".sl_config_lock;select
> "_slony_securithor2".dropNode(ARRAY[3]);
>   - ERROR:  Slony-I: DROP_NODE cannot initiate on the dropped node
>
>
> Now I need to setup another node which must have id=3. I run a script on
> the main db(the one pgadmin does not show a node 3). The following is the
> script that I used to setup the node and the error that I get:
>
>
> cluster name = slony_cluster;
>
> node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser
> password = slonPASS port = 5432';
> node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user =
> slonyuser password = slonPASS port = 5432';
>
> store node (id=3, comment = 'Slave node 3', event node=1);
> store path (server = 1, client = 3,
> conninfo='dbname=dbNAME host=172.16.10.3 user=slonyuser password = slonPASS
> port = 5432');
> store path (server = 3, client = 1,
> conninfo='dbname=dbNAME host=172.16.10.4 user=slonyuser password = slonPASS
> port = 5432');
>
> subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);
>
>
>
>
>
> C:\Program Files\PostgreSQL\9.3\bin>slonik create.txt
> drop.txt:6: Error: namespace "_slony_cluster" already exists in database of
> node 3
>
>
>
> Is there another way to drop nodes? Can I recover from this without
> dropping the cluster and restarting from scratch?
>
>
> Thanks.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160204/f335c6ad/attachment.htm 

From sungh.lei at gmail.com  Thu Feb  4 07:24:11 2016
From: sungh.lei at gmail.com (Sung Hsin Lei)
Date: Thu, 4 Feb 2016 10:24:11 -0500
Subject: [Slony1-general] Cannot fully drop slony node
In-Reply-To: <CAHD_kvmaefZz=pd1ogsSdtJnZza-7Da7JvUSfsEuNkZ4zpiLGQ@mail.gmail.com>
References: <CAHD_kvkQaS+3M-OAaAuPJJEVkQK2D=ooKs+3WXaW2563Tv=4bg@mail.gmail.com>
	<D2D8C231.4CC58%ttignor@akamai.com>
	<CAHD_kvmaefZz=pd1ogsSdtJnZza-7Da7JvUSfsEuNkZ4zpiLGQ@mail.gmail.com>
Message-ID: <CAHD_kv=zNg0u2Jz2v_wGHXZj3TEPBGSzXMPpcAygbe3c6UWKTQ@mail.gmail.com>

One more question,

After I re-created node 3 and run(on replicated db):


slon slony_Securithor2 "dbname = dbNAME user = slonyuser password =
slonPASS port = 5432"


I get:


2016-02-04 17:15:05 GTB Standard Time FATAL  main: Node is not initialized
prope
rly - sleep 10s


slon then stops after 10 seconds. Any idea what happened?

Thanks again.

On Thu, Feb 4, 2016 at 9:48 AM, Sung Hsin Lei <sungh.lei at gmail.com> wrote:

> yes... that's it!!
>
> On Thu, Feb 4, 2016 at 8:58 AM, Tignor, Tom <ttignor at akamai.com> wrote:
>
>>
>> If I?m reading right, did you run the drop node op at some point on node
>> 1 and see it succeed? If it did, the sl_node table on each other node in
>> the cluster (save perhaps node 3) should show it gone.
>> If that?s the case, your cluster is fine and you can just run ?DROP
>> SCHEMA mycluster CASCADE? on node 3 and then retry your store node script.
>>
>> Tom    :-)
>>
>>
>> From: Sung Hsin Lei <sungh.lei at gmail.com>
>> Date: Wednesday, February 3, 2016 at 11:37 PM
>> To: slony <slony1-general at lists.slony.info>
>> Subject: [Slony1-general] Cannot fully drop slony node
>>
>> Hey guys,
>>
>> I have a cluster with 3 nodes. On the main db, I run the following script:
>>
>>
>> cluster name = slony_cluster;
>>
>> node 1 admin conninfo = 'dbname = dbNAME host = localhost user =
>> slonyuser password = slonPASS port = 5432';
>> node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user =
>> slonyuser password = slonPASS port = 5432';
>>
>> DROP NODE ( ID = 3, EVENT NODE = 1 );
>>
>>
>>
>> I open pdadmin on the main db and I don't see node 3 anymore. However,
>> when I open pgadmin on the replicated db, I still see node 3. The
>> replicated db is the one associated with node 3. I run the above script
>> again on the replicated db but get the following error:
>>
>>
>> C:\Program Files\PostgreSQL\9.3\bin>slonik drop.txt
>> debug: waiting for 3,5000000004 on 1
>> drop.txt:4: PGRES_FATAL_ERROR lock table
>> "_slony_securithor2".sl_event_lock, "_s
>> lony_cluster".sl_config_lock;select
>> "_slony_securithor2".dropNode(ARRAY[3]);
>>   - ERROR:  Slony-I: DROP_NODE cannot initiate on the dropped node
>>
>>
>> Now I need to setup another node which must have id=3. I run a script on
>> the main db(the one pgadmin does not show a node 3). The following is the
>> script that I used to setup the node and the error that I get:
>>
>>
>> cluster name = slony_cluster;
>>
>> node 1 admin conninfo = 'dbname = dbNAME host = localhost user =
>> slonyuser password = slonPASS port = 5432';
>> node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user =
>> slonyuser password = slonPASS port = 5432';
>>
>> store node (id=3, comment = 'Slave node 3', event node=1);
>> store path (server = 1, client = 3,
>> conninfo='dbname=dbNAME host=172.16.10.3 user=slonyuser password = slonPASS
>> port = 5432');
>> store path (server = 3, client = 1,
>> conninfo='dbname=dbNAME host=172.16.10.4 user=slonyuser password = slonPASS
>> port = 5432');
>>
>> subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);
>>
>>
>>
>>
>>
>> C:\Program Files\PostgreSQL\9.3\bin>slonik create.txt
>> drop.txt:6: Error: namespace "_slony_cluster" already exists in database
>> of
>> node 3
>>
>>
>>
>> Is there another way to drop nodes? Can I recover from this without
>> dropping the cluster and restarting from scratch?
>>
>>
>> Thanks.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160204/393d9902/attachment-0001.htm 

From ttignor at akamai.com  Thu Feb  4 07:35:49 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 4 Feb 2016 15:35:49 +0000
Subject: [Slony1-general] Cannot fully drop slony node
In-Reply-To: <CAHD_kv=zNg0u2Jz2v_wGHXZj3TEPBGSzXMPpcAygbe3c6UWKTQ@mail.gmail.com>
References: <CAHD_kvkQaS+3M-OAaAuPJJEVkQK2D=ooKs+3WXaW2563Tv=4bg@mail.gmail.com>
	<D2D8C231.4CC58%ttignor@akamai.com>
	<CAHD_kvmaefZz=pd1ogsSdtJnZza-7Da7JvUSfsEuNkZ4zpiLGQ@mail.gmail.com>
	<CAHD_kv=zNg0u2Jz2v_wGHXZj3TEPBGSzXMPpcAygbe3c6UWKTQ@mail.gmail.com>
Message-ID: <D2D8D917.4CC76%ttignor@akamai.com>


Hmm. Hard to say. I recall some prior advice on putting subscribe ops in their own separate scripts...
After any op, you can check the local sl_node, sl_set, sl_subscribe, sl_path to see that particular node's view of the universe. The path info between subscribers and their providers of course is important. For new nodes not yet setup with subscriptions, it may be most expedient to drop a problem node and start again (as you already have.)

Tom    :-)

From: Sung Hsin Lei <sungh.lei at gmail.com<mailto:sungh.lei at gmail.com>>
Date: Thursday, February 4, 2016 at 10:24 AM
To: Tom Tignor <ttignor at akamai.com<mailto:ttignor at akamai.com>>
Cc: slony <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Subject: Re: [Slony1-general] Cannot fully drop slony node

One more question,

After I re-created node 3 and run(on replicated db):


slon slony_Securithor2 "dbname = dbNAME user = slonyuser password = slonPASS port = 5432"


I get:


2016-02-04 17:15:05 GTB Standard Time FATAL  main: Node is not initialized prope
rly - sleep 10s


slon then stops after 10 seconds. Any idea what happened?

Thanks again.

On Thu, Feb 4, 2016 at 9:48 AM, Sung Hsin Lei <sungh.lei at gmail.com<mailto:sungh.lei at gmail.com>> wrote:
yes... that's it!!

On Thu, Feb 4, 2016 at 8:58 AM, Tignor, Tom <ttignor at akamai.com<mailto:ttignor at akamai.com>> wrote:

If I'm reading right, did you run the drop node op at some point on node 1 and see it succeed? If it did, the sl_node table on each other node in the cluster (save perhaps node 3) should show it gone.
If that's the case, your cluster is fine and you can just run 'DROP SCHEMA mycluster CASCADE' on node 3 and then retry your store node script.

Tom    :-)


From: Sung Hsin Lei <sungh.lei at gmail.com<mailto:sungh.lei at gmail.com>>
Date: Wednesday, February 3, 2016 at 11:37 PM
To: slony <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Subject: [Slony1-general] Cannot fully drop slony node

Hey guys,

I have a cluster with 3 nodes. On the main db, I run the following script:


cluster name = slony_cluster;

node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser password = slonPASS port = 5432';
node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user = slonyuser password = slonPASS port = 5432';

DROP NODE ( ID = 3, EVENT NODE = 1 );



I open pdadmin on the main db and I don't see node 3 anymore. However, when I open pgadmin on the replicated db, I still see node 3. The replicated db is the one associated with node 3. I run the above script again on the replicated db but get the following error:


C:\Program Files\PostgreSQL\9.3\bin>slonik drop.txt
debug: waiting for 3,5000000004 on 1
drop.txt:4: PGRES_FATAL_ERROR lock table "_slony_securithor2".sl_event_lock, "_s
lony_cluster".sl_config_lock;select "_slony_securithor2".dropNode(ARRAY[3]);
  - ERROR:  Slony-I: DROP_NODE cannot initiate on the dropped node


Now I need to setup another node which must have id=3. I run a script on the main db(the one pgadmin does not show a node 3). The following is the script that I used to setup the node and the error that I get:


cluster name = slony_cluster;

node 1 admin conninfo = 'dbname = dbNAME host = localhost user = slonyuser password = slonPASS port = 5432';
node 3 admin conninfo = 'dbname = dbNAME host = 172.16.10.4 user = slonyuser password = slonPASS port = 5432';

store node (id=3, comment = 'Slave node 3', event node=1);
store path (server = 1, client = 3, conninfo='dbname=dbNAME host=172.16.10.3 user=slonyuser password = slonPASS port = 5432');
store path (server = 3, client = 1, conninfo='dbname=dbNAME host=172.16.10.4 user=slonyuser password = slonPASS port = 5432');

subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);





C:\Program Files\PostgreSQL\9.3\bin>slonik create.txt
drop.txt:6: Error: namespace "_slony_cluster" already exists in database of
node 3



Is there another way to drop nodes? Can I recover from this without dropping the cluster and restarting from scratch?


Thanks.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160204/2b1f25a8/attachment.htm 

From gvasquez at waypoint.cl  Fri Feb  5 10:15:51 2016
From: gvasquez at waypoint.cl (=?utf-8?Q?=22Gonzalo_V=C3=A1squez_=40=C2=A0Waypoint=22?=)
Date: Fri, 5 Feb 2016 15:15:51 -0300
Subject: [Slony1-general] Pending synch size?
Message-ID: <713EE9A0-9E18-4E67-81F8-C17B88B3CBA8@waypoint.cl>

Is there some way to actually see how much data is still pending to be synch between nodes?

Atentamente / Regards,

Gonzalo V?squez S?ez
gvasquez at waypoint.cl <mailto:gvasquez at waypoint.cl>
+56 (2) 2963 4180
Director I+D / R&D Director
Waypoint Telecomunicaciones S.A.

Alfredo Barros Err?zuriz 1953 Of. 1004
7500550
Providencia, Santiago, Chile
Mapcode: R3.BR










-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160205/54ca99f8/attachment.htm 

From asad68 at gmail.com  Tue Feb  9 12:46:34 2016
From: asad68 at gmail.com (Asad Shah)
Date: Tue, 9 Feb 2016 15:46:34 -0500
Subject: [Slony1-general] Is slony.info down?
Message-ID: <CAGAi+v1tcaNkfVc-0kK7BmJgi8ATw3DvsLoaRP+HN43CAtdfig@mail.gmail.com>

I cannot get to the slony site right now?

Is it down?

Regards,
Asad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160209/bf5a1052/attachment.html 

From steve at ssinger.info  Wed Feb 10 17:58:23 2016
From: steve at ssinger.info (Steve Singer)
Date: Wed, 10 Feb 2016 20:58:23 -0500 (EST)
Subject: [Slony1-general] Is slony.info down?
In-Reply-To: <CAGAi+v1tcaNkfVc-0kK7BmJgi8ATw3DvsLoaRP+HN43CAtdfig@mail.gmail.com>
References: <CAGAi+v1tcaNkfVc-0kK7BmJgi8ATw3DvsLoaRP+HN43CAtdfig@mail.gmail.com>
Message-ID: <alpine.DEB.2.02.1602102057440.4720@mini.atlantida>

On Tue, 9 Feb 2016, Asad Shah wrote:

> I cannot get to the slony site right now?
> Is it down?

Working for me right now, but it could have been down.

> 
> Regards,
> Asad
> 
> 
> 
>


From ttignor at akamai.com  Thu Feb 11 05:54:55 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 11 Feb 2016 13:54:55 +0000
Subject: [Slony1-general] Is slony.info down?
In-Reply-To: <alpine.DEB.2.02.1602102057440.4720@mini.atlantida>
References: <CAGAi+v1tcaNkfVc-0kK7BmJgi8ATw3DvsLoaRP+HN43CAtdfig@mail.gmail.com>
	<alpine.DEB.2.02.1602102057440.4720@mini.atlantida>
Message-ID: <D2E1FCB9.4D885%ttignor@akamai.com>


	I also noticed it down yesterday morning.

	Tom    :-)


On 2/10/16, 8:58 PM, "Steve Singer" <steve at ssinger.info> wrote:

>On Tue, 9 Feb 2016, Asad Shah wrote:
>
>> I cannot get to the slony site right now?
>> Is it down?
>
>Working for me right now, but it could have been down.
>
>> 
>> Regards,
>> Asad
>> 
>> 
>> 
>>
>
>_______________________________________________
>Slony1-general mailing list
>Slony1-general at lists.slony.info
>http://lists.slony.info/mailman/listinfo/slony1-general


From cbbrowne at afilias.info  Thu Feb 11 07:14:54 2016
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 11 Feb 2016 10:14:54 -0500
Subject: [Slony1-general] Is slony.info down?
In-Reply-To: <alpine.DEB.2.02.1602102057440.4720@mini.atlantida>
References: <CAGAi+v1tcaNkfVc-0kK7BmJgi8ATw3DvsLoaRP+HN43CAtdfig@mail.gmail.com>
	<alpine.DEB.2.02.1602102057440.4720@mini.atlantida>
Message-ID: <CANfbgbajsrLqF6E7A3_q4D6hOBGtDvPOUp43mu5fO32bqNxc0w@mail.gmail.com>

The site was down for a while, not quite sure how long.  I reckon a VM got
rebooted.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160211/06dc0fe8/attachment.htm 

From rob.brucks at rackspace.com  Fri Feb 26 12:51:14 2016
From: rob.brucks at rackspace.com (Rob Brucks)
Date: Fri, 26 Feb 2016 20:51:14 +0000
Subject: [Slony1-general] Replication Lag?
Message-ID: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>

I have a fairly simple test cluster, three PG instances running on different ports:  a master and two slave DBs both subscribed to the master.  Everything is running on the same server (it's a playground), so I have three PG instances (ports 5432, 5433, 5434; connecting via sockets in /tmp) and a slony daemon for each instance.

I'm running Postgres 9.3.9 and slony1 2.2.4 on Centos 6.7 x86_64.  Both postgres and slony were installed via yum using the PGDG repo.

I used the config below to initialize a very simple replication setup of only one table and one sequence.

My problem is that if I shut down just one slave postgres instance then sl_status on the master instance shows replication stalling on both slave DBs, instead of just one.

But, if I insert some test data into the master DB, I see the data show up on the remaining active slave.  So replication to the remaining slave DB is obviously working.

We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.  The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.

Why does sl_status report lag on the active slave even though replication appears to be working fine?

Do I have a misconfiguration somewhere?

Thanks,
Rob


Here's my slony config:


      CLUSTER NAME = slony;
      NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
      NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
      NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';

############ CLUSTERS

      INIT CLUSTER (ID = 1, COMMENT = 'Master');


############ NODES

      STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
      STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);


############ PATHS

      STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
      STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
      STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
      STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
      STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
      STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');


############ SETS

      CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');

############ SEQUENCES

      SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');

############ TABLES

      SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');

############ SUBSCRIPTIONS

      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);



From melvin6925 at yahoo.com  Fri Feb 26 18:19:34 2016
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Sat, 27 Feb 2016 02:19:34 +0000 (UTC)
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
References: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
Message-ID: <1081050347.118265.1456539574996.JavaMail.yahoo@mail.yahoo.com>




      From: Rob Brucks <rob.brucks at rackspace.com>
 To: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
 Sent: Friday, February 26, 2016 3:51 PM
 Subject: [Slony1-general] Replication Lag?
   
I have a fairly simple test cluster, three PG instances running on different ports:? a master and two slave DBs both subscribed to the master.? Everything is running on the same server (it's a playground), so I have three PG instances (ports 5432, 5433, 5434; connecting via sockets in /tmp) and a slony daemon for each instance.

I'm running Postgres 9.3.9 and slony1 2.2.4 on Centos 6.7 x86_64.? Both postgres and slony were installed via yum using the PGDG repo.

I used the config below to initialize a very simple replication setup of only one table and one sequence.

My problem is that if I shut down just one slave postgres instance then sl_status on the master instance shows replication stalling on both slave DBs, instead of just one.

But, if I insert some test data into the master DB, I see the data show up on the remaining active slave.? So replication to the remaining slave DB is obviously working.

We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.? The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.

Why does sl_status report lag on the active slave even though replication appears to be working fine?

Do I have a misconfiguration somewhere?

Thanks,
Rob


Here's my slony config:


? ? ? CLUSTER NAME = slony;
? ? ? NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
? ? ? NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
? ? ? NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';

############ CLUSTERS

? ? ? INIT CLUSTER (ID = 1, COMMENT = 'Master');


############ NODES

? ? ? STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
? ? ? STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);


############ PATHS

? ? ? STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
? ? ? STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
? ? ? STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
? ? ? STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
? ? ? STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
? ? ? STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');


############ SETS

? ? ? CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');

############ SEQUENCES

? ? ? SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');

############ TABLES

? ? ? SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');

############ SUBSCRIPTIONS

? ? ? SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
? ? ? SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);

_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


========================================================================================================The config looks good.On which server are you running the slon process?
What does your query for sl_status look like?

Melvin Davidson 
  ??? Cell 720-320-0155 

I reserve the right to fantasize.? Whether or not you 
 wish to share my fantasy is entirely up to you. 
www.youtube.com/unusedhero
 Folk Alley - All Folk - 24 Hours a day 
www.folkalley.com


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160227/bc81f671/attachment.htm 

From steve at ssinger.info  Fri Feb 26 18:38:55 2016
From: steve at ssinger.info (Steve Singer)
Date: Fri, 26 Feb 2016 21:38:55 -0500 (EST)
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
References: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
Message-ID: <alpine.DEB.2.02.1602262135270.4720@mini.atlantida>

On Fri, 26 Feb 2016, Rob Brucks wrote:

>
> But, if I insert some test data into the master DB, I see the data show up 
> on the remaining active slave.  So replication to the remaining slave DB 
> is obviously working.

Replication with slony has two parts

1. Does the data replicate from the origin to the subscriber. When this 
happens a row is added to the subscriber's sl_event table with 
ev_origin=$origin_node and a confirm is added to the subscribers sl_confirm 
table with con_origin=$origin_node and con_received=$subscriber_node

2. The sl_confirm row mentioned above needs to then get picked up by the 
slon for the origin node and brought back from the subscriber to the origin.

Are your confirms making it back?


>
> We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.  The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.
>
> Why does sl_status report lag on the active slave even though replication appears to be working fine?
>
> Do I have a misconfiguration somewhere?
>
> Thanks,
> Rob
>
>
> Here's my slony config:
>
>
>      CLUSTER NAME = slony;
>      NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
>      NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
>      NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';
>
> ############ CLUSTERS
>
>      INIT CLUSTER (ID = 1, COMMENT = 'Master');
>
>
> ############ NODES
>
>      STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
>      STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);
>
>
> ############ PATHS
>
>      STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>      STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>      STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>      STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>      STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>      STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>
>
> ############ SETS
>
>      CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');
>
> ############ SEQUENCES
>
>      SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');
>
> ############ TABLES
>
>      SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');
>
> ############ SUBSCRIPTIONS
>
>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From rob.brucks at rackspace.com  Mon Feb 29 07:49:47 2016
From: rob.brucks at rackspace.com (Rob Brucks)
Date: Mon, 29 Feb 2016 15:49:47 +0000
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <alpine.DEB.2.02.1602262135270.4720@mini.atlantida>
References: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
	<alpine.DEB.2.02.1602262135270.4720@mini.atlantida>
Message-ID: <73E68821-04FC-432E-8D10-CAE3B568B573@rackspace.com>

1. Yes, the sl_confirm data is showing up on the subscriber.

2. No, the origin node is not getting back the sl_confirm data from the active subscriber.


Thanks,
Rob



On 2/26/16, 8:38 PM, "Steve Singer" <steve at ssinger.info> wrote:

>On Fri, 26 Feb 2016, Rob Brucks wrote:
>
>>
>> But, if I insert some test data into the master DB, I see the data show up 
>> on the remaining active slave.  So replication to the remaining slave DB 
>> is obviously working.
>
>Replication with slony has two parts
>
>1. Does the data replicate from the origin to the subscriber. When this 
>happens a row is added to the subscriber's sl_event table with 
>ev_origin=$origin_node and a confirm is added to the subscribers sl_confirm 
>table with con_origin=$origin_node and con_received=$subscriber_node
>
>2. The sl_confirm row mentioned above needs to then get picked up by the 
>slon for the origin node and brought back from the subscriber to the origin.
>
>Are your confirms making it back?
>
>
>>
>> We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.  The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.
>>
>> Why does sl_status report lag on the active slave even though replication appears to be working fine?
>>
>> Do I have a misconfiguration somewhere?
>>
>> Thanks,
>> Rob
>>
>>
>> Here's my slony config:
>>
>>
>>      CLUSTER NAME = slony;
>>      NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
>>      NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
>>      NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';
>>
>> ############ CLUSTERS
>>
>>      INIT CLUSTER (ID = 1, COMMENT = 'Master');
>>
>>
>> ############ NODES
>>
>>      STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
>>      STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);
>>
>>
>> ############ PATHS
>>
>>      STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>>      STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>>      STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>>      STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>>      STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>>      STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>>
>>
>> ############ SETS
>>
>>      CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');
>>
>> ############ SEQUENCES
>>
>>      SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');
>>
>> ############ TABLES
>>
>>      SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');
>>
>> ############ SUBSCRIPTIONS
>>
>>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
>>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>

From rob.brucks at rackspace.com  Mon Feb 29 08:00:43 2016
From: rob.brucks at rackspace.com (Rob Brucks)
Date: Mon, 29 Feb 2016 16:00:43 +0000
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <1081050347.118265.1456539574996.JavaMail.yahoo@mail.yahoo.com>
References: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
	<1081050347.118265.1456539574996.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <B2EAC7C7-B044-4E61-A418-E32937D558D7@rackspace.com>

Everything is running on the same server.  One subscriber instance has been shut down.

test_db=# select * from _sloncluster.sl_status;
-[ RECORD 1 ]-------------+------------------------------
st_origin                 | 1
st_received               | 3
st_last_event             | 5000000264
st_last_event_ts          | 2016-02-29 09:58:24.455544-06
st_last_received          | 5000000175
st_last_received_ts       | 2016-02-29 09:43:47.461217-06
st_last_received_event_ts | 2016-02-29 09:43:38.254386-06
st_lag_num_events         | 89
st_lag_time               | 00:14:55.35601
-[ RECORD 2 ]-------------+------------------------------
st_origin                 | 1
st_received               | 2
st_last_event             | 5000000264
st_last_event_ts          | 2016-02-29 09:58:24.455544-06
st_last_received          | 5000000176
st_last_received_ts       | 2016-02-29 09:43:52.284371-06
st_last_received_event_ts | 2016-02-29 09:43:48.258894-06
st_lag_num_events         | 88
st_lag_time               | 00:14:45.351502

Thanks,
--Rob

From: Melvin Davidson <melvin6925 at yahoo.com<mailto:melvin6925 at yahoo.com>>
Reply-To: Melvin Davidson <melvin6925 at yahoo.com<mailto:melvin6925 at yahoo.com>>
Date: Friday, February 26, 2016 at 8:19 PM
To: Rob Brucks <rob.brucks at rackspace.com<mailto:rob.brucks at rackspace.com>>, "slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>" <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Subject: Re: [Slony1-general] Replication Lag?




________________________________
From: Rob Brucks <rob.brucks at rackspace.com<mailto:rob.brucks at rackspace.com>>
To: "slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>" <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Sent: Friday, February 26, 2016 3:51 PM
Subject: [Slony1-general] Replication Lag?

I have a fairly simple test cluster, three PG instances running on different ports:  a master and two slave DBs both subscribed to the master.  Everything is running on the same server (it's a playground), so I have three PG instances (ports 5432, 5433, 5434; connecting via sockets in /tmp) and a slony daemon for each instance.

I'm running Postgres 9.3.9 and slony1 2.2.4 on Centos 6.7 x86_64.  Both postgres and slony were installed via yum using the PGDG repo.

I used the config below to initialize a very simple replication setup of only one table and one sequence.

My problem is that if I shut down just one slave postgres instance then sl_status on the master instance shows replication stalling on both slave DBs, instead of just one.

But, if I insert some test data into the master DB, I see the data show up on the remaining active slave.  So replication to the remaining slave DB is obviously working.

We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.  The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.

Why does sl_status report lag on the active slave even though replication appears to be working fine?

Do I have a misconfiguration somewhere?

Thanks,
Rob


Here's my slony config:


      CLUSTER NAME = slony;
      NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
      NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
      NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';

############ CLUSTERS

      INIT CLUSTER (ID = 1, COMMENT = 'Master');


############ NODES

      STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
      STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);


############ PATHS

      STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
      STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
      STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
      STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
      STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
      STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');


############ SETS

      CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');

############ SEQUENCES

      SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');

############ TABLES

      SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');

############ SUBSCRIPTIONS

      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);

_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info<mailto:Slony1-general at lists.slony.info>
http://lists.slony.info/mailman/listinfo/slony1-general


========================================================================================================
The config looks good.
On which server are you running the slon process?
What does your query for sl_status look like?

Melvin Davidson
    Cell 720-320-0155

I reserve the right to fantasize.  Whether or not you
wish to share my fantasy is entirely up to you. [http://us.i1.yimg.com/us.yimg.com/i/mesg/tsmileys2/01.gif]
www.youtube.com/unusedhero
Folk Alley - All Folk - 24 Hours a day
www.folkalley.com


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160229/d81b69d7/attachment.htm 

From jan at wi3ck.info  Mon Feb 29 08:25:36 2016
From: jan at wi3ck.info (Jan Wieck)
Date: Mon, 29 Feb 2016 11:25:36 -0500
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <73E68821-04FC-432E-8D10-CAE3B568B573@rackspace.com>
References: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
	<alpine.DEB.2.02.1602262135270.4720@mini.atlantida>
	<73E68821-04FC-432E-8D10-CAE3B568B573@rackspace.com>
Message-ID: <56D47100.2080206@wi3ck.info>

On 02/29/2016 10:49 AM, Rob Brucks wrote:
> 1. Yes, the sl_confirm data is showing up on the subscriber.
>
> 2. No, the origin node is not getting back the sl_confirm data from the active subscriber.

Does the origin node log any errors that it cannot connect to that 
subscriber node?


Jan



>
>
> Thanks,
> Rob
>
>
>
> On 2/26/16, 8:38 PM, "Steve Singer" <steve at ssinger.info> wrote:
>
>>On Fri, 26 Feb 2016, Rob Brucks wrote:
>>
>>>
>>> But, if I insert some test data into the master DB, I see the data show up
>>> on the remaining active slave.  So replication to the remaining slave DB
>>> is obviously working.
>>
>>Replication with slony has two parts
>>
>>1. Does the data replicate from the origin to the subscriber. When this
>>happens a row is added to the subscriber's sl_event table with
>>ev_origin=$origin_node and a confirm is added to the subscribers sl_confirm
>>table with con_origin=$origin_node and con_received=$subscriber_node
>>
>>2. The sl_confirm row mentioned above needs to then get picked up by the
>>slon for the origin node and brought back from the subscriber to the origin.
>>
>>Are your confirms making it back?
>>
>>
>>>
>>> We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.  The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.
>>>
>>> Why does sl_status report lag on the active slave even though replication appears to be working fine?
>>>
>>> Do I have a misconfiguration somewhere?
>>>
>>> Thanks,
>>> Rob
>>>
>>>
>>> Here's my slony config:
>>>
>>>
>>>      CLUSTER NAME = slony;
>>>      NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
>>>      NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
>>>      NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';
>>>
>>> ############ CLUSTERS
>>>
>>>      INIT CLUSTER (ID = 1, COMMENT = 'Master');
>>>
>>>
>>> ############ NODES
>>>
>>>      STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
>>>      STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);
>>>
>>>
>>> ############ PATHS
>>>
>>>      STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>>>      STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>>>      STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>>>      STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>>>      STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>>>      STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>>>
>>>
>>> ############ SETS
>>>
>>>      CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');
>>>
>>> ############ SEQUENCES
>>>
>>>      SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');
>>>
>>> ############ TABLES
>>>
>>>      SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');
>>>
>>> ############ SUBSCRIPTIONS
>>>
>>>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
>>>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);
>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Jan Wieck
Database Architect

From rob.brucks at rackspace.com  Mon Feb 29 08:28:23 2016
From: rob.brucks at rackspace.com (Rob Brucks)
Date: Mon, 29 Feb 2016 16:28:23 +0000
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <56D47100.2080206@wi3ck.info>
References: <D12F03E4-5350-4B86-A75F-3B88012BB6B9@rackspace.com>
	<alpine.DEB.2.02.1602262135270.4720@mini.atlantida>
	<73E68821-04FC-432E-8D10-CAE3B568B573@rackspace.com>
	<56D47100.2080206@wi3ck.info>
Message-ID: <FAB4ADE0-A904-4246-9937-812CBDFB0592@rackspace.com>

Only errors connecting to the node I intentionally brought down as part of the test.

No errors in the origin daemon connecting to the subscriber that is still up and running.  If I check pg_stat_activity I see connections from both daemons on both DB instances.

Thanks,
Rob




On 2/29/16, 10:25 AM, "Jan Wieck" <jan at wi3ck.info> wrote:

>On 02/29/2016 10:49 AM, Rob Brucks wrote:
>> 1. Yes, the sl_confirm data is showing up on the subscriber.
>>
>> 2. No, the origin node is not getting back the sl_confirm data from the active subscriber.
>
>Does the origin node log any errors that it cannot connect to that 
>subscriber node?
>
>
>Jan
>
>
>
>>
>>
>> Thanks,
>> Rob
>>
>>
>>
>> On 2/26/16, 8:38 PM, "Steve Singer" <steve at ssinger.info> wrote:
>>
>>>On Fri, 26 Feb 2016, Rob Brucks wrote:
>>>
>>>>
>>>> But, if I insert some test data into the master DB, I see the data show up
>>>> on the remaining active slave.  So replication to the remaining slave DB
>>>> is obviously working.
>>>
>>>Replication with slony has two parts
>>>
>>>1. Does the data replicate from the origin to the subscriber. When this
>>>happens a row is added to the subscriber's sl_event table with
>>>ev_origin=$origin_node and a confirm is added to the subscribers sl_confirm
>>>table with con_origin=$origin_node and con_received=$subscriber_node
>>>
>>>2. The sl_confirm row mentioned above needs to then get picked up by the
>>>slon for the origin node and brought back from the subscriber to the origin.
>>>
>>>Are your confirms making it back?
>>>
>>>
>>>>
>>>> We use sl_status to monitor replication so we need it to accurately report lag if there's an issue.  The Slony 1.2 version we used before did not behave this way, it accurately reported which slave was not replicating.
>>>>
>>>> Why does sl_status report lag on the active slave even though replication appears to be working fine?
>>>>
>>>> Do I have a misconfiguration somewhere?
>>>>
>>>> Thanks,
>>>> Rob
>>>>
>>>>
>>>> Here's my slony config:
>>>>
>>>>
>>>>      CLUSTER NAME = slony;
>>>>      NODE 1 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony';
>>>>      NODE 2 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony';
>>>>      NODE 3 ADMIN CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony';
>>>>
>>>> ############ CLUSTERS
>>>>
>>>>      INIT CLUSTER (ID = 1, COMMENT = 'Master');
>>>>
>>>>
>>>> ############ NODES
>>>>
>>>>      STORE NODE (ID = 2, COMMENT = 'Slave1', EVENT NODE = 1);
>>>>      STORE NODE (ID = 3, COMMENT = 'Slave2', EVENT NODE = 1);
>>>>
>>>>
>>>> ############ PATHS
>>>>
>>>>      STORE PATH (SERVER = 1, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>>>>      STORE PATH (SERVER = 1, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5432 user=slony');
>>>>      STORE PATH (SERVER = 2, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>>>>      STORE PATH (SERVER = 2, CLIENT = 3, CONNINFO = 'dbname=test_db host=/tmp port=5433 user=slony');
>>>>      STORE PATH (SERVER = 3, CLIENT = 1, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>>>>      STORE PATH (SERVER = 3, CLIENT = 2, CONNINFO = 'dbname=test_db host=/tmp port=5434 user=slony');
>>>>
>>>>
>>>> ############ SETS
>>>>
>>>>      CREATE SET (ID = 1, ORIGIN = 1, COMMENT = 'TEST Set 1');
>>>>
>>>> ############ SEQUENCES
>>>>
>>>>      SET ADD SEQUENCE (SET ID = 1, ORIGIN = 1, ID = 1, FULLY QUALIFIED NAME = '"public"."test_seq"');
>>>>
>>>> ############ TABLES
>>>>
>>>>      SET ADD TABLE (SET ID = 1, ORIGIN = 1, ID = 2, FULLY QUALIFIED NAME = '"public"."test"');
>>>>
>>>> ############ SUBSCRIPTIONS
>>>>
>>>>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 2, FORWARD = YES);
>>>>      SUBSCRIBE SET (ID = 1, PROVIDER = 1, RECEIVER = 3, FORWARD = YES);
>>>>
>>>>
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>
>>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
>-- 
>Jan Wieck
>Database Architect

From melvin6925 at yahoo.com  Mon Feb 29 08:32:31 2016
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon, 29 Feb 2016 16:32:31 +0000 (UTC)
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <B2EAC7C7-B044-4E61-A418-E32937D558D7@rackspace.com>
References: <B2EAC7C7-B044-4E61-A418-E32937D558D7@rackspace.com>
Message-ID: <342046034.1053143.1456763551176.JavaMail.yahoo@mail.yahoo.com>

Which server are you executing the query on?If you are executing on the master, then this definitely looks like a bug.Otherwise, if you are executing on a slave, then I would say that is the source of the problem.
?Melvin Davidson 
  ??? Cell 720-320-0155 

I reserve the right to fantasize.? Whether or not you 
 wish to share my fantasy is entirely up to you. 
www.youtube.com/unusedhero
 Folk Alley - All Folk - 24 Hours a day 
www.folkalley.com




      From: Rob Brucks <rob.brucks at rackspace.com>
 To: Melvin Davidson <melvin6925 at yahoo.com>; "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
 Sent: Monday, February 29, 2016 11:00 AM
 Subject: Re: [Slony1-general] Replication Lag?
   
Everything is running on the same server. ?One subscriber instance has been shut down.
test_db=# select * from _sloncluster.sl_status;-[ RECORD 1 ]-------------+------------------------------st_origin ? ? ? ? ? ? ? ? | 1st_received ? ? ? ? ? ? ? | 3st_last_event ? ? ? ? ? ? | 5000000264st_last_event_ts ? ? ? ? ?| 2016-02-29 09:58:24.455544-06st_last_received ? ? ? ? ?| 5000000175st_last_received_ts ? ? ? | 2016-02-29 09:43:47.461217-06st_last_received_event_ts | 2016-02-29 09:43:38.254386-06st_lag_num_events ? ? ? ? | 89st_lag_time ? ? ? ? ? ? ? | 00:14:55.35601-[ RECORD 2 ]-------------+------------------------------st_origin ? ? ? ? ? ? ? ? | 1st_received ? ? ? ? ? ? ? | 2st_last_event ? ? ? ? ? ? | 5000000264st_last_event_ts ? ? ? ? ?| 2016-02-29 09:58:24.455544-06st_last_received ? ? ? ? ?| 5000000176st_last_received_ts ? ? ? | 2016-02-29 09:43:52.284371-06st_last_received_event_ts | 2016-02-29 09:43:48.258894-06st_lag_num_events ? ? ? ? | 88st_lag_time ? ? ? ? ? ? ? | 00:14:45.351502
Thanks,--Rob

  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160229/2a01ae0c/attachment.htm 

From rob.brucks at rackspace.com  Mon Feb 29 08:36:32 2016
From: rob.brucks at rackspace.com (Rob Brucks)
Date: Mon, 29 Feb 2016 16:36:32 +0000
Subject: [Slony1-general] Replication Lag?
In-Reply-To: <342046034.1053143.1456763551176.JavaMail.yahoo@mail.yahoo.com>
References: <B2EAC7C7-B044-4E61-A418-E32937D558D7@rackspace.com>
	<342046034.1053143.1456763551176.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5C4EEC36-97DF-417A-AD92-FFCA135ECE91@rackspace.com>

No, this is on the master.

--Rob

From: Melvin Davidson <melvin6925 at yahoo.com<mailto:melvin6925 at yahoo.com>>
Reply-To: Melvin Davidson <melvin6925 at yahoo.com<mailto:melvin6925 at yahoo.com>>
Date: Monday, February 29, 2016 at 10:32 AM
To: Rob Brucks <rob.brucks at rackspace.com<mailto:rob.brucks at rackspace.com>>, "slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>" <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Subject: Re: [Slony1-general] Replication Lag?

Which server are you executing the query on?
If you are executing on the master, then this definitely looks like a bug.
Otherwise, if you are executing on a slave, then I would say that is the source of the problem.

Melvin Davidson
    Cell 720-320-0155

I reserve the right to fantasize.  Whether or not you
wish to share my fantasy is entirely up to you. [http://us.i1.yimg.com/us.yimg.com/i/mesg/tsmileys2/01.gif]
www.youtube.com/unusedhero
Folk Alley - All Folk - 24 Hours a day
www.folkalley.com




________________________________
From: Rob Brucks <rob.brucks at rackspace.com<mailto:rob.brucks at rackspace.com>>
To: Melvin Davidson <melvin6925 at yahoo.com<mailto:melvin6925 at yahoo.com>>; "slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>" <slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>>
Sent: Monday, February 29, 2016 11:00 AM
Subject: Re: [Slony1-general] Replication Lag?

Everything is running on the same server.  One subscriber instance has been shut down.

test_db=# select * from _sloncluster.sl_status;
-[ RECORD 1 ]-------------+------------------------------
st_origin                 | 1
st_received               | 3
st_last_event             | 5000000264
st_last_event_ts          | 2016-02-29 09:58:24.455544-06
st_last_received          | 5000000175
st_last_received_ts       | 2016-02-29 09:43:47.461217-06
st_last_received_event_ts | 2016-02-29 09:43:38.254386-06
st_lag_num_events         | 89
st_lag_time               | 00:14:55.35601
-[ RECORD 2 ]-------------+------------------------------
st_origin                 | 1
st_received               | 2
st_last_event             | 5000000264
st_last_event_ts          | 2016-02-29 09:58:24.455544-06
st_last_received          | 5000000176
st_last_received_ts       | 2016-02-29 09:43:52.284371-06
st_last_received_event_ts | 2016-02-29 09:43:48.258894-06
st_lag_num_events         | 88
st_lag_time               | 00:14:45.351502

Thanks,
--Rob


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160229/c8808de9/attachment-0001.htm 

