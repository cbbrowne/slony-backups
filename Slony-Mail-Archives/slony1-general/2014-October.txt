From cbbrowne at afilias.info  Wed Oct  1 14:56:10 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 1 Oct 2014 17:56:10 -0400
Subject: [Slony1-general] PostgreSQL 9.4 support?
In-Reply-To: <CANk4eaXY0W_No88n7by61Q9NjCTLk1-fpmZF8Azhpx5N9PzuNQ@mail.gmail.com>
References: <CANk4eaXY0W_No88n7by61Q9NjCTLk1-fpmZF8Azhpx5N9PzuNQ@mail.gmail.com>
Message-ID: <CANfbgbb_5J2qWAYXFSR0Acxn+s62XHseeZj_xJaA3YaodOHsmA@mail.gmail.com>

On Tue, Sep 30, 2014 at 8:09 PM, Quinn David Weaver <quinn at fairpath.com>
wrote:

> Apologies if this has been answered elsewhere; my searches of the list
> archives, docs, and web at large didn't turn up anything. What is the
> status of PostgreSQL 9.4 (beta, obviously) support in Slony?
>

We do periodically check for regressions, and it is interesting that in
pretty well every major release, there is *some* regression due to
code moving around.

(I just did a run against HEAD, and will have to do something about
the fact that INT64_FORMAT has moved from pg_config.h to c.h.
So the tale continues even after 9.4!  :-) )

We are pretty careful about making sure of things once it comes
time for a major release to come out, as people certainly take
interest in Slony at such moments.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141001/767711e5/attachment.htm 

From glynastill at yahoo.co.uk  Fri Oct  3 05:35:49 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri, 3 Oct 2014 13:35:49 +0100
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider when
	origin down
Message-ID: <1412339749.70834.YahooMailNeo@web133202.mail.ir2.yahoo.com>

Hi All,

I'm looking at a slony setup using 2.1.4, with 4 nodes in the following configuration:

    Node 1 --> Node 2
    Node 1 --> Node 3 --> Node 4

Node
 1 is the origin of all sets, and node 3 is a provider of all to node 
4.  What I'm looking to do is fail over to node 2 when both nodes 1 and 3
 have gone down.

Is this possible? 

In both a live environment that I've not had chance
 to move to 2.2 and my test environment I'm seeing the same issues, for my test environment the slonik script is:

    CLUSTER NAME = test_replication;

    NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432 user=slony';
    NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433 user=slony';
    NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434 user=slony';
    NODE 4 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5435 user=slony';

    SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
    WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
    SUBSCRIBE
 SET (ID = 2, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
    WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
    SUBSCRIBE SET (ID = 3, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
    WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);

    DROP NODE (ID = 3, EVENT NODE = 2);

    FAILOVER (
        ID = 1, BACKUP NODE = 2
    );

    DROP NODE (ID = 1, EVENT NODE = 2);

slonik is failing at the first subscribe set line as follows:

    $ slonik test.scr
    test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
    test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5434?
    test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
    Segmentation fault

I get the same behaviour until I bring node 1 back up, then the script almost succeeds, but for an error
stating that a record in sl_event already exists:

    $ slonik ~/test.scr
    ~/test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5434?
    waiting for events  (1,5000000172) only at (1,5000000162) to be confirmed on node 4
    executing failedNode() on 2
    ~/test.scr:17: NOTICE:  failedNode: set 1 has no other
 direct receivers - move now
    ~/test.scr:17: NOTICE:  failedNode: set 2 has no other direct receivers - move now
    ~/test.scr:17: NOTICE:  failedNode: set 3 has no other direct receivers - move now
    ~/test.scr:17: NOTICE:  failedNode: set 1 has other direct receivers - change providers only
    ~/test.scr:17: NOTICE:  failedNode: set 2 has other direct receivers - change providers only
    ~/test.scr:17: NOTICE:  failedNode: set 3 has other direct receivers - change providers only
    NOTICE: executing "_test_replication".failedNode2 on node 2
    ~/test.scr:17: waiting for event (1,5000000175).  node 4 only on event 5000000162
   
 NOTICE: executing "_test_replication".failedNode2 on node 2
   
 ~/test.scr:17: PGRES_FATAL_ERROR lock table 
"_test_replication".sl_event_lock, 
"_test_replication".sl_config_lock;select 
"_test_replication".failedNode2(1,2,2,'5000000174','5000000176');  - 
ERROR:  duplicate key value violates unique constraint "sl_event-pkey"
    DETAIL:  Key (ev_origin, ev_seqno)=(1, 5000000176) already exists.
    CONTEXT:  SQL statement "insert into "_test_replication".sl_event
                (ev_origin, ev_seqno, ev_timestamp,
                ev_snapshot,
                ev_type, ev_data1, ev_data2,
 ev_data3)
                values
                (p_failed_node, p_ev_seqfake, CURRENT_TIMESTAMP,
                v_row.ev_snapshot,
                'FAILOVER_SET', p_failed_node::text, p_backup_node::text,
                p_set_id::text)"
    PL/pgSQL function _test_replication.failednode2(integer,integer,integer,bigint,bigint) line 14 at SQL statement
    NOTICE: executing "_test_replication".failedNode2 on node 2
    ~/test.scr:17: waiting for
 event (1,5000000177).  node 4 only on event 5000000175
    ~/test.scr:21: begin transaction; - 

 After this sl_set on node 4 still has node 1 as the origin for one of the sets
 (Is this possibly becasuse I'm not waiting properly or waiting on the wrong node?):

    TEST=# table _test_replication.sl_set;
     set_id | set_origin | set_locked |    set_comment
    --------+------------+------------+-------------------
          2 |          1 |            | Replication set 2
   
       1 |          2 |            | Replication set 1
          3 |          2 |            | Replication set 3
    (3 rows)


I had attached the slon logs, but my mail to the list bounced, if that would provide any better insight I can provide them.


Any help would be greatly appreciated.

ThanksGlyn
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141003/21c6a823/attachment.htm 

From ssinger at ca.afilias.info  Fri Oct  3 10:40:45 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 03 Oct 2014 13:40:45 -0400
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
Message-ID: <542EDF9D.9050303@ca.afilias.info>

On 10/03/2014 08:27 AM, Glyn Astill wrote:
> Hi All,
>
> I'm looking at a slony setup using 2.1.4, with 4 nodes in the following
> configuration:
>
>      Node 1 --> Node 2
>      Node 1 --> Node 3 --> Node 4
>
> Node 1 is the origin of all sets, and node 3 is a provider of all to
> node 4.  What I'm looking to do is fail over to node 2 when both nodes 1
> and 3 have gone down.
>
> Is this possible?


Improvements with dealing with multiple nodes failing at once was one of 
the big changes with 2.2

You might want to try something like

NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432
NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433
NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434

   FAILOVER (
           ID = 1, BACKUP NODE = 2);
  SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);

DROP NODE (ID = 3, EVENT NODE = 2);
DROP NODE (ID = 1, EVENT NODE = 2);

But I haven't tried to setup a cluster in this configuration so I can't 
say for sure if it will work or not.  As a general comment I think 
trying to reshape the cluster before the FAILOVER command will be 
problematic.

When I started doing a lot of failover tests with 2.1 I discovered a lot 
of cases that wouldn't work, or wouldn't work reliably.  That lead to 
major changes in the 2.2 for failover.




>
> In both a live environment that I've not had chance to move to 2.2 and
> my test environment I'm seeing the same issues, for my test environment
> the slonik script is:
>
>      CLUSTER NAME = test_replication;
>
>      NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432
> user=slony';
>      NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433
> user=slony';
>      NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434
> user=slony';
>      NODE 4 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5435
> user=slony';
>
>      SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>      WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
>      SUBSCRIBE SET (ID = 2, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>      WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
>      SUBSCRIBE SET (ID = 3, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>      WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
>
>      DROP NODE (ID = 3, EVENT NODE = 2);
>
>      FAILOVER (
>          ID = 1, BACKUP NODE = 2
>      );
>
>      DROP NODE (ID = 1, EVENT NODE = 2);
>
> slonik is failing at the first subscribe set line as follows:
>
>      $ slonik test.scr
>      test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5432?
>      test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5434?
>      test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5432?
>      Segmentation fault
>
> I get the same behaviour until I bring node 1 back up, then the script
> almost succeeds, but for an error
> stating that a record in sl_event already exists:
>
>      $ slonik ~/test.scr
>      ~/test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5434?
>      waiting for events  (1,5000000172) only at (1,5000000162) to be
> confirmed on node 4
>      executing failedNode() on 2
>      ~/test.scr:17: NOTICE:  failedNode: set 1 has no other direct
> receivers - move now
>      ~/test.scr:17: NOTICE:  failedNode: set 2 has no other direct
> receivers - move now
>      ~/test.scr:17: NOTICE:  failedNode: set 3 has no other direct
> receivers - move now
>      ~/test.scr:17: NOTICE:  failedNode: set 1 has other direct
> receivers - change providers only
>      ~/test.scr:17: NOTICE:  failedNode: set 2 has other direct
> receivers - change providers only
>      ~/test.scr:17: NOTICE:  failedNode: set 3 has other direct
> receivers - change providers only
>      NOTICE: executing "_test_replication".failedNode2 on node 2
>      ~/test.scr:17: waiting for event (1,5000000175).  node 4 only on
> event 5000000162
> NOTICE: executing "_test_replication".failedNode2 on node 2
>      ~/test.scr:17: PGRES_FATAL_ERROR lock table
> "_test_replication".sl_event_lock,
> "_test_replication".sl_config_lock;select
> "_test_replication".failedNode2(1,2,2,'5000000174','5000000176');  -
> ERROR:  duplicate key value violates unique constraint "sl_event-pkey"
>      DETAIL:  Key (ev_origin, ev_seqno)=(1, 5000000176) already exists.
>      CONTEXT:  SQL statement "insert into "_test_replication".sl_event
>                  (ev_origin, ev_seqno, ev_timestamp,
>                  ev_snapshot,
>                  ev_type, ev_data1, ev_data2, ev_data3)
>                  values
>                  (p_failed_node, p_ev_seqfake, CURRENT_TIMESTAMP,
>                  v_row.ev_snapshot,
>                  'FAILOVER_SET', p_failed_node::text, p_backup_node::text,
>                  p_set_id::text)"
>      PL/pgSQL function
> _test_replication.failednode2(integer,integer,integer,bigint,bigint)
> line 14 at SQL statement
>      NOTICE: executing "_test_replication".failedNode2 on node 2
>      ~/test.scr:17: waiting for event (1,5000000177).  node 4 only on
> event 5000000175
>      ~/test.scr:21: begin transaction; -
>
>   After this sl_set on node 4 still has node 1 as the origin for one of
> the sets
>   (Is this possibly becasuse I'm not waiting properly or waiting on the
> wrong node?):
>
>      TEST=# table _test_replication.sl_set;
>       set_id | set_origin | set_locked |    set_comment
>      --------+------------+------------+-------------------
>            2 |          1 |            | Replication set 2
>        1 |          2 |            | Replication set 1
>            3 |          2 |            | Replication set 3
>      (3 rows)
>
> I've attached the slon logs if that would provide any better insight.
>
> Any help would be greatly appreciated.
>
> Thanks
> Glyn
>


From glynastill at yahoo.co.uk  Mon Oct  6 09:13:41 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 6 Oct 2014 17:13:41 +0100
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <542EDF9D.9050303@ca.afilias.info>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
	<542EDF9D.9050303@ca.afilias.info>
Message-ID: <1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>


> From: Steve Singer <ssinger at ca.afilias.info>
>To: Glyn Astill <glynastill at yahoo.co.uk>; "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
>Sent: Friday, 3 October 2014, 18:40
>Subject: Re: Slony 2.1.4 - Issues re-subscribing provider when origin down
> 
>
>On 10/03/2014 08:27 AM, Glyn Astill wrote:
>> Hi All,
>>
>> I'm looking at a slony setup using 2.1.4, with 4 nodes in the following
>> configuration:
>>
>>      Node 1 --> Node 2
>>      Node 1 --> Node 3 --> Node 4
>>
>> Node 1 is the origin of all sets, and node 3 is a provider of all to
>> node 4.  What I'm looking to do is fail over to node 2 when both nodes 1
>> and 3 have gone down.
>>
>> Is this possible?
>
>
>Improvements with dealing with multiple nodes failing at once was one of 
>the big changes with 2.2
>
>You might want to try something like
>
>NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432
>NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433
>NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434
>
>   FAILOVER (
>           ID = 1, BACKUP NODE = 2);
>  SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>
>DROP NODE (ID = 3, EVENT NODE = 2);
>DROP NODE (ID = 1, EVENT NODE = 2);
>
>But I haven't tried to setup a cluster in this configuration so I can't 
>say for sure if it will work or not.  As a general comment I think 
>trying to reshape the cluster before the FAILOVER command will be 
>problematic.
>
>When I started doing a lot of failover tests with 2.1 I discovered a lot 
>of cases that wouldn't work, or wouldn't work reliably.  That lead to 
>major changes in the 2.2 for failover.
>
>

Thanks Steve.

Yeah this same scenario in 2.2 works fine, but 2.1 is looking pretty broken to me in terms of failing over at all if another node is down in addition to the origin.  Reordering the resubscribe of node 4 after the failover doesn't work, but also just attempting to failover fails when it can't contact the non-origin failed node; it appears slonik want's to connect to the other downed node regardless complaining "ERROR: no admin conninfo for node 3" if I remove the conninfo or failing when it can't connect when I leave it in.

Was this something that was always impossible, or was it canges in 2.0/2.1 that introduced the issues?  I don't recall having the same issue in 1.2 - but it's a while ago now since I checked.


From ssinger at ca.afilias.info  Mon Oct  6 09:59:08 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 06 Oct 2014 12:59:08 -0400
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
	<542EDF9D.9050303@ca.afilias.info>
	<1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>
Message-ID: <5432CA5C.8030704@ca.afilias.info>

On 10/06/2014 12:13 PM, Glyn Astill wrote:
>
>>
>
> Thanks Steve.
>
> Yeah this same scenario in 2.2 works fine, but 2.1 is looking pretty broken to me in terms of failing over at all if another node is down in addition to the origin.  Reordering the resubscribe of node 4 after the failover doesn't work, but also just attempting to failover fails when it can't contact the non-origin failed node; it appears slonik want's to connect to the other downed node regardless complaining "ERROR: no admin conninfo for node 3" if I remove the conninfo or failing when it can't connect when I leave it in.
>
> Was this something that was always impossible, or was it canges in 2.0/2.1 that introduced the issues?  I don't recall having the same issue in 1.2 - but it's a while ago now since I checked.
>

You could try disabling the automatic wait for stuff with the -w option 
to slonik to see if it makes a difference.


From glynastill at yahoo.co.uk  Mon Oct  6 10:21:33 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 6 Oct 2014 18:21:33 +0100
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <5432CA5C.8030704@ca.afilias.info>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
	<542EDF9D.9050303@ca.afilias.info>
	<1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>
	<5432CA5C.8030704@ca.afilias.info>
Message-ID: <1412616093.88642.YahooMailNeo@web133202.mail.ir2.yahoo.com>

----- Original Message -----

> From: Steve Singer <ssinger at ca.afilias.info>
> To: Glyn Astill <glynastill at yahoo.co.uk>; "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
> Cc: 
> Sent: Monday, 6 October 2014, 17:59
> Subject: Re: Slony 2.1.4 - Issues re-subscribing provider when origin down
> 
> On 10/06/2014 12:13 PM, Glyn Astill wrote:
> 
>> 
>>> 
>> 
>>  Thanks Steve.
>> 
>>  Yeah this same scenario in 2.2 works fine, but 2.1 is looking pretty broken 
> to me in terms of failing over at all if another node is down in addition to the 
> origin.  Reordering the resubscribe of node 4 after the failover doesn't 
> work, but also just attempting to failover fails when it can't contact the 
> non-origin failed node; it appears slonik want's to connect to the other 
> downed node regardless complaining "ERROR: no admin conninfo for node 
> 3" if I remove the conninfo or failing when it can't connect when I 
> leave it in.
>> 
>>  Was this something that was always impossible, or was it canges in 2.0/2.1 
> that introduced the issues?  I don't recall having the same issue in 1.2 - 
> but it's a while ago now since I checked.
>> 
> 
> You could try disabling the automatic wait for stuff with the -w option 
> to slonik to see if it makes a difference.
>

No luck unfortunately.


From davecramer at gmail.com  Tue Oct  7 03:51:26 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Tue, 7 Oct 2014 06:51:26 -0400
Subject: [Slony1-general] replicating from 9.3 to 8.4
Message-ID: <CADK3HH+0HaNkqUQKK0R3vTt0unHO7xPBCbnqa_2v136dOLL2LA@mail.gmail.com>

I'm running into http://www.slony.info/bugzilla/show_bug.cgi?id=331

The reason for copying from 9.3 to 8.4 is that an 8.4 is in the upgrade
path.

It will be temporary, but necessary.

Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141007/ef7070b8/attachment.htm 

From vivek at khera.org  Tue Oct  7 11:14:06 2014
From: vivek at khera.org (Vick Khera)
Date: Tue, 7 Oct 2014 14:14:06 -0400
Subject: [Slony1-general] replicating from 9.3 to 8.4
In-Reply-To: <CADK3HH+0HaNkqUQKK0R3vTt0unHO7xPBCbnqa_2v136dOLL2LA@mail.gmail.com>
References: <CADK3HH+0HaNkqUQKK0R3vTt0unHO7xPBCbnqa_2v136dOLL2LA@mail.gmail.com>
Message-ID: <CALd+dce=hpterZPr5M1A_vMA3ufg9mvF=kr2H4kM6K=qJU7aAQ@mail.gmail.com>

can you then just set the bytea_output setting on the 9.3 server to
'escape' for this duration?

or perhaps set that on the slony user only?

On Tue, Oct 7, 2014 at 6:51 AM, Dave Cramer <davecramer at gmail.com> wrote:
> I'm running into http://www.slony.info/bugzilla/show_bug.cgi?id=331
>
> The reason for copying from 9.3 to 8.4 is that an 8.4 is in the upgrade
> path.
>
> It will be temporary, but necessary.
>
> Dave Cramer
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From davecramer at gmail.com  Tue Oct  7 11:35:39 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Tue, 7 Oct 2014 14:35:39 -0400
Subject: [Slony1-general] replicating from 9.3 to 8.4
In-Reply-To: <CALd+dce=hpterZPr5M1A_vMA3ufg9mvF=kr2H4kM6K=qJU7aAQ@mail.gmail.com>
References: <CADK3HH+0HaNkqUQKK0R3vTt0unHO7xPBCbnqa_2v136dOLL2LA@mail.gmail.com>
	<CALd+dce=hpterZPr5M1A_vMA3ufg9mvF=kr2H4kM6K=qJU7aAQ@mail.gmail.com>
Message-ID: <CADK3HHKd5LOv-uyd8AuWiJ57x1PMTD8D3rhmf24-yFyhs_RD=Q@mail.gmail.com>

I like the second solution ... and yes, I can

Dave Cramer

On 7 October 2014 14:14, Vick Khera <vivek at khera.org> wrote:

> can you then just set the bytea_output setting on the 9.3 server to
> 'escape' for this duration?
>
> or perhaps set that on the slony user only?
>
> On Tue, Oct 7, 2014 at 6:51 AM, Dave Cramer <davecramer at gmail.com> wrote:
> > I'm running into http://www.slony.info/bugzilla/show_bug.cgi?id=331
> >
> > The reason for copying from 9.3 to 8.4 is that an 8.4 is in the upgrade
> > path.
> >
> > It will be temporary, but necessary.
> >
> > Dave Cramer
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141007/b11cf1e0/attachment.htm 

From granthana.biswas at gmail.com  Thu Oct  9 05:18:38 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Thu, 9 Oct 2014 17:48:38 +0530
Subject: [Slony1-general] Error with Slony replication from another slony
	cluster slave
Message-ID: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>

Hi All,

I am trying to replicate from another Slony cluster's slave node.

Cluster1 -> replicating from DB1 to -> DB2

Cluster2 -> replicating from DB2 to -> DB3

The initial sync up went fine without any errors. There are no errors in
logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
Cluster2.

But the data added in DB2 since I started slony Cluster2 is not reflecting
in DB3.

Does slony allow replication from another cluster's slony slave? Or did I
miss something?

Thanks & Regards,
Granthana Biswas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/d7fc2429/attachment.htm 

From jan at wi3ck.info  Thu Oct  9 05:30:19 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 09 Oct 2014 08:30:19 -0400
Subject: [Slony1-general] Error with Slony replication from another
 slony cluster slave
In-Reply-To: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>
References: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>
Message-ID: <54367FDB.7090202@wi3ck.info>

On 10/09/2014 08:18 AM, Granthana Biswas wrote:
> Hi All,
>
> I am trying to replicate from another Slony cluster's slave node.
>
> Cluster1 -> replicating from DB1 to -> DB2
>
> Cluster2 -> replicating from DB2 to -> DB3
>
> The initial sync up went fine without any errors. There are no errors in
> logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
> Cluster2.
>
> But the data added in DB2 since I started slony Cluster2 is not
> reflecting in DB3.
>
> Does slony allow replication from another cluster's slony slave? Or did
> I miss something?

This does not work because the logTrigger will only record information 
when firing in session_replication_role 'origin'. When Slony is applying 
changes from DB1 to DB2, the session_replication_role is 'replica', so 
the logTrigger action is suppressed inside the trigger function.

You will have to make DB3 a member of Cluster1 and use DB2 as the data 
provider to achieve this. This has the added benefit that you can easily 
change the data provider of DB3 to DB1 in case you need to perform any 
maintenance on DB2.


Regards,
Jan

-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From glynastill at yahoo.co.uk  Thu Oct  9 05:40:53 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 9 Oct 2014 13:40:53 +0100
Subject: [Slony1-general] Error with Slony replication from another
	slony	cluster slave
In-Reply-To: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>
References: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>
Message-ID: <1412858453.95727.YahooMailNeo@web133203.mail.ir2.yahoo.com>






>________________________________
> From: Granthana Biswas <granthana.biswas at gmail.com>
>To: slony1-general at lists.slony.info 
>Sent: Thursday, 9 October 2014, 13:18
>Subject: [Slony1-general] Error with Slony replication from another slony    cluster slave
> 
>
>
>Hi All,
>
>
>I am trying to replicate from another Slony cluster's slave node. 
>
>
>Cluster1 -> replicating from DB1 to -> DB2
>
>
>Cluster2 -> replicating from DB2 to -> DB3
>
>
>The initial sync up went fine without any errors. There are no errors in logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for Cluster2.
>
>
>But the data added in DB2 since I started slony Cluster2 is not reflecting in DB3.
>
>
>Does slony allow replication from another cluster's slony slave? Or did I miss something?
>
>

Perhaps you could show us what you've done so far, to me it's unclear if you are setting up a totally seperate slony cluster for DB2->DB3, or just trying to subscribe DB3 to sets in the existing cluster.


If it's the latter, then as long as the slave you want to use as the provider was subscribed to the sets you want with "FORWARD = YES" it should be ok


See: http://main.slony.info/documentation/stmtsubscribeset.html

You should be able to check by looking at sub_forward in sl_subscribe, some thing like:

select * from _<cluster name>.sl_subscribe where sub_receiver = <id of DB2>;

>Thanks & Regards,
>Granthana Biswas
>
>

From granthana.biswas at gmail.com  Thu Oct  9 06:14:04 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Thu, 9 Oct 2014 18:44:04 +0530
Subject: [Slony1-general] Error with Slony replication from another
 slony cluster slave
In-Reply-To: <1412858453.95727.YahooMailNeo@web133203.mail.ir2.yahoo.com>
References: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>
	<1412858453.95727.YahooMailNeo@web133203.mail.ir2.yahoo.com>
Message-ID: <CAAPsc-RCfP6Va8R_BZRWaZcxMTuwVsFbaTeHo7YxdXMtc4NdEg@mail.gmail.com>

I am trying to set up a totally separate cluster for DB2 -> DB3  :

Cluster1 -> replicating from DB1 to -> DB2

Cluster2 -> replicating from DB2 to -> DB3

On Thu, Oct 9, 2014 at 6:10 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:

>
>
>
>
>
> >________________________________
> > From: Granthana Biswas <granthana.biswas at gmail.com>
> >To: slony1-general at lists.slony.info
> >Sent: Thursday, 9 October 2014, 13:18
> >Subject: [Slony1-general] Error with Slony replication from another
> slony    cluster slave
> >
> >
> >
> >Hi All,
> >
> >
> >I am trying to replicate from another Slony cluster's slave node.
> >
> >
> >Cluster1 -> replicating from DB1 to -> DB2
> >
> >
> >Cluster2 -> replicating from DB2 to -> DB3
> >
> >
> >The initial sync up went fine without any errors. There are no errors in
> logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
> Cluster2.
> >
> >
> >But the data added in DB2 since I started slony Cluster2 is not
> reflecting in DB3.
> >
> >
> >Does slony allow replication from another cluster's slony slave? Or did I
> miss something?
> >
> >
>
> Perhaps you could show us what you've done so far, to me it's unclear if
> you are setting up a totally seperate slony cluster for DB2->DB3, or just
> trying to subscribe DB3 to sets in the existing cluster.
>
>
> If it's the latter, then as long as the slave you want to use as the
> provider was subscribed to the sets you want with "FORWARD = YES" it should
> be ok
>
>
> See: http://main.slony.info/documentation/stmtsubscribeset.html
>
> You should be able to check by looking at sub_forward in sl_subscribe,
> some thing like:
>
> select * from _<cluster name>.sl_subscribe where sub_receiver = <id of
> DB2>;
>
> >Thanks & Regards,
> >Granthana Biswas
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/4a3b2e12/attachment.htm 

From granthana.biswas at gmail.com  Thu Oct  9 06:16:37 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Thu, 9 Oct 2014 18:46:37 +0530
Subject: [Slony1-general] Error with Slony replication from another
 slony cluster slave
In-Reply-To: <54367FDB.7090202@wi3ck.info>
References: <CAAPsc-Sva1=QXOCpQg-PupVW4AdQMuGDSa2gfPKJihvNCuOH3w@mail.gmail.com>
	<54367FDB.7090202@wi3ck.info>
Message-ID: <CAAPsc-T8khs6K17ZTa8OC6Bwz-n85-k4_aVNUn2xUukeXQGwcQ@mail.gmail.com>

Hi Jan,

Thanks for your quick reply!

What if DB1 goes out in future? Can I make DB2 the new origin of same
cluster, i.e. master of DB3 without down time for DB2?



On Thu, Oct 9, 2014 at 6:00 PM, Jan Wieck <jan at wi3ck.info> wrote:

> On 10/09/2014 08:18 AM, Granthana Biswas wrote:
>
>> Hi All,
>>
>> I am trying to replicate from another Slony cluster's slave node.
>>
>> Cluster1 -> replicating from DB1 to -> DB2
>>
>> Cluster2 -> replicating from DB2 to -> DB3
>>
>> The initial sync up went fine without any errors. There are no errors in
>> logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
>> Cluster2.
>>
>> But the data added in DB2 since I started slony Cluster2 is not
>> reflecting in DB3.
>>
>> Does slony allow replication from another cluster's slony slave? Or did
>> I miss something?
>>
>
> This does not work because the logTrigger will only record information
> when firing in session_replication_role 'origin'. When Slony is applying
> changes from DB1 to DB2, the session_replication_role is 'replica', so the
> logTrigger action is suppressed inside the trigger function.
>
> You will have to make DB3 a member of Cluster1 and use DB2 as the data
> provider to achieve this. This has the added benefit that you can easily
> change the data provider of DB3 to DB1 in case you need to perform any
> maintenance on DB2.
>
>
> Regards,
> Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/1dfede9a/attachment.htm 

From granthana.biswas at gmail.com  Thu Oct  9 06:18:21 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Thu, 9 Oct 2014 18:48:21 +0530
Subject: [Slony1-general] Error with Slony replication from another slony
 cluster slave
Message-ID: <CAAPsc-SeOk8sgzGwes2k4DWJQhbfCN6Nag3ZqDx76nwLFGGmxA@mail.gmail.com>

Hi Jan,

Thanks for your quick reply!

What if DB1 goes out in future? Can I make DB2 the new origin of same
cluster, i.e. master of DB3 without down time for DB2?



On Thu, Oct 9, 2014 at 6:00 PM, Jan Wieck <jan at wi3ck.info> wrote:

> On 10/09/2014 08:18 AM, Granthana Biswas wrote:
>
>> Hi All,
>>
>> I am trying to replicate from another Slony cluster's slave node.
>>
>> Cluster1 -> replicating from DB1 to -> DB2
>>
>> Cluster2 -> replicating from DB2 to -> DB3
>>
>> The initial sync up went fine without any errors. There are no errors in
>> logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
>> Cluster2.
>>
>> But the data added in DB2 since I started slony Cluster2 is not
>> reflecting in DB3.
>>
>> Does slony allow replication from another cluster's slony slave? Or did
>> I miss something?
>>
>
> This does not work because the logTrigger will only record information
> when firing in session_replication_role 'origin'. When Slony is applying
> changes from DB1 to DB2, the session_replication_role is 'replica', so the
> logTrigger action is suppressed inside the trigger function.
>
> You will have to make DB3 a member of Cluster1 and use DB2 as the data
> provider to achieve this. This has the added benefit that you can easily
> change the data provider of DB3 to DB1 in case you need to perform any
> maintenance on DB2.
>
>
> Regards,
> Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/d8a39e6a/attachment-0001.htm 

From granthana.biswas at gmail.com  Thu Oct  9 06:19:05 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Thu, 9 Oct 2014 18:49:05 +0530
Subject: [Slony1-general] Error with Slony replication from another slony
 cluster slave
Message-ID: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>

Hi Glyn,


I am trying to set up a totally separate cluster for DB2 -> DB3  :

Cluster1 -> replicating from DB1 to -> DB2

Cluster2 -> replicating from DB2 to -> DB3

Granthana

On Thu, Oct 9, 2014 at 6:10 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:

>
>
>
>
>
> >________________________________
> > From: Granthana Biswas <granthana.biswas at gmail.com>
> >To: slony1-general at lists.slony.info
> >Sent: Thursday, 9 October 2014, 13:18
> >Subject: [Slony1-general] Error with Slony replication from another
> slony    cluster slave
> >
> >
> >
> >Hi All,
> >
> >
> >I am trying to replicate from another Slony cluster's slave node.
> >
> >
> >Cluster1 -> replicating from DB1 to -> DB2
> >
> >
> >Cluster2 -> replicating from DB2 to -> DB3
> >
> >
> >The initial sync up went fine without any errors. There are no errors in
> logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
> Cluster2.
> >
> >
> >But the data added in DB2 since I started slony Cluster2 is not
> reflecting in DB3.
> >
> >
> >Does slony allow replication from another cluster's slony slave? Or did I
> miss something?
> >
> >
>
> Perhaps you could show us what you've done so far, to me it's unclear if
> you are setting up a totally seperate slony cluster for DB2->DB3, or just
> trying to subscribe DB3 to sets in the existing cluster.
>
>
> If it's the latter, then as long as the slave you want to use as the
> provider was subscribed to the sets you want with "FORWARD = YES" it should
> be ok
>
>
> See: http://main.slony.info/documentation/stmtsubscribeset.html
>
> You should be able to check by looking at sub_forward in sl_subscribe,
> some thing like:
>
> select * from _<cluster name>.sl_subscribe where sub_receiver = <id of
> DB2>;
>
> >Thanks & Regards,
> >Granthana Biswas
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/71859545/attachment.htm 

From granthana.biswas at gmail.com  Thu Oct  9 06:41:14 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Thu, 9 Oct 2014 19:11:14 +0530
Subject: [Slony1-general] Error with Slony replication from another slony
 cluster slave
Message-ID: <CAAPsc-QMwn_owCSoD=KWyobuRPrOOpdozKpbryzXnja47qkt2A@mail.gmail.com>

Hi Jan,

Will it work if I change the slony user for cluster2?

Regards,
Granthana

On Thu, Oct 9, 2014 at 6:00 PM, Jan Wieck <jan at wi3ck.info> wrote:

> On 10/09/2014 08:18 AM, Granthana Biswas wrote:
>
>> Hi All,
>>
>> I am trying to replicate from another Slony cluster's slave node.
>>
>> Cluster1 -> replicating from DB1 to -> DB2
>>
>> Cluster2 -> replicating from DB2 to -> DB3
>>
>> The initial sync up went fine without any errors. There are no errors in
>> logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for
>> Cluster2.
>>
>> But the data added in DB2 since I started slony Cluster2 is not
>> reflecting in DB3.
>>
>> Does slony allow replication from another cluster's slony slave? Or did
>> I miss something?
>>
>
> This does not work because the logTrigger will only record information
> when firing in session_replication_role 'origin'. When Slony is applying
> changes from DB1 to DB2, the session_replication_role is 'replica', so the
> logTrigger action is suppressed inside the trigger function.
>
> You will have to make DB3 a member of Cluster1 and use DB2 as the data
> provider to achieve this. This has the added benefit that you can easily
> change the data provider of DB3 to DB1 in case you need to perform any
> maintenance on DB2.
>
>
> Regards,
> Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/1e4b911a/attachment.htm 

From glynastill at yahoo.co.uk  Thu Oct  9 07:02:10 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 9 Oct 2014 15:02:10 +0100
Subject: [Slony1-general] Error with Slony replication from another
	slony cluster slave
In-Reply-To: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>
References: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>
Message-ID: <1412863330.25877.YahooMailNeo@web133201.mail.ir2.yahoo.com>

________________________________
> From: Granthana Biswas <granthana.biswas at gmail.com>
>To: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
>Sent: Thursday, 9 October 2014, 14:19
>Subject: [Slony1-general] Error with Slony replication from another slony cluster slave
> 
>
>
>Hi Glyn,
>
>
>
>I am trying to set up a totally separate cluster for DB2 -> DB3  :
>
>Cluster1 -> replicating from DB1 to -> DB2
>
>
>Cluster2 -> replicating from DB2 to -> DB3
>
>
>Granthana
>
>

As Jan has already stated, it's not possible to do this with 2 separate clusters because the logtriggers won't fire on DB2 due to the session replication role.

You want to get rid of Cluster2 and subscribe DB3 into Cluster1 with DB2 as it's provider.


>On Thu, Oct 9, 2014 at 6:10 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:
>
>
>>
>>
>>
>>
>>>________________________________
>>> From: Granthana Biswas <granthana.biswas at gmail.com>
>>>To: slony1-general at lists.slony.info
>>>Sent: Thursday, 9 October 2014, 13:18
>>>Subject: [Slony1-general] Error with Slony replication from another slony    cluster slave
>>
>>>
>>>
>>>
>>>Hi All,
>>>
>>>
>>>I am trying to replicate from another Slony cluster's slave node.
>>>
>>>
>>>Cluster1 -> replicating from DB1 to -> DB2
>>>
>>>
>>>Cluster2 -> replicating from DB2 to -> DB3
>>>
>>>
>>>The initial sync up went fine without any errors. There are no errors in logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for Cluster2.
>>>
>>>
>>>But the data added in DB2 since I started slony Cluster2 is not reflecting in DB3.
>>>
>>>
>>>Does slony allow replication from another cluster's slony slave? Or did I miss something?
>>>
>>>
>>
>>Perhaps you could show us what you've done so far, to me it's unclear if you are setting up a totally seperate slony cluster for DB2->DB3, or just trying to subscribe DB3 to sets in the existing cluster.
>>
>>
>>If it's the latter, then as long as the slave you want to use as the provider was subscribed to the sets you want with "FORWARD = YES" it should be ok
>>
>>
>>See: http://main.slony.info/documentation/stmtsubscribeset.html
>>
>>You should be able to check by looking at sub_forward in sl_subscribe, some thing like:
>>
>>select * from _<cluster name>.sl_subscribe where sub_receiver = <id of DB2>;
>>
>>
>>>Thanks & Regards,
>>>Granthana Biswas

From glynastill at yahoo.co.uk  Thu Oct  9 14:26:57 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 9 Oct 2014 22:26:57 +0100
Subject: [Slony1-general] Error with Slony replication from another
	slony cluster slave
In-Reply-To: <CAAPsc-SMde27UpeHyb7JLVLeDpmaQC1J9Fa+GF5p9=2ade7H5w@mail.gmail.com>
References: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>
	<1412863330.25877.YahooMailNeo@web133201.mail.ir2.yahoo.com>
	<CAAPsc-SMde27UpeHyb7JLVLeDpmaQC1J9Fa+GF5p9=2ade7H5w@mail.gmail.com>
Message-ID: <1412890017.53155.YahooMailNeo@web133205.mail.ir2.yahoo.com>

> From: Granthana Biswas <granthana.biswas at gmail.com>
>To: Glyn Astill <glynastill at yahoo.co.uk> 
>Sent: Thursday, 9 October 2014, 16:34
>Subject: Re: [Slony1-general] Error with Slony replication from another slony cluster slave
> 
>
>
>Hi Glyn,
>
>
>In my case I have two clusters:
>
>
>Cluster1 ->  replicating from DB1 -> DB2
>Cluster2 ->  replicating from DB1 -> DB3
>
>
>Can I stop Cluster2 and add DB3 to Cluster1 with DB2 as its master? Or do I have to delete the data first in DB3?
>
>

You'll want to run DROP NODE against each node in Cluster2, (or if on 2.0+ you can get away with just DROP SCHEMA _Cluster2 CASCADE) and stop the slons for Cluster2.

Then just run through adding the node into Cluster1 i.e. STORE NODE, STORE PATH for DB3 on Cluster1 and then SUBSCRIBE SET with the provider as DB2.

As long as your schemas are as you want, the tables will be truncated on DB3 when you subscribe the sets.


>I want to stop DB1 eventually and make DB2 the master.
>
>
>Regards,
>Granthana
>
>
>
>
>
>
>
>
>
>On Thu, Oct 9, 2014 at 7:32 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:
>
>________________________________
>>> From: Granthana Biswas <granthana.biswas at gmail.com>
>>>To: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
>>>Sent: Thursday, 9 October 2014, 14:19
>>>Subject: [Slony1-general] Error with Slony replication from another slony cluster slave
>>>
>>>
>>>
>>>Hi Glyn,
>>>
>>>
>>>
>>>I am trying to set up a totally separate cluster for DB2 -> DB3  :
>>>
>>>Cluster1 -> replicating from DB1 to -> DB2
>>>
>>>
>>>Cluster2 -> replicating from DB2 to -> DB3
>>>
>>>
>>>Granthana
>>>
>>>
>>
>>As Jan has already stated, it's not possible to do this with 2 separate clusters because the logtriggers won't fire on DB2 due to the session replication role.
>>
>>You want to get rid of Cluster2 and subscribe DB3 into Cluster1 with DB2 as it's provider.
>>
>>
>>
>>>On Thu, Oct 9, 2014 at 6:10 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:
>>>
>>>
>>>>
>>>>
>>>>
>>>>
>>>>>________________________________
>>>>> From: Granthana Biswas <granthana.biswas at gmail.com>
>>>>>To: slony1-general at lists.slony.info
>>>>>Sent: Thursday, 9 October 2014, 13:18
>>>>>Subject: [Slony1-general] Error with Slony replication from another slony    cluster slave
>>>>
>>>>>
>>>>>
>>>>>
>>>>>Hi All,
>>>>>
>>>>>
>>>>>I am trying to replicate from another Slony cluster's slave node.
>>>>>
>>>>>
>>>>>Cluster1 -> replicating from DB1 to -> DB2
>>>>>
>>>>>
>>>>>Cluster2 -> replicating from DB2 to -> DB3
>>>>>
>>>>>
>>>>>The initial sync up went fine without any errors. There are no errors in logs of both the clusters. Also no st_lag_num_events in DB3 or DB2 for Cluster2.
>>>>>
>>>>>
>>>>>But the data added in DB2 since I started slony Cluster2 is not reflecting in DB3.
>>>>>
>>>>>
>>>>>Does slony allow replication from another cluster's slony slave? Or did I miss something?
>>>>>
>>>>>
>>>>
>>>>Perhaps you could show us what you've done so far, to me it's unclear if you are setting up a totally seperate slony cluster for DB2->DB3, or just trying to subscribe DB3 to sets in the existing cluster.
>>>>
>>>>
>>>>If it's the latter, then as long as the slave you want to use as the provider was subscribed to the sets you want with "FORWARD = YES" it should be ok
>>>>
>>>>
>>>>See: http://main.slony.info/documentation/stmtsubscribeset.html
>>>>
>>>>You should be able to check by looking at sub_forward in sl_subscribe, some thing like:
>>>>
>>>>select * from _<cluster name>.sl_subscribe where sub_receiver = <id of DB2>;
>>>>
>>>>
>>>>>Thanks & Regards,
>>>>>Granthana Biswas
>>
>
>
>

From jan at wi3ck.info  Thu Oct  9 15:31:59 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 09 Oct 2014 18:31:59 -0400
Subject: [Slony1-general] Error with Slony replication from another
 slony cluster slave
In-Reply-To: <1412890017.53155.YahooMailNeo@web133205.mail.ir2.yahoo.com>
References: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>	<1412863330.25877.YahooMailNeo@web133201.mail.ir2.yahoo.com>	<CAAPsc-SMde27UpeHyb7JLVLeDpmaQC1J9Fa+GF5p9=2ade7H5w@mail.gmail.com>
	<1412890017.53155.YahooMailNeo@web133205.mail.ir2.yahoo.com>
Message-ID: <54370CDF.7060104@wi3ck.info>

On 10/09/2014 05:26 PM, Glyn Astill wrote:
>> From: Granthana Biswas <granthana.biswas at gmail.com>
>>To: Glyn Astill <glynastill at yahoo.co.uk>
>>Sent: Thursday, 9 October 2014, 16:34
>>Subject: Re: [Slony1-general] Error with Slony replication from another slony cluster slave
>>
>>
>>
>>Hi Glyn,
>>
>>
>>In my case I have two clusters:
>>
>>
>>Cluster1 ->  replicating from DB1 -> DB2
>>Cluster2 ->  replicating from DB1 -> DB3
>>
>>
>>Can I stop Cluster2 and add DB3 to Cluster1 with DB2 as its master? Or do I have to delete the data first in DB3?
>>
>>
>
> You'll want to run DROP NODE against each node in Cluster2, (or if on 2.0+ you can get away with just DROP SCHEMA _Cluster2 CASCADE) and stop the slons for Cluster2.


Cascading, forwarding and all that is one of the core concepts of Slony. 
If you create one single setup looking like this:

     DB1 -> DB2 -> DB3

then you have a lot more flexibility and functionality than is obvious 
at first. Aside from being able to fail over to DB2.

Slony will let you "MOVE" the master role from DB1 to DB2. That 
operation will not just make DB2 the master, but at the same time DB1 
becomes a replica that doesn't need an initial sync, so your setup now 
would look like this:

     DB1 <- DB2 -> DB3

This is useful if you need to perform some maintenance on DB1. At this 
point DB2 is your master an you would just stop the slon process for 
DB1, do whatever you need to do, and start the slon process again. Once 
DB1 has caught up, you just transfer the master role back and everything 
is as it was.

What also works is that if you need to perform maintenance on DB2, it 
doesn't mean that DB3 has to fall behind too. You can easily change the 
data provider for DB3 to be DB1, so your configuration changes to

     DB1 -> DB2
      |
      V
     DB3

At this point you stop the slon process for DB2, perform what you need 
to do on DB2, start the slon process and after DB2 has caught up, make 
it the data provider for DB3 again.


Regards,
Jan

-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From davecramer at gmail.com  Thu Oct  9 15:34:10 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Thu, 9 Oct 2014 18:34:10 -0400
Subject: [Slony1-general] Error with Slony replication from another
 slony cluster slave
In-Reply-To: <54370CDF.7060104@wi3ck.info>
References: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>
	<1412863330.25877.YahooMailNeo@web133201.mail.ir2.yahoo.com>
	<CAAPsc-SMde27UpeHyb7JLVLeDpmaQC1J9Fa+GF5p9=2ade7H5w@mail.gmail.com>
	<1412890017.53155.YahooMailNeo@web133205.mail.ir2.yahoo.com>
	<54370CDF.7060104@wi3ck.info>
Message-ID: <CADK3HH+Gxype20zxrsrgftKN3GBvCCJ6CVm=JnYuuHM_vMv2SQ@mail.gmail.com>

Jan,

But as you said they all have to be in the same cluster, correct ?

Dave Cramer

On 9 October 2014 18:31, Jan Wieck <jan at wi3ck.info> wrote:

> On 10/09/2014 05:26 PM, Glyn Astill wrote:
> >> From: Granthana Biswas <granthana.biswas at gmail.com>
> >>To: Glyn Astill <glynastill at yahoo.co.uk>
> >>Sent: Thursday, 9 October 2014, 16:34
> >>Subject: Re: [Slony1-general] Error with Slony replication from another
> slony cluster slave
> >>
> >>
> >>
> >>Hi Glyn,
> >>
> >>
> >>In my case I have two clusters:
> >>
> >>
> >>Cluster1 ->  replicating from DB1 -> DB2
> >>Cluster2 ->  replicating from DB1 -> DB3
> >>
> >>
> >>Can I stop Cluster2 and add DB3 to Cluster1 with DB2 as its master? Or
> do I have to delete the data first in DB3?
> >>
> >>
> >
> > You'll want to run DROP NODE against each node in Cluster2, (or if on
> 2.0+ you can get away with just DROP SCHEMA _Cluster2 CASCADE) and stop the
> slons for Cluster2.
>
>
> Cascading, forwarding and all that is one of the core concepts of Slony.
> If you create one single setup looking like this:
>
>      DB1 -> DB2 -> DB3
>
> then you have a lot more flexibility and functionality than is obvious
> at first. Aside from being able to fail over to DB2.
>
> Slony will let you "MOVE" the master role from DB1 to DB2. That
> operation will not just make DB2 the master, but at the same time DB1
> becomes a replica that doesn't need an initial sync, so your setup now
> would look like this:
>
>      DB1 <- DB2 -> DB3
>
> This is useful if you need to perform some maintenance on DB1. At this
> point DB2 is your master an you would just stop the slon process for
> DB1, do whatever you need to do, and start the slon process again. Once
> DB1 has caught up, you just transfer the master role back and everything
> is as it was.
>
> What also works is that if you need to perform maintenance on DB2, it
> doesn't mean that DB3 has to fall behind too. You can easily change the
> data provider for DB3 to be DB1, so your configuration changes to
>
>      DB1 -> DB2
>       |
>       V
>      DB3
>
> At this point you stop the slon process for DB2, perform what you need
> to do on DB2, start the slon process and after DB2 has caught up, make
> it the data provider for DB3 again.
>
>
> Regards,
> Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141009/8e702ea1/attachment.htm 

From jan at wi3ck.info  Thu Oct  9 15:34:59 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 09 Oct 2014 18:34:59 -0400
Subject: [Slony1-general] Error with Slony replication from another
 slony cluster slave
In-Reply-To: <CADK3HH+Gxype20zxrsrgftKN3GBvCCJ6CVm=JnYuuHM_vMv2SQ@mail.gmail.com>
References: <CAAPsc-QfdSZhu4nVbc4Ze_z=E6csuUKXE8CRdaV+B9eq=bQ+Ag@mail.gmail.com>
	<1412863330.25877.YahooMailNeo@web133201.mail.ir2.yahoo.com>
	<CAAPsc-SMde27UpeHyb7JLVLeDpmaQC1J9Fa+GF5p9=2ade7H5w@mail.gmail.com>
	<1412890017.53155.YahooMailNeo@web133205.mail.ir2.yahoo.com>
	<54370CDF.7060104@wi3ck.info>
	<CADK3HH+Gxype20zxrsrgftKN3GBvCCJ6CVm=JnYuuHM_vMv2SQ@mail.gmail.com>
Message-ID: <54370D93.3090306@wi3ck.info>

On 10/09/2014 06:34 PM, Dave Cramer wrote:
> Jan,
>
> But as you said they all have to be in the same cluster, correct ?

Yes.


Jan


>
> Dave Cramer
>
> On 9 October 2014 18:31, Jan Wieck <jan at wi3ck.info
> <mailto:jan at wi3ck.info>> wrote:
>
>     On 10/09/2014 05:26 PM, Glyn Astill wrote:
>      >> From: Granthana Biswas <granthana.biswas at gmail.com
>     <mailto:granthana.biswas at gmail.com>>
>      >>To: Glyn Astill <glynastill at yahoo.co.uk
>     <mailto:glynastill at yahoo.co.uk>>
>      >>Sent: Thursday, 9 October 2014, 16:34
>      >>Subject: Re: [Slony1-general] Error with Slony replication from
>     another slony cluster slave
>      >>
>      >>
>      >>
>      >>Hi Glyn,
>      >>
>      >>
>      >>In my case I have two clusters:
>      >>
>      >>
>      >>Cluster1 ->  replicating from DB1 -> DB2
>      >>Cluster2 ->  replicating from DB1 -> DB3
>      >>
>      >>
>      >>Can I stop Cluster2 and add DB3 to Cluster1 with DB2 as its
>     master? Or do I have to delete the data first in DB3?
>      >>
>      >>
>      >
>      > You'll want to run DROP NODE against each node in Cluster2, (or
>     if on 2.0+ you can get away with just DROP SCHEMA _Cluster2 CASCADE)
>     and stop the slons for Cluster2.
>
>
>     Cascading, forwarding and all that is one of the core concepts of Slony.
>     If you create one single setup looking like this:
>
>           DB1 -> DB2 -> DB3
>
>     then you have a lot more flexibility and functionality than is obvious
>     at first. Aside from being able to fail over to DB2.
>
>     Slony will let you "MOVE" the master role from DB1 to DB2. That
>     operation will not just make DB2 the master, but at the same time DB1
>     becomes a replica that doesn't need an initial sync, so your setup now
>     would look like this:
>
>           DB1 <- DB2 -> DB3
>
>     This is useful if you need to perform some maintenance on DB1. At this
>     point DB2 is your master an you would just stop the slon process for
>     DB1, do whatever you need to do, and start the slon process again. Once
>     DB1 has caught up, you just transfer the master role back and everything
>     is as it was.
>
>     What also works is that if you need to perform maintenance on DB2, it
>     doesn't mean that DB3 has to fall behind too. You can easily change the
>     data provider for DB3 to be DB1, so your configuration changes to
>
>           DB1 -> DB2
>            |
>            V
>           DB3
>
>     At this point you stop the slon process for DB2, perform what you need
>     to do on DB2, start the slon process and after DB2 has caught up, make
>     it the data provider for DB3 again.
>
>
>     Regards,
>     Jan
>
>     --
>     Jan Wieck
>     Senior Software Engineer
>     http://slony.info
>     _______________________________________________
>     Slony1-general mailing list
>     Slony1-general at lists.slony.info <mailto:Slony1-general at lists.slony.info>
>     http://lists.slony.info/mailman/listinfo/slony1-general
>
>


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From granthana.biswas at gmail.com  Tue Oct 14 03:15:40 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Tue, 14 Oct 2014 15:45:40 +0530
Subject: [Slony1-general] Changing master node's IP & port
Message-ID: <CAAPsc-RP48G+8bBtbEZNGvHf2rNN55keo3LKtwY8_TMkhoTD-g@mail.gmail.com>

Hi All,

I am trying to change the master node's IP and port

After stopping the slon process, I ran store path as follows:

store path (server = 1, client = 2, conninfo='host=new_ip dbname=$DB1
user=$USER1 port=$PORT1');
store path (server = 1, client = 3, conninfo='host=new_ip dbname=$DB1
user=$USER1 port=$PORT1');


But on trying to start slon process again after changing the node config, I
got the following error:

nohup /usr/bin/slon -d 2 -p /var/run/slony1/cl1node1.pid -f
/home/postgres/slony_test/cl1_node1.conf >
/home/postgres/slony_test/log/cl1node1.log 2>&1 &
[1] 5626
postgres at GB:~/slony_test$ 2014-10-14 13:09:13 IST ERROR:  duplicate key
value violates unique constraint "sl_nodelock-pkey"
2014-10-14 13:09:13 IST DETAIL:  Key (nl_nodeid, nl_conncnt)=(1, 0) already
exists.
2014-10-14 13:09:13 IST STATEMENT:  select "_Cluster1".cleanupNodelock();
insert into "_Cluster1".sl_nodelock values (    1, 0,
"pg_catalog".pg_backend_pid());


I even tried the drop path but it gives the following error:

sh drop_path.sh
2014-10-14 13:52:06 IST ERROR:  Slony-I: Path cannot be dropped,
subscription of set 1 needs it
2014-10-14 13:52:06 IST STATEMENT:  lock table "_Cluster1".sl_event_lock,
"_Cluster1".sl_config_lock;select "_Cluster1".dropPath(1, 2);
<stdin>:8: PGRES_FATAL_ERROR lock table "_Cluster1".sl_event_lock,
"_Cluster1".sl_config_lock;select "_Cluster1".dropPath(1, 2);  - ERROR:
Slony-I: Path cannot be dropped, subscription of set 1 needs it


Did anyone face the same issue when trying to change ip of a node? In my
case, I am trying to change IP of master node.

Regards,
Granthana
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141014/4b956d23/attachment.htm 

From glynastill at yahoo.co.uk  Tue Oct 14 08:43:11 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue, 14 Oct 2014 16:43:11 +0100
Subject: [Slony1-general] Changing master node's IP & port
In-Reply-To: <CAAPsc-RP48G+8bBtbEZNGvHf2rNN55keo3LKtwY8_TMkhoTD-g@mail.gmail.com>
References: <CAAPsc-RP48G+8bBtbEZNGvHf2rNN55keo3LKtwY8_TMkhoTD-g@mail.gmail.com>
Message-ID: <1413301391.19481.YahooMailNeo@web133201.mail.ir2.yahoo.com>


> From: Granthana Biswas <granthana.biswas at gmail.com>
>To: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
>Sent: Tuesday, 14 October 2014, 11:15
>Subject: [Slony1-general] Changing master node's IP & port
> 
>
>
>Hi All,
>
>
>I am trying to change the master node's IP and port
>
>After stopping the slon process, I ran store path as follows:
>
>store path (server = 1, client = 2, conninfo='host=new_ip dbname=$DB1 user=$USER1 port=$PORT1');
>store path (server = 1, client = 3, conninfo='host=new_ip dbname=$DB1 user=$USER1 port=$PORT1');
>
>
>But on trying to start slon process again after changing the node config, I got the following error:
>
>nohup /usr/bin/slon -d 2 -p /var/run/slony1/cl1node1.pid -f /home/postgres/slony_test/cl1_node1.conf > /home/postgres/slony_test/log/cl1node1.log 2>&1 &
>[1] 5626
>postgres at GB:~/slony_test$ 2014-10-14 13:09:13 IST ERROR:  duplicate key value violates unique constraint "sl_nodelock-pkey"
>2014-10-14 13:09:13 IST DETAIL:  Key (nl_nodeid, nl_conncnt)=(1, 0) already exists.
>2014-10-14
13:09:13 IST STATEMENT:  select "_Cluster1".cleanupNodelock(); insert 
into "_Cluster1".sl_nodelock values (    1, 0, 
"pg_catalog".pg_backend_pid()); 
>
>

Hi Granthana,

Are you sure you'd stopped all the slons?  A quick "ps aux | grep slon" on each machine where you have slons running should be enough to check.

As long as you're positive you've stopped all the slon processes, you could clean up sl_nodelock by just doing DELETE FROM "_Cluster1".sl_nodelock WHERE nl_nodeid = 1 AND nl_conncnt = 0;



>I even tried the drop path but it gives the following error:
>
>sh drop_path.sh 
>2014-10-14 13:52:06 IST ERROR:  Slony-I: Path cannot be dropped, subscription of set 1 needs it
>2014-10-14 13:52:06 IST STATEMENT:  lock table "_Cluster1".sl_event_lock, "_Cluster1".sl_config_lock;select "_Cluster1".dropPath(1, 2); 
><stdin>:8: PGRES_FATAL_ERROR lock table "_Cluster1".sl_event_lock, "_Cluster1".sl_config_lock;select "_Cluster1".dropPath(1, 2);  - ERROR:  Slony-I: Path cannot be dropped, subscription of set 1 needs it
>
>
>Did anyone face the same issue when trying to change ip of a node? In my case, I am trying to change IP of master node.
>
>
>
>Regards,
>Granthana
>_______________________________________________
>Slony1-general mailing list
>Slony1-general at lists.slony.info
>http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>

From granthana.biswas at gmail.com  Tue Oct 14 08:48:24 2014
From: granthana.biswas at gmail.com (Granthana Biswas)
Date: Tue, 14 Oct 2014 21:18:24 +0530
Subject: [Slony1-general]  Changing master node's IP & port
Message-ID: <CAAPsc-T0b97td_8xZn0py=9f_eSc-sHBO=Zjvu8qVQ9d4v6ayg@mail.gmail.com>

Hi Glyn,

Yes I had stopped all the slons for every node but I did not DELETE FROM
"_Cluster1".sl_nodelock WHERE nl_nodeid = 1 AND nl_conncnt = 0;

Regards,
Granthana

On Tue, Oct 14, 2014 at 9:13 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:

>
> > From: Granthana Biswas <granthana.biswas at gmail.com>
> >To: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
> >Sent: Tuesday, 14 October 2014, 11:15
> >Subject: [Slony1-general] Changing master node's IP & port
> >
> >
> >
> >Hi All,
> >
> >
> >I am trying to change the master node's IP and port
> >
> >After stopping the slon process, I ran store path as follows:
> >
> >store path (server = 1, client = 2, conninfo='host=new_ip dbname=$DB1
> user=$USER1 port=$PORT1');
> >store path (server = 1, client = 3, conninfo='host=new_ip dbname=$DB1
> user=$USER1 port=$PORT1');
> >
> >
> >But on trying to start slon process again after changing the node config,
> I got the following error:
> >
> >nohup /usr/bin/slon -d 2 -p /var/run/slony1/cl1node1.pid -f
> /home/postgres/slony_test/cl1_node1.conf >
> /home/postgres/slony_test/log/cl1node1.log 2>&1 &
> >[1] 5626
> >postgres at GB:~/slony_test$ 2014-10-14 13:09:13 IST ERROR:  duplicate key
> value violates unique constraint "sl_nodelock-pkey"
> >2014-10-14 13:09:13 IST DETAIL:  Key (nl_nodeid, nl_conncnt)=(1, 0)
> already exists.
> >2014-10-14
> 13:09:13 IST STATEMENT:  select "_Cluster1".cleanupNodelock(); insert
> into "_Cluster1".sl_nodelock values (    1, 0,
> "pg_catalog".pg_backend_pid());
> >
> >
>
> Hi Granthana,
>
> Are you sure you'd stopped all the slons?  A quick "ps aux | grep slon" on
> each machine where you have slons running should be enough to check.
>
> As long as you're positive you've stopped all the slon processes, you
> could clean up sl_nodelock by just doing DELETE FROM
> "_Cluster1".sl_nodelock WHERE nl_nodeid = 1 AND nl_conncnt = 0;
>
>
>
> >I even tried the drop path but it gives the following error:
> >
> >sh drop_path.sh
> >2014-10-14 13:52:06 IST ERROR:  Slony-I: Path cannot be dropped,
> subscription of set 1 needs it
> >2014-10-14 13:52:06 IST STATEMENT:  lock table "_Cluster1".sl_event_lock,
> "_Cluster1".sl_config_lock;select "_Cluster1".dropPath(1, 2);
> ><stdin>:8: PGRES_FATAL_ERROR lock table "_Cluster1".sl_event_lock,
> "_Cluster1".sl_config_lock;select "_Cluster1".dropPath(1, 2);  - ERROR:
> Slony-I: Path cannot be dropped, subscription of set 1 needs it
> >
> >
> >Did anyone face the same issue when trying to change ip of a node? In my
> case, I am trying to change IP of master node.
> >
> >
> >
> >Regards,
> >Granthana
> >_______________________________________________
> >Slony1-general mailing list
> >Slony1-general at lists.slony.info
> >http://lists.slony.info/mailman/listinfo/slony1-general
> >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141014/ef168339/attachment.htm 

From glynastill at yahoo.co.uk  Tue Oct 14 08:57:44 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue, 14 Oct 2014 16:57:44 +0100
Subject: [Slony1-general] Changing master node's IP & port
In-Reply-To: <CAAPsc-T0b97td_8xZn0py=9f_eSc-sHBO=Zjvu8qVQ9d4v6ayg@mail.gmail.com>
References: <CAAPsc-T0b97td_8xZn0py=9f_eSc-sHBO=Zjvu8qVQ9d4v6ayg@mail.gmail.com>
Message-ID: <1413302264.76871.YahooMailNeo@web133201.mail.ir2.yahoo.com>

> From: Granthana Biswas <granthana.biswas at gmail.com>
>To: Glyn Astill <glynastill at yahoo.co.uk> 
>Cc: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
>Sent: Tuesday, 14 October 2014, 16:48
>Subject: [Slony1-general] Changing master node's IP & port
> 
>
>
>Hi Glyn,
>
>
>Yes I had stopped all the slons for every node but I did not DELETE FROM "_Cluster1".sl_nodelock WHERE nl_nodeid = 1 AND nl_conncnt = 0;
>
>

Well you shouldn't have had to run the delete, that's just to get you going.  I assume you're up and running now?

Glyn


>Regards,
>Granthana
>

From jan at wi3ck.info  Tue Oct 14 09:20:30 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Tue, 14 Oct 2014 12:20:30 -0400
Subject: [Slony1-general] Changing master node's IP & port
In-Reply-To: <1413302264.76871.YahooMailNeo@web133201.mail.ir2.yahoo.com>
References: <CAAPsc-T0b97td_8xZn0py=9f_eSc-sHBO=Zjvu8qVQ9d4v6ayg@mail.gmail.com>
	<1413302264.76871.YahooMailNeo@web133201.mail.ir2.yahoo.com>
Message-ID: <543D4D4E.5060607@wi3ck.info>

On 10/14/2014 11:57 AM, Glyn Astill wrote:
>> From: Granthana Biswas <granthana.biswas at gmail.com>
>>To: Glyn Astill <glynastill at yahoo.co.uk>
>>Cc: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
>>Sent: Tuesday, 14 October 2014, 16:48
>>Subject: [Slony1-general] Changing master node's IP & port
>>
>>
>>
>>Hi Glyn,
>>
>>
>>Yes I had stopped all the slons for every node but I did not DELETE FROM "_Cluster1".sl_nodelock WHERE nl_nodeid = 1 AND nl_conncnt = 0;
>>
>>
>
> Well you shouldn't have had to run the delete, that's just to get you going.  I assume you're up and running now?

The cleanup procedure for nodelock checks if the backend process, 
holding the lock (if any) is still alive. If I had to guess my guess 
would be that someone/something changed the IP address of the server 
without stopping the slon processes first and that IP address change 
leads to a connection loss without the TCP connections getting RST or 
FIN packets. In that situation it is likely that the database backend 
from the old slon connection is waiting on a blocking read and will only 
notice that the connection is gone after a full TCP keepalive timeout, 
which defaults to several hours.

Terminating the slony related database connections via 
pg_terminate_backend() will make the nodelock cleanup succeed.

In any case it is a good practice to have TCP keepalive settings a lot 
more aggressive on both sides, PostgreSQL and Slony. At my former work 
place we used to set them to 60 seconds idle, then 9 keepalive packets 
in 7 second interval. That will let the connections time out in about 2 
minutes.


Regards, Jan


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From davecramer at gmail.com  Thu Oct 16 03:19:54 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Thu, 16 Oct 2014 06:19:54 -0400
Subject: [Slony1-general] Lag time increasing but there are no events
Message-ID: <CADK3HHJFyyxDs32wiKYe0bEReR3sB_q+xLyhBJ3=MSGtKkvZGQ@mail.gmail.com>

I have a situation I can't explain.

sl_status shows lag time increasing. num events is 0, and the data is being
replicated.

What exactly does lag_time represent ?


Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141016/9baaa6ea/attachment.htm 

