From cbbrowne at afilias.info  Wed Oct  1 14:56:10 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 1 Oct 2014 17:56:10 -0400
Subject: [Slony1-general] PostgreSQL 9.4 support?
In-Reply-To: <CANk4eaXY0W_No88n7by61Q9NjCTLk1-fpmZF8Azhpx5N9PzuNQ@mail.gmail.com>
References: <CANk4eaXY0W_No88n7by61Q9NjCTLk1-fpmZF8Azhpx5N9PzuNQ@mail.gmail.com>
Message-ID: <CANfbgbb_5J2qWAYXFSR0Acxn+s62XHseeZj_xJaA3YaodOHsmA@mail.gmail.com>

On Tue, Sep 30, 2014 at 8:09 PM, Quinn David Weaver <quinn at fairpath.com>
wrote:

> Apologies if this has been answered elsewhere; my searches of the list
> archives, docs, and web at large didn't turn up anything. What is the
> status of PostgreSQL 9.4 (beta, obviously) support in Slony?
>

We do periodically check for regressions, and it is interesting that in
pretty well every major release, there is *some* regression due to
code moving around.

(I just did a run against HEAD, and will have to do something about
the fact that INT64_FORMAT has moved from pg_config.h to c.h.
So the tale continues even after 9.4!  :-) )

We are pretty careful about making sure of things once it comes
time for a major release to come out, as people certainly take
interest in Slony at such moments.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141001/767711e5/attachment.htm 

From glynastill at yahoo.co.uk  Fri Oct  3 05:35:49 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri, 3 Oct 2014 13:35:49 +0100
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider when
	origin down
Message-ID: <1412339749.70834.YahooMailNeo@web133202.mail.ir2.yahoo.com>

Hi All,

I'm looking at a slony setup using 2.1.4, with 4 nodes in the following configuration:

    Node 1 --> Node 2
    Node 1 --> Node 3 --> Node 4

Node
 1 is the origin of all sets, and node 3 is a provider of all to node 
4.  What I'm looking to do is fail over to node 2 when both nodes 1 and 3
 have gone down.

Is this possible? 

In both a live environment that I've not had chance
 to move to 2.2 and my test environment I'm seeing the same issues, for my test environment the slonik script is:

    CLUSTER NAME = test_replication;

    NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432 user=slony';
    NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433 user=slony';
    NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434 user=slony';
    NODE 4 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5435 user=slony';

    SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
    WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
    SUBSCRIBE
 SET (ID = 2, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
    WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
    SUBSCRIBE SET (ID = 3, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
    WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);

    DROP NODE (ID = 3, EVENT NODE = 2);

    FAILOVER (
        ID = 1, BACKUP NODE = 2
    );

    DROP NODE (ID = 1, EVENT NODE = 2);

slonik is failing at the first subscribe set line as follows:

    $ slonik test.scr
    test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
    test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5434?
    test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?
    Segmentation fault

I get the same behaviour until I bring node 1 back up, then the script almost succeeds, but for an error
stating that a record in sl_event already exists:

    $ slonik ~/test.scr
    ~/test.scr:8: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5434?
    waiting for events  (1,5000000172) only at (1,5000000162) to be confirmed on node 4
    executing failedNode() on 2
    ~/test.scr:17: NOTICE:  failedNode: set 1 has no other
 direct receivers - move now
    ~/test.scr:17: NOTICE:  failedNode: set 2 has no other direct receivers - move now
    ~/test.scr:17: NOTICE:  failedNode: set 3 has no other direct receivers - move now
    ~/test.scr:17: NOTICE:  failedNode: set 1 has other direct receivers - change providers only
    ~/test.scr:17: NOTICE:  failedNode: set 2 has other direct receivers - change providers only
    ~/test.scr:17: NOTICE:  failedNode: set 3 has other direct receivers - change providers only
    NOTICE: executing "_test_replication".failedNode2 on node 2
    ~/test.scr:17: waiting for event (1,5000000175).  node 4 only on event 5000000162
   
 NOTICE: executing "_test_replication".failedNode2 on node 2
   
 ~/test.scr:17: PGRES_FATAL_ERROR lock table 
"_test_replication".sl_event_lock, 
"_test_replication".sl_config_lock;select 
"_test_replication".failedNode2(1,2,2,'5000000174','5000000176');  - 
ERROR:  duplicate key value violates unique constraint "sl_event-pkey"
    DETAIL:  Key (ev_origin, ev_seqno)=(1, 5000000176) already exists.
    CONTEXT:  SQL statement "insert into "_test_replication".sl_event
                (ev_origin, ev_seqno, ev_timestamp,
                ev_snapshot,
                ev_type, ev_data1, ev_data2,
 ev_data3)
                values
                (p_failed_node, p_ev_seqfake, CURRENT_TIMESTAMP,
                v_row.ev_snapshot,
                'FAILOVER_SET', p_failed_node::text, p_backup_node::text,
                p_set_id::text)"
    PL/pgSQL function _test_replication.failednode2(integer,integer,integer,bigint,bigint) line 14 at SQL statement
    NOTICE: executing "_test_replication".failedNode2 on node 2
    ~/test.scr:17: waiting for
 event (1,5000000177).  node 4 only on event 5000000175
    ~/test.scr:21: begin transaction; - 

 After this sl_set on node 4 still has node 1 as the origin for one of the sets
 (Is this possibly becasuse I'm not waiting properly or waiting on the wrong node?):

    TEST=# table _test_replication.sl_set;
     set_id | set_origin | set_locked |    set_comment
    --------+------------+------------+-------------------
          2 |          1 |            | Replication set 2
   
       1 |          2 |            | Replication set 1
          3 |          2 |            | Replication set 3
    (3 rows)


I had attached the slon logs, but my mail to the list bounced, if that would provide any better insight I can provide them.


Any help would be greatly appreciated.

ThanksGlyn
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141003/21c6a823/attachment.htm 

From ssinger at ca.afilias.info  Fri Oct  3 10:40:45 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 03 Oct 2014 13:40:45 -0400
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
Message-ID: <542EDF9D.9050303@ca.afilias.info>

On 10/03/2014 08:27 AM, Glyn Astill wrote:
> Hi All,
>
> I'm looking at a slony setup using 2.1.4, with 4 nodes in the following
> configuration:
>
>      Node 1 --> Node 2
>      Node 1 --> Node 3 --> Node 4
>
> Node 1 is the origin of all sets, and node 3 is a provider of all to
> node 4.  What I'm looking to do is fail over to node 2 when both nodes 1
> and 3 have gone down.
>
> Is this possible?


Improvements with dealing with multiple nodes failing at once was one of 
the big changes with 2.2

You might want to try something like

NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432
NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433
NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434

   FAILOVER (
           ID = 1, BACKUP NODE = 2);
  SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);

DROP NODE (ID = 3, EVENT NODE = 2);
DROP NODE (ID = 1, EVENT NODE = 2);

But I haven't tried to setup a cluster in this configuration so I can't 
say for sure if it will work or not.  As a general comment I think 
trying to reshape the cluster before the FAILOVER command will be 
problematic.

When I started doing a lot of failover tests with 2.1 I discovered a lot 
of cases that wouldn't work, or wouldn't work reliably.  That lead to 
major changes in the 2.2 for failover.




>
> In both a live environment that I've not had chance to move to 2.2 and
> my test environment I'm seeing the same issues, for my test environment
> the slonik script is:
>
>      CLUSTER NAME = test_replication;
>
>      NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432
> user=slony';
>      NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433
> user=slony';
>      NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434
> user=slony';
>      NODE 4 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5435
> user=slony';
>
>      SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>      WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
>      SUBSCRIBE SET (ID = 2, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>      WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
>      SUBSCRIBE SET (ID = 3, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>      WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 4, WAIT ON = 2);
>
>      DROP NODE (ID = 3, EVENT NODE = 2);
>
>      FAILOVER (
>          ID = 1, BACKUP NODE = 2
>      );
>
>      DROP NODE (ID = 1, EVENT NODE = 2);
>
> slonik is failing at the first subscribe set line as follows:
>
>      $ slonik test.scr
>      test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5432?
>      test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5434?
>      test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5432?
>      Segmentation fault
>
> I get the same behaviour until I bring node 1 back up, then the script
> almost succeeds, but for an error
> stating that a record in sl_event already exists:
>
>      $ slonik ~/test.scr
>      ~/test.scr:8: could not connect to server: Connection refused
>          Is the server running on host "localhost" (127.0.0.1) and accepting
>          TCP/IP connections on port 5434?
>      waiting for events  (1,5000000172) only at (1,5000000162) to be
> confirmed on node 4
>      executing failedNode() on 2
>      ~/test.scr:17: NOTICE:  failedNode: set 1 has no other direct
> receivers - move now
>      ~/test.scr:17: NOTICE:  failedNode: set 2 has no other direct
> receivers - move now
>      ~/test.scr:17: NOTICE:  failedNode: set 3 has no other direct
> receivers - move now
>      ~/test.scr:17: NOTICE:  failedNode: set 1 has other direct
> receivers - change providers only
>      ~/test.scr:17: NOTICE:  failedNode: set 2 has other direct
> receivers - change providers only
>      ~/test.scr:17: NOTICE:  failedNode: set 3 has other direct
> receivers - change providers only
>      NOTICE: executing "_test_replication".failedNode2 on node 2
>      ~/test.scr:17: waiting for event (1,5000000175).  node 4 only on
> event 5000000162
> NOTICE: executing "_test_replication".failedNode2 on node 2
>      ~/test.scr:17: PGRES_FATAL_ERROR lock table
> "_test_replication".sl_event_lock,
> "_test_replication".sl_config_lock;select
> "_test_replication".failedNode2(1,2,2,'5000000174','5000000176');  -
> ERROR:  duplicate key value violates unique constraint "sl_event-pkey"
>      DETAIL:  Key (ev_origin, ev_seqno)=(1, 5000000176) already exists.
>      CONTEXT:  SQL statement "insert into "_test_replication".sl_event
>                  (ev_origin, ev_seqno, ev_timestamp,
>                  ev_snapshot,
>                  ev_type, ev_data1, ev_data2, ev_data3)
>                  values
>                  (p_failed_node, p_ev_seqfake, CURRENT_TIMESTAMP,
>                  v_row.ev_snapshot,
>                  'FAILOVER_SET', p_failed_node::text, p_backup_node::text,
>                  p_set_id::text)"
>      PL/pgSQL function
> _test_replication.failednode2(integer,integer,integer,bigint,bigint)
> line 14 at SQL statement
>      NOTICE: executing "_test_replication".failedNode2 on node 2
>      ~/test.scr:17: waiting for event (1,5000000177).  node 4 only on
> event 5000000175
>      ~/test.scr:21: begin transaction; -
>
>   After this sl_set on node 4 still has node 1 as the origin for one of
> the sets
>   (Is this possibly becasuse I'm not waiting properly or waiting on the
> wrong node?):
>
>      TEST=# table _test_replication.sl_set;
>       set_id | set_origin | set_locked |    set_comment
>      --------+------------+------------+-------------------
>            2 |          1 |            | Replication set 2
>        1 |          2 |            | Replication set 1
>            3 |          2 |            | Replication set 3
>      (3 rows)
>
> I've attached the slon logs if that would provide any better insight.
>
> Any help would be greatly appreciated.
>
> Thanks
> Glyn
>


From glynastill at yahoo.co.uk  Mon Oct  6 09:13:41 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 6 Oct 2014 17:13:41 +0100
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <542EDF9D.9050303@ca.afilias.info>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
	<542EDF9D.9050303@ca.afilias.info>
Message-ID: <1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>


> From: Steve Singer <ssinger at ca.afilias.info>
>To: Glyn Astill <glynastill at yahoo.co.uk>; "slony1-general at lists.slony.info" <slony1-general at lists.slony.info> 
>Sent: Friday, 3 October 2014, 18:40
>Subject: Re: Slony 2.1.4 - Issues re-subscribing provider when origin down
> 
>
>On 10/03/2014 08:27 AM, Glyn Astill wrote:
>> Hi All,
>>
>> I'm looking at a slony setup using 2.1.4, with 4 nodes in the following
>> configuration:
>>
>>      Node 1 --> Node 2
>>      Node 1 --> Node 3 --> Node 4
>>
>> Node 1 is the origin of all sets, and node 3 is a provider of all to
>> node 4.  What I'm looking to do is fail over to node 2 when both nodes 1
>> and 3 have gone down.
>>
>> Is this possible?
>
>
>Improvements with dealing with multiple nodes failing at once was one of 
>the big changes with 2.2
>
>You might want to try something like
>
>NODE 1 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5432
>NODE 2 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5433
>NODE 3 ADMIN CONNINFO = 'dbname=TEST host=localhost port=5434
>
>   FAILOVER (
>           ID = 1, BACKUP NODE = 2);
>  SUBSCRIBE SET (ID = 1, PROVIDER = 2, RECEIVER = 4, FORWARD = YES);
>
>DROP NODE (ID = 3, EVENT NODE = 2);
>DROP NODE (ID = 1, EVENT NODE = 2);
>
>But I haven't tried to setup a cluster in this configuration so I can't 
>say for sure if it will work or not.  As a general comment I think 
>trying to reshape the cluster before the FAILOVER command will be 
>problematic.
>
>When I started doing a lot of failover tests with 2.1 I discovered a lot 
>of cases that wouldn't work, or wouldn't work reliably.  That lead to 
>major changes in the 2.2 for failover.
>
>

Thanks Steve.

Yeah this same scenario in 2.2 works fine, but 2.1 is looking pretty broken to me in terms of failing over at all if another node is down in addition to the origin.  Reordering the resubscribe of node 4 after the failover doesn't work, but also just attempting to failover fails when it can't contact the non-origin failed node; it appears slonik want's to connect to the other downed node regardless complaining "ERROR: no admin conninfo for node 3" if I remove the conninfo or failing when it can't connect when I leave it in.

Was this something that was always impossible, or was it canges in 2.0/2.1 that introduced the issues?  I don't recall having the same issue in 1.2 - but it's a while ago now since I checked.


From ssinger at ca.afilias.info  Mon Oct  6 09:59:08 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 06 Oct 2014 12:59:08 -0400
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
	<542EDF9D.9050303@ca.afilias.info>
	<1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>
Message-ID: <5432CA5C.8030704@ca.afilias.info>

On 10/06/2014 12:13 PM, Glyn Astill wrote:
>
>>
>
> Thanks Steve.
>
> Yeah this same scenario in 2.2 works fine, but 2.1 is looking pretty broken to me in terms of failing over at all if another node is down in addition to the origin.  Reordering the resubscribe of node 4 after the failover doesn't work, but also just attempting to failover fails when it can't contact the non-origin failed node; it appears slonik want's to connect to the other downed node regardless complaining "ERROR: no admin conninfo for node 3" if I remove the conninfo or failing when it can't connect when I leave it in.
>
> Was this something that was always impossible, or was it canges in 2.0/2.1 that introduced the issues?  I don't recall having the same issue in 1.2 - but it's a while ago now since I checked.
>

You could try disabling the automatic wait for stuff with the -w option 
to slonik to see if it makes a difference.


From glynastill at yahoo.co.uk  Mon Oct  6 10:21:33 2014
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 6 Oct 2014 18:21:33 +0100
Subject: [Slony1-general] Slony 2.1.4 - Issues re-subscribing provider
	when origin down
In-Reply-To: <5432CA5C.8030704@ca.afilias.info>
References: <1412339223.8330.YahooMailNeo@web133203.mail.ir2.yahoo.com>
	<542EDF9D.9050303@ca.afilias.info>
	<1412612021.43355.YahooMailNeo@web133206.mail.ir2.yahoo.com>
	<5432CA5C.8030704@ca.afilias.info>
Message-ID: <1412616093.88642.YahooMailNeo@web133202.mail.ir2.yahoo.com>

----- Original Message -----

> From: Steve Singer <ssinger at ca.afilias.info>
> To: Glyn Astill <glynastill at yahoo.co.uk>; "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
> Cc: 
> Sent: Monday, 6 October 2014, 17:59
> Subject: Re: Slony 2.1.4 - Issues re-subscribing provider when origin down
> 
> On 10/06/2014 12:13 PM, Glyn Astill wrote:
> 
>> 
>>> 
>> 
>>  Thanks Steve.
>> 
>>  Yeah this same scenario in 2.2 works fine, but 2.1 is looking pretty broken 
> to me in terms of failing over at all if another node is down in addition to the 
> origin.  Reordering the resubscribe of node 4 after the failover doesn't 
> work, but also just attempting to failover fails when it can't contact the 
> non-origin failed node; it appears slonik want's to connect to the other 
> downed node regardless complaining "ERROR: no admin conninfo for node 
> 3" if I remove the conninfo or failing when it can't connect when I 
> leave it in.
>> 
>>  Was this something that was always impossible, or was it canges in 2.0/2.1 
> that introduced the issues?  I don't recall having the same issue in 1.2 - 
> but it's a while ago now since I checked.
>> 
> 
> You could try disabling the automatic wait for stuff with the -w option 
> to slonik to see if it makes a difference.
>

No luck unfortunately.


