From david at fetter.org  Tue Apr 14 15:56:05 2015
From: david at fetter.org (David Fetter)
Date: Tue, 14 Apr 2015 15:56:05 -0700
Subject: [Slony1-general] Multiple slons per node pair?
Message-ID: <20150414225605.GB23915@fetter.org>

Folks,

This came up in the context of making slony k-safe for some k>0.

Naively, a simple way to do this would be to have >1 machine, each
running all the slons for a cluster, replacing any machines that fail.

Would Bad Things? happen as a consequence?

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From ajs at crankycanuck.ca  Tue Apr 14 16:18:28 2015
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue, 14 Apr 2015 19:18:28 -0400
Subject: [Slony1-general] Multiple slons per node pair?
In-Reply-To: <20150414225605.GB23915@fetter.org>
References: <20150414225605.GB23915@fetter.org>
Message-ID: <20150414231828.GK43114@crankycanuck.ca>

On Tue, Apr 14, 2015 at 03:56:05PM -0700, David Fetter wrote:
> 
> Naively, a simple way to do this would be to have >1 machine, each
> running all the slons for a cluster, replacing any machines that fail.
> 
> Would Bad Things? happen as a consequence?

I seem to recall doing this by accident some years ago, and getting a
lot of deadlocks (and resulting rollbacks).  I know the whole system
is carefully designed for safety, so I don't think it'll break
anything, but I think you'll get a lot of non-optimal locking that
will block stuff.  Also, your troubleshooting will be a nightmare.

I suspect you'd be much better off to run some sort of watchdog across
machines and start in the event you can't reach through.  If you have
a network problem between the nodes, you still shouldn't break
anything, but it's more likely to work smoothly, I think.

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From diptatapa at gmail.com  Thu Apr 16 20:44:46 2015
From: diptatapa at gmail.com (Soni M)
Date: Fri, 17 Apr 2015 10:44:46 +0700
Subject: [Slony1-general] long 'idle in transaction' from remote slon
Message-ID: <CAAMgDXnsNMgzEaWnzJG6UpaOrEqvn5RPdMGfoEjzStsjJ737jw@mail.gmail.com>

Hello All,
2 nodes configured for slony 2.0.7, on RHEL 6.5 using postgres 9.1.14. Each
slon manage local postgres.
Slony and RHEL installed from postgres yum repo.

On some occasion, on master db, the cleanupEvent last for long time, up to
5 minutes, normally it finish for a few seconds. The 'truncate sl_log_x' is
waiting for a lock which takes most time. This make all write operation to
postgres have to wait also, some get failed. As I inspected, what makes
truncate wait is another slon transaction made by slon slave process, that
is transaction which run 'fetch 500 from LOG'. This transaction left 'idle
on transaction' for a long time on some occasion.

Why is this happen ? Is this due to network latency between nodes ? Is
there any work around for this?

Many thanks, cheers...

-- 
Regards,

Soni Maula Harriz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150417/797f1921/attachment.html 

From ssinger at ca.afilias.info  Fri Apr 17 06:40:59 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 17 Apr 2015 09:40:59 -0400
Subject: [Slony1-general] long 'idle in transaction' from remote slon
In-Reply-To: <CAAMgDXnsNMgzEaWnzJG6UpaOrEqvn5RPdMGfoEjzStsjJ737jw@mail.gmail.com>
References: <CAAMgDXnsNMgzEaWnzJG6UpaOrEqvn5RPdMGfoEjzStsjJ737jw@mail.gmail.com>
Message-ID: <55310D6B.5090007@ca.afilias.info>

On 04/16/2015 11:44 PM, Soni M wrote:
> Hello All,
> 2 nodes configured for slony 2.0.7, on RHEL 6.5 using postgres 9.1.14.
> Each slon manage local postgres.
> Slony and RHEL installed from postgres yum repo.
>

Slony 2.0.x is no longer supported and was never properly supported 
against PG 9.1 +.  You should upgrade to 2.2

Your issue looks a lot like bug 258 (
http://www.slony.info/bugzilla/show_bug.cgi?id=258) which was fixed in 
the 2.1 and 2.2 series.



> On some occasion, on master db, the cleanupEvent last for long time, up
> to 5 minutes, normally it finish for a few seconds. The 'truncate
> sl_log_x' is waiting for a lock which takes most time. This make all
> write operation to postgres have to wait also, some get failed. As I
> inspected, what makes truncate wait is another slon transaction made by
> slon slave process, that is transaction which run 'fetch 500 from LOG'.
> This transaction left 'idle on transaction' for a long time on some
> occasion.
>
> Why is this happen ? Is this due to network latency between nodes ? Is
> there any work around for this?
>
> Many thanks, cheers...
>
> --
> Regards,
>
> Soni Maula Harriz
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From diptatapa at gmail.com  Fri Apr 17 06:57:01 2015
From: diptatapa at gmail.com (Soni M)
Date: Fri, 17 Apr 2015 20:57:01 +0700
Subject: [Slony1-general] long 'idle in transaction' from remote slon
In-Reply-To: <55310D6B.5090007@ca.afilias.info>
References: <CAAMgDXnsNMgzEaWnzJG6UpaOrEqvn5RPdMGfoEjzStsjJ737jw@mail.gmail.com>
	<55310D6B.5090007@ca.afilias.info>
Message-ID: <CAAMgDXk9Byi4cVLbexvRyyaXpF5Q7ELo=TVRFdi89qOQOUCOJA@mail.gmail.com>

Wow, Thanks a lot, that bug definitely what we experience. Will prepare for
upgrade.

On Fri, Apr 17, 2015 at 8:40 PM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 04/16/2015 11:44 PM, Soni M wrote:
>
>> Hello All,
>> 2 nodes configured for slony 2.0.7, on RHEL 6.5 using postgres 9.1.14.
>> Each slon manage local postgres.
>> Slony and RHEL installed from postgres yum repo.
>>
>>
> Slony 2.0.x is no longer supported and was never properly supported
> against PG 9.1 +.  You should upgrade to 2.2
>
> Your issue looks a lot like bug 258 (
> http://www.slony.info/bugzilla/show_bug.cgi?id=258) which was fixed in
> the 2.1 and 2.2 series.
>
>
>
>  On some occasion, on master db, the cleanupEvent last for long time, up
>> to 5 minutes, normally it finish for a few seconds. The 'truncate
>> sl_log_x' is waiting for a lock which takes most time. This make all
>> write operation to postgres have to wait also, some get failed. As I
>> inspected, what makes truncate wait is another slon transaction made by
>> slon slave process, that is transaction which run 'fetch 500 from LOG'.
>> This transaction left 'idle on transaction' for a long time on some
>> occasion.
>>
>> Why is this happen ? Is this due to network latency between nodes ? Is
>> there any work around for this?
>>
>> Many thanks, cheers...
>>
>> --
>> Regards,
>>
>> Soni Maula Harriz
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>


-- 
Regards,

Soni Maula Harriz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150417/1a08a724/attachment.htm 

