From david at fetter.org  Tue Dec  2 10:18:34 2014
From: david at fetter.org (David Fetter)
Date: Tue, 2 Dec 2014 10:18:34 -0800
Subject: [Slony1-general] RDS PostgreSQL?
Message-ID: <20141202181834.GA11035@fetter.org>

Folks,

Is there any way to make an Amazon RDS PostgreSQL database a Slony node?

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From jimmy76 at gmail.com  Tue Dec  2 11:07:08 2014
From: jimmy76 at gmail.com (Jim Mlodgenski)
Date: Tue, 2 Dec 2014 14:07:08 -0500
Subject: [Slony1-general] RDS PostgreSQL?
In-Reply-To: <20141202181834.GA11035@fetter.org>
References: <20141202181834.GA11035@fetter.org>
Message-ID: <CAB_5SRcdZFuu-7zucMk2WrybmWFURGr6oJRVzaedMGtpPryWhA@mail.gmail.com>

On Tue, Dec 2, 2014 at 1:18 PM, David Fetter <david at fetter.org> wrote:

> Folks,
>
> Is there any way to make an Amazon RDS PostgreSQL database a Slony node?
>

I don't believe they've deployed the Slony libraries on RDS yet, but you
can use Londiste or Bucardo. They just recently announced support for both
of them.


> Cheers,
> David.
> --
> David Fetter <david at fetter.org> http://fetter.org/
> Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
> Skype: davidfetter      XMPP: david.fetter at gmail.com
>
> Remember to vote!
> Consider donating to Postgres: http://www.postgresql.org/about/donate
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141202/c7d05a0f/attachment.htm 

From david at fetter.org  Tue Dec  2 14:04:58 2014
From: david at fetter.org (David Fetter)
Date: Tue, 2 Dec 2014 14:04:58 -0800
Subject: [Slony1-general] RDS PostgreSQL?
In-Reply-To: <CAB_5SRcdZFuu-7zucMk2WrybmWFURGr6oJRVzaedMGtpPryWhA@mail.gmail.com>
References: <20141202181834.GA11035@fetter.org>
	<CAB_5SRcdZFuu-7zucMk2WrybmWFURGr6oJRVzaedMGtpPryWhA@mail.gmail.com>
Message-ID: <20141202220458.GA15144@fetter.org>

On Tue, Dec 02, 2014 at 02:07:08PM -0500, Jim Mlodgenski wrote:
> On Tue, Dec 2, 2014 at 1:18 PM, David Fetter <david at fetter.org> wrote:
> > Folks,
> >
> > Is there any way to make an Amazon RDS PostgreSQL database a Slony
> > node?
> >
> 
> I don't believe they've deployed the Slony libraries on RDS yet, but
> you can use Londiste or Bucardo. They just recently announced
> support for both of them.

It's great to know there are logical replication systems available.

Thanks, Jim!

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From jan at wi3ck.info  Tue Dec  2 14:49:45 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Tue, 02 Dec 2014 17:49:45 -0500
Subject: [Slony1-general] RDS PostgreSQL?
In-Reply-To: <20141202220458.GA15144@fetter.org>
References: <20141202181834.GA11035@fetter.org>	<CAB_5SRcdZFuu-7zucMk2WrybmWFURGr6oJRVzaedMGtpPryWhA@mail.gmail.com>
	<20141202220458.GA15144@fetter.org>
Message-ID: <547E4209.3080202@wi3ck.info>

On 12/02/2014 05:04 PM, David Fetter wrote:
> On Tue, Dec 02, 2014 at 02:07:08PM -0500, Jim Mlodgenski wrote:
>> On Tue, Dec 2, 2014 at 1:18 PM, David Fetter <david at fetter.org> wrote:
>> > Folks,
>> >
>> > Is there any way to make an Amazon RDS PostgreSQL database a Slony
>> > node?
>> >
>>
>> I don't believe they've deployed the Slony libraries on RDS yet, but
>> you can use Londiste or Bucardo. They just recently announced
>> support for both of them.
>
> It's great to know there are logical replication systems available.

I appears that the people, working for Amazon in India, don't have what 
it takes to master Slony. They do well on MySQL, but that's about it.


Jan


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From steve at ssinger.info  Tue Dec  2 18:56:33 2014
From: steve at ssinger.info (Steve Singer)
Date: Tue, 2 Dec 2014 21:56:33 -0500
Subject: [Slony1-general] Why helper-threads log is remained?
In-Reply-To: <5477C37F.9010809@lab.ntt.co.jp>
References: <5477C37F.9010809@lab.ntt.co.jp>
Message-ID: <BLU436-SMTP16790204C7ADBC250506631DC7B0@phx.gbl>

On Fri, 28 Nov 2014, ?? ?? wrote:

> Hi,
>
> Please tell me.
> I have understood that helper threads was removed at 2.1.4 ? 2.2.0.
> Why this log is remained here?
>

I think the log text referencing a helper thread is now out dated.  The 
connection to the remote nodes used to query sl_log data is used by the 
remoteWorkerThread's in 2.2

The block of code following that log message (to disconnect from the remote 
database) I think still would apply in certain cases.



>
> slony1-2.2.0/src/slon/remote_worker.c
>
> 1695         if (provider->set_head == NULL)
> 1696         {
> 1697             /*
> 1698              * Tell this helper thread to exit, join him and destroy thread
> 1699              * related data.
> 1700              */
> 1701             slon_log(SLON_CONFIG, "remoteWorkerThread_%d: "
> 1702                      "helper thread for provider %d terminated?n",
> 1703                      node->no_id, provider->no_id);
> ...
>
> Does this slon_log only inform user about whether provider is empty?
>
> regards,
> uehara
>
>
> -- 
> ?????
> NTT OSS??? DBMS??
> Mail : uehara.kazuki at lab.ntt.co.jp
> Phone: 03-5860-5115
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From pwnell at me.com  Fri Dec  5 12:33:08 2014
From: pwnell at me.com (Waldo Nell)
Date: Fri, 05 Dec 2014 12:33:08 -0800
Subject: [Slony1-general] Slony-I 2.2.3 fails on execute script
Message-ID: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>

I have a working Slony-I setup with one master and one slave.  I tried to run a simple alter table add column statement via execute script.  This always worked for me, however today it failed.

I got this error (truncated) on the slave:

2014-12-05 21:51:14 SAST ERROR  remoteWorkerThread_1_1: error at end of COPY IN: ERROR:  schema "_appcluster" does not exist
CONTEXT:  SQL statement "select _AppCluster.sequenceSetValue($1,$2,NULL,$3,true); "
COPY sl_log_1, line 1: "1       4065328 \N      13346355        \N      \N      S       \N      {"alter table future_details add note_adm text;"..."
2014-12-05 21:51:14 SAST ERROR  remoteWorkerThread_1_1: failed SYNC's log selection query was 'COPY ( select log_origin, log_txid, NULL::integer, log_actionseq, NULL::text, NUL
L::text, log_cmdtype, NULL::integer, log_cmdargs from "_AppCluster".sl_log_script where log_origin = 1 and log_txid >= "pg_catalog".txid_snapshot_xmax('4065328:4065330:406532
8') and log_txid < '4065334' and "pg_catalog".txid_visible_in_snapshot(log_txid, '4065334:4065334:') union all select log_origin, log_txid, NULL::integer, log_actionseq, NULL::
...

Why am I getting this error that the schema does not exist?  If I insert data into a replicated table on the master, it appears correctly on the slave.  

On the slave:

oasis=# \dn
    List of schemas
    Name      |  Owner   
---------------+----------
_AppCluster | postgres
public        | postgres
(2 rows)

Can anyone provide advise?

Thanks
Waldo


From ssinger at ca.afilias.info  Fri Dec  5 13:09:39 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 05 Dec 2014 16:09:39 -0500
Subject: [Slony1-general] Slony-I 2.2.3 fails on execute script
In-Reply-To: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
References: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
Message-ID: <54821F13.20308@ca.afilias.info>

On 12/05/2014 03:33 PM, Waldo Nell wrote:

I think because your slony cluster name is mixed case (AppCluster)

This looks like http://bugs.slony.info/bugzilla/show_bug.cgi?id=349

Chris has a patch attached to that bug
You might want to apply that patch and rebuild slony.



> I have a working Slony-I setup with one master and one slave.  I tried to run a simple alter table add column statement via execute script.  This always worked for me, however today it failed.
>
> I got this error (truncated) on the slave:
>
> 2014-12-05 21:51:14 SAST ERROR  remoteWorkerThread_1_1: error at end of COPY IN: ERROR:  schema "_appcluster" does not exist
> CONTEXT:  SQL statement "select _AppCluster.sequenceSetValue($1,$2,NULL,$3,true); "
> COPY sl_log_1, line 1: "1       4065328 \N      13346355        \N      \N      S       \N      {"alter table future_details add note_adm text;"..."
> 2014-12-05 21:51:14 SAST ERROR  remoteWorkerThread_1_1: failed SYNC's log selection query was 'COPY ( select log_origin, log_txid, NULL::integer, log_actionseq, NULL::text, NUL
> L::text, log_cmdtype, NULL::integer, log_cmdargs from "_AppCluster".sl_log_script where log_origin = 1 and log_txid >= "pg_catalog".txid_snapshot_xmax('4065328:4065330:406532
> 8') and log_txid < '4065334' and "pg_catalog".txid_visible_in_snapshot(log_txid, '4065334:4065334:') union all select log_origin, log_txid, NULL::integer, log_actionseq, NULL::
> ...
>
> Why am I getting this error that the schema does not exist?  If I insert data into a replicated table on the master, it appears correctly on the slave.
>
> On the slave:
>
> oasis=# \dn
>      List of schemas
>      Name      |  Owner
> ---------------+----------
> _AppCluster | postgres
> public        | postgres
> (2 rows)
>
> Can anyone provide advise?
>
> Thanks
> Waldo
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From pwnell at me.com  Fri Dec  5 13:20:49 2014
From: pwnell at me.com (Waldo Nell)
Date: Fri, 05 Dec 2014 13:20:49 -0800
Subject: [Slony1-general] Slony-I 2.2.3 fails on execute script
In-Reply-To: <54821F13.20308@ca.afilias.info>
References: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
	<54821F13.20308@ca.afilias.info>
Message-ID: <C8D3B5EC-B31D-4166-9743-CEFF3B5C6CDC@me.com>

That did it, thank you so much.

Why has this patch not been applied to the main trunk of 2.2.3?


> On Dec 5, 2014, at 13:09, Steve Singer <ssinger at ca.afilias.info> wrote:
> 
> On 12/05/2014 03:33 PM, Waldo Nell wrote:
> 
> I think because your slony cluster name is mixed case (AppCluster)
> 
> This looks like http://bugs.slony.info/bugzilla/show_bug.cgi?id=349
> 
> Chris has a patch attached to that bug
> You might want to apply that patch and rebuild slony.


From pwnell at me.com  Fri Dec  5 13:23:52 2014
From: pwnell at me.com (Waldo Nell)
Date: Fri, 05 Dec 2014 13:23:52 -0800
Subject: [Slony1-general] Slony-I 2.2.3 fails on execute script
In-Reply-To: <C8D3B5EC-B31D-4166-9743-CEFF3B5C6CDC@me.com>
References: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
	<54821F13.20308@ca.afilias.info>
	<C8D3B5EC-B31D-4166-9743-CEFF3B5C6CDC@me.com>
Message-ID: <DB265BD6-9505-4824-BD84-8E8B88EE4CFD@me.com>

Never mind my last question - I see that it was misreported on 1.2, and is actually present in the current trunk, and that it was only picked up recently.


> On Dec 5, 2014, at 13:20, Waldo Nell <pwnell at me.com> wrote:
> 
> 
> Why has this patch not been applied to the main trunk of 2.2.3?


From dustink at consistentstate.com  Fri Dec  5 13:36:33 2014
From: dustink at consistentstate.com (dustin kempter)
Date: Fri, 05 Dec 2014 14:36:33 -0700
Subject: [Slony1-general] slony 2.2 with postgres 9.3 and 8.4?
Message-ID: <54822561.4080801@consistentstate.com>

Hi all, were trying to get slony 2.2.3 working between 2 nodes. 1 with 
postgres 9.3 installed. and another with 8.4. But we keep getting this error

<stdin>:506: PGRES_FATAL_ERROR load '$libdir/slony1_funcs.2.2.3'; - 
ERROR: could not access file "$libdir/slony1_funcs.2.2.3": No such file 
or directory
<stdin>:506: Error: the extension for the Slony-I C functions cannot be 
loaded in database 'dbname=tigris port=5432 host=192.168.2.59 user=postgres'

ive added the slon bin dir to my path but it still seems to have issues. 
any help would be great thanks!



From ssinger at ca.afilias.info  Fri Dec  5 13:39:59 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 05 Dec 2014 16:39:59 -0500
Subject: [Slony1-general] slony 2.2 with postgres 9.3 and 8.4?
In-Reply-To: <54822561.4080801@consistentstate.com>
References: <54822561.4080801@consistentstate.com>
Message-ID: <5482262F.9040403@ca.afilias.info>

On 12/05/2014 04:36 PM, dustin kempter wrote:
> Hi all, were trying to get slony 2.2.3 working between 2 nodes. 1 with
> postgres 9.3 installed. and another with 8.4. But we keep getting this error
>
> <stdin>:506: PGRES_FATAL_ERROR load '$libdir/slony1_funcs.2.2.3'; -
> ERROR: could not access file "$libdir/slony1_funcs.2.2.3": No such file
> or directory
> <stdin>:506: Error: the extension for the Slony-I C functions cannot be
> loaded in database 'dbname=tigris port=5432 host=192.168.2.59 user=postgres'
>
> ive added the slon bin dir to my path but it still seems to have issues.
> any help would be great thanks!
>

Is the slony shared library Slony 2.2.3 installed on the database server 
192.168.2.59

You need to get/build/install slony for PG 9.3 on the 9.3 server
and get/build/install slony against PG 8.4 on the 8.4 server

A shared library slony1_funcs.2.2.3.so should be in the postgresql lib 
directory on both machines




>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From pwnell at me.com  Fri Dec  5 13:43:20 2014
From: pwnell at me.com (Waldo Nell)
Date: Fri, 05 Dec 2014 13:43:20 -0800
Subject: [Slony1-general] slony 2.2 with postgres 9.3 and 8.4?
In-Reply-To: <54822561.4080801@consistentstate.com>
References: <54822561.4080801@consistentstate.com>
Message-ID: <BA504FA0-D49F-4098-8982-AF87753BAEA9@me.com>


> 
> Hi all, were trying to get slony 2.2.3 working between 2 nodes. 1 with 
> postgres 9.3 installed. and another with 8.4. But we keep getting this error
> 
> <stdin>:506: PGRES_FATAL_ERROR load '$libdir/slony1_funcs.2.2.3'; - 
> ERROR: could not access file "$libdir/slony1_funcs.2.2.3": No such file 
> or directory
> <stdin>:506: Error: the extension for the Slony-I C functions cannot be 
> loaded in database 'dbname=tigris port=5432 host=192.168.2.59 user=postgres'
> 
> ive added the slon bin dir to my path but it still seems to have issues. 
> any help would be great thanks!
> 

When you ran ./configure, did you specify ?with-pgconfigdir=<Path to PostgreSQL bin dir> ?



From jan at wi3ck.info  Fri Dec  5 21:39:06 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Sat, 06 Dec 2014 00:39:06 -0500
Subject: [Slony1-general] Slony-I 2.2.3 fails on execute script
In-Reply-To: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
References: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
Message-ID: <5482967A.2030602@wi3ck.info>

On 12/05/2014 03:33 PM, Waldo Nell wrote:
> I have a working Slony-I setup with one master and one slave.  I tried to run a simple alter table add column statement via execute script.  This always worked for me, however today it failed.

Please stop using terms like "master" and "slave" in a Slony context.

There is no "master" or "slave". There are origins of sets and 
subscribers of sets. The latter are sometimes referred to as replica.


Thanks,
Jan


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From greg at endpoint.com  Sat Dec  6 04:26:27 2014
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Sat, 6 Dec 2014 07:26:27 -0500
Subject: [Slony1-general] Slony-I 2.2.3 fails on execute script
In-Reply-To: <5482967A.2030602@wi3ck.info>
References: <8ECC1485-988A-4787-A3C1-D0407FF9D6D7@me.com>
	<5482967A.2030602@wi3ck.info>
Message-ID: <20141206122627.GZ18797@broken.home>

On Sat, Dec 06, 2014 at 12:39:06AM -0500, Jan Wieck wrote:
> Please stop using terms like "master" and "slave" in a Slony context.
...
> Jan Wieck
> Senior Software Engineer
> http://slony.info

I don't think it is fair to pick on the original poster for using 
those terms, when they are used in the *very first sentence* on 
slony.info. If you want to move away from those terms, I suggest 
cleaning up the documenation first. :)

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: Digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20141206/74ca3b47/attachment-0001.pgp 

From tmblue at gmail.com  Thu Dec 11 11:36:01 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 11 Dec 2014 11:36:01 -0800
Subject: [Slony1-general] Slonik lock set, verifying
Message-ID: <CAEaSS0buS8PXoz_Ky5kvcH-5Et256m30fEJ6Gs=X6_MCWvk3dA@mail.gmail.com>

Just want to verify that a slonik lock set during the slon upgrade, only
causes locking for updates, and that  my read and selects are unaffected?

Just not 100%, probably a dumb question

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141211/42a713a7/attachment.htm 

From cbbrowne at afilias.info  Thu Dec 11 12:32:48 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 11 Dec 2014 15:32:48 -0500
Subject: [Slony1-general] Slonik lock set, verifying
In-Reply-To: <CAEaSS0buS8PXoz_Ky5kvcH-5Et256m30fEJ6Gs=X6_MCWvk3dA@mail.gmail.com>
References: <CAEaSS0buS8PXoz_Ky5kvcH-5Et256m30fEJ6Gs=X6_MCWvk3dA@mail.gmail.com>
Message-ID: <CANfbgbb0Z4t0gtez9fq_VKxsm_x1KAJZH=V6WcO=zyieSVdOVg@mail.gmail.com>

Well, when slony gets around to the cases where it needs to alter tables,
it'll be a problem if it's fighting for exclusive locks against the
requests for lesser locks by the read-oriented connections.

But the crucial thing about the LOCK SET is that it forcibly prevents data
from going into the replicated tables so that the queue of updates in
sl_log_{1|2} are able to flush with the guarantee that no more data goes in.

So I'd say the reads aren't 100% unaffected; they're 95% unaffected.  (And
the 5% represents "I WANT EXCLUSIVE LOCKS, NOW!!!  Gotta finish this
upgrade!")

Hopefully that helps?  Knowing what the 5% is represents the battle!  :-)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141211/ecf843a2/attachment.htm 

From tmblue at gmail.com  Mon Dec 15 22:35:44 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 15 Dec 2014 22:35:44 -0800
Subject: [Slony1-general] PGRES_FATAL_ERROR ERROR: could not access file
 "$libdir/slony1_funcs.2.2.3": No such file or directory
Message-ID: <CAEaSS0YsHQpgirW0bRdEbA9YbUFRjK7dTZ+dKmFBXTr8+dSjgw@mail.gmail.com>

Trying a production upgrade, after upgrade 3 different environments and I'm
getting this error.

PGRES_FATAL_ERROR ERROR:  could not access file
"$libdir/slony1_funcs.2.2.3": No such file or directory

The files are there, anyway to force this? even tried to create a export
libdir with no success.

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141215/683a63c0/attachment.htm 

From tmblue at gmail.com  Mon Dec 15 23:31:38 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 15 Dec 2014 23:31:38 -0800
Subject: [Slony1-general] PGRES_FATAL_ERROR ERROR: could not access file
 "$libdir/slony1_funcs.2.2.3": No such file or directory
In-Reply-To: <CAEaSS0YsHQpgirW0bRdEbA9YbUFRjK7dTZ+dKmFBXTr8+dSjgw@mail.gmail.com>
References: <CAEaSS0YsHQpgirW0bRdEbA9YbUFRjK7dTZ+dKmFBXTr8+dSjgw@mail.gmail.com>
Message-ID: <CAEaSS0bwynxKqNHEXRMttnihkJUT=La3JPzunu8QXTyHQ8EysQ@mail.gmail.com>

On Mon, Dec 15, 2014 at 10:35 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
> Trying a production upgrade, after upgrade 3 different environments and
> I'm getting this error.
>
> PGRES_FATAL_ERROR ERROR:  could not access file
> "$libdir/slony1_funcs.2.2.3": No such file or directory
>
> The files are there, anyway to force this? even tried to create a export
> libdir with no success.
>
> Thanks
> Tory
>
>
I read over the various threads so many times and it never clicked. But you
have to upgrade all your nodes at the same time. Figured I would be safer
to upgrade just the master, have slon not running on the slaves and query
slaves and things would be fine, safer, cleaner. But no, that error was
coming from a slave node... Upgraded all boxes to the new slony and the
command ran without a hitch.

OOFDA!!!
Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141215/deb1e765/attachment.htm 

From carlos.reimer at opendb.com.br  Tue Dec 16 02:32:03 2014
From: carlos.reimer at opendb.com.br (Carlos Henrique Reimer)
Date: Tue, 16 Dec 2014 08:32:03 -0200
Subject: [Slony1-general] Duplicate key while merging temporary to main set
Message-ID: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>

Hi,

I'm trying to replicate a postgresql database 8.3 with 1TB data and about
20.000 tables using slony 2.2.3 but I'm getting the following error in the
middle of the process:

I created a main set with id number 1 and then small temporary sets were
created and merged to the main set.

Currently there are about 15.000 tables being replicated in the main set
and when I was adding another small set I got this error during the MERGE
command:

2014-12-16 07:08:25 BRST CONFIG remoteWorkerThread_1: Begin COPY of table
"8357_isarq"."wm_bco_pla"
NOTICE:  truncate of "8357_isarq"."wm_bco_pla" succeeded
2014-12-16 07:08:25 BRST CONFIG remoteWorkerThread_1: 74168 bytes copied
for table "8357_isarq"."wm_bco_pla"
NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
CONTEXT:  SQL statement "SELECT "_slcluster".logswitch_start()"
PL/pgSQL function cleanupevent(interval) line 97 at PERFORM
2014-12-16 07:08:25 BRST INFO   cleanupThread:   89.011 seconds for
cleanupEvent()
2014-12-16 07:08:25 BRST ERROR  remoteWorkerThread_1: "select
"_slcluster".finishTableAfterCopy(15564); analyze
"8357_isarq"."wm_bco_pla"; " PGRES_FATAL_ERROR ERROR:  could not create
unique index "pk_wm_bco_pla"
DETAIL:  Key (bco_cod, pla_cod)=(399, 5056110) is duplicated.
CONTEXT:  SQL statement "reindex table "8357_isarq"."wm_bco_pla""
PL/pgSQL function finishtableaftercopy(integer) line 26 at EXECUTE statement
2014-12-16 07:08:25 BRST WARN   remoteWorkerThread_1: data copy for set 999
failed 108 times - sleep 60 seconds

The error indicates "8357_isarq"."wm_bco_pla" has duplicated rows for
primary key  (bco_cod, pla_cod)=(399, 5056110) but when I query the table
using this pk I got only one row:

select * from "8357_isarq"."wm_bco_pla" where bco_cod=399 and pla_cod =
'5056110';
 bco_cod | pla_cod |                pla_des                | ativo |
conta_pai | pla_itlistserv | pla_ctacosif
---------+---------+---------------------------------------+-------+-----------+----------------+--------------
     399 | 5056110 | RENDAS TRANSACOES VISA ELETRON - HBBR | S     |
5056004   |           1501 | 71799003
(1 row)

Slony is trying to merge the temporary set to the main set every 60 seconds
and getting this error. I do not want to loose three days of replication
processing.

Is there anything I can do to fix this error or at least can I remove this
table from the temporary set and work on this issue afterwards?

Thank you!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141216/ee794a36/attachment.htm 

From ajs at crankycanuck.ca  Tue Dec 16 06:16:10 2014
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue, 16 Dec 2014 09:16:10 -0500
Subject: [Slony1-general] Duplicate key while merging temporary to main
 set
In-Reply-To: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
References: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
Message-ID: <20141216141610.GC31758@crankycanuck.ca>

On Tue, Dec 16, 2014 at 08:32:03AM -0200, Carlos Henrique Reimer wrote:
> Hi,
> 
> I'm trying to replicate a postgresql database 8.3 with 1TB data and about

8.3.what?  And also, is that a typo, or do you really mean 8.3?  If
so, I hope you're aware that you're running a database system that was
EOL'd by the community almost two years ago.

On the issue:

> "8357_isarq"."wm_bco_pla"; " PGRES_FATAL_ERROR ERROR:  could not create
> unique index "pk_wm_bco_pla"
> DETAIL:  Key (bco_cod, pla_cod)=(399, 5056110) is duplicated.
> CONTEXT:  SQL statement "reindex table "8357_isarq"."wm_bco_pla""

> The error indicates "8357_isarq"."wm_bco_pla" has duplicated rows for
> primary key  (bco_cod, pla_cod)=(399, 5056110) but when I query the table
> using this pk I got only one row:
> 
> select * from "8357_isarq"."wm_bco_pla" where bco_cod=399 and pla_cod =
> '5056110';
>  bco_cod | pla_cod |                pla_des                | ativo |
> conta_pai | pla_itlistserv | pla_ctacosif
> ---------+---------+---------------------------------------+-------+-----------+----------------+--------------
>      399 | 5056110 | RENDAS TRANSACOES VISA ELETRON - HBBR | S     |
> 5056004   |           1501 | 71799003
> (1 row)

Are you quite sure that your source table isn't corrupt?  That'd be my
first suspicion.  I'd dump the source table and look for a duplicate
before jumping to the conclusion that you are going to "lose three
days of replication processing".  Your replica may in fact contain a
copy of a corrupted database, and it might be tripping on this.

Best regards,

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From carlos.reimer at opendb.com.br  Tue Dec 16 07:11:40 2014
From: carlos.reimer at opendb.com.br (Carlos Henrique Reimer)
Date: Tue, 16 Dec 2014 13:11:40 -0200
Subject: [Slony1-general] Duplicate key while merging temporary to main
	set
In-Reply-To: <20141216141610.GC31758@crankycanuck.ca>
References: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
	<20141216141610.GC31758@crankycanuck.ca>
Message-ID: <CAJnnue2yBcdmW+V4Vchsur2mArar=Wcg9oH2_o0vL8xBdmOgVg@mail.gmail.com>

As you told, table seemed to be corrupted. I got two rows from this select:
SELECT bco_cod, pla_cod, COUNT(*) from "8357_isarq"."wm_bco_pla" GROUP BY
bco_cod, pla_cod HAVING      COUNT(*) > 1;

Ran a delete to remove the two rows and SLONY could continue on with the
processing.

Hope to get the replication finished soon in order to end the migration
from 8.3 to 9.3.

Thank you!

On Tue, Dec 16, 2014 at 12:16 PM, Andrew Sullivan <ajs at crankycanuck.ca>
wrote:
>
> On Tue, Dec 16, 2014 at 08:32:03AM -0200, Carlos Henrique Reimer wrote:
> > Hi,
> >
> > I'm trying to replicate a postgresql database 8.3 with 1TB data and about
>
> 8.3.what?  And also, is that a typo, or do you really mean 8.3?  If
> so, I hope you're aware that you're running a database system that was
> EOL'd by the community almost two years ago.
>
> On the issue:
>
> > "8357_isarq"."wm_bco_pla"; " PGRES_FATAL_ERROR ERROR:  could not create
> > unique index "pk_wm_bco_pla"
> > DETAIL:  Key (bco_cod, pla_cod)=(399, 5056110) is duplicated.
> > CONTEXT:  SQL statement "reindex table "8357_isarq"."wm_bco_pla""
>
> > The error indicates "8357_isarq"."wm_bco_pla" has duplicated rows for
> > primary key  (bco_cod, pla_cod)=(399, 5056110) but when I query the table
> > using this pk I got only one row:
> >
> > select * from "8357_isarq"."wm_bco_pla" where bco_cod=399 and pla_cod =
> > '5056110';
> >  bco_cod | pla_cod |                pla_des                | ativo |
> > conta_pai | pla_itlistserv | pla_ctacosif
> >
> ---------+---------+---------------------------------------+-------+-----------+----------------+--------------
> >      399 | 5056110 | RENDAS TRANSACOES VISA ELETRON - HBBR | S     |
> > 5056004   |           1501 | 71799003
> > (1 row)
>
> Are you quite sure that your source table isn't corrupt?  That'd be my
> first suspicion.  I'd dump the source table and look for a duplicate
> before jumping to the conclusion that you are going to "lose three
> days of replication processing".  Your replica may in fact contain a
> copy of a corrupted database, and it might be tripping on this.
>
> Best regards,
>
> A
>
> --
> Andrew Sullivan
> ajs at crankycanuck.ca
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Reimer
47-3347-1724 47-9183-0547 msn: carlos.reimer at opendb.com.br
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141216/5b7b664b/attachment.htm 

From ajs at crankycanuck.ca  Tue Dec 16 07:14:44 2014
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue, 16 Dec 2014 10:14:44 -0500
Subject: [Slony1-general] Duplicate key while merging temporary to main
 set
In-Reply-To: <CAJnnue2yBcdmW+V4Vchsur2mArar=Wcg9oH2_o0vL8xBdmOgVg@mail.gmail.com>
References: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
	<20141216141610.GC31758@crankycanuck.ca>
	<CAJnnue2yBcdmW+V4Vchsur2mArar=Wcg9oH2_o0vL8xBdmOgVg@mail.gmail.com>
Message-ID: <20141216151444.GF31758@crankycanuck.ca>

On Tue, Dec 16, 2014 at 01:11:40PM -0200, Carlos Henrique Reimer wrote:
> As you told, table seemed to be corrupted. I got two rows from this select:

My only worry now would be that you have other problems in the origin.
This sort of index corruption tends not to be isolated to a single
case.  Good luck!

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From jan at wi3ck.info  Tue Dec 16 09:56:06 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Tue, 16 Dec 2014 12:56:06 -0500
Subject: [Slony1-general] Duplicate key while merging temporary to main
 set
In-Reply-To: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
References: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
Message-ID: <54907236.2090303@wi3ck.info>

On 12/16/2014 05:32 AM, Carlos Henrique Reimer wrote:

> "8357_isarq"."wm_bco_pla"; " PGRES_FATAL_ERROR ERROR:  could not create
> unique index "pk_wm_bco_pla"
> DETAIL:  Key (bco_cod, pla_cod)=(399, 5056110) is duplicated.
> CONTEXT:  SQL statement "reindex table "8357_isarq"."wm_bco_pla""
> PL/pgSQL function finishtableaftercopy(integer) line 26 at EXECUTE statement
> 2014-12-16 07:08:25 BRST WARN   remoteWorkerThread_1: data copy for set
> 999 failed 108 times - sleep 60 seconds
>
> The error indicates "8357_isarq"."wm_bco_pla" has duplicated rows for
> primary key  (bco_cod, pla_cod)=(399, 5056110) but when I query the
> table using this pk I got only one row:
>
> select * from "8357_isarq"."wm_bco_pla" where bco_cod=399 and pla_cod =
> '5056110';
>   bco_cod | pla_cod |                pla_des                | ativo |
> conta_pai | pla_itlistserv | pla_ctacosif
> ---------+---------+---------------------------------------+-------+-----------+----------------+--------------
>       399 | 5056110 | RENDAS TRANSACOES VISA ELETRON - HBBR | S     |
> 5056004   |           1501 | 71799003
> (1 row)

This means that a sequential scan (done by the COPY) produces multiple 
rows with that primary key, while only one of them has an index entry. 
As Andrew pointed out, you have index corruption on the source database. 
I bet a REINDEX of that table on the source will fail too.

> Slony is trying to merge the temporary set to the main set every 60
> seconds and getting this error. I do not want to loose three days of
> replication processing.

No, Slony is still processing the SUBSCRIBE SET for the small temporary 
set (and repeatedly failing on that). The MERGE SET is queued behind and 
not touched yet.

> Is there anything I can do to fix this error or at least can I remove
> this table from the temporary set and work on this issue afterwards?

Reading your follow up, you only got a single duplicate on this one. 
However, there is a high chance that your DELETE actually deleted the 
wrong row version. Since you are running a really outdated version of 
PostgreSQL I would not be surprised if some bug made an outdated version 
of that row visible again. The index entry for that PK would be pointing 
at the last row version, which you now deleted. The replica would now 
contain the old row version again.

Check if you have a row with that PK in the replica and what that 
actually looks like.


Regards, Jan


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From carlos.reimer at opendb.com.br  Tue Dec 16 10:12:41 2014
From: carlos.reimer at opendb.com.br (Carlos Henrique Reimer)
Date: Tue, 16 Dec 2014 16:12:41 -0200
Subject: [Slony1-general] Duplicate key while merging temporary to main
	set
In-Reply-To: <54907236.2090303@wi3ck.info>
References: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
	<54907236.2090303@wi3ck.info>
Message-ID: <CAJnnue01EmPemk6xfAHzVA-kKQqscHGG1n_ynU3M-Y_Oh1QPZQ@mail.gmail.com>

Hi Jan,

Just verified the row again and I see it is visible again even after
removing it with DELETE this morning. Checked the replica and the table
contains the same row with same contents.

Tried a reindex table and it worked fine.

Hope no more discrepancies will be found until the complete cluster is
replicated.

Thanks!


On Tue, Dec 16, 2014 at 3:56 PM, Jan Wieck <jan at wi3ck.info> wrote:
>
> On 12/16/2014 05:32 AM, Carlos Henrique Reimer wrote:
>
>  "8357_isarq"."wm_bco_pla"; " PGRES_FATAL_ERROR ERROR:  could not create
>> unique index "pk_wm_bco_pla"
>> DETAIL:  Key (bco_cod, pla_cod)=(399, 5056110) is duplicated.
>> CONTEXT:  SQL statement "reindex table "8357_isarq"."wm_bco_pla""
>> PL/pgSQL function finishtableaftercopy(integer) line 26 at EXECUTE
>> statement
>> 2014-12-16 07:08:25 BRST WARN   remoteWorkerThread_1: data copy for set
>> 999 failed 108 times - sleep 60 seconds
>>
>> The error indicates "8357_isarq"."wm_bco_pla" has duplicated rows for
>> primary key  (bco_cod, pla_cod)=(399, 5056110) but when I query the
>> table using this pk I got only one row:
>>
>> select * from "8357_isarq"."wm_bco_pla" where bco_cod=399 and pla_cod =
>> '5056110';
>>   bco_cod | pla_cod |                pla_des                | ativo |
>> conta_pai | pla_itlistserv | pla_ctacosif
>> ---------+---------+---------------------------------------+
>> -------+-----------+----------------+--------------
>>       399 | 5056110 | RENDAS TRANSACOES VISA ELETRON - HBBR | S     |
>> 5056004   |           1501 | 71799003
>> (1 row)
>>
>
> This means that a sequential scan (done by the COPY) produces multiple
> rows with that primary key, while only one of them has an index entry. As
> Andrew pointed out, you have index corruption on the source database. I bet
> a REINDEX of that table on the source will fail too.
>
>  Slony is trying to merge the temporary set to the main set every 60
>> seconds and getting this error. I do not want to loose three days of
>> replication processing.
>>
>
> No, Slony is still processing the SUBSCRIBE SET for the small temporary
> set (and repeatedly failing on that). The MERGE SET is queued behind and
> not touched yet.
>
>  Is there anything I can do to fix this error or at least can I remove
>> this table from the temporary set and work on this issue afterwards?
>>
>
> Reading your follow up, you only got a single duplicate on this one.
> However, there is a high chance that your DELETE actually deleted the wrong
> row version. Since you are running a really outdated version of PostgreSQL
> I would not be surprised if some bug made an outdated version of that row
> visible again. The index entry for that PK would be pointing at the last
> row version, which you now deleted. The replica would now contain the old
> row version again.
>
> Check if you have a row with that PK in the replica and what that actually
> looks like.
>
>
> Regards, Jan
>
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>


-- 
Reimer
47-3347-1724 47-9183-0547 msn: carlos.reimer at opendb.com.br
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141216/ec3f055b/attachment.htm 

From carlos.reimer at opendb.com.br  Wed Dec 17 05:48:55 2014
From: carlos.reimer at opendb.com.br (Carlos Henrique Reimer)
Date: Wed, 17 Dec 2014 11:48:55 -0200
Subject: [Slony1-general] WARNING: out of shared memory
Message-ID: <CAJnnue3BBPWoNRSQ8fe5RUQytYamRwJsxTsS2bVVECiFGzD+2g@mail.gmail.com>

Hi,

We are trying to upgrade our PG from 8.3 to 9.3 with slony 2.2.3.

The complete database (21.000 tables) is now being sincronized with SLONY
and I would like to terminate the replication processing in order to
another team test the application with the new 9.3 replicated database.
They will need several hours to test the application. Once I get they green
light, will repeat the processing again and replicate the database from
scratch.

I'm trying to unsubscribe a receiver from the master set but I'm getting an
out of shared memory message:

cat unsubscribe2.sl
cluster name = slcluster;
node 1 admin conninfo = 'dbname=FiscalWeb host=192.168.23.10 user=slonyo';
node 2 admin conninfo = 'dbname=FiscalWeb host=192.168.23.11 user=slonyn';
unsubscribe set ( id = 1 , receiver = 2);

slonik < unsubscribe2.sl
<stdin>:4: WARNING:  out of shared memory
CONTEXT:  SQL statement "drop trigger "_slcluster_logtrigger" on
"8359_wsn"."tbpgdas01502""
PL/pgSQL function altertabledroptriggers(integer) line 47 at EXECUTE
statement
SQL statement "SELECT "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at PERFORM
<stdin>:4: PGRES_FATAL_ERROR lock table "_slcluster".sl_event_lock,
"_slcluster".sl_config_lock;select "_slcluster".unsubscribeSet(1,
2,false);  - ERROR:  out of shared memory
HINT:  You might need to increase max_locks_per_transaction.
CONTEXT:  SQL statement "drop trigger "_slcluster_logtrigger" on
"8359_wsn"."tbpgdas01502""
PL/pgSQL function altertabledroptriggers(integer) line 47 at EXECUTE
statement
SQL statement "SELECT "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at PERFORM
[postgres at 00002-NfseNet-SGDB reimer]$ ^C

max_locks_per_transaction is set to 255. Changed to 4096 and will restart
the database during the night change window.

Tried to repeat the process again and now I'm getting always the same error:
[postgres at 00002-NfseNet-SGDB reimer]$ slonik < unsubscribe2.sl
<stdin>:4: PGRES_FATAL_ERROR lock table "_slcluster".sl_event_lock,
"_slcluster".sl_config_lock;select "_slcluster".unsubscribeSet(1,
2,false);  - ERROR:  deadlock detected
DETAIL:  Process 15366 waits for AccessExclusiveLock on relation 29564 of
database 16384; blocked by process 14994.
Process 14994 waits for RowExclusiveLock on relation 84222 of database
16384; blocked by process 15366.
HINT:  See server log for query details.
CONTEXT:  SQL statement "lock table "7481_spunico"."sincdc" in access
exclusive mode"
PL/pgSQL function altertabledroptriggers(integer) line 42 at EXECUTE
statement
SQL statement "SELECT "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at PERFORM
[postgres at 00002-NfseNet-SGDB reimer]$

The two pids reported by the deadlock message are probably temporary
processed created by SLONY as I did not find them in the system.

The increase in the max_locks_per_transactions and the server restart will
fix this issue?

Thank you!

-- 
Reimer
47-3347-1724 47-9183-0547 msn: carlos.reimer at opendb.com.br
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141217/0b6435dd/attachment.htm 

From ssinger at ca.afilias.info  Wed Dec 17 07:22:00 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 17 Dec 2014 10:22:00 -0500
Subject: [Slony1-general] WARNING: out of shared memory
In-Reply-To: <CAJnnue3BBPWoNRSQ8fe5RUQytYamRwJsxTsS2bVVECiFGzD+2g@mail.gmail.com>
References: <CAJnnue3BBPWoNRSQ8fe5RUQytYamRwJsxTsS2bVVECiFGzD+2g@mail.gmail.com>
Message-ID: <54919F98.5010905@ca.afilias.info>

On 12/17/2014 08:48 AM, Carlos Henrique Reimer wrote:
> Hi,
>
> We are trying to upgrade our PG from 8.3 to 9.3 with slony 2.2.3.
>
> The complete database (21.000 tables) is now being sincronized with
> SLONY and I would like to terminate the replication processing in order
> to another team test the application with the new 9.3 replicated
> database. They will need several hours to test the application. Once I
> get they green light, will repeat the processing again and replicate the
> database from scratch.
>
> I'm trying to unsubscribe a receiver from the master set but I'm getting
> an out of shared memory message:
>
> cat unsubscribe2.sl <http://unsubscribe2.sl>
> cluster name = slcluster;
> node 1 admin conninfo = 'dbname=FiscalWeb host=192.168.23.10 user=slonyo';
> node 2 admin conninfo = 'dbname=FiscalWeb host=192.168.23.11 user=slonyn';
> unsubscribe set ( id = 1 , receiver = 2);
>
> slonik < unsubscribe2.sl <http://unsubscribe2.sl>
> <stdin>:4: WARNING:  out of shared memory
> CONTEXT:  SQL statement "drop trigger "_slcluster_logtrigger" on
> "8359_wsn"."tbpgdas01502""
> PL/pgSQL function altertabledroptriggers(integer) line 47 at EXECUTE
> statement
> SQL statement "SELECT "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
> PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at PERFORM
> <stdin>:4: PGRES_FATAL_ERROR lock table "_slcluster".sl_event_lock,
> "_slcluster".sl_config_lock;select "_slcluster".unsubscribeSet(1,
> 2,false);  - ERROR:  out of shared memory
> HINT:  You might need to increase max_locks_per_transaction.
> CONTEXT:  SQL statement "drop trigger "_slcluster_logtrigger" on
> "8359_wsn"."tbpgdas01502""
> PL/pgSQL function altertabledroptriggers(integer) line 47 at EXECUTE
> statement
> SQL statement "SELECT "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
> PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at PERFORM
> [postgres at 00002-NfseNet-SGDB reimer]$ ^C
>
> max_locks_per_transaction is set to 255. Changed to 4096 and will
> restart the database during the night change window.
>
> Tried to repeat the process again and now I'm getting always the same error:
> [postgres at 00002-NfseNet-SGDB reimer]$ slonik < unsubscribe2.sl
> <http://unsubscribe2.sl>
> <stdin>:4: PGRES_FATAL_ERROR lock table "_slcluster".sl_event_lock,
> "_slcluster".sl_config_lock;select "_slcluster".unsubscribeSet(1,
> 2,false);  - ERROR:  deadlock detected
> DETAIL:  Process 15366 waits for AccessExclusiveLock on relation 29564
> of database 16384; blocked by process 14994.
> Process 14994 waits for RowExclusiveLock on relation 84222 of database
> 16384; blocked by process 15366.
> HINT:  See server log for query details.
> CONTEXT:  SQL statement "lock table "7481_spunico"."sincdc" in access
> exclusive mode"
> PL/pgSQL function altertabledroptriggers(integer) line 42 at EXECUTE
> statement
> SQL statement "SELECT "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
> PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at PERFORM
> [postgres at 00002-NfseNet-SGDB reimer]$
>
> The two pids reported by the deadlock message are probably temporary
> processed created by SLONY as I did not find them in the system.
>
> The increase in the max_locks_per_transactions and the server restart
> will fix this issue?
>


If you have 27,000 tables in your replication set then I think 
max_locks_per_transaction might needs to be at least that big.  The 
alterTableDropTriggers function  will take a lock on each table in the set.

As for your deadlock you should figure out what process 15366 is
Since this operation takes an exclusive lock on all tables in the set on 
the replica you probably don't want any other processes accessing those 
tables at that point in time.





> Thank you!
>
> --
> Reimer
> 47-3347-1724 47-9183-0547 msn: carlos.reimer at opendb.com.br
> <mailto:carlos.reimer at opendb.com.br>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From carlos.reimer at opendb.com.br  Wed Dec 17 10:44:27 2014
From: carlos.reimer at opendb.com.br (Carlos Henrique Reimer)
Date: Wed, 17 Dec 2014 16:44:27 -0200
Subject: [Slony1-general] WARNING: out of shared memory
In-Reply-To: <54919F98.5010905@ca.afilias.info>
References: <CAJnnue3BBPWoNRSQ8fe5RUQytYamRwJsxTsS2bVVECiFGzD+2g@mail.gmail.com>
	<54919F98.5010905@ca.afilias.info>
Message-ID: <CAJnnue3NXZH8cyr5RA7rVGrAo=xCc31V3cpcEQwpowQ1bXqp2g@mail.gmail.com>

Hi Steve,

As you said the deadlock was caused by process 15366 but I did not find it
because I was looking in the wrong server. Once the process was identified
I could stop it and run the commands successfully.

Thank you for your help!

On Wed, Dec 17, 2014 at 1:22 PM, Steve Singer <ssinger at ca.afilias.info>
wrote:
>
> On 12/17/2014 08:48 AM, Carlos Henrique Reimer wrote:
> > Hi,
> >
> > We are trying to upgrade our PG from 8.3 to 9.3 with slony 2.2.3.
> >
> > The complete database (21.000 tables) is now being sincronized with
> > SLONY and I would like to terminate the replication processing in order
> > to another team test the application with the new 9.3 replicated
> > database. They will need several hours to test the application. Once I
> > get they green light, will repeat the processing again and replicate the
> > database from scratch.
> >
> > I'm trying to unsubscribe a receiver from the master set but I'm getting
> > an out of shared memory message:
> >
> > cat unsubscribe2.sl <http://unsubscribe2.sl>
> > cluster name = slcluster;
> > node 1 admin conninfo = 'dbname=FiscalWeb host=192.168.23.10
> user=slonyo';
> > node 2 admin conninfo = 'dbname=FiscalWeb host=192.168.23.11
> user=slonyn';
> > unsubscribe set ( id = 1 , receiver = 2);
> >
> > slonik < unsubscribe2.sl <http://unsubscribe2.sl>
> > <stdin>:4: WARNING:  out of shared memory
> > CONTEXT:  SQL statement "drop trigger "_slcluster_logtrigger" on
> > "8359_wsn"."tbpgdas01502""
> > PL/pgSQL function altertabledroptriggers(integer) line 47 at EXECUTE
> > statement
> > SQL statement "SELECT
> "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
> > PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at
> PERFORM
> > <stdin>:4: PGRES_FATAL_ERROR lock table "_slcluster".sl_event_lock,
> > "_slcluster".sl_config_lock;select "_slcluster".unsubscribeSet(1,
> > 2,false);  - ERROR:  out of shared memory
> > HINT:  You might need to increase max_locks_per_transaction.
> > CONTEXT:  SQL statement "drop trigger "_slcluster_logtrigger" on
> > "8359_wsn"."tbpgdas01502""
> > PL/pgSQL function altertabledroptriggers(integer) line 47 at EXECUTE
> > statement
> > SQL statement "SELECT
> "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
> > PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at
> PERFORM
> > [postgres at 00002-NfseNet-SGDB reimer]$ ^C
> >
> > max_locks_per_transaction is set to 255. Changed to 4096 and will
> > restart the database during the night change window.
> >
> > Tried to repeat the process again and now I'm getting always the same
> error:
> > [postgres at 00002-NfseNet-SGDB reimer]$ slonik < unsubscribe2.sl
> > <http://unsubscribe2.sl>
> > <stdin>:4: PGRES_FATAL_ERROR lock table "_slcluster".sl_event_lock,
> > "_slcluster".sl_config_lock;select "_slcluster".unsubscribeSet(1,
> > 2,false);  - ERROR:  deadlock detected
> > DETAIL:  Process 15366 waits for AccessExclusiveLock on relation 29564
> > of database 16384; blocked by process 14994.
> > Process 14994 waits for RowExclusiveLock on relation 84222 of database
> > 16384; blocked by process 15366.
> > HINT:  See server log for query details.
> > CONTEXT:  SQL statement "lock table "7481_spunico"."sincdc" in access
> > exclusive mode"
> > PL/pgSQL function altertabledroptriggers(integer) line 42 at EXECUTE
> > statement
> > SQL statement "SELECT
> "_slcluster".alterTableDropTriggers(v_tab_row.tab_id)"
> > PL/pgSQL function unsubscribeset(integer,integer,boolean) line 49 at
> PERFORM
> > [postgres at 00002-NfseNet-SGDB reimer]$
> >
> > The two pids reported by the deadlock message are probably temporary
> > processed created by SLONY as I did not find them in the system.
> >
> > The increase in the max_locks_per_transactions and the server restart
> > will fix this issue?
> >
>
>
> If you have 27,000 tables in your replication set then I think
> max_locks_per_transaction might needs to be at least that big.  The
> alterTableDropTriggers function  will take a lock on each table in the set.
>
> As for your deadlock you should figure out what process 15366 is
> Since this operation takes an exclusive lock on all tables in the set on
> the replica you probably don't want any other processes accessing those
> tables at that point in time.
>
>
>
>
>
> > Thank you!
> >
> > --
> > Reimer
> > 47-3347-1724 47-9183-0547 msn: carlos.reimer at opendb.com.br
> > <mailto:carlos.reimer at opendb.com.br>
> >
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> >
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Reimer
47-3347-1724 47-9183-0547 msn: carlos.reimer at opendb.com.br
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141217/34b77f43/attachment-0001.htm 

From jan at wi3ck.info  Wed Dec 17 15:02:35 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Wed, 17 Dec 2014 18:02:35 -0500
Subject: [Slony1-general] Duplicate key while merging temporary to main
	set
In-Reply-To: <CAJnnue01EmPemk6xfAHzVA-kKQqscHGG1n_ynU3M-Y_Oh1QPZQ@mail.gmail.com>
References: <CAJnnue3xNufp-GXSWD7XAfx=UNig8fVprMc43DSOB2sPM892Qw@mail.gmail.com>
	<54907236.2090303@wi3ck.info>
	<CAJnnue01EmPemk6xfAHzVA-kKQqscHGG1n_ynU3M-Y_Oh1QPZQ@mail.gmail.com>
Message-ID: <CAGBW59fhLwi02g7ZgvtCy4qVGKu02XqMRcGB2mKQMBpQH0PS-g@mail.gmail.com>

On Dec 16, 2014 1:12 PM, "Carlos Henrique Reimer" <
carlos.reimer at opendb.com.br> wrote:
>
> Hi Jan,
>
> Just verified the row again and I see it is visible again even after
removing it with DELETE this morning. Checked the replica and the table
contains the same row with same contents.
>
> Tried a reindex table and it worked fine.
>

The reindex worked because you deleted one of the heap row versions. As
said, you just have a good chance to have deleted the wrong one.

Jan

--
Jan Wieck
Senior Software Engineer
http://slony.info
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141217/49731972/attachment.htm 

From jan at wi3ck.info  Wed Dec 17 15:05:50 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Wed, 17 Dec 2014 18:05:50 -0500
Subject: [Slony1-general] PGRES_FATAL_ERROR ERROR: could not access file
 "$libdir/slony1_funcs.2.2.3": No such file or directory
In-Reply-To: <CAEaSS0YsHQpgirW0bRdEbA9YbUFRjK7dTZ+dKmFBXTr8+dSjgw@mail.gmail.com>
References: <CAEaSS0YsHQpgirW0bRdEbA9YbUFRjK7dTZ+dKmFBXTr8+dSjgw@mail.gmail.com>
Message-ID: <CAGBW59erGnr00Awyjr5MTt2LuFY5-xKA2sY71=v3vGwUxuRa_w@mail.gmail.com>

On Dec 16, 2014 1:35 AM, "Tory M Blue" <tmblue at gmail.com> wrote:
>
> Trying a production upgrade, after upgrade 3 different environments and
I'm getting this error.
>
> PGRES_FATAL_ERROR ERROR:  could not access file
"$libdir/slony1_funcs.2.2.3": No such file or directory
>
> The files are there, anyway to force this? even tried to create a export
libdir with no success.

Are the files there on all the involved nodes?

Jan

--
Jan Wieck
Senior Software Engineer
http://slony.info
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141217/d6d5dcd7/attachment.htm 

From tmblue at gmail.com  Wed Dec 17 15:13:58 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 17 Dec 2014 15:13:58 -0800
Subject: [Slony1-general] PGRES_FATAL_ERROR ERROR: could not access file
 "$libdir/slony1_funcs.2.2.3": No such file or directory
In-Reply-To: <CAGBW59erGnr00Awyjr5MTt2LuFY5-xKA2sY71=v3vGwUxuRa_w@mail.gmail.com>
References: <CAEaSS0YsHQpgirW0bRdEbA9YbUFRjK7dTZ+dKmFBXTr8+dSjgw@mail.gmail.com>
	<CAGBW59erGnr00Awyjr5MTt2LuFY5-xKA2sY71=v3vGwUxuRa_w@mail.gmail.com>
Message-ID: <CAEaSS0Yqa1i4Bbx6ffbfLTXRp3aNS1QKgvs-NHU2C7MmSt2Vrg@mail.gmail.com>

On Wed, Dec 17, 2014 at 3:05 PM, Jan Wieck <jan at wi3ck.info> wrote:
>
> On Dec 16, 2014 1:35 AM, "Tory M Blue" <tmblue at gmail.com> wrote:
> >
> > Trying a production upgrade, after upgrade 3 different environments and
> I'm getting this error.
> >
> > PGRES_FATAL_ERROR ERROR:  could not access file
> "$libdir/slony1_funcs.2.2.3": No such file or directory
> >
> > The files are there, anyway to force this? even tried to create a export
> libdir with no success.
>
> Are the files there on all the involved nodes?
>
> Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>

Yes that was the rub and where the documentation needs to be more clear. It
was not and the error doesn't bubble up the fact that the error came from a
slave or other node. So I beat myself around the head for an hour, looking
at one post where your exact response was provided, 3-5 times, never once
having it register that was my issue :)  Once i upgraded the other 3 nodes,
things worked perfectly, but I was spending a lot of time looking at paths,
looking at my scripts on the box that I had upgraded.

So it would have been nice if the error stated that so and so node failed :)

But I finally got it, thanks Jan!

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141217/e3f23163/attachment.htm 

From scott.marlowe at gmail.com  Wed Dec 17 20:35:16 2014
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 17 Dec 2014 21:35:16 -0700
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
	9.2.9/Slony 2.2.1
Message-ID: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>

So as the title states, we're running pg 9.2.9 and slony 2.2.1

If you create a parent / child table with inherit, then truncate only
the parent table, it will work properly on the source but will
propagate as a regular truncate on the destinations.

create table parent yada;
create table chlid inherits parent yada;
insert rows into parent, and rows into child tables

subscribe parent and child to another node...

on source run "truncate only parent" and note that child table there
still has its own rows. Check on subscriber and children tables are
now empty.

1: Is this a known bug?
2: Is it planned to fix it?

We were bitten by it quite by surprise and lost a lot of data in child
tables. Luckily we noticed before switching over from the master node
to another and resubed the child tables.

-- 
To understand recursion, one must first understand recursion.

From ssinger at ca.afilias.info  Thu Dec 18 04:33:40 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 18 Dec 2014 07:33:40 -0500
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
Message-ID: <5492C9A4.5010802@ca.afilias.info>

On 12/17/2014 11:35 PM, Scott Marlowe wrote:
> So as the title states, we're running pg 9.2.9 and slony 2.2.1
>
> If you create a parent / child table with inherit, then truncate only
> the parent table, it will work properly on the source but will
> propagate as a regular truncate on the destinations.
>
> create table parent yada;
> create table chlid inherits parent yada;
> insert rows into parent, and rows into child tables
>
> subscribe parent and child to another node...
>
> on source run "truncate only parent" and note that child table there
> still has its own rows. Check on subscriber and children tables are
> now empty.
>
> 1: Is this a known bug?
> 2: Is it planned to fix it?
>

No this isn't a known bug, this worked properly prior to 2.1 when the 
pl/sql function was used to apply the truncate on a replica (it did a 
truncate only).  The C apply trigger in 2.2 is missing the 'only'.

I think we'll need to fix this.
There is a handful of bugs that we are close to committing fixes for, 
once those are in we should do a 2.2.4 release


> We were bitten by it quite by surprise and lost a lot of data in child
> tables. Luckily we noticed before switching over from the master node
> to another and resubed the child tables.
>


From scott.marlowe at gmail.com  Thu Dec 18 09:42:58 2014
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 18 Dec 2014 10:42:58 -0700
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <5492C9A4.5010802@ca.afilias.info>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
Message-ID: <CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>

On Thu, Dec 18, 2014 at 5:33 AM, Steve Singer <ssinger at ca.afilias.info> wrote:
> On 12/17/2014 11:35 PM, Scott Marlowe wrote:
>>
>> So as the title states, we're running pg 9.2.9 and slony 2.2.1
>>
>> If you create a parent / child table with inherit, then truncate only
>> the parent table, it will work properly on the source but will
>> propagate as a regular truncate on the destinations.
>>
>> create table parent yada;
>> create table chlid inherits parent yada;
>> insert rows into parent, and rows into child tables
>>
>> subscribe parent and child to another node...
>>
>> on source run "truncate only parent" and note that child table there
>> still has its own rows. Check on subscriber and children tables are
>> now empty.
>>
>> 1: Is this a known bug?
>> 2: Is it planned to fix it?
>>
>
> No this isn't a known bug, this worked properly prior to 2.1 when the pl/sql
> function was used to apply the truncate on a replica (it did a truncate
> only).  The C apply trigger in 2.2 is missing the 'only'.
>
> I think we'll need to fix this.
> There is a handful of bugs that we are close to committing fixes for, once
> those are in we should do a 2.2.4 release
>

Thanks! (and we're on 2.2.2 not 2.2.1 i just checked)

From ssinger at ca.afilias.info  Thu Dec 18 12:40:47 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 18 Dec 2014 15:40:47 -0500
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
Message-ID: <54933BCF.5040003@ca.afilias.info>

On 12/18/2014 12:42 PM, Scott Marlowe wrote:

I have added a patch for this against

http://bugs.slony.info/bugzilla/show_bug.cgi?id=356


> On Thu, Dec 18, 2014 at 5:33 AM, Steve Singer <ssinger at ca.afilias.info> wrote:
>> On 12/17/2014 11:35 PM, Scott Marlowe wrote:
>>>
>>> So as the title states, we're running pg 9.2.9 and slony 2.2.1
>>>
>>> If you create a parent / child table with inherit, then truncate only
>>> the parent table, it will work properly on the source but will
>>> propagate as a regular truncate on the destinations.
>>>
>>> create table parent yada;
>>> create table chlid inherits parent yada;
>>> insert rows into parent, and rows into child tables
>>>
>>> subscribe parent and child to another node...
>>>
>>> on source run "truncate only parent" and note that child table there
>>> still has its own rows. Check on subscriber and children tables are
>>> now empty.
>>>
>>> 1: Is this a known bug?
>>> 2: Is it planned to fix it?
>>>
>>
>> No this isn't a known bug, this worked properly prior to 2.1 when the pl/sql
>> function was used to apply the truncate on a replica (it did a truncate
>> only).  The C apply trigger in 2.2 is missing the 'only'.
>>
>> I think we'll need to fix this.
>> There is a handful of bugs that we are close to committing fixes for, once
>> those are in we should do a 2.2.4 release
>>
>
> Thanks! (and we're on 2.2.2 not 2.2.1 i just checked)
>


From davecramer at gmail.com  Wed Dec 24 04:32:19 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Wed, 24 Dec 2014 07:32:19 -0500
Subject: [Slony1-general] 9.3 origin,
 8.3 subscriber bytea_output =escape for the slony user doesn't seem
 to work
Message-ID: <CADK3HHKVPn7PUjwG8EFmriiyYi4zgaHR7G+n-wiTDvZh_8dwyA@mail.gmail.com>

I have checked and rechecked this. The slony user on 9.3 has bytea_output
set to escape, but for some reason hex encoded data is in the logs.

Where in the code can I check to see how or why this is happening ?

Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141224/1bab865d/attachment.htm 

From cbbrowne at afilias.info  Wed Dec 24 07:20:22 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 24 Dec 2014 10:20:22 -0500
Subject: [Slony1-general] 9.3 origin,
 8.3 subscriber bytea_output =escape for the slony user doesn't seem
 to work
In-Reply-To: <CADK3HHKVPn7PUjwG8EFmriiyYi4zgaHR7G+n-wiTDvZh_8dwyA@mail.gmail.com>
References: <CADK3HHKVPn7PUjwG8EFmriiyYi4zgaHR7G+n-wiTDvZh_8dwyA@mail.gmail.com>
Message-ID: <CANfbgbaunzbzbMDis8cBbjOwWtBZ3zA7FYBRShMi0OiCOP-G0w@mail.gmail.com>

I'll bet the log trigger (C/SPI) hard codes this encoding in
src/backend/slony1_funcs.c, but it's not too obvious in the code.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141224/4a90dc06/attachment.htm 

From davecramer at gmail.com  Wed Dec 24 10:16:45 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Wed, 24 Dec 2014 13:16:45 -0500
Subject: [Slony1-general] 9.3 origin,
 8.3 subscriber bytea_output =escape for the slony user doesn't seem
 to work
In-Reply-To: <CANfbgbaunzbzbMDis8cBbjOwWtBZ3zA7FYBRShMi0OiCOP-G0w@mail.gmail.com>
References: <CADK3HHKVPn7PUjwG8EFmriiyYi4zgaHR7G+n-wiTDvZh_8dwyA@mail.gmail.com>
	<CANfbgbaunzbzbMDis8cBbjOwWtBZ3zA7FYBRShMi0OiCOP-G0w@mail.gmail.com>
Message-ID: <CADK3HH+6Q2YEajE0OaP7bA5n19_BcWGunJYfO7DdjtJASFNoOg@mail.gmail.com>

So I changed the bytea_output at the cluster level and it now works ???

This is not consistent with my understanding of how pg should handle user
level config settings ?

Dave Cramer

On 24 December 2014 at 10:20, Christopher Browne <cbbrowne at afilias.info>
wrote:
>
> I'll bet the log trigger (C/SPI) hard codes this encoding in
> src/backend/slony1_funcs.c, but it's not too obvious in the code.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20141224/8c2b1e91/attachment.htm 

