From dragospruteanu at yahoo.com  Tue Apr  2 01:37:48 2013
From: dragospruteanu at yahoo.com (Pruteanu Dragos)
Date: Tue, 2 Apr 2013 01:37:48 -0700 (PDT)
Subject: [Slony1-general] Slony on High Load
Message-ID: <1364891868.43995.YahooMailNeo@web140105.mail.bf1.yahoo.com>

Hi Slony admins,
?
Here I have a problem for a Slony setup on a really loaded primary database.?
I try to build the slony and got from time to time an error.
Maybe is related to the high load we have. I hope you can help.


?
PGRES_FATAL_ERROR ERROR:? stack depth limit exceeded
HINT:? Increase the
configuration parameter "max_stack_depth", after ensuring the
platform's stack depth limit is adequate.
?
The line before this error message in the slony logs has
~11MB worth of text consisting mainly in a long concatenation of:
?
... and? log_actionseq <> '...'
?
This data is also present in the sl_setsync table.
?
The problem happens immediately after the slave finishes
syncing the set, enables the subscription and tries to do the first sync.
?
I found a thread about it here:
?
http://old.nabble.com/Slave-can%27t-catch-up,-postgres-error-%
27stack-depth-limit-exceeded%27-td33182661.html
?
We're running on postgres 9.0.10 and slony1 version
2.0.7, and upgrading is not an option in the near future (eventually we will
upgrade both postgres and slony).
?
The problem is that we hit this issue now more and more
regularly - and it is a killer for the slony replication, as it is not possible
to reliably set it up...
?
What I already tried and didn't help:
?
?* set
max_stack_depth up to ridiculous amounts (10s of GB) - not sure if I got the OS
side of it right, but I did my best;
?
?* decrease the
slon deamon's SYNC_CHECK_INTERVAL to 1 second;
?
With both those I still get the error regularly...
?
I wonder if this is fixed in newer slony releases, or if
there's any chance I can get some help/directions on how to fix/patch it in the
version we use to avoid this problem ?
?
Jan Wieck mentions in the thread cited above that the a
solution would
be:
?
<quote>
The improvement for a future release would be to have the
remote worker get the log_actionseq list at the beginning of copy_set. If that
list is longer than a configurable maximum, it would abort the subscribe and
retry in a few seconds. It may take a couple of retries, but it should
eventually hit a moment where a SYNC event was created recently enough so that
there are only a few hundred log rows to ignore.
</quote>
?
Was this already implemented in a newer release ?
?
If not I would like to work on it, including back-patch
for the 2.0.7 version we use...
?
I would appreciate any help/hints on how to approach this
!
?
Cheers,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130402/e875c5b8/attachment.htm 

From ssinger at ca.afilias.info  Tue Apr  2 06:42:44 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 02 Apr 2013 09:42:44 -0400
Subject: [Slony1-general] Slony on High Load
In-Reply-To: <1364891868.43995.YahooMailNeo@web140105.mail.bf1.yahoo.com>
References: <1364891868.43995.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Message-ID: <515AE054.60000@ca.afilias.info>

On 13-04-02 04:37 AM, Pruteanu Dragos wrote:
> Hi Slony admins,
> Here I have a problem for a Slony setup on a really loaded primary
> database.
> I try to build the slony and got from time to time an error.
> Maybe is related to the high load we have. I hope you can help.
>
>
> PGRES_FATAL_ERROR ERROR: stack depth limit exceeded
> HINT:  Increase the configuration parameter "max_stack_depth", after
> ensuring the platform's stack depth limit is adequate.
> The line before this error message in the slony logs has ~11MB worth of
> text consisting mainly in a long concatenation of:
> ... and log_actionseq <> '...'
> This data is also present in the sl_setsync table.
> The problem happens immediately after the slave finishes syncing the
> set, enables the subscription and tries to do the first sync.
> I found a thread about it here:
> http://old.nabble.com/Slave-can%27t-catch-up,-postgres-error-%
> <http://old.nabble.com/Slave-can%27t-catch-up,-postgres-error-%25>
> 27stack-depth-limit-exceeded%27-td33182661.html
> We're running on postgres 9.0.10 and slony1 version 2.0.7, and upgrading
> is not an option in the near future (eventually we will upgrade both
> postgres and slony).
> The problem is that we hit this issue now more and more regularly - and
> it is a killer for the slony replication, as it is not possible to
> reliably set it up...
> What I already tried and didn't help:
>   * set max_stack_depth up to ridiculous amounts (10s of GB) - not sure
> if I got the OS side of it right, but I did my best;
>   * decrease the slon deamon's SYNC_CHECK_INTERVAL to 1 second;
> With both those I still get the error regularly...
> I wonder if this is fixed in newer slony releases, or if there's any
> chance I can get some help/directions on how to fix/patch it in the
> version we use to avoid this problem ?
> Jan Wieck mentions in the thread cited above that the a solution would
> be:
> <quote>
> The improvement for a future release would be to have the remote worker
> get the log_actionseq list at the beginning of copy_set. If that list is
> longer than a configurable maximum, it would abort the subscribe and
> retry in a few seconds. It may take a couple of retries, but it should
> eventually hit a moment where a SYNC event was created recently enough
> so that there are only a few hundred log rows to ignore.
> </quote>
> Was this already implemented in a newer release ?
> If not I would like to work on it, including back-patch for the 2.0.7
> version we use...
> I would appreciate any help/hints on how to approach this !
> Cheers,

See bug 264 http://www.slony.info/bugzilla/show_bug.cgi?id=264 and the 
patches referenced.




>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From vivek at khera.org  Tue Apr 16 06:10:58 2013
From: vivek at khera.org (Vick Khera)
Date: Tue, 16 Apr 2013 09:10:58 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
Message-ID: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>

For years, I have run slony (currently version 2.1.3) with one origin and
one replica. Every night at midnight, I run vacuum analyze on the whole DB.
 I still run autovacuum with its default settings. The midnight vacuum
takes approximately 4.5 hours to run.

All was well until I upgraded the DB from 8.4 to 9.2.x. (at the same time I
upgraded slony from 2.0).  Now, every night, the replication basically
stalls mid-way through the vacuum.

Here is what I observe:

midnight - vacuum starts on origin and replica
3:12am - replication delay reaches > 7 minutes
3:15am - replication delay = 624 seconds
3:30am - replication delay = 1524 seconds
3:45am - replication delay = 2423 seconds
... basically replication has stopped
4:30am - replication delay = 5124 seconds
4:40am - vacuum ends on replica
4:41am - vacuum ends on origind
4:45am - replication delay = 1018 seconds
4:49am - replication lag drops to under 5 minutes (I consider this
recovered)


At no other time during the day, even when the DB is very very busy doing
lots of writes and a fair number of reads, does the replication lag more
than 5 or 10 seconds.

I have another DB on another pair of machines that is reasonably large as
well, that does nightly vacuum similarly. It is running slony 2.1 also, but
the DB version it replicates from is 8.3 to a 9.1. I never see any massive
delay in replication on there.

So my instinct is that there is some change in 9.2 that slony is tripping
over that is causing it to lock something for way too long. I would
appreciate any guidance on figuring out what that is, so I can avoid having
long delays in my replication while vacuum is running.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130416/9ce315bd/attachment.htm 

From vivek at khera.org  Tue Apr 16 08:18:16 2013
From: vivek at khera.org (Vick Khera)
Date: Tue, 16 Apr 2013 11:18:16 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
Message-ID: <CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>

On Tue, Apr 16, 2013 at 9:10 AM, Vick Khera <vivek at khera.org> wrote:

> So my instinct is that there is some change in 9.2 that slony is tripping
> over that is causing it to lock something for way too long. I would
> appreciate any guidance on figuring out what that is, so I can avoid having
> long delays in my replication while vacuum is running.


Hmmm... I just realized that there is a pg_dump that starts at about 3am. I
wonder of that's the cause, and not slony/vacuum interaction after all. I
will experiment by changing the time of the vacuum tonight.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130416/6545b956/attachment.htm 

From ssinger at ca.afilias.info  Tue Apr 16 08:35:55 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 16 Apr 2013 11:35:55 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
	<CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
Message-ID: <516D6FDB.1070808@ca.afilias.info>

On 13-04-16 11:18 AM, Vick Khera wrote:
>
> On Tue, Apr 16, 2013 at 9:10 AM, Vick Khera <vivek at khera.org
> <mailto:vivek at khera.org>> wrote:
>
>     So my instinct is that there is some change in 9.2 that slony is
>     tripping over that is causing it to lock something for way too long.
>     I would appreciate any guidance on figuring out what that is, so I
>     can avoid having long delays in my replication while vacuum is running.
>
>
> Hmmm... I just realized that there is a pg_dump that starts at about
> 3am. I wonder of that's the cause, and not slony/vacuum interaction
> after all. I will experiment by changing the time of the vacuum tonight.
>

Slony takes needs an exclusive lock on sl_event to create SYNC events. 
If your pg_dump includes the slony schema then you should expect 
replication to pause.  A lot of people exclude the slony schema from 
their pg_dumps for this reason.




>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From vivek at khera.org  Tue Apr 16 09:06:20 2013
From: vivek at khera.org (Vick Khera)
Date: Tue, 16 Apr 2013 12:06:20 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <516D6FDB.1070808@ca.afilias.info>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
	<CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
	<516D6FDB.1070808@ca.afilias.info>
Message-ID: <CALd+dccjaZYMZyn5gHihBvfj-hM5VK5_4DkXvA+jsr_CWTOJQw@mail.gmail.com>

On Tue, Apr 16, 2013 at 11:35 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Slony takes needs an exclusive lock on sl_event to create SYNC events. If
> your pg_dump includes the slony schema then you should expect replication
> to pause.  A lot of people exclude the slony schema from their pg_dumps for
> this reason.
>

Good idea. Seems contrary to the purpose of these particular backups to
even need the slony schema. Thanks for the tip!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130416/f3c83439/attachment.htm 

From vivek at khera.org  Wed Apr 17 11:36:36 2013
From: vivek at khera.org (Vick Khera)
Date: Wed, 17 Apr 2013 14:36:36 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <CALd+dccjaZYMZyn5gHihBvfj-hM5VK5_4DkXvA+jsr_CWTOJQw@mail.gmail.com>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
	<CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
	<516D6FDB.1070808@ca.afilias.info>
	<CALd+dccjaZYMZyn5gHihBvfj-hM5VK5_4DkXvA+jsr_CWTOJQw@mail.gmail.com>
Message-ID: <CALd+dce6THpWrp5-AriZj97ksOTfDJiUEuDaFZDsB2982nLBVQ@mail.gmail.com>

To close this out for the list, this did solve my issue. No replication
delays were reported when the dump was told to skip the slony schema.


On Tue, Apr 16, 2013 at 12:06 PM, Vick Khera <vivek at khera.org> wrote:

>
> On Tue, Apr 16, 2013 at 11:35 AM, Steve Singer <ssinger at ca.afilias.info>wrote:
>
>> Slony takes needs an exclusive lock on sl_event to create SYNC events. If
>> your pg_dump includes the slony schema then you should expect replication
>> to pause.  A lot of people exclude the slony schema from their pg_dumps for
>> this reason.
>>
>
> Good idea. Seems contrary to the purpose of these particular backups to
> even need the slony schema. Thanks for the tip!
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130417/e17fd20a/attachment.htm 

From mjames at profitpoint.com  Thu Apr 18 07:19:39 2013
From: mjames at profitpoint.com (Mike James)
Date: Thu, 18 Apr 2013 14:19:39 +0000
Subject: [Slony1-general] yum error uninstalling slony1
Message-ID: <F024DCE3402750409CD407C414C07E1A1D39AA3C@BY2PRD0811MB404.namprd08.prod.outlook.com>

Hi, I'm running RHEL 5.9, 64-bit, fully patched. I've installed the pgdg92 repo and I've installed Postgresql-9.2.4 and slony1. Today, I was doing some troubleshooting and I found two versions of slony1-92 installed on the system. I tried to uninstall the older version but I get an error. Any ideas what caused this error, or how to resolve it? I rebuilt the rpm database, also ran "yum clean all". The problem is not fixed.

[root at pg-server ~]# rpm -qa | grep slony1
slony1-92-2.1.2-1.rhel5
slony1-92-2.1.3-1.rhel5

[root at pg-server ~]# rpm -e slony1-92-2.1.2
error: %postun(slony1-92-2.1.2-1.rhel5.x86_64) scriptlet failed, exit status 5

And there are still 2 versions of slony in the rpm database:

[root at pg-server ~]# rpm -qa | grep slony1
slony1-92-2.1.2-1.rhel5
slony1-92-2.1.3-1.rhel5

thanks, Mike
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130418/760e9378/attachment.htm 

From devrim at gunduz.org  Sun Apr 21 13:11:20 2013
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Sun, 21 Apr 2013 23:11:20 +0300
Subject: [Slony1-general] yum error uninstalling slony1
In-Reply-To: <F024DCE3402750409CD407C414C07E1A1D39AA3C@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A1D39AA3C@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <1366575080.2968.1.camel@lenovo01-laptop03.gunduz.org>


Hi,

On Thu, 2013-04-18 at 14:19 +0000, Mike James wrote:
> Hi, I'm running RHEL 5.9, 64-bit, fully patched. I've installed the pgdg92 repo and I've installed Postgresql-9.2.4 and slony1. Today, I was doing some troubleshooting and I found two versions of slony1-92 installed on the system. I tried to uninstall the older version but I get an error. Any ideas what caused this error, or how to resolve it? I rebuilt the rpm database, also ran "yum clean all". The problem is not fixed.
> 
> [root at pg-server ~]# rpm -qa | grep slony1
> slony1-92-2.1.2-1.rhel5
> slony1-92-2.1.3-1.rhel5
> 
> [root at pg-server ~]# rpm -e slony1-92-2.1.2
> error: %postun(slony1-92-2.1.2-1.rhel5.x86_64) scriptlet failed, exit status 5

This is something that I fixed in 2.1.3:

* Tue Feb 19 2013 Devrim Gunduz <devrim at gunduz.org> 2.1.3-1
- Update to 2.1.3
- Fix init script names in %%postun and %%preun.

> And there are still 2 versions of slony in the rpm database:
> 
> [root at pg-server ~]# rpm -qa | grep slony1
> slony1-92-2.1.2-1.rhel5
> slony1-92-2.1.3-1.rhel5

rpm -e --noscripts slony1-91-2.1.2 

will do the trick. It will not appear in other updates.

Regards,
-- 
Devrim G?ND?Z
Principal Systems Engineer @ EnterpriseDB: http://www.enterprisedb.com
PostgreSQL Dan??man?/Consultant, Red Hat Certified Engineer
Community: devrim~PostgreSQL.org, devrim.gunduz~linux.org.tr
http://www.gunduz.org  Twitter: http://twitter.com/devrimgunduz
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20130421/aaa490c0/attachment.pgp 

From andrew at asi-web.com  Mon Apr 22 15:45:34 2013
From: andrew at asi-web.com (andrew)
Date: Mon, 22 Apr 2013 22:45:34 +0000
Subject: [Slony1-general] Slony Set Sync Tracking and Archive Tracking - how
 to shift between these?
Message-ID: <88CAA3D677DBF347A10B6CA1B28D6DFE3C1A179A@MSSERVER.asi-web.local>

For some time now, we've been working with an older SuSE 11.3 server that had slony 1.1 (I think 1.1.2, possibly 1.1.5) on it.  Our remote servers were being replicated to by means of the log shipping setup.  As a result, the remote machines are all prepared for log files that have entries like the following:



select "_cpc_replic".setsyncTracking_offline(12, '476698', '476700', '2013-04-22 11:00:32.050019');



However, for the past week, I've been working on getting a new server (RHEL 6.4 virtual machine) in place as our primary replication box.  I couldn't get the old slony version to install on that correctly, so I ended up downloading and installing Slony 2.0.7 on it.  As a result, the logshipping files now no longer have the line above, they instead have a line much like the one below:



select "_cpc_replic".archiveTracking_offline('5945', '2013-04-22 11:07:38.617205');



How do I go about converting those?  My preference would be to modify the process itself so it goes back to generating the .setsyncTracking_offline statements, but if that cannot be done, where can I find a good set of instructions on converting our current database replication schemas to correctly handle the new statement types?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130422/552dcf1a/attachment.htm 

From mtsahakis at gmail.com  Wed Apr 24 01:01:16 2013
From: mtsahakis at gmail.com (Manos Tsahakis)
Date: Wed, 24 Apr 2013 11:01:16 +0300
Subject: [Slony1-general] session_replication_role 'replica' behavior and
	referential integrity constraints
Message-ID: <CABuvXqoC7op4y3E_Bf+m_MPu2bLn=3NbzS-OkNb-FOQdH0GZWA@mail.gmail.com>

Hello all,

In our application we are enabling session_replication_role TO 'replica' in
certain situations so that triggers will not fire in a table during DML
operations. However, we observed that when setting session_replication_role
TO 'replica' referential integrity constraints will not fire on a table
either.

A simple example is given bellow:

dynacom=# create table parent (id serial primary key, name text not null);

dynacom=# create table child (id serial primary key, name text not null,pid
int NOT NULL REFERENCES parent(id) ON DELETE CASCADE);

dynacom=# insert into parent (name) values ('test 1');
INSERT 0 1

dynacom=# insert into parent (name) values ('test 2');
INSERT 0 1

dynacom=# insert into child (name,pid) values ('test kid2',2);
INSERT 0 1
dynacom=# begin ;
BEGIN
dynacom=# set session_replication_role TO 'replica';
SET
dynacom=# delete from parent where id=2;
DELETE 1
dynacom=# commit ;
COMMIT

dynacom=# select * from child;
 id |   name    | pid
----+-----------+-----
  2 | test kid2 |   2
(1 row)

dynacom=# select * from parent;
 id | name
----+------
(0 rows)

So we are a left, basically, with an inconsistent database.

1. 9.2 documentation (
http://www.postgresql.org/docs/9.2/static/sql-altertable.html) in the "
DISABLE/ENABLE [ REPLICA | ALWAYS ] TRIGGER" section, makes a distinction
between USER (non system-constraint related) and ALL triggers, but does not
state that simply(??) enabled system (non-user) constraint triggers will
not fire in case of session_replication_role = replica. Shouldn't non-user
triggers *not* be affected by session_replication_role ?

2. Is there any way to just find the name of the FK constraint trigger and
convert it to
ENABLE ALWAYS?

For the above test we used postgresql 9.2, currently we are running
postgresql 9.0 in production.

Kind Regards,
manos
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130424/14e23c95/attachment.htm 

From tmblue at gmail.com  Thu Apr 25 11:29:10 2013
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 25 Apr 2013 11:29:10 -0700
Subject: [Slony1-general] Long slon replication times, 3 hours for 87, 873,
	597 rows
Message-ID: <CAEaSS0ZQqH94Jyqd7=_Rmz=HacLSDfN-0EaKY6LV99gqyOCfmw@mail.gmail.com>

Postgres 9.2.4, slony 2.1.2 (ya will be pushing to 2.1.3), but with my
postgres upgrade had to keep slony version the same for the first go
through.

But for  87,873,597 rows it took 3 hours to replicate over a gig link.
There were no signs of system health issues, iostat showed hardly anything,
no CPU/memory issues, so I'm wondering if there is some tuning that needs
to happen?

These are my master/slave so do have fsync on, maybe during a rebuild a
fsync off is in order, but not sure if that will truly help.

slon config is pretty basic

everything is default except for these values:

log_level=2
sync_interval=1000
remote_listen_timeout=10000

spotlightimpressions has  87,873,597 rows, took  12845.025 seconds
adimpressions has 1,542,169 rows, took  2098.679 seconds
listings has 4,080,091 rows . took 1491.740 seconds

2013-04-24 23:10:19 PDT CONFIG remoteWorkerThread_1: copy table
"db"."listings"

2013-04-24 23:10:19 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"db"."listings"

2013-04-24 23:35:10 PDT CONFIG remoteWorkerThread_1: 1491.740 seconds to
copy table "db"."listings"

2013-04-24 23:35:23 PDT CONFIG remoteWorkerThread_1: copy table
"db"."spotlightimpressions"

2013-04-24 23:35:23 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"db"."spotlightimpressions"

2013-04-25 03:09:28 PDT CONFIG remoteWorkerThread_1: 12845.025 seconds to
copy table "db"."spotlightimpressions"

2013-04-25 03:09:28 PDT CONFIG remoteWorkerThread_1: copy table
"db"."adimpressions"

2013-04-25 03:09:28 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"db"."adimpressions"

2013-04-25 03:44:26 PDT CONFIG remoteWorkerThread_1: 2098.679 seconds to
copy table "db"."adimpressions"

Where do I start looking? These tables will only grow and since upgrades
require full rebuilds etc, This is already painful, but will only get worse.

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130425/1bc2ab80/attachment.htm 

From cbbrowne at afilias.info  Thu Apr 25 12:40:37 2013
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 25 Apr 2013 15:40:37 -0400
Subject: [Slony1-general] Long slon replication times, 3 hours for 87,
 873, 597 rows
In-Reply-To: <CAEaSS0ZQqH94Jyqd7=_Rmz=HacLSDfN-0EaKY6LV99gqyOCfmw@mail.gmail.com>
References: <CAEaSS0ZQqH94Jyqd7=_Rmz=HacLSDfN-0EaKY6LV99gqyOCfmw@mail.gmail.com>
Message-ID: <CANfbgbYm9idvLYK7=LYxw3DC6RhwN6x4r5=mOth-_aRsexx72Q@mail.gmail.com>

I wonder what the amount of data is; that's not exactly the same as the
number of tuples...

At any rate, for it to take several hours to copy these pretty big tables
is not super-surprising.

A thing to compare with is to compare this with how long it takes to dump
"db.spotlightimpressions" using pg_dump and load it into another database;
the combination of COPY TO and COPY FROM would be expected to take somewhat
less than 12845s; add in generating indexes on the table and I'd expect
dump/restore to take very nearly the same amount of time.

Slony doesn't do "magic"; it's using the same features, albeit wrapped a
little differently.

If you can dump/restore that table in an hour, and it takes Slony 3-4
hours, then I'd be concerned, but if dump/restore takes ~3.5hr, then I'd
say Slony, running in similar time, is working as expected.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130425/3a4c8cc3/attachment.htm 

From tmblue at gmail.com  Thu Apr 25 13:25:21 2013
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 25 Apr 2013 13:25:21 -0700
Subject: [Slony1-general] Long slon replication times, 3 hours for 87,
 873, 597 rows
In-Reply-To: <CANfbgbYm9idvLYK7=LYxw3DC6RhwN6x4r5=mOth-_aRsexx72Q@mail.gmail.com>
References: <CAEaSS0ZQqH94Jyqd7=_Rmz=HacLSDfN-0EaKY6LV99gqyOCfmw@mail.gmail.com>
	<CANfbgbYm9idvLYK7=LYxw3DC6RhwN6x4r5=mOth-_aRsexx72Q@mail.gmail.com>
Message-ID: <CAEaSS0aLGb30dRmYu-daWAHRJSibZpVr7kxW5o=czZY7gjCogQ@mail.gmail.com>

On Thu, Apr 25, 2013 at 12:40 PM, Christopher Browne
<cbbrowne at afilias.info>wrote:

> I wonder what the amount of data is; that's not exactly the same as the
> number of tuples...
>
> At any rate, for it to take several hours to copy these pretty big tables
> is not super-surprising.
>
> A thing to compare with is to compare this with how long it takes to dump
> "db.spotlightimpressions" using pg_dump and load it into another database;
> the combination of COPY TO and COPY FROM would be expected to take somewhat
> less than 12845s; add in generating indexes on the table and I'd expect
> dump/restore to take very nearly the same amount of time.
>
> Slony doesn't do "magic"; it's using the same features, albeit wrapped a
> little differently.
>
> If you can dump/restore that table in an hour, and it takes Slony 3-4
> hours, then I'd be concerned, but if dump/restore takes ~3.5hr, then I'd
> say Slony, running in similar time, is working as expected.
>

Thanks Christopher, makes sense and I'll give it a try.

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130425/a606cd46/attachment.htm 

