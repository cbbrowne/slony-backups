From dragospruteanu at yahoo.com  Tue Apr  2 01:37:48 2013
From: dragospruteanu at yahoo.com (Pruteanu Dragos)
Date: Tue, 2 Apr 2013 01:37:48 -0700 (PDT)
Subject: [Slony1-general] Slony on High Load
Message-ID: <1364891868.43995.YahooMailNeo@web140105.mail.bf1.yahoo.com>

Hi Slony admins,
?
Here I have a problem for a Slony setup on a really loaded primary database.?
I try to build the slony and got from time to time an error.
Maybe is related to the high load we have. I hope you can help.


?
PGRES_FATAL_ERROR ERROR:? stack depth limit exceeded
HINT:? Increase the
configuration parameter "max_stack_depth", after ensuring the
platform's stack depth limit is adequate.
?
The line before this error message in the slony logs has
~11MB worth of text consisting mainly in a long concatenation of:
?
... and? log_actionseq <> '...'
?
This data is also present in the sl_setsync table.
?
The problem happens immediately after the slave finishes
syncing the set, enables the subscription and tries to do the first sync.
?
I found a thread about it here:
?
http://old.nabble.com/Slave-can%27t-catch-up,-postgres-error-%
27stack-depth-limit-exceeded%27-td33182661.html
?
We're running on postgres 9.0.10 and slony1 version
2.0.7, and upgrading is not an option in the near future (eventually we will
upgrade both postgres and slony).
?
The problem is that we hit this issue now more and more
regularly - and it is a killer for the slony replication, as it is not possible
to reliably set it up...
?
What I already tried and didn't help:
?
?* set
max_stack_depth up to ridiculous amounts (10s of GB) - not sure if I got the OS
side of it right, but I did my best;
?
?* decrease the
slon deamon's SYNC_CHECK_INTERVAL to 1 second;
?
With both those I still get the error regularly...
?
I wonder if this is fixed in newer slony releases, or if
there's any chance I can get some help/directions on how to fix/patch it in the
version we use to avoid this problem ?
?
Jan Wieck mentions in the thread cited above that the a
solution would
be:
?
<quote>
The improvement for a future release would be to have the
remote worker get the log_actionseq list at the beginning of copy_set. If that
list is longer than a configurable maximum, it would abort the subscribe and
retry in a few seconds. It may take a couple of retries, but it should
eventually hit a moment where a SYNC event was created recently enough so that
there are only a few hundred log rows to ignore.
</quote>
?
Was this already implemented in a newer release ?
?
If not I would like to work on it, including back-patch
for the 2.0.7 version we use...
?
I would appreciate any help/hints on how to approach this
!
?
Cheers,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130402/e875c5b8/attachment.htm 

From ssinger at ca.afilias.info  Tue Apr  2 06:42:44 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 02 Apr 2013 09:42:44 -0400
Subject: [Slony1-general] Slony on High Load
In-Reply-To: <1364891868.43995.YahooMailNeo@web140105.mail.bf1.yahoo.com>
References: <1364891868.43995.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Message-ID: <515AE054.60000@ca.afilias.info>

On 13-04-02 04:37 AM, Pruteanu Dragos wrote:
> Hi Slony admins,
> Here I have a problem for a Slony setup on a really loaded primary
> database.
> I try to build the slony and got from time to time an error.
> Maybe is related to the high load we have. I hope you can help.
>
>
> PGRES_FATAL_ERROR ERROR: stack depth limit exceeded
> HINT:  Increase the configuration parameter "max_stack_depth", after
> ensuring the platform's stack depth limit is adequate.
> The line before this error message in the slony logs has ~11MB worth of
> text consisting mainly in a long concatenation of:
> ... and log_actionseq <> '...'
> This data is also present in the sl_setsync table.
> The problem happens immediately after the slave finishes syncing the
> set, enables the subscription and tries to do the first sync.
> I found a thread about it here:
> http://old.nabble.com/Slave-can%27t-catch-up,-postgres-error-%
> <http://old.nabble.com/Slave-can%27t-catch-up,-postgres-error-%25>
> 27stack-depth-limit-exceeded%27-td33182661.html
> We're running on postgres 9.0.10 and slony1 version 2.0.7, and upgrading
> is not an option in the near future (eventually we will upgrade both
> postgres and slony).
> The problem is that we hit this issue now more and more regularly - and
> it is a killer for the slony replication, as it is not possible to
> reliably set it up...
> What I already tried and didn't help:
>   * set max_stack_depth up to ridiculous amounts (10s of GB) - not sure
> if I got the OS side of it right, but I did my best;
>   * decrease the slon deamon's SYNC_CHECK_INTERVAL to 1 second;
> With both those I still get the error regularly...
> I wonder if this is fixed in newer slony releases, or if there's any
> chance I can get some help/directions on how to fix/patch it in the
> version we use to avoid this problem ?
> Jan Wieck mentions in the thread cited above that the a solution would
> be:
> <quote>
> The improvement for a future release would be to have the remote worker
> get the log_actionseq list at the beginning of copy_set. If that list is
> longer than a configurable maximum, it would abort the subscribe and
> retry in a few seconds. It may take a couple of retries, but it should
> eventually hit a moment where a SYNC event was created recently enough
> so that there are only a few hundred log rows to ignore.
> </quote>
> Was this already implemented in a newer release ?
> If not I would like to work on it, including back-patch for the 2.0.7
> version we use...
> I would appreciate any help/hints on how to approach this !
> Cheers,

See bug 264 http://www.slony.info/bugzilla/show_bug.cgi?id=264 and the 
patches referenced.




>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From vivek at khera.org  Tue Apr 16 06:10:58 2013
From: vivek at khera.org (Vick Khera)
Date: Tue, 16 Apr 2013 09:10:58 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
Message-ID: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>

For years, I have run slony (currently version 2.1.3) with one origin and
one replica. Every night at midnight, I run vacuum analyze on the whole DB.
 I still run autovacuum with its default settings. The midnight vacuum
takes approximately 4.5 hours to run.

All was well until I upgraded the DB from 8.4 to 9.2.x. (at the same time I
upgraded slony from 2.0).  Now, every night, the replication basically
stalls mid-way through the vacuum.

Here is what I observe:

midnight - vacuum starts on origin and replica
3:12am - replication delay reaches > 7 minutes
3:15am - replication delay = 624 seconds
3:30am - replication delay = 1524 seconds
3:45am - replication delay = 2423 seconds
... basically replication has stopped
4:30am - replication delay = 5124 seconds
4:40am - vacuum ends on replica
4:41am - vacuum ends on origind
4:45am - replication delay = 1018 seconds
4:49am - replication lag drops to under 5 minutes (I consider this
recovered)


At no other time during the day, even when the DB is very very busy doing
lots of writes and a fair number of reads, does the replication lag more
than 5 or 10 seconds.

I have another DB on another pair of machines that is reasonably large as
well, that does nightly vacuum similarly. It is running slony 2.1 also, but
the DB version it replicates from is 8.3 to a 9.1. I never see any massive
delay in replication on there.

So my instinct is that there is some change in 9.2 that slony is tripping
over that is causing it to lock something for way too long. I would
appreciate any guidance on figuring out what that is, so I can avoid having
long delays in my replication while vacuum is running.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130416/9ce315bd/attachment.htm 

From vivek at khera.org  Tue Apr 16 08:18:16 2013
From: vivek at khera.org (Vick Khera)
Date: Tue, 16 Apr 2013 11:18:16 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
Message-ID: <CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>

On Tue, Apr 16, 2013 at 9:10 AM, Vick Khera <vivek at khera.org> wrote:

> So my instinct is that there is some change in 9.2 that slony is tripping
> over that is causing it to lock something for way too long. I would
> appreciate any guidance on figuring out what that is, so I can avoid having
> long delays in my replication while vacuum is running.


Hmmm... I just realized that there is a pg_dump that starts at about 3am. I
wonder of that's the cause, and not slony/vacuum interaction after all. I
will experiment by changing the time of the vacuum tonight.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130416/6545b956/attachment.htm 

From ssinger at ca.afilias.info  Tue Apr 16 08:35:55 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 16 Apr 2013 11:35:55 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
	<CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
Message-ID: <516D6FDB.1070808@ca.afilias.info>

On 13-04-16 11:18 AM, Vick Khera wrote:
>
> On Tue, Apr 16, 2013 at 9:10 AM, Vick Khera <vivek at khera.org
> <mailto:vivek at khera.org>> wrote:
>
>     So my instinct is that there is some change in 9.2 that slony is
>     tripping over that is causing it to lock something for way too long.
>     I would appreciate any guidance on figuring out what that is, so I
>     can avoid having long delays in my replication while vacuum is running.
>
>
> Hmmm... I just realized that there is a pg_dump that starts at about
> 3am. I wonder of that's the cause, and not slony/vacuum interaction
> after all. I will experiment by changing the time of the vacuum tonight.
>

Slony takes needs an exclusive lock on sl_event to create SYNC events. 
If your pg_dump includes the slony schema then you should expect 
replication to pause.  A lot of people exclude the slony schema from 
their pg_dumps for this reason.




>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From vivek at khera.org  Tue Apr 16 09:06:20 2013
From: vivek at khera.org (Vick Khera)
Date: Tue, 16 Apr 2013 12:06:20 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <516D6FDB.1070808@ca.afilias.info>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
	<CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
	<516D6FDB.1070808@ca.afilias.info>
Message-ID: <CALd+dccjaZYMZyn5gHihBvfj-hM5VK5_4DkXvA+jsr_CWTOJQw@mail.gmail.com>

On Tue, Apr 16, 2013 at 11:35 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Slony takes needs an exclusive lock on sl_event to create SYNC events. If
> your pg_dump includes the slony schema then you should expect replication
> to pause.  A lot of people exclude the slony schema from their pg_dumps for
> this reason.
>

Good idea. Seems contrary to the purpose of these particular backups to
even need the slony schema. Thanks for the tip!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130416/f3c83439/attachment.htm 

From vivek at khera.org  Wed Apr 17 11:36:36 2013
From: vivek at khera.org (Vick Khera)
Date: Wed, 17 Apr 2013 14:36:36 -0400
Subject: [Slony1-general] replication stalls with long-running vacuum
In-Reply-To: <CALd+dccjaZYMZyn5gHihBvfj-hM5VK5_4DkXvA+jsr_CWTOJQw@mail.gmail.com>
References: <CALd+dcev3kiev+qYibgUKRQTZU4m-zKfH0Oe1jFWoHCHQi0VLg@mail.gmail.com>
	<CALd+dcd_m8VUU4b2YMNNQMvOaFSmVnm4zmB+d2rVugdVdxRgWQ@mail.gmail.com>
	<516D6FDB.1070808@ca.afilias.info>
	<CALd+dccjaZYMZyn5gHihBvfj-hM5VK5_4DkXvA+jsr_CWTOJQw@mail.gmail.com>
Message-ID: <CALd+dce6THpWrp5-AriZj97ksOTfDJiUEuDaFZDsB2982nLBVQ@mail.gmail.com>

To close this out for the list, this did solve my issue. No replication
delays were reported when the dump was told to skip the slony schema.


On Tue, Apr 16, 2013 at 12:06 PM, Vick Khera <vivek at khera.org> wrote:

>
> On Tue, Apr 16, 2013 at 11:35 AM, Steve Singer <ssinger at ca.afilias.info>wrote:
>
>> Slony takes needs an exclusive lock on sl_event to create SYNC events. If
>> your pg_dump includes the slony schema then you should expect replication
>> to pause.  A lot of people exclude the slony schema from their pg_dumps for
>> this reason.
>>
>
> Good idea. Seems contrary to the purpose of these particular backups to
> even need the slony schema. Thanks for the tip!
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130417/e17fd20a/attachment.htm 

From mjames at profitpoint.com  Thu Apr 18 07:19:39 2013
From: mjames at profitpoint.com (Mike James)
Date: Thu, 18 Apr 2013 14:19:39 +0000
Subject: [Slony1-general] yum error uninstalling slony1
Message-ID: <F024DCE3402750409CD407C414C07E1A1D39AA3C@BY2PRD0811MB404.namprd08.prod.outlook.com>

Hi, I'm running RHEL 5.9, 64-bit, fully patched. I've installed the pgdg92 repo and I've installed Postgresql-9.2.4 and slony1. Today, I was doing some troubleshooting and I found two versions of slony1-92 installed on the system. I tried to uninstall the older version but I get an error. Any ideas what caused this error, or how to resolve it? I rebuilt the rpm database, also ran "yum clean all". The problem is not fixed.

[root at pg-server ~]# rpm -qa | grep slony1
slony1-92-2.1.2-1.rhel5
slony1-92-2.1.3-1.rhel5

[root at pg-server ~]# rpm -e slony1-92-2.1.2
error: %postun(slony1-92-2.1.2-1.rhel5.x86_64) scriptlet failed, exit status 5

And there are still 2 versions of slony in the rpm database:

[root at pg-server ~]# rpm -qa | grep slony1
slony1-92-2.1.2-1.rhel5
slony1-92-2.1.3-1.rhel5

thanks, Mike
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130418/760e9378/attachment.htm 

