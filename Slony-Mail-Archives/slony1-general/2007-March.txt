From cbbrowne at ca.afilias.info  Mon Mar  5 08:41:02 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar  5 08:41:11 2007
Subject: [Slony1-general] Migration to new site - main.slony.info
	/	lists.slony.info
In-Reply-To: <45EC4208.2040007@Yahoo.com> (Jan Wieck's message of "Mon,
	05 Mar 2007 11:15:04 -0500")
References: <45E851F1.6050007@ca.afilias.info> <45EC4208.2040007@Yahoo.com>
Message-ID: <60vehfve1t.fsf@dba2.int.libertyrms.com>

Jan Wieck <JanWieck@Yahoo.com> writes:
> Is it time now to switch www.slony.info and slony.info directly to
> that web site?

I think so.

I think it's also time to send messages to <slony1-general@lists.slony.info> :-)
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From ajs at crankycanuck.ca  Mon Mar  5 09:07:29 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Mar  5 09:07:49 2007
Subject: [Slony1-general] Migration to new site - main.slony.info
	/	lists.slony.info
In-Reply-To: <60vehfve1t.fsf@dba2.int.libertyrms.com>
References: <45E851F1.6050007@ca.afilias.info> <45EC4208.2040007@Yahoo.com>
	<60vehfve1t.fsf@dba2.int.libertyrms.com>
Message-ID: <20070305170729.GC10153@phlogiston.dyndns.org>

On Mon, Mar 05, 2007 at 11:41:02AM -0500, Christopher Browne wrote:
> 
> I think it's also time to send messages to <slony1-general@lists.slony.info> :-)

Is there a way to put a filter on the old list to forward everything
to the new?  I seem to remember this ability in mailman, but I forget
how you do it (it's a pity what not doing things for a yeaar or two
will do to knowledge you thought you had.  Sigh).  It's a
"forward-only" list or something.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The plural of anecdote is not data.
		--Roger Brinner
From cbbrowne at ca.afilias.info  Wed Mar  7 05:25:07 2007
From: cbbrowne at ca.afilias.info (cbbrowne@ca.afilias.info)
Date: Wed Mar  7 05:25:13 2007
Subject: [Slony1-general] Slave system becomes Slow
In-Reply-To: <585C7AB730180C4CB3D086B7256531FB6D5EC4@srit_mail.renaissance-it.com>
References: <585C7AB730180C4CB3D086B7256531FB6D5EC4@srit_mail.renaissance-it.com>
Message-ID: <64646.64.229.224.57.1173273907.squirrel@look.libertyrms.info>

> Dear Andrew
>
> Your right, the postgresql.conf and the Server OS kernel parameters are
> not tuned. After tuning the replication working fine.
> Thank you for your tech-help.

It's worth observing that while there is a certain amount of "magical
complexity" involved in replication (the way that one or two of the
queries are generated are pretty frightful to try to understand!),
ultimately it all runs as pretty plain, ordinary database operations.

Those operations are:

- All susceptible to any database configuration problems (as you
discovered!);

- Happily, they should also be susceptible to being fixed by common
improvements to the tuning of the DBMS;

- Susceptible to being reported in the logs, which should give you a
handle to be able to notice them (even if the query may be too "magic" to
understand :-)).

The work you do to tune the DBMS will help Slony-I too, which is good news...

From cbbrowne at ca.afilias.info  Wed Mar  7 15:32:41 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar  7 15:32:51 2007
Subject: [Slony1-general] Release of 1.1.7 and 1.2.7
Message-ID: <60tzwwr5nq.fsf@dba2.int.libertyrms.com>

We are pleased to release versions 1.1.7 and 1.2.7, which contain
fixes for various problems found in these branches over the last
several months.

Below are listed the release notes for these two releases.  Please see
<http://main.slony.info/> to find CVS and downloadable tarballs.

----------------------------------------------------------------------
$Id: RELEASE-1.1.7,v 1.1.2.6 2007-03-06 18:47:23 cbbrowne Exp $

- Add remote_listen_timeout parameter to slon.conf

This addresses the problem where a slon times out when accessing
sl_event if a node has been out of commission for a long time (several
days)

- Add node numbers to error reports in slonik

If portions of UNINSTALL NODE break, report in the error message which node
it was working on.  That way the gentle user gets an indication as to 
which node 'broke' if the slonik script contained multiple such
requests.

- Applied bug fix for #1538

If cluster has only one node, then remove all events up to the last
SYNC.  That allows the cleanup loop to clear out sl_log_{1/2}.
Otherwise, the log tables will forever bloat until you add a second
node...

- Added test to test1 for function generate_sync_event() and make_function_strict

- Added "v81" files (for slony1_base.v81.sql, slony1_funcs.v81.sql,
xxid.v81.sql), necessary to support 8.1 "ALTER FUNCTION ... STRICT";

- Fixed quoting problem in generate_sync_event()

- Added functionality to UPDATE FUNCTIONS to make xxidin() function
STRICT; the absence of this caused postmaster to fall over when
processing MOVE SET event in PG v8.2 (not to say that Slony-I 1.1 now
*supports* 8.2; it does not)

- When you run MOVE SET, this populates sl_setsync for the moved set
  even on nodes that are not subscribed.  If, subsequent to doing this, 
  you attempt a SUBSCRIBE SET for a formerly-unsubscribed node, the
  subscription will fail right at the end when the slon tries to insert a
  new value to sl_setsync.

  The fix: DELETE from sl_setsync immediately before the INSERT.  This
  will silently blow away any 'offending' sl_setsync row.

  (As observed by Afilias staff...)

- Log shipping fix - storage of sl_setsync_offline call had a wrong
  printf type; change from %d to %s

----------------------------------------------------------------------
$Id: RELEASE-1.2.7,v 1.1.2.7 2007-03-06 18:47:45 cbbrowne Exp $

- Add remote_listen_timeout parameter to slon.conf

This addresses the problem where a slon times out when accessing
sl_event if a node has been out of commission for a long time (several
days)

- Resolve bug #1623

In this bug, big "action lists" that need to get compressed could cause
a logging printf to blow up.  Changed the logging level so that detail
is only shown at level 4, which won't bite people by default.

- UNINSTALL NODE failures now show node # in slonik error messages

If a user ran several UNINSTALL NODE requests in a single slonik
script, and one of them broke, you'd have no ready way to tell which
node this failed on.  Added code to report the node # where it failed.

- Added test to test1 for function generate_sync_event() and make_function_strict

- Added "v81" files (for slony1_base.v81.sql, slony1_funcs.v81.sql,
xxid.v81.sql), necessary to support 8.1 "ALTER FUNCTION ... STRICT";

- Fixed quoting problem in generate_sync_event()

- Added functionality to UPDATE FUNCTIONS to make xxidin() function
STRICT; the absence of this caused postmaster to fall over when
processing MOVE SET event in PG v8.2

- Added documentation of an issue surrounding NULLABLE columns to the
log analysis chapter of the admin guide and to the UPGRADING docs.

- When you run MOVE SET, this populates sl_setsync for the moved set
  even on nodes that are not subscribed.  If, subsequent to doing this, 
  you attempt a SUBSCRIBE SET for a formerly-unsubscribed node, the
  subscription will fail right at the end when the slon tries to insert a
  new value to sl_setsync.

  The fix: DELETE from sl_setsync immediately before the INSERT.  This
  will silently blow away any 'offending' sl_setsync row.

  (As observed by Afilias staff...)

- Log shipping fix - storage of sl_setsync_offline call had a wrong
  printf type; change from %d to %s
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From jg_web at seacode.com  Thu Mar  8 05:31:41 2007
From: jg_web at seacode.com (John Goetsch)
Date: Thu Mar  8 05:31:47 2007
Subject: [Slony1-general] Architectural guidance - cascaded replication
Message-ID: <45F0103D.1000804@seacode.com>

Hi,
I have a master-slave model running just great.

I now want to replicate a subset of the above database to a number of
other nodes.  I thought of replicating this from the aforementioned
slave (to reduce the the load on the master).

The question is:  should all the nodes be one cluster ie master, slave
and all subnodes, or should I have one cluster for the master-slave
replication and one cluster for the slave-multinode replication.

I can't seem to find the policy/rules/conventions about cluster creation.

can any offer pointers or references....

Thanks
John

From darcyb at commandprompt.com  Thu Mar  8 06:28:26 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Thu Mar  8 06:28:35 2007
Subject: [Slony1-general] Architectural guidance - cascaded replication
In-Reply-To: <45F0103D.1000804@seacode.com>
References: <45F0103D.1000804@seacode.com>
Message-ID: <200703080628.27142.darcyb@commandprompt.com>

On March 8, 2007 05:31 am, John Goetsch wrote:
> Hi,
> I have a master-slave model running just great.
>
> I now want to replicate a subset of the above database to a number of
> other nodes.  I thought of replicating this from the aforementioned
> slave (to reduce the the load on the master).
>
> The question is:  should all the nodes be one cluster ie master, slave
> and all subnodes, or should I have one cluster for the master-slave
> replication and one cluster for the slave-multinode replication.
>
> I can't seem to find the policy/rules/conventions about cluster creation.

A single cluster with multiple sets.

One set is the subset of relations that you want to cascade, the other set is 
the non cascaded relations.

>
> can any offer pointers or references....
>
> Thanks
> John
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Darcy Buskermolen
Command Prompt, Inc.
Sales/Support: +1.503.667.4564 || 24x7/Emergency: +1.800.492.2240
PostgreSQL solutions since 1997
http://www.commandprompt.com/
From andrew.george.hammond at gmail.com  Thu Mar  8 14:51:31 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Mar  8 14:51:41 2007
Subject: [Slony1-general] Architectural guidance - cascaded replication
In-Reply-To: <200703080628.27142.darcyb@commandprompt.com>
References: <45F0103D.1000804@seacode.com>
	<200703080628.27142.darcyb@commandprompt.com>
Message-ID: <5a0a9d6f0703081451v753b6dd1y84ad99cf70cdcf66@mail.gmail.com>

On 3/8/07, Darcy Buskermolen <darcyb@commandprompt.com> wrote:
> On March 8, 2007 05:31 am, John Goetsch wrote:
> > The question is:  should all the nodes be one cluster ie master, slave
> > and all subnodes, or should I have one cluster for the master-slave
> > replication and one cluster for the slave-multinode replication.
> >
> > I can't seem to find the policy/rules/conventions about cluster creation.
>
> A single cluster with multiple sets.
>
> One set is the subset of relations that you want to cascade, the other set is
> the non cascaded relations.

As Darcy says, however be aware that you need to have the exact same
schema on all the members of the cluster. For example, you need tables
to exist on the slaves even if you're not replicating data into them.

See the email to slony1-general list on 2007-Feb-20 from Sven
Willenberger, subject "EXECUTE SCRIPT operates on non set member
tables". I'd link to the mailing list archives, but they seem to be
broken following the move.

Andrew
From jg_web at seacode.com  Thu Mar  8 17:47:41 2007
From: jg_web at seacode.com (John Goetsch)
Date: Thu Mar  8 17:47:53 2007
Subject: [Slony1-general] Architectural guidance - cascaded replication
In-Reply-To: <5a0a9d6f0703081451v753b6dd1y84ad99cf70cdcf66@mail.gmail.com>
References: <45F0103D.1000804@seacode.com>	
	<200703080628.27142.darcyb@commandprompt.com>
	<5a0a9d6f0703081451v753b6dd1y84ad99cf70cdcf66@mail.gmail.com>
Message-ID: <45F0BCBD.8060101@seacode.com>

Andrew -

Thanks for pointing out the subtly.  I may well have violated that....

John

Andrew Hammond wrote:
> On 3/8/07, Darcy Buskermolen <darcyb@commandprompt.com> wrote:
>> On March 8, 2007 05:31 am, John Goetsch wrote:
>> > The question is:  should all the nodes be one cluster ie master, slave
>> > and all subnodes, or should I have one cluster for the master-slave
>> > replication and one cluster for the slave-multinode replication.
>> >
>> > I can't seem to find the policy/rules/conventions about cluster 
>> creation.
>>
>> A single cluster with multiple sets.
>>
>> One set is the subset of relations that you want to cascade, the 
>> other set is
>> the non cascaded relations.
>
> As Darcy says, however be aware that you need to have the exact same
> schema on all the members of the cluster. For example, you need tables
> to exist on the slaves even if you're not replicating data into them.
>
> See the email to slony1-general list on 2007-Feb-20 from Sven
> Willenberger, subject "EXECUTE SCRIPT operates on non set member
> tables". I'd link to the mailing list archives, but they seem to be
> broken following the move.
>
> Andrew
>

From cbbrowne at ca.afilias.info  Fri Mar  9 08:06:20 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar  9 08:06:29 2007
Subject: [Slony1-general] Some questions...
In-Reply-To: <20070308092705.GC5318@sv.lnf.it>
References: <20070308092705.GC5318@sv.lnf.it>
Message-ID: <45F185FC.2090308@ca.afilias.info>

Marco Gaiarin wrote:
> As you ave seen i'm still a novice on slony, but i've just some doubt
> on it.
>
> 1) master have all the connect information for the slave(s), so why
>  there's a slon daemon also on the slave? master can simply
> insert/delete/update data on their own?
>
>   
As far as the slon daemon is concerned, there is no such thing as a
"master" or a "slave."

There is just:

1.  The node that I am managing, and
2.  Other nodes that I draw events from.

The result is that switching roles (e.g. - when you do MOVE SET or
FAILOVER) does not require shifting processes around.  When a node takes
over the "provider" role for a set, there's nothing too special about
that - the slon managing the node can already manage that.

> 2) (probably conflicting with 1) why on slave there's no a trigger that
> simply redirect insert/delete/update on the master, instead of
> preventing it?
>   
That would mean building a multimaster replication system, and conflict
resolution is a Big Problem with doing that.

Slony-I was designed as a single-master replication system; let it be
good at what it is, rather than trying to force Big Problems onto it...
From gaio at sv.lnf.it  Thu Mar  8 01:23:34 2007
From: gaio at sv.lnf.it (Marco Gaiarin)
Date: Fri Mar  9 08:15:46 2007
Subject: [Slony1-general] Trouble starting up: missing something?
Message-ID: <20070308092332.GB5318@sv.lnf.it>


[i've tried to subscribe to the new list on
http://lists.slony.info/mailman/listinfo/slony1-general diuble, but the
confirmation email does not arrive; i think i'm simply coming here in
the middle of a site migration... ;) ]

I've to deploy in my organization a database replica for an internal
LAPP application, and clearly i've choosed slony-I.

We are using here Debian, keeping stable (sarge), so postgres 7.4.X.
The real intention was to move after etch, but i need  this replica now
and so i've firstly recompiled etch slony1 package for sarge, without
no trouble at all.
So i'm using:

 mouse:~# dpkg -l | grep "postgres"
 ii  postgresql     7.4.7-6sarge4  object-relational SQL database management sy
 mouse:~# dpkg -l | grep "slony"
 ii  slony1-bin     1.2.1-0sarge1  replication system for PostgreSQL


As a debian afecionados the first docs i read was
/usr/share/doc/slony1-bin/README.Debian.gz, that warn to read the docs
and then suggest at least to try the 'perl tools'.

So i've setup a quick testbed, compiled the slon_tools.conf
configuration files and start testing.
I suppose no error done on create the database, create schema, add language
and copy schema on the slave, simple tasks.

Then i do a slonik_init_cluster | slonik :

 mouse:~# slonik_init_cluster 

 # INIT CLUSTER
 cluster name = test;
  node 1 admin conninfo='host=mouse.sv.lnf.it dbname=test user=postgres port=5432';
  node 2 admin conninfo='host=eli.ud.lnf.it dbname=test user=postgres port=5432 password=test';
   init cluster (id = 1, comment = 'Node 1 - test@mouse.sv.lnf.it');

 # STORE NODE
   store node (id = 2, event node = 1, comment = 'Node 2 - test@eli.ud.lnf.it');
   echo 'Set up replication nodes';

 # STORE PATH
   echo 'Next: configure paths for each node/origin';
   echo 'Replication nodes prepared';
   echo 'Please start a slon replication daemon for each node';

and slonik_create_set set1 | slonik :

mouse:~# slonik_create_set set1
 cluster name = test;
  node 1 admin conninfo='host=mouse.sv.lnf.it dbname=test user=postgres port=5432';
  node 2 admin conninfo='host=eli.ud.lnf.it dbname=test user=postgres port=5432 password=test';

 # TABLE ADD KEY

 # CREATE SET
   try {
     create set (id = 1, origin = 1, comment = 'Set 1 for test');
   } on error {
     echo 'Could not create subscription set 1 for test!';
     exit -1;
   }

 # SET ADD TABLE
   echo 'Subscription set 1 created';
   echo 'Adding tables to the subscription set';
   set add table (set id = 1, origin = 1, id = 1,
                  full qualified name = 'public.assi_accogl_dimiss',
                  comment = 'Table public.assi_accogl_dimiss with primary key');
   echo 'Add primary keyed table public.assi_accogl_dimiss';
   set add table (set id = 1, origin = 1, id = 2,
                  full qualified name = 'public.assi_trattamenti',
                  comment = 'Table public.assi_trattamenti with primary key');
   echo 'Add primary keyed table public.assi_trattamenti';
   set add table (set id = 1, origin = 1, id = 3,
                  full qualified name = 'public.assi_impegnative',
                  comment = 'Table public.assi_impegnative with primary key');
   echo 'Add primary keyed table public.assi_impegnative';

 # SET ADD SEQUENCE
   echo 'Adding sequences to the subscription set';
   echo 'All tables added';

the commmand execute with no error at all, but the first strangness are
that on master database i can see the trigger that 'log' data, on the
slave there's no trigger that prevent write operation (i've read about
it on the docs).
Second strangness: reading on the docs for the 'manual mode' i suspect
that the missin of a 'STORE PATH' sections it is not good...


Doing that i've started the slon daemons on master and slave, and i can
see that communicate each other (via tcpdump or seen in the log

 2007-03-07 17:06:13 CET DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
 2007-03-07 17:06:13 CET DEBUG2 localListenThread: Received event 2,36 SYNC

).

So after that the only pass missed was the subscribing, so i've done a
slonik_subscribe_set set1 node2 | slonik :

 mouse:~# slonik_subscribe_set set1 node2
 cluster name = test;
  node 1 admin conninfo='host=mouse.sv.lnf.it dbname=test user=postgres port=5432';
  node 2 admin conninfo='host=eli.ud.lnf.it dbname=test user=postgres port=5432 password=test';
   try {
     subscribe set (id = 1, provider = 1, receiver = 2, forward = yes);
   }
   on error {
     exit 1;
   }
   echo 'Subscribed nodes to set 1';

and i've seen on the *slave* log (nothing on master log):

 2007-03-07 16:52:04 CET DEBUG2 localListenThread: Received event 1,13 SUBSCRIBE_SET
 2007-03-07 16:52:04 CET DEBUG2 localListenThread: Received event 1,14 ENABLE_SUBSCRIPTION

but now if i add some data to the table, i see nothing on the salve,
and nothing on the logs.


What i'm missing?

-- 
dott. Marco Gaiarin				    GNUPG Key ID: 240A3D66
  Associazione ``La Nostra Famiglia''                http://www.sv.lnf.it/
  Polo FVG  -  Via della Bont?, 7 - 33078  -  San Vito al Tagliamento (PN)
  marco.gaiarin(at)sv.lnf.it	  tel +39-0434-842711  fax +39-0434-842797
From gaio at sv.lnf.it  Thu Mar  8 01:27:05 2007
From: gaio at sv.lnf.it (Marco Gaiarin)
Date: Fri Mar  9 08:15:46 2007
Subject: [Slony1-general] Some questions...
Message-ID: <20070308092705.GC5318@sv.lnf.it>


As you ave seen i'm still a novice on slony, but i've just some doubt
on it.

1) master have all the connect information for the slave(s), so why
 there's a slon daemon also on the slave? master can simply
insert/delete/update data on their own?

2) (probably conflicting with 1) why on slave there's no a trigger that
simply redirect insert/delete/update on the master, instead of
preventing it?


Sorry if they are stupid questions. Thanks.

-- 
dott. Marco Gaiarin				    GNUPG Key ID: 240A3D66
  Associazione ``La Nostra Famiglia''                http://www.sv.lnf.it/
  Polo FVG  -  Via della Bont?, 7 - 33078  -  San Vito al Tagliamento (PN)
  marco.gaiarin(at)sv.lnf.it	  tel +39-0434-842711  fax +39-0434-842797
From johng at seacode.com  Thu Mar  8 05:26:02 2007
From: johng at seacode.com (John Goetsch)
Date: Fri Mar  9 08:15:48 2007
Subject: [Slony1-general] Architectural guidance - cascaded replication
Message-ID: <45F00EEA.3090508@seacode.com>

Hi,
I have a master-slave model running just great.

I now want to replicate a subset of the above database to a number of 
other nodes.  I thought of replicating this from the aforementioned 
slave (to reduce the the load on the master).

The question is:  should all the nodes be one cluster ie master, slave 
and all subnodes, or should I have one cluster for the master-slave 
replication and one cluster for the slave-multinode replication.

I can't seem to find the policy/rules/conventions about cluster creation.

can any offer pointers or references....

Thanks
John
From andrew.george.hammond at gmail.com  Fri Mar  9 10:34:51 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar  9 10:35:00 2007
Subject: [Slony1-general] slon documentation bugs and annoyances
Message-ID: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>

So, I'm reading the slon documentation while waiting for stuff to
compile and these things jump out at me.

1) Debug level lists 8 levels of logging as error through debug4 and
then gives a numeric example. If it's a numeric log level, why aren't
there numbers beside the text labels?

Also, this disagrees with the run-time config docs which say that
log_level can be from 0 to 4. The difference should be explained.

2) Debug level suggest a log level of 2 (WARN) and says this is "a
routine, and, seemingly preferable choice". This is not what
experience has demonstrated. As I recall we ran all logging at debug1
since higher levels proved insufficient for troubleshooting. It seems
reasonable to recommend debug1 for production systems where
troubleshooting capabilities are required.

3) There is no listed default for the -c option (should be 3).

Andrew

P.S. I went to www.slony1.info to see if I could find a bug reporting
tool there, but no luck. I always used to just heckle Chris across the
cubicles, but it's a little far to do that now.
From andrew at ca.afilias.info  Fri Mar  9 10:42:32 2007
From: andrew at ca.afilias.info (Andrew Sullivan)
Date: Fri Mar  9 10:42:52 2007
Subject: [Slony1-general] slon documentation bugs and annoyances
In-Reply-To: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
References: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
Message-ID: <20070309184232.GE4966@afilias.info>

On Fri, Mar 09, 2007 at 10:34:51AM -0800, Andrew Hammond wrote:

> P.S. I went to www.slony1.info to see if I could find a bug reporting
> tool there, but no luck. 

That'd be because the domain you wanted isn't slony1.info:

whois slony1.info
NOT FOUND


slony.info works (I just tried it).

A

-- 
Andrew Sullivan                         204-4141 Yonge Street
Afilias Canada                        Toronto, Ontario Canada
<andrew@ca.afilias.info>                              M2P 2A8
jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
From andrew.george.hammond at gmail.com  Fri Mar  9 13:56:56 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar  9 13:57:04 2007
Subject: [Slony1-general] slon documentation bugs and annoyances
In-Reply-To: <20070309184232.GE4966@afilias.info>
References: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
	<20070309184232.GE4966@afilias.info>
Message-ID: <5a0a9d6f0703091356i5eca7680xd05db581800b6d24@mail.gmail.com>

On 3/9/07, Andrew Sullivan <andrew@ca.afilias.info> wrote:
> On Fri, Mar 09, 2007 at 10:34:51AM -0800, Andrew Hammond wrote:
>
> > P.S. I went to www.slony1.info to see if I could find a bug reporting
> > tool there, but no luck.
>
> That'd be because the domain you wanted isn't slony1.info:
>
> whois slony1.info
> NOT FOUND
>
> slony.info works (I just tried it).

Brain fart on my side. I was at slony.info. Since we've already
established that I'm stupid and blind, I tried to search for bug,
issue, problem and report and still couldn't find anything on the
subject of bug reporting on the main page. Same in the docs directory
and elsewhere on the site. There's a slony1-bugs mailing list to which
I have subscribed, but it doesn't appear to have any traffic on it.

Seriously though, now that the "turn around and holler over my
shoulder" technique isn't available, what's the right way to report
bugs? And how can we better expose that to other users?

Andrew
From andrew.george.hammond at gmail.com  Fri Mar  9 14:17:09 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar  9 14:17:18 2007
Subject: [Slony1-general] Re: slon documentation bugs and annoyances
In-Reply-To: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
References: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
Message-ID: <5a0a9d6f0703091417k132d472dtb45242907d16cc83@mail.gmail.com>

On 3/9/07, Andrew Hammond <andrew.george.hammond@gmail.com> wrote:
> So, I'm reading the slon documentation while waiting for stuff to
> compile and these things jump out at me.
>
> 1) Debug level lists 8 levels of logging as error through debug4 and
> then gives a numeric example. If it's a numeric log level, why aren't
> there numbers beside the text labels?
>
> Also, this disagrees with the run-time config docs which say that
> log_level can be from 0 to 4. The difference should be explained.
>
> 2) Debug level suggest a log level of 2 (WARN) and says this is "a
> routine, and, seemingly preferable choice". This is not what
> experience has demonstrated. As I recall we ran all logging at debug1
> since higher levels proved insufficient for troubleshooting. It seems
> reasonable to recommend debug1 for production systems where
> troubleshooting capabilities are required.
>
> 3) There is no listed default for the -c option (should be 3).

4) There's no definition of conninfo. This should at least be a link
to http://www.postgresql.org/docs/current/interactive/libpq-connect.html

Andrew
From andrew.george.hammond at gmail.com  Fri Mar  9 16:53:02 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar  9 16:53:12 2007
Subject: [Slony1-general] slon <> russian elephant?
Message-ID: <5a0a9d6f0703091653k2ed5505ame1545b944c5d07ce@mail.gmail.com>

http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0107e&L=ads-l&F=&S=&m=37769&P=328

"SLON is a naive slut."

Made my day.

Andrew
From cbbrowne at ca.afilias.info  Sat Mar 10 07:33:20 2007
From: cbbrowne at ca.afilias.info (cbbrowne@ca.afilias.info)
Date: Sat Mar 10 07:33:26 2007
Subject: [Slony1-general] slon documentation bugs and annoyances
In-Reply-To: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
References: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
Message-ID: <63949.64.229.224.57.1173540800.squirrel@look.libertyrms.info>

> So, I'm reading the slon documentation while waiting for stuff to
> compile and these things jump out at me.
>
> 1) Debug level lists 8 levels of logging as error through debug4 and
> then gives a numeric example. If it's a numeric log level, why aren't
> there numbers beside the text labels?
>
> Also, this disagrees with the run-time config docs which say that
> log_level can be from 0 to 4. The difference should be explained.

I'm quite sure I did revise that; perhaps it needs a few more words.

There are 8 levels; ERROR/WARN/CONFIG/INFO are ones that cannot be shut
off.  Your choices of log levels are 0 thru 4; at 0, only the
ERROR/WARN/CONFIG/INFO messages are shown.  At 1, DEBUG1 is added in.  At
2, DEBUG2 is added as well.  DEBUG4 will show all possible levels.

> 2) Debug level suggest a log level of 2 (WARN) and says this is "a
> routine, and, seemingly preferable choice". This is not what
> experience has demonstrated. As I recall we ran all logging at debug1
> since higher levels proved insufficient for troubleshooting. It seems
> reasonable to recommend debug1 for production systems where
> troubleshooting capabilities are required.

No, you're mis-recalling this.  DEBUG2 is the level we have always been
using.  Levels 3 and 4 show too much stuff; there's something at level 3
that spits out a line for each tuple that is replicated.

> 3) There is no listed default for the -c option (should be 3).

Perhaps I ought to add that...

> P.S. I went to www.slony1.info to see if I could find a bug reporting
> tool there, but no luck. I always used to just heckle Chris across the
> cubicles, but it's a little far to do that now.

And postcards are a bit slow for the purpose :-).  (Got it...)

Getting a new bug tracker is one of the subsequent steps.  There's a
Bugzilla instance that CommandPrompt set up to help with PostgreSQL; it
would be slick to add Slony-I to that.  That would probably make the
instance "in use," which I think it isn't, right now...

From nyamada at millburncorp.com  Sat Mar 10 18:23:29 2007
From: nyamada at millburncorp.com (Norman Yamada)
Date: Sat Mar 10 18:23:39 2007
Subject: [Slony1-general] Problems upgrading to 1.2.7 --
Message-ID: <3DDBF867-FDE7-498B-B2D3-8B498976E0B4@millburncorp.com>

We have a relatively simple setup.

One master; three slaves. All boxes on Debian linux (testing) running  
postgresql 8.2.3 and slony 1.2.6, both compiled from source.

2 problems:

1) After compiling Slony 1.2.7 on all four boxes, the UPDATE  
FUNCTIONS call failed on the master box failed. I'm afraid I was  
stupid, and didn't save the error message -- but I believe it was  
complaining about not finding a function in my replication cluster.  
Has anyone else had problems upgrading from 1.2.6 to 1.2.7? If not,  
I'll try again later and if it throws up an error again, I'll be more  
diligent and record it and report here.

2) How soon will Slony be officially sanctioned for 8.2.x? We  
upgraded our servers without realizing it might cause problems with  
Slony; so far, all seems to be working, but should I be concerned?

TIA for any help.

From cbbrowne at ca.afilias.info  Mon Mar 12 15:18:52 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar 12 15:19:01 2007
Subject: [Slony1-general] PATCH: add compatibility with older versions of
	rotate logs
In-Reply-To: <et4c9c$u5g$1@sea.gmane.org> (Mark Stosberg's message of "Mon,
	12 Mar 2007 16:13:00 -0400")
References: <et4c9c$u5g$1@sea.gmane.org>
Message-ID: <60d53enm0j.fsf@dba2.int.libertyrms.com>

Mark Stosberg <mark@summersault.com> writes:
> Patch attached.

Committed...
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From mlists at rp-online.de  Tue Mar 13 02:34:22 2007
From: mlists at rp-online.de (Thomas Pundt)
Date: Tue Mar 13 02:32:17 2007
Subject: [Slony1-general] Release of 1.1.7 and 1.2.7
In-Reply-To: <60tzwwr5nq.fsf@dba2.int.libertyrms.com>
References: <60tzwwr5nq.fsf@dba2.int.libertyrms.com>
Message-ID: <200703131034.22865.mlists@rp-online.de>

Hi,

On Thursday 08 March 2007 00:32, Christopher Browne wrote:
| We are pleased to release versions 1.1.7 and 1.2.7, which contain
| fixes for various problems found in these branches over the last
| several months.

I just tried to compile slony-1.1.7 on an Linux box and got the
following error:

# make
make[1]: Entering directory `/usr/local/src/slony1-1.1.7/src'
make[2]: Entering directory `/usr/local/src/slony1-1.1.7/src/xxid'
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -fpic -I../.. -I/usr/local/pg81/include/ -I/usr/local/pg81/include/postgresql/server/  -c -o 
xxid.o xxid.c
gcc -shared -o xxid.so xxid.o
make[2]: *** No rule to make target `xxid.v74.sql', needed by `all'.  Stop.
make[2]: Leaving directory `/usr/local/src/slony1-1.1.7/src/xxid'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/usr/local/src/slony1-1.1.7/src'
make: *** [all] Error 2

and indeed, there is no file xxid.v74.sql in the archive. It only has
a xxid.v73.sql...

Ciao,
Thomas

-- 
Thomas Pundt <thomas.pundt@rp-online.de> ---- http://rp-online.de/ ----
From John.Parnefjord at kib.ki.se  Tue Mar 13 05:08:42 2007
From: John.Parnefjord at kib.ki.se (John Parnefjord)
Date: Tue Mar 13 05:08:47 2007
Subject: [Slony1-general] ROOT page split
Message-ID: <76C73999206F3145A2E3046A5C32A103122641@kibmail.kib.local>

Hi!
 
I got this message from slony:
 
DEBUG:  concurrent ROOT page split
CONTEXT:  SQL statement "INSERT INTO _replication.sl_log_1 (log_origin, log_xid, log_tableid, log_actionseq, log_cmdtype, log_cmddata) VALUES (1, $1, $2, nextval('_replication.sl_action_seq'), $3, $4);"
SQL statement "INSERT INTO schema.items (id,source_id,item_nr_id,article_nr,title,title_enhancement,document_type,page_begin,page_end,pages,meeting_abstract,reviewed_work_year,cited_items_count,abstract_available,abstract,ki_insert_time,ki_publication_year) VALUES ( $1 , $2 , $3 , $4 , $5 , $6 , $7 , $8 , $9 , $10 , $11 , $12 , $13 , $14 , $15 ,LOCALTIMESTAMP, $16 )"
PL/pgSQL function "insert_item" line 3 at SQL statement
 
Is this an error or just an informational message? All seems well and all data seems to have been inserted.
 
Cheers!
// John
From andreas at kostyrka.org  Tue Mar 13 07:03:28 2007
From: andreas at kostyrka.org (Andreas Kostyrka)
Date: Tue Mar 13 07:02:28 2007
Subject: [Slony1-general] ROOT page split
In-Reply-To: <76C73999206F3145A2E3046A5C32A103122641@kibmail.kib.local>
References: <76C73999206F3145A2E3046A5C32A103122641@kibmail.kib.local>
Message-ID: <20070313140327.GD5720@andi-lap.la.revver.com>

* John Parnefjord <John.Parnefjord@kib.ki.se> [070313 13:16]:
> Hi!
>  
> I got this message from slony:
>  
> DEBUG:  concurrent ROOT page split
> CONTEXT:  SQL statement "INSERT INTO _replication.sl_log_1 (log_origin, log_xid, log_tableid, log_actionseq, log_cmdtype, log_cmddata) VALUES (1, $1, $2, nextval('_replication.sl_action_seq'), $3, $4);"
> SQL statement "INSERT INTO schema.items (id,source_id,item_nr_id,article_nr,title,title_enhancement,document_type,page_begin,page_end,pages,meeting_abstract,reviewed_work_year,cited_items_count,abstract_available,abstract,ki_insert_time,ki_publication_year) VALUES ( $1 , $2 , $3 , $4 , $5 , $6 , $7 , $8 , $9 , $10 , $11 , $12 , $13 , $14 , $15 ,LOCALTIMESTAMP, $16 )"
> PL/pgSQL function "insert_item" line 3 at SQL statement
>  
> Is this an error or just an informational message? All seems well and all data seems to have been inserted.
I'd say informational message :)

=> you sure that all your slaves confirming events back?

Andreas
From John.Parnefjord at kib.ki.se  Tue Mar 13 07:36:00 2007
From: John.Parnefjord at kib.ki.se (John Parnefjord)
Date: Tue Mar 13 07:39:12 2007
Subject: SV: [Slony1-general] ROOT page split
References: <76C73999206F3145A2E3046A5C32A103122641@kibmail.kib.local>
	<20070313140327.GD5720@andi-lap.la.revver.com>
Message-ID: <76C73999206F3145A2E3046A5C32A103122642@kibmail.kib.local>

> DEBUG:  concurrent ROOT page split
> CONTEXT:  SQL statement "INSERT INTO _replication.sl_log_1 (log_origin, log_xid, log_tableid, log_actionseq, log_cmdtype, log_cmddata) VALUES (1, $1, $2, nextval('_replication.sl_action_seq'), $3, $4);"
> SQL statement "INSERT INTO schema.items (id,source_id,item_nr_id,article_nr,title,title_enhancement,document_type,page_begin,page_end,pages,meeting_abstract,reviewed_work_year,cited_items_count,abstract_available,abstract,ki_insert_time,ki_publication_year) VALUES ( $1 , $2 , $3 , $4 , $5 , $6 , $7 , $8 , $9 , $10 , $11 , $12 , $13 , $14 , $15 ,LOCALTIMESTAMP, $16 )"
> PL/pgSQL function "insert_item" line 3 at SQL statement
> 
> Is this an error or just an informational message? All seems well and all data seems to have been inserted.
I'd say informational message :)

As I have log level debug2 turned on for Postgresql at the moment that makes sense.

=> you sure that all your slaves confirming events back?

 
It's set up that way. But if I want to confirm that how can I tell the easiest way?
 
// John
From cbbrowne at ca.afilias.info  Tue Mar 13 08:57:54 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Mar 13 08:57:59 2007
Subject: [Slony1-general] Release of 1.1.7 and 1.2.7
In-Reply-To: <200703131034.22865.mlists@rp-online.de> (Thomas Pundt's message
	of "Tue, 13 Mar 2007 10:34:22 +0100")
References: <60tzwwr5nq.fsf@dba2.int.libertyrms.com>
	<200703131034.22865.mlists@rp-online.de>
Message-ID: <604popnnjx.fsf@dba2.int.libertyrms.com>

Thomas Pundt <mlists@rp-online.de> writes:
> On Thursday 08 March 2007 00:32, Christopher Browne wrote:
> | We are pleased to release versions 1.1.7 and 1.2.7, which contain
> | fixes for various problems found in these branches over the last
> | several months.
>
> I just tried to compile slony-1.1.7 on an Linux box and got the
> following error:
>
> # make
> make[1]: Entering directory `/usr/local/src/slony1-1.1.7/src'
> make[2]: Entering directory `/usr/local/src/slony1-1.1.7/src/xxid'
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -fpic -I../.. -I/usr/local/pg81/include/ -I/usr/local/pg81/include/postgresql/server/  -c -o 
> xxid.o xxid.c
> gcc -shared -o xxid.so xxid.o
> make[2]: *** No rule to make target `xxid.v74.sql', needed by `all'.  Stop.
> make[2]: Leaving directory `/usr/local/src/slony1-1.1.7/src/xxid'
> make[1]: *** [all] Error 2
> make[1]: Leaving directory `/usr/local/src/slony1-1.1.7/src'
> make: *** [all] Error 2
>
> and indeed, there is no file xxid.v74.sql in the archive. It only has
> a xxid.v73.sql...

Oh, dear.  That is a regression that is presumably my fault.

In version 1.1, the "ultimate source" for the xxid SQL is
xxid.v73.sql; in version 1.2, that changes to xxid.v74.sql (as we drop
PostgreSQL 7.3 support).  Evidently I have made the mistake of
applying some of those 1.2 revisions to 1.1 :-(.  

I'm changing that now.

You can work around this simply via:
  cp xxid.v73.sql xxid.v74.sql

I just committed a fix to the 1.1 branch for this that visibly works better:

cbbrowne@dba2:CMD/slony1-1.1/src/xxid> ls -l
total 116
drwxr-xr-x 2 cbbrowne users  4096 Mar 13 15:56 CVS
-rwxr-xr-- 1 cbbrowne users  1754 Mar 13 15:56 Makefile
-rwxr-x--- 1 cbbrowne users  9037 Jan  6  2006 xxid.c
-rw-r--r-- 1 cbbrowne users 31684 Mar 13 15:56 xxid.o
-rwxr-xr-x 1 cbbrowne users 30524 Mar 13 15:56 xxid.so
-rwxr-x--- 1 cbbrowne users  5376 Jan  6  2006 xxid.v73.sql
-rwxr-x--- 1 cbbrowne users  5376 Mar 13 15:56 xxid.v74.sql
-rwxr-x--- 1 cbbrowne users  5376 Mar 13 15:56 xxid.v80.sql
-rwxr-x--- 1 cbbrowne users  5376 Mar 13 15:56 xxid.v81.sql
cbbrowne@dba2:CMD/slony1-1.1/src/xxid> make clean
rm -f xxid.so xxid.o 
rm -f xxid.v74.sql xxid.v80.sql xxid.v81.sql
cbbrowne@dba2:CMD/slony1-1.1/src/xxid> make
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -fpic -I../.. -I/opt/OXRS/dbs/pgsql745/include/ -I/opt/OXRS/dbs/pgsql745/include/server/  -c -o xxid.o xxid.c
gcc -shared -o xxid.so xxid.o
cp xxid.v73.sql xxid.v74.sql
cp xxid.v74.sql xxid.v80.sql
cp xxid.v80.sql xxid.v81.sql
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From andrew.george.hammond at gmail.com  Tue Mar 13 11:09:17 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue Mar 13 11:09:27 2007
Subject: [Slony1-general] slon documentation bugs and annoyances
In-Reply-To: <63949.64.229.224.57.1173540800.squirrel@look.libertyrms.info>
References: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
	<63949.64.229.224.57.1173540800.squirrel@look.libertyrms.info>
Message-ID: <5a0a9d6f0703131109s327525aubdd5d89e2ae2f0e6@mail.gmail.com>

On 3/10/07, cbbrowne@ca.afilias.info <cbbrowne@ca.afilias.info> wrote:
> > So, I'm reading the slon documentation while waiting for stuff to
> > compile and these things jump out at me.
> >
> > 1) Debug level lists 8 levels of logging as error through debug4 and
> > then gives a numeric example. If it's a numeric log level, why aren't
> > there numbers beside the text labels?

Can we update the docs so that the numbers are directly related to the
log levels?

> > Also, this disagrees with the run-time config docs which say that
> > log_level can be from 0 to 4. The difference should be explained.
>
> I'm quite sure I did revise that; perhaps it needs a few more words.
>
> There are 8 levels; ERROR/WARN/CONFIG/INFO are ones that cannot be shut
> off.  Your choices of log levels are 0 thru 4; at 0, only the
> ERROR/WARN/CONFIG/INFO messages are shown.  At 1, DEBUG1 is added in.  At
> 2, DEBUG2 is added as well.  DEBUG4 will show all possible levels.

So we have -d and log_level both take a numeric argument that
specifies how verbose the logging should be, but the numbers mean
different log levels depending on which way they're passed in.

> > 2) Debug level suggest a log level of 2 (WARN) and says this is "a
> > routine, and, seemingly preferable choice". This is not what
> > experience has demonstrated. As I recall we ran all logging at debug1
> > since higher levels proved insufficient for troubleshooting. It seems
> > reasonable to recommend debug1 for production systems where
> > troubleshooting capabilities are required.
>
> No, you're mis-recalling this.  DEBUG2 is the level we have always been
> using.  Levels 3 and 4 show too much stuff; there's something at level 3
> that spits out a line for each tuple that is replicated.

So... everyone agrees that DEBUG2 is the level that should be
suggested for logging? And if that's the case, then perhaps the
documentation should also be updated?

Andrew
From andrew.george.hammond at gmail.com  Tue Mar 13 11:57:55 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue Mar 13 11:58:04 2007
Subject: [Slony1-general] documentation slon logging output
Message-ID: <5a0a9d6f0703131157k48f3223dhefefa9f3afaef691@mail.gmail.com>

In the process of hacking on the slon-mkservice stuff, I got all
excited about multilog's filtering capabilities and wanted to write
some clever filters. So, I went looking for documentation on the
output of slons and I can't find anything definitive, except the
source.

Is there a document anywhere that clearly lays out what the different
outputs mean? Heck, even a list of the different types of log
statements possible would probably be sufficient. If not, I can grep
it out of the source. If there a wiki for this stuff I'd be happy to
put it there to get annotated.

Andrew
From cbbrowne at ca.afilias.info  Tue Mar 13 12:23:09 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Mar 13 12:23:25 2007
Subject: [Slony1-general] Can't init cluster with slony 1.2.7
In-Reply-To: <45F5C233.8040909@selectacast.net> (Joseph Shraibman's message of
	"Mon, 12 Mar 2007 17:12:19 -0400")
References: <2CC69F840555CB43B04195F218CCB57F7CD948@COENGEX01.cctus.com>
	<45F5C233.8040909@selectacast.net>
Message-ID: <60zm6hlzhe.fsf@dba2.int.libertyrms.com>

Joseph Shraibman <jks@selectacast.net> writes:
> I was able to solve the problem by remming out these lines:
>
> comment on function @NAMESPACE@.make_table_strict (text, text) is
> 'Equivalent to 8.1+ ALTER FUNCTION ... STRICT';
>
> Hopefully that is all I have to do.

Argh.

I ran thru tests on 7.4, 8.1, and 8.2.  And the one version of
PostgreSQL I didn't test 1.2.7 on (8.0, which, as it happens, is a
version that Afilias will ultimately never have used), there's a typo
on it :-(.

You could also fix the problem by changing that line to:

    comment on function @NAMESPACE@.make_function_strict (text, text) is

Obviously we'll need 1.2.8 sooner than later :-(.

Fixed in CVS in branches 1.1 and 1.2.  HEAD had it right already...
-- 
"cbbrowne","@","cbbrowne.com"
http://linuxfinances.info/info/lsf.html
"You're one of those condescending Unix computer users!"
"Here's a nickel, kid.  Get yourself a real computer" - Dilbert.
From cbbrowne at ca.afilias.info  Tue Mar 13 14:02:58 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Mar 13 14:03:05 2007
Subject: [Slony1-general] PATCH: document that slonik_execute_script is an
	easy way to make DLL changes
In-Reply-To: <eoof7s$j3a$1@sea.gmane.org> (Mark Stosberg's message of "Thu,
	18 Jan 2007 13:44:56 -0500")
References: <eoof7s$j3a$1@sea.gmane.org>
Message-ID: <60veh4n9fh.fsf@dba2.int.libertyrms.com>

Mark Stosberg <mark@summersault.com> writes:
> The attach patch updates addthings.html to mention
> slonik_execute_script, which I find significantly easier to use than raw
> slonik commands.

Committed to HEAD...
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Wed Mar 14 08:24:18 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar 14 08:24:32 2007
Subject: [Slony1-general] Howto instll and admin rpm slony1?
In-Reply-To: <3af7f950703132135m76201d23p479e3568de6ff6bd@mail.gmail.com>
References: <3af7f950703132135m76201d23p479e3568de6ff6bd@mail.gmail.com>
Message-ID: <45F813A2.10202@ca.afilias.info>

amrit angsusingh wrote:
> I'm new to slony1 and try to install slony1 in FC6 x86_64 postgrsesql
> 8.14 with 
> # rpm -Uvh ./postgresql-slony1-engine-1.1.6-1_PG8.1.5.x86_64.rpm as root
> and find the find conf in /etc pg_config  in /usr/bin but cannot find
> /usr/share/doc/postgresql-slony1-engine .
>  
> I try to set cluster but no actual guildline or document for rpm
> , which way would be the best way for me to get start with this slony
> , via pgadmin lll or via slon command in /usr/bin.
> And how could I use this slon command?
> Do I need two machines during replication testing?
> Anyone could suggest or help me ?
> Best regard.
>
> Amrit Angsusingh
> Thailand
Well, if the documentation isn't included, you could always consult a
copy online:

<http://main.slony.info/documentation/>
From cbbrowne at acm.org  Wed Mar 14 13:20:25 2007
From: cbbrowne at acm.org (Chris Browne)
Date: Wed Mar 14 13:47:12 2007
Subject: [Slony1-general] Cutting down size of sl_seqlog
Message-ID: <60lkhzmvau.fsf@dba2.int.libertyrms.com>

People have observed that the table sl_seqlog can grow to be rather
large, if you have a lot of sequences.  It, in effect, generates one
tuple per sequence per SYNC.

If you have 30 sequences, and generate a SYNC per second, then you
will have 1800 records generated per minute, irrespective of whether
or not any given sequence value changed during that period of time.

create table @NAMESPACE@.sl_seqlog (
	seql_seqid			int4,
	seql_origin			int4,
	seql_ev_seqno		int8,
	seql_last_value		int8,
) WITHOUT OIDS;

create index sl_seqlog_idx on @NAMESPACE@.sl_seqlog
	(seql_origin, seql_ev_seqno, seql_seqid);

There have been some vague mutterings going around now and again for
quite some time now to the effect that we really ought to find some
way of cutting down on the number of tuples at least for cases where a
sequence isn't being updated regularly.

My inchoate thoughts finally coalesced into something today.  The
thought...

- Augment sl_seqlog to present a range rather than a value, thus...

alter table sl_seqlog alter column seql_ev_seqno rename to seql_min_seqno;
alter table sl_seqlog add column seql_max_seqno int8;

drop index sl_seqlog_idx;
create index sl_seqlog_idx1 on sl_seqlog(seql_origin, seql_seqid);
create index sl_seqlog_idx2 on sl_seqlog(seql_min_seqno);
create index sl_seqlog_idx3 on sl_seqlog(seql_max_seqno);

- Rather than doing straight inserts,  we do one of two things:

   - If the current sequence value <> the most recent value, then

     insert into sl_seqlog (seql_seqid, seql_origin, seql_min_seqno, seql_max_seqno, seql_last_value)
       values (seql_seqid, node->no_id, [seqbuf], [seqbuf], seql_last_value);

    In this case, our tuple has an extra column, but there is not
    anything materially worse than what we have today.

  - If the current sequence value is equal to the previous one, then...

    update sl_seqlog set seql_max_seqno = [seqbuf] where seql_seqid = [seql_seqid] 
          and seql_origin = node->no_id  and seql_last_value = [seql_last_value] and
          seql_max_seqno = (select max(seql_max_seqno) from sl_seqlog 
                                           where [match sequence, origin]);

   In this case, we wind up with a dead tuple for each update, but
   this isn't really worse than what is the case now, as there are no
   extra tuples being generated, and, with this revision, there's only
   one live tuple that needs to be kept around for a sequence that is
   very infrequently updated.

Overall, this isn't worse than before as we aren't generating more
tuples than before.  It is *somewhat* better because we wind up
"killing" obsolete tuples (e.g. - ones where a SYNC does not lead to a
change in value) immediately.

It's not a LOT better, but I think it's at least *somewhat* better.
And perhaps someone can point out a way of doing better still, perhaps
to not bother generating some of the tuples altogether.

Thoughts?
-- 
"cbbrowne","@","linuxdatabases.info"
http://www3.sympatico.ca/cbbrowne/multiplexor.html
Rules of the Evil Overlord #206. "When my Legions of Terror park their
vehicle  to do  reconnaissance on  foot,  they will  be instructed  to
employ The Club." <http://www.eviloverlord.com/>
From andrew.george.hammond at gmail.com  Wed Mar 14 14:41:28 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Wed Mar 14 14:41:43 2007
Subject: [Slony1-general] slon documentation bugs and annoyances
In-Reply-To: <5a0a9d6f0703131109s327525aubdd5d89e2ae2f0e6@mail.gmail.com>
References: <5a0a9d6f0703091034g587dd5bate0859f120f574e32@mail.gmail.com>
	<63949.64.229.224.57.1173540800.squirrel@look.libertyrms.info>
	<5a0a9d6f0703131109s327525aubdd5d89e2ae2f0e6@mail.gmail.com>
Message-ID: <5a0a9d6f0703141441t5ac6f286kdbc444614ba3af22@mail.gmail.com>

> > > Also, this disagrees with the run-time config docs which say that
> > > log_level can be from 0 to 4. The difference should be explained.
> >
> > I'm quite sure I did revise that; perhaps it needs a few more words.
> >
> > There are 8 levels; ERROR/WARN/CONFIG/INFO are ones that cannot be shut
> > off.  Your choices of log levels are 0 thru 4; at 0, only the
> > ERROR/WARN/CONFIG/INFO messages are shown.  At 1, DEBUG1 is added in.  At
> > 2, DEBUG2 is added as well.  DEBUG4 will show all possible levels.
>
> So we have -d and log_level both take a numeric argument that
> specifies how verbose the logging should be, but the numbers mean
> different log levels depending on which way they're passed in.

Better still, the command line behaviour appears to have been modified
to match the new config file behaviour, or at least the slon barfed
with

[0] WARN   5 is outside the valid range for parameter "log_level" (0 .. 4)

> > > 2) Debug level suggest a log level of 2 (WARN) and says this is "a
> > > routine, and, seemingly preferable choice". This is not what
> > > experience has demonstrated. As I recall we ran all logging at debug1
> > > since higher levels proved insufficient for troubleshooting. It seems
> > > reasonable to recommend debug1 for production systems where
> > > troubleshooting capabilities are required.
> >
> > No, you're mis-recalling this.  DEBUG2 is the level we have always been
> > using.  Levels 3 and 4 show too much stuff; there's something at level 3
> > that spits out a line for each tuple that is replicated.
>
> So... everyone agrees that DEBUG2 is the level that should be
> suggested for logging? And if that's the case, then perhaps the
> documentation should also be updated?

This is even better. If you follow the suggestion in the docs of using
log level "WARN", you count down the list of log levels and decide
that it's level 2. But that's actually DEBUG2, which appears to be a
pretty good choice for the actual log level of a production system.
Cool.

Andrew
From bnichols at ca.afilias.info  Thu Mar 15 05:49:57 2007
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Thu Mar 15 05:50:03 2007
Subject: [Slony1-general] Cutting down size of sl_seqlog
In-Reply-To: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
References: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
Message-ID: <1173962997.5446.74.camel@dba5.int.libertyrms.com>

On Wed, 2007-03-14 at 16:20 -0400, Chris Browne wrote:
> It's not a LOT better, but I think it's at least *somewhat* better.
> And perhaps someone can point out a way of doing better still, perhaps
> to not bother generating some of the tuples altogether.
> 
> Thoughts?

Is there a reason that there has to be an entry in sl_seqlog if the
sequence doesn't change?  If not, then you should be able to insert on
change, and do nothing for case where it doesn't change.
-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.

From lists_slony1-general at avsupport.com  Thu Mar 15 06:15:03 2007
From: lists_slony1-general at avsupport.com (Dan Falconer)
Date: Thu Mar 15 06:15:18 2007
Subject: [Slony1-general] Cutting down size of sl_seqlog
In-Reply-To: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
References: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
Message-ID: <200703150815.03804.lists_slony1-general@avsupport.com>

	DISCLAIMER: I don't know a lot about the internals of Slony, especially when 
it comes to how records get into the tables within Slony's replication 
schema.

	That said, it seems to me that there should be a way to wholly reduce the 
apparent amount of work involved in maintaining sequences: in sl_sequence, 
create a column that contains the last value for each of the sequences.  
Presumably, there would be a process that checks the current value of each 
sequence... if it determined that the sequence had changed, run an update on 
the last sequence (for the master), and do whatever it normally did to note 
that the slaves should update their sequences.

	I apologize for the amount of vagueness in my reply.  It was quite apparent 
to me, while attempting to formulate words to express my idea, that I knew 
very little of the actual inner workings of Slony.  I hope the idea makes 
sense and doesn't cause any eye-bleeding.  :) 

On Wednesday 14 March 2007 3:20 pm, Chris Browne wrote:
> People have observed that the table sl_seqlog can grow to be rather
> large, if you have a lot of sequences.  It, in effect, generates one
> tuple per sequence per SYNC.
>
> If you have 30 sequences, and generate a SYNC per second, then you
> will have 1800 records generated per minute, irrespective of whether
> or not any given sequence value changed during that period of time.
>
> create table @NAMESPACE@.sl_seqlog (
> 	seql_seqid			int4,
> 	seql_origin			int4,
> 	seql_ev_seqno		int8,
> 	seql_last_value		int8,
> ) WITHOUT OIDS;
>
> create index sl_seqlog_idx on @NAMESPACE@.sl_seqlog
> 	(seql_origin, seql_ev_seqno, seql_seqid);
>
> There have been some vague mutterings going around now and again for
> quite some time now to the effect that we really ought to find some
> way of cutting down on the number of tuples at least for cases where a
> sequence isn't being updated regularly.
>
> My inchoate thoughts finally coalesced into something today.  The
> thought...
>
> - Augment sl_seqlog to present a range rather than a value, thus...
>
> alter table sl_seqlog alter column seql_ev_seqno rename to seql_min_seqno;
> alter table sl_seqlog add column seql_max_seqno int8;
>
> drop index sl_seqlog_idx;
> create index sl_seqlog_idx1 on sl_seqlog(seql_origin, seql_seqid);
> create index sl_seqlog_idx2 on sl_seqlog(seql_min_seqno);
> create index sl_seqlog_idx3 on sl_seqlog(seql_max_seqno);
>
> - Rather than doing straight inserts,  we do one of two things:
>
>    - If the current sequence value <> the most recent value, then
>
>      insert into sl_seqlog (seql_seqid, seql_origin, seql_min_seqno,
> seql_max_seqno, seql_last_value) values (seql_seqid, node->no_id, [seqbuf],
> [seqbuf], seql_last_value);
>
>     In this case, our tuple has an extra column, but there is not
>     anything materially worse than what we have today.
>
>   - If the current sequence value is equal to the previous one, then...
>
>     update sl_seqlog set seql_max_seqno = [seqbuf] where seql_seqid =
> [seql_seqid] and seql_origin = node->no_id  and seql_last_value =
> [seql_last_value] and seql_max_seqno = (select max(seql_max_seqno) from
> sl_seqlog where [match sequence, origin]);
>
>    In this case, we wind up with a dead tuple for each update, but
>    this isn't really worse than what is the case now, as there are no
>    extra tuples being generated, and, with this revision, there's only
>    one live tuple that needs to be kept around for a sequence that is
>    very infrequently updated.
>
> Overall, this isn't worse than before as we aren't generating more
> tuples than before.  It is *somewhat* better because we wind up
> "killing" obsolete tuples (e.g. - ones where a SYNC does not lead to a
> change in value) immediately.
>
> It's not a LOT better, but I think it's at least *somewhat* better.
> And perhaps someone can point out a way of doing better still, perhaps
> to not bother generating some of the tuples altogether.
>
> Thoughts?

-- 
Best Regards,


Dan Falconer
"Head Geek",
AvSupport, Inc. (http://www.partslogistics.com)
From bnichols at ca.afilias.info  Thu Mar 15 06:35:12 2007
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Thu Mar 15 06:35:18 2007
Subject: [Slony1-general] Cutting down size of sl_seqlog
In-Reply-To: <200703150815.03804.lists_slony1-general@avsupport.com>
References: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
	<200703150815.03804.lists_slony1-general@avsupport.com>
Message-ID: <1173965712.5446.94.camel@dba5.int.libertyrms.com>

On Thu, 2007-03-15 at 08:15 -0500, Dan Falconer wrote:
> 	DISCLAIMER: I don't know a lot about the internals of Slony, especially when 
> it comes to how records get into the tables within Slony's replication 
> schema.
> 
> 	That said, it seems to me that there should be a way to wholly reduce the 
> apparent amount of work involved in maintaining sequences: in sl_sequence, 
> create a column that contains the last value for each of the sequences.  
> Presumably, there would be a process that checks the current value of each 
> sequence... if it determined that the sequence had changed, run an update on 
> the last sequence (for the master), and do whatever it normally did to note 
> that the slaves should update their sequences.

This won't work.  It only tracks the last value of the sequence, not the
changes to the sequence data.  It's entirely likely that nodes will be
processing different syncs at different times.

For example

sync 1
 seq_y=1

sync 2
 seq_y=2

Node 1 is the provider and node 2 is the subscriber

A time t

Node1
-Has generated sync 2 - seq_y=2

Node 2 has only processed sync 1

On node 2 - seq_y should = 1, but that sequence data has been
overwritten, and all you know is the value of seq_y=2.  The node is in
an inconsistent state.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.

From cbbrowne at ca.afilias.info  Thu Mar 15 09:32:57 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 15 09:33:02 2007
Subject: [Slony1-general] Cutting down size of sl_seqlog
In-Reply-To: <200703150815.03804.lists_slony1-general@avsupport.com>
References: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
	<200703150815.03804.lists_slony1-general@avsupport.com>
Message-ID: <45F97539.5080302@ca.afilias.info>

Dan Falconer wrote:
> 	DISCLAIMER: I don't know a lot about the internals of Slony, especially when 
> it comes to how records get into the tables within Slony's replication 
> schema.
>
> 	That said, it seems to me that there should be a way to wholly reduce the 
> apparent amount of work involved in maintaining sequences: in sl_sequence, 
> create a column that contains the last value for each of the sequences.  
> Presumably, there would be a process that checks the current value of each 
> sequence... if it determined that the sequence had changed, run an update on 
> the last sequence (for the master), and do whatever it normally did to note 
> that the slaves should update their sequences.
>
> 	I apologize for the amount of vagueness in my reply.  It was quite apparent 
> to me, while attempting to formulate words to express my idea, that I knew 
> very little of the actual inner workings of Slony.  I hope the idea makes 
> sense and doesn't cause any eye-bleeding.  :) 
>   
No problem; I understand what you mean...

There are two problems with that approach:

1.  As Brad mentioned in his followup, this forces there to be Only One
Value for the sequence in flight at any given time.  That doesn't allow
properly handling the case where we have a series of updates thus:

SYNC 452 - seq_a = 121
SYNC 453 - seq_a = 124
SYNC 454 - seq_a = 124
SYNC 455 - seq_a = 125
SYNC 456 - seq_a = 127
SYNC 457 - seq_a = 135
SYNC 458 - seq_a = 124  (And it jumps back to 124, for some reason...)

If the value gets stowed on sl_sequence, and node 5 happens to fall a
bit behind so that we have to apply SYNCs 452 thru 458, we haven't any
place to get those interim values.

The typical usage of a sequence value is to provide values for primary
keys; in that case, losing interim values isn't terribly important.

But sometimes sequences are characterizing the state of something;
rotating between 0 and 1 or otherwise cycling through some small set of
values that actually imply some symbolic meaning.  And we may want that
to be consistent at the end of each SYNC.

This is why the idea isn't good enough.

Then there's problem #2...

2.  If we're continually updating the contents of sl_sequence, then that
means the table will be blowing through dead tuples "like nobody's
business."

That's already a problem with sl_seqlog; making it a problem with
sl_sequence seems to me to worsen things more, notably because it's a
much wider table.

I'm not pointing out the problems to be unkind or anything; the point of
the exercise is to better understand the usage of sl_seqlog.  Finding
worse alternatives   After pointing out enough flaws in various
proposals, however well-baked, inspiration will hopefully strike and
someone may perceive a "non-flawed" approach that we *can* use.
From cbbrowne at ca.afilias.info  Thu Mar 15 09:45:53 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 15 09:45:59 2007
Subject: [Slony1-general] Cutting down size of sl_seqlog
In-Reply-To: <1173962997.5446.74.camel@dba5.int.libertyrms.com>
References: <60lkhzmvau.fsf@dba2.int.libertyrms.com>
	<1173962997.5446.74.camel@dba5.int.libertyrms.com>
Message-ID: <45F97841.3080705@ca.afilias.info>

Brad Nicholson wrote:
> On Wed, 2007-03-14 at 16:20 -0400, Chris Browne wrote:
>   
>> It's not a LOT better, but I think it's at least *somewhat* better.
>> And perhaps someone can point out a way of doing better still, perhaps
>> to not bother generating some of the tuples altogether.
>>
>> Thoughts?
>>     
>
> Is there a reason that there has to be an entry in sl_seqlog if the
> sequence doesn't change?  If not, then you should be able to insert on
> change, and do nothing for case where it doesn't change.
>   
Hmm.  That's a thought.

We pull the value as part of the COPY_SET event; that is SURE to
initialize the sequence.

If we don't bother recording a value, when it doesn't change, that means
there won't be an update to be found.  Which isn't wrong.

The case I see being a *tad* troublesome is if we are processing SYNCs
4781 thru 4820 as one "sync group."  In that case, I think the slon
looks at 4820, at present.  If the last sequence update for #21 was in
SYNC 4600, then we don't really need to change that sequence.  If the
last sequence update was in SYNC 4812, then we have to search for the
last sequence update within the current "sync group" range.

Thus, for each sequence, we're looking for...

select seql_last_value from sl_seqlog
where
   seql_seqid = 21 and
   seql_origin = 2 and
   seql_ev_seqno = (select max(seql_ev_seqno) from sl_seqlog where
seql_seqid = 21 and seql_origin = 2 and seql_ev_seqno between 4781 and
4820);

Doing this in one "swell foop" as one query seems troublesome :-(

But I think you may be onto something here...
From andrew.george.hammond at gmail.com  Thu Mar 15 11:43:13 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Mar 15 11:43:22 2007
Subject: [Slony1-general] slony1-announce? rss feed?
Message-ID: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>

So what's the right way for a user to find out about 1.2.8 when it
gets released? I was thinking that a -announce mailing list would make
pretty good sense (not that -general is particularly high traffic).
But then I thought, "dude, that's so 90's. We should have an RSS
feed!" Which probably means that the California sun is going to my
head, but in all seriousness, is there any reason not to have an RSS
feed for announcements?

Andrew
From andrew at ca.afilias.info  Thu Mar 15 11:45:46 2007
From: andrew at ca.afilias.info (Andrew Sullivan)
Date: Thu Mar 15 11:46:24 2007
Subject: [Slony1-general] slony1-announce? rss feed?
In-Reply-To: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
References: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
Message-ID: <20070315184546.GA7096@afilias.info>

On Thu, Mar 15, 2007 at 11:43:13AM -0700, Andrew Hammond wrote:
> head, but in all seriousness, is there any reason not to have an RSS
> feed for announcements?

Well, one reason is that it's another piece of infrastructure that has
to be maintained by someone.  Are you volunteering?

A

-- 
Andrew Sullivan                         204-4141 Yonge Street
Afilias Canada                        Toronto, Ontario Canada
<andrew@ca.afilias.info>                              M2P 2A8
jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
From andrew.george.hammond at gmail.com  Thu Mar 15 11:51:57 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Mar 15 11:52:07 2007
Subject: [Slony1-general] slony1-announce? rss feed?
In-Reply-To: <20070315184546.GA7096@afilias.info>
References: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
	<20070315184546.GA7096@afilias.info>
Message-ID: <5a0a9d6f0703151151w41c5a891r79693f964a19a0a2@mail.gmail.com>

On 3/15/07, Andrew Sullivan <andrew@ca.afilias.info> wrote:
> On Thu, Mar 15, 2007 at 11:43:13AM -0700, Andrew Hammond wrote:
> > head, but in all seriousness, is there any reason not to have an RSS
> > feed for announcements?
>
> Well, one reason is that it's another piece of infrastructure that has
> to be maintained by someone.  Are you volunteering?

Sure.

Andrew
From cbbrowne at ca.afilias.info  Thu Mar 15 12:02:22 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 15 12:02:29 2007
Subject: [Slony1-general] slony1-announce? rss feed?
In-Reply-To: <5a0a9d6f0703151151w41c5a891r79693f964a19a0a2@mail.gmail.com>
References: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>	<20070315184546.GA7096@afilias.info>
	<5a0a9d6f0703151151w41c5a891r79693f964a19a0a2@mail.gmail.com>
Message-ID: <45F9983E.1040405@ca.afilias.info>

Andrew Hammond wrote:
> On 3/15/07, Andrew Sullivan <andrew@ca.afilias.info> wrote:
>> On Thu, Mar 15, 2007 at 11:43:13AM -0700, Andrew Hammond wrote:
>> > head, but in all seriousness, is there any reason not to have an RSS
>> > feed for announcements?
>>
>> Well, one reason is that it's another piece of infrastructure that has
>> to be maintained by someone.  Are you volunteering?
>
> Sure.
>
> Andrew
FYI, our "news" feed comes in a manifestly simple fashion.

It is generated from the contents of the file:

slony1-www/content/news.txt

The file looks like the following; each news item consists of:
1.  A line like "---"
2.  A Title
3.  A URL for that title to point to
4.  A date
5.  The name of the news-maker
6.  A series of lines of HTML, none of which consist of "---"

An idea would be to check this out of CVS on a regular basis, and
transform that into some form of RSS stream. 

One approach would be to do so on main.slony.info, so that we'd run
"make" in the content directory every so often so as to regenerate the
XML file that someone could then point to.

Another would be that you could do this where ever you like, doing a
"CVS update" every so often, and regenerating your XML file, and we
could publish a URL to your server...

Sample:
---
Slony-I Quick downloads
http://main.slony.info/downloads
2007-02-28
Chris Browne
Slony-1 1.2.7 <a
href="http://main.slony.info/downloads/1.2/slony1-1.2.7.tar.bz2">engine</a>
<a
href="http://main.slony.info/downloads/1.2/slony1-1.2.7-docs.tar.bz2">documentation</a>
<br />
Slony-1 1.1.7 <a
href="http://main.slony.info/downloads/1.1/slony1-1.1.7.tar.bz2">engine</a>
<!-- Please keep this item at the top of the news list -->
---
Slony-I Release - 1.2.7
http://main.slony.info/downloads/1.2
2007-03-07
Chris Browne
We are pleased to announce the release of version 1.2.7, which may be
found <a
href="http://main.slony.info/downloads/1.2/slony1-1.2.7.tar.bz2">here</a>. 
There is also a <a
href="http://main.slony.info/downloads/1.2/slony1-1.2.7-docs.tar.bz2">Documentation
tarball </a>.

<P> This release fixes several problems that were found in the 1.2 stream:
<ul>
<li> Add remote_listen_timeout parameter to slon.conf

<p> This addresses the problem where a slon times out when accessing
sl_event if a node has been out of commission for a long time (several
days)

<li> Resolve bug #1623

<p> In this bug, big "action lists" that need to get compressed could cause
a logging printf to blow up.  Changed the logging level so that detail
is only shown at level 4, which won't bite people by default.

<li> UNINSTALL NODE failures now show node # in slonik error messages

<P> If a user ran several UNINSTALL NODE requests in a single slonik
script, and one of them broke, you'd have no ready way to tell which
node this failed on.  Added code to report the node # where it failed.

<li> Added test to test1 for function generate_sync_event() and
make_function_strict

<li> Added "v81" files (for slony1_base.v81.sql, slony1_funcs.v81.sql,
xxid.v81.sql), necessary to support 8.1 "ALTER FUNCTION ... STRICT";

<li> Fixed quoting problem in generate_sync_event()

<li> Added functionality to UPDATE FUNCTIONS to make xxidin() function
STRICT; the absence of this caused postmaster to fall over when
processing MOVE SET event in PG v8.2

<li> Added documentation of an issue surrounding NULLABLE columns to the
log analysis chapter of the admin guide and to the UPGRADING docs.

<li> When you run MOVE SET, this populates sl_setsync for the moved set
  even on nodes that are not subscribed.  If, subsequent to doing this,
  you attempt a SUBSCRIBE SET for a formerly-unsubscribed node, the
  subscription will fail right at the end when the slon tries to insert a
  new value to sl_setsync.

<P>   The fix: DELETE from sl_setsync immediately before the INSERT.  This
  will silently blow away any 'offending' sl_setsync row.

<p>  (As observed by Afilias staff...)

<li> Log shipping fix - storage of sl_setsync_offline call had a wrong
  printf type; change from %d to %s
</ul>
---
Slony-I Release - 1.1.7
http://main.slony.info/downloads/1.1
2007-03-07
Chris Browne
We are pleased to announce the release of version 1.1.7, which may be
found <a
href="http://main.slony.info/downloads/1.1/slony1-1.1.7.tar.bz2">here</a>. 
It fixes several problems that were found in the 1.2 stream that were
readily back-ported to 1.1:
<ul>
<li>Add remote_listen_timeout parameter to <tt>slon.conf</tt>

<p> This addresses the problem where a slon times out when accessing
sl_event if a node has been out of commission for a long time (several
days)

<li> Add node numbers to error reports in slonik

<p> If portions of UNINSTALL NODE break, report in the error message
which node
it was working on.  That way the gentle user gets an indication as to
which node 'broke' if the slonik script contained multiple such
requests.

<li> Applied bug fix for #1538

<p> If cluster has only one node, then remove all events up to the last
SYNC.  That allows the cleanup loop to clear out sl_log_{1/2}.
Otherwise, the log tables will forever bloat until you add a second
node...

<li> Added test to test1 for function generate_sync_event() and
make_function_strict

<li> Added "v81" files (for slony1_base.v81.sql, slony1_funcs.v81.sql,
xxid.v81.sql), necessary to support 8.1 "ALTER FUNCTION ... STRICT";

<li> Fixed quoting problem in generate_sync_event()

<li> Added functionality to UPDATE FUNCTIONS to make xxidin() function
STRICT; the absence of this caused postmaster to fall over when
processing MOVE SET event in PG v8.2 (not to say that Slony-I 1.1 now
*supports* 8.2; it does not)

<li> When you run MOVE SET, this populates sl_setsync for the moved set
  even on nodes that are not subscribed.  If, subsequent to doing this,
  you attempt a SUBSCRIBE SET for a formerly-unsubscribed node, the
  subscription will fail right at the end when the slon tries to insert a
  new value to sl_setsync.

<p>  The fix: DELETE from sl_setsync immediately before the INSERT.  This
  will silently blow away any 'offending' sl_setsync row.

<P>  (As observed by Afilias staff...)

<li> Log shipping fix - storage of sl_setsync_offline call had a wrong
  printf type; change from %d to %s
</ul>
---
Slony-I New Web Site!
http://main.slony.info
2007-03-02
Chris Browne
We are pleased to announce the release of the new Slony-I web site at
<tt>main.slony.info</tt>.
---
Slony-I New CVS Site!
http://main.slony.info/cvs.html
2007-01-20
<a href="mailto:cbbrowne@acm.org">Chris Browne</a>
The Slony-I project is now making active use of CVS repositories at
<tt>cvs.slony.info</tt>; this replaces the former
CVS repositories at gBorg.  We thank the gBorg operators for their
support over the last few years, and Command Prompt for their provision
of hardware, administrative, and hosting services for
<tt>cvs.slony.info</tt>.
---
From jks at selectacast.net  Thu Mar 15 12:02:47 2007
From: jks at selectacast.net (Joseph S)
Date: Thu Mar 15 12:02:56 2007
Subject: [Slony1-general] slony1-announce? rss feed?
In-Reply-To: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
References: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
Message-ID: <45F99857.6010005@selectacast.net>

Shouldn't everyone who uses slony be on the postgresql announce list anyway?

Andrew Hammond wrote:
> So what's the right way for a user to find out about 1.2.8 when it
> gets released? I was thinking that a -announce mailing list would make
> pretty good sense (not that -general is particularly high traffic).
> But then I thought, "dude, that's so 90's. We should have an RSS
> feed!" Which probably means that the California sun is going to my
> head, but in all seriousness, is there any reason not to have an RSS
> feed for announcements?
> 
From cedric at over-blog.com  Fri Mar 16 02:37:49 2007
From: cedric at over-blog.com (cedric)
Date: Fri Mar 16 02:38:07 2007
Subject: [Slony1-general] slony1-announce? rss feed?
In-Reply-To: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
References: <5a0a9d6f0703151143v59e9cbbbme2099d0f9f527e6a@mail.gmail.com>
Message-ID: <200703161037.49640.cedric@over-blog.com>

Le jeudi 15 mars 2007 19:43, Andrew Hammond a ?crit?:
> So what's the right way for a user to find out about 1.2.8 when it
> gets released? I was thinking that a -announce mailing list would make
> pretty good sense (not that -general is particularly high traffic).
> But then I thought, "dude, that's so 90's. We should have an RSS 
> feed!" 
Yeah, but RSS are so web, and Atom so web2.0 :-D 
> Which probably means that the California sun is going to my 
> head, but in all seriousness, is there any reason not to have an RSS
> feed for announcements?
>
> Andrew
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
From david at fetter.org  Fri Mar 16 05:56:46 2007
From: david at fetter.org (David Fetter)
Date: Fri Mar 16 05:56:54 2007
Subject: [Slony1-general] EXECUTE SCRIPT and Combinations of DDL and DML
Message-ID: <20070316125646.GA29668@fetter.org>

Folks,

When managing database changes on a single database, I frequently do
things like:

BEGIN;
CREATE TABLE ...;
COPY ... ; -- Populate the new table
ALTER TABLE ... ADD COLUMN ... USING ... ; -- From previous statements
ALTER TABLE ...;
DROP TABLE ....;
...
COMMIT;

This way, only whole schema changes are visible.

How do people approach this with Slony?

Cheers,
D
-- 
David Fetter <david@fetter.org> http://fetter.org/
phone: +1 415 235 3778        AIM: dfetter666
                              Skype: davidfetter

Remember to vote!
Consider donating to PostgreSQL: http://www.postgresql.org/about/donate
From dbrain at bandwidth.com  Fri Mar 16 07:07:00 2007
From: dbrain at bandwidth.com (David Brain)
Date: Fri Mar 16 07:10:14 2007
Subject: [Slony1-general] Configuration/Tuning Question
Message-ID: <45FAA484.2080702@bandwidth.com>

Hi,

I have a working Slony environment, but I would really appreciate some 
assistance on tuning/configuration for better performance.

My situation is that I have source servers (masters) replicating to one 
archive (slave) server - each of the source servers basically replicates 
one table (which has a unique name) to the archive server.

The problem I am having is that there is a significant number of writes 
(maybe 20-60 rows per second) from each source server into the archive, 
and the large number of small writes is taking a toll on the archive 
server in terms of disk performance.

Is there a way to configure the Slony such that the writes take place in 
a more 'batched' fashion, say writing all the rows from 10-15 seconds 
worth of imports in 1 transaction.  In our situation there is no vital 
need for the archive server to be in tight sync with the source server - 
it could lag by as much as 5 mins without there being a significant problem.

Thanks,

David.

From tilman.baumann at collax.com  Fri Mar 16 07:22:45 2007
From: tilman.baumann at collax.com (Tilman Baumann)
Date: Fri Mar 16 07:23:06 2007
Subject: [Slony1-general] missed to create tables on slave
Message-ID: <45FAA835.7000407@collax.com>

Hi,

in a inattentive moment i addes tome table to the replacation without
creating the tables on the slave fist.
I used the slony helpers in pgadmin3.
I added the new tables to a new replication set. After that i merged the
new set with my previous one. And than it struk me that i missed a step.
I guess if i had not merged the sets. I could remove the slave note from
that set, create the tables and add ist again.

Can anyone please give me a direction what i could do to bring the
synchronisation in 'sync' again?
Settiung the slave up new would be a ok option. The main thing is that
it syncs all tables after that. :)

Are there some tricks to tell slony to have a look for not yet synced
tables?

Thank you.

-- 
Tilman Baumann
Software Developer
Collax GmbH . Boetzinger Strasse 60 . 79111 Freiburg . Germany

p: +49 (0) 89-990157-0
f: +49 (0) 89-990157-11

Geschaeftsfuehrer: William K. Hite / Boris Nalbach
AG Muenchen HRB 158898, Ust.-IdNr: DE 814464942

From cbbrowne at ca.afilias.info  Fri Mar 16 10:17:26 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar 16 10:17:32 2007
Subject: [Slony1-general] documentation slon logging output
In-Reply-To: <5a0a9d6f0703131157k48f3223dhefefa9f3afaef691@mail.gmail.com>
References: <5a0a9d6f0703131157k48f3223dhefefa9f3afaef691@mail.gmail.com>
Message-ID: <45FAD126.2050904@ca.afilias.info>

Andrew Hammond wrote:
> In the process of hacking on the slon-mkservice stuff, I got all
> excited about multilog's filtering capabilities and wanted to write
> some clever filters. So, I went looking for documentation on the
> output of slons and I can't find anything definitive, except the
> source.
>
> Is there a document anywhere that clearly lays out what the different
> outputs mean? Heck, even a list of the different types of log
> statements possible would probably be sufficient. If not, I can grep
> it out of the source. If there a wiki for this stuff I'd be happy to
> put it there to get annotated.
Do you mean like this?
http://slony.info/documentation/loganalysis.html

That was added to the documentation set (at Elein's suggestion) during
OSCON 2006 last year.
From cbbrowne at ca.afilias.info  Fri Mar 16 11:30:14 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar 16 11:30:25 2007
Subject: [Slony1-general] Configuration/Tuning Question
In-Reply-To: <45FAA484.2080702@bandwidth.com>
References: <45FAA484.2080702@bandwidth.com>
Message-ID: <45FAE236.3070202@ca.afilias.info>

David Brain wrote:
> Hi,
>
> I have a working Slony environment, but I would really appreciate some
> assistance on tuning/configuration for better performance.
>
> My situation is that I have source servers (masters) replicating to
> one archive (slave) server - each of the source servers basically
> replicates one table (which has a unique name) to the archive server.
>
> The problem I am having is that there is a significant number of
> writes (maybe 20-60 rows per second) from each source server into the
> archive, and the large number of small writes is taking a toll on the
> archive server in terms of disk performance.
>
> Is there a way to configure the Slony such that the writes take place
> in a more 'batched' fashion, say writing all the rows from 10-15
> seconds worth of imports in 1 transaction.  In our situation there is
> no vital need for the archive server to be in tight sync with the
> source server - it could lag by as much as 5 mins without there being
> a significant problem.
>
The parameter that you'll want to change is "sync_interval".  It can be
set on the command line via the "-s" option.

The default value is likely 2000 (2000ms, or 2s); if you increase that,
you will reduce how often SYNCs get generated, which will have the
effect that you are looking for.

To increase the time covered to 15s, you'd run:

slon -s 15000 [and other parms, of course :-)]
From andrew.george.hammond at gmail.com  Fri Mar 16 11:58:00 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar 16 11:58:11 2007
Subject: [Slony1-general] updated slon-mkservice (and logrep-mkservice too)
Message-ID: <5a0a9d6f0703161158u63edd393rbbef3389ddd31793@mail.gmail.com>

On 3/9/07, Andrew Hammond <andrew.george.hammond@gmail.com> wrote:
> On 3/9/07, Vivek Khera <vivek@khera.org> wrote:
> >
> > On Mar 9, 2007, at 12:59 PM, Andrew Hammond wrote:
> >
> > > I'll give a +1 on using daemontools to run the slons. It's far cleaner
> > > and easier than anything else I've tried. I've also put some more work
> > > into polishing the slon-mkservice.sh script, which I would be happy to
> > > contribute back if there's any interest.
> >
> > absolutely!
>
> Ok, I'll spit-n-polish then email my version to you.

Attached. Feedback is very welcome. Yes, I realize it creates
annoyingly verbose directory names for the services.

logrep is a tool intended to allow efficient real-time filtering of
log files so that you can pull semantically related information from a
highly verbose log (ie DEBUG2) and put it all in one place. This is
very handy if you want to implement automated monitoring scripts for
stuff like nagios. It relies on the tail -F functionality (tested on
FreeBSD, OSX, Linux).

I've included a multilog filter for use with logrep that plucks out
everything related to subscribe as an example. I'll eventually get
around to writing a nagios monitor that does something clever with
this log output (like say warn when a slon is doing the initial copy
so you know not to clobber it). I'd love to see other filters that
pull out other interesting stuff, like specific error conditions, for
example.

Andrew
-------------- next part --------------
A non-text attachment was scrubbed...
Name: logrep-mkservice.sh
Type: application/x-sh
Size: 3943 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20070316=
/1dc718ce/logrep-mkservice-0001.sh
-------------- next part --------------
A non-text attachment was scrubbed...
Name: slon-mkservice.sh
Type: application/x-sh
Size: 13125 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20070316=
/1dc718ce/slon-mkservice-0001.sh
From andrew.george.hammond at gmail.com  Fri Mar 16 14:16:36 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar 16 14:16:44 2007
Subject: [Slony1-general] documentation slon logging output
In-Reply-To: <45FAD126.2050904@ca.afilias.info>
References: <5a0a9d6f0703131157k48f3223dhefefa9f3afaef691@mail.gmail.com>
	<45FAD126.2050904@ca.afilias.info>
Message-ID: <5a0a9d6f0703161416g4892df5dlcf395e160f2fca92@mail.gmail.com>

> > Is there a document anywhere that clearly lays out what the different
> > outputs mean? Heck, even a list of the different types of log
> > statements possible would probably be sufficient. If not, I can grep
> > it out of the source. If there a wiki for this stuff I'd be happy to
> > put it there to get annotated.
> Do you mean like this?
> http://slony.info/documentation/loganalysis.html
>
> That was added to the documentation set (at Elein's suggestion) during
> OSCON 2006 last year.

Yeah, exactly that. Someone on the IRC channel pointed it out to me
moments after posting to the mailing list. Isn't that always the way?

I noticed that a number of things in the document don't quite match
what the code is actually doing in trivial ways. For example there are
no colons after DEBUG2 in the actual output, but they're in the
document. Makes writing filters more interesting.

Andrew
From cbbrowne at ca.afilias.info  Fri Mar 16 16:10:09 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar 16 16:10:20 2007
Subject: [Slony1-general] Slony-I Releases 1.1.8, 1.2.8
Message-ID: <606490n5ta.fsf@dba2.int.libertyrms.com>

Listed below are the release notes for these releases, now available
at main.slony.info...  

I ran 1.2.8 thru tests on each major versions > 7.3 of PostgreSQL, so
we're definitely not missing any version tests there.  

I only tested 1.1.8 on 7.4, 8.0, and 8.1; if you're still on 7.3, you
may want to do a bit more testing before deploying...

-------------
$Id: RELEASE-1.1.8,v 1.1.2.5 2007-03-15 18:55:34 cbbrowne Exp $

- Change to rotatelogs configuration to support older versions of Apache
log rotator

- Fix to altperl "execute script" script to pass the filename properly

- Fix to xxid Makefile: in the 1.1 branch, xxid.v73.sql is still in
use, and should be copied to generate the other versions.

- Fix to src/backend/slony1_funcs.v80.sql - comment on the right function in v8.0

- Fix to src/slonik/slonik.c - it wasn't pulling in the right version of slony1_funcs.?.sql for late breaking versions of PostgreSQL
-------------
$Id: RELEASE-1.2.8,v 1.1.2.5 2007-03-16 22:34:46 cbbrowne Exp $

- Change to rotatelogs configuration to support older versions of Apache
log rotator

- Fix to altperl "execute script" script to pass the filename properly

- Fix to src/backend/slony1_funcs.v80.sql - comment on the right function in v8.0

- Fix to src/slonik/slonik.c - it wasn't pulling in the right version of slony1_funcs.?.sql in some cases

- Updated docs on creating releases to describe the version mismatch problem found above in slonik.c
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From andrew.george.hammond at gmail.com  Fri Mar 16 17:11:15 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Fri Mar 16 17:11:26 2007
Subject: [Slony1-general] EXECUTE SCRIPT and Combinations of DDL and DML
In-Reply-To: <20070316125646.GA29668@fetter.org>
References: <20070316125646.GA29668@fetter.org>
Message-ID: <5a0a9d6f0703161711i4b442e3bj41b40df9c2822a8e@mail.gmail.com>

On 3/16/07, David Fetter <david@fetter.org> wrote:
> Folks,
>
> When managing database changes on a single database, I frequently do
> things like:
>
> BEGIN;
> CREATE TABLE ...;
either slonik execute script or via psql against all members in the cluster

slonik create new set
slonik add table,
slonik subscribe set as necessary

> COPY ... ; -- Populate the new table

Against the origin, as you'd expect.

> ALTER TABLE ... ADD COLUMN ... USING ... ; -- From previous statements
> ALTER TABLE ...;

slonik execute script,

> DROP TABLE ....;

slonik drop set,
slonik execute script or via psql on all nodes

> COMMIT;
>
> This way, only whole schema changes are visible.

I am not aware of any way to achieve this in a single transaction.

> How do people approach this with Slony?

I recently submitted an RFC to both this list and pgsql-general for a
tool we're building here that's intended to address automation of this
kind of multi-phased upgrade.

http://archives.postgresql.org/pgsql-general/2007-03/msg00272.php

There has been further thinking about this tool locally so that
posting is a bit out of date. If you're interested. I need to clean up
the wiki page that hosts the design anyway. I'll put it back on the
list if you're intereested. We're already  into coding at this point.
I intend to release the tool once it's got the basic functionality
implemented.

Andrew
From dun at haisuli.net  Sat Mar 17 02:28:25 2007
From: dun at haisuli.net (Mikko Partio)
Date: Sat Mar 17 02:28:43 2007
Subject: [Slony1-general] Execute script and "execute only on"
Message-ID: <45FBB4B9.30501@haisuli.net>

Hi,

slonik's EXECUTE SCRIPT -documentation says that:

EXECUTE ONLY ON = ival

    (Optional) The ID of the only node to actually execute the script.
    This option causes the script to be propagated by all nodes but
    executed only by one. The default is to execute the script on all
    nodes that are subscribed to the set.


In my experience this property is not working correctly, and here's the 
proof ("tiuhti" is origin and "viuhti" subscriber):

slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h tiuhti
CREATE TABLE
slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h viuhti
CREATE TABLE

slony1@tiuhti:~$ cat drop_table_testtable.sql
DROP TABLE testtable;

slony1@tiuhti:~$ cat droptest.slonik
#!/usr/bin/slonik

CLUSTER NAME=climate;

NODE 1 ADMIN CONNINFO = 'dbname=cldb host=tiuhti user=slony1';
NODE 2 ADMIN CONNINFO = 'dbname=cldb host=viuhti user=slony1';

EXECUTE SCRIPT (
        SET ID = 1,
        FILENAME = '/home/slony1/drop_table_testtable.sql',
        EVENT NODE = 1,
        EXECUTE ONLY ON = 2
);

slony1@tiuhti:~$ slonik droptest.slonik
DDL script consisting of 1 SQL statements
DDL Statement 0: (0,21) [DROP TABLE testtable;]
Submit DDL Event to subscribers...
DDL on origin - PGRES_TUPLES_OK

slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h viuhti
Did not find any relation named "testtable".

This is what I expected, but

slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h tiuhti
Did not find any relation named "testtable".

Wooah - the script dropped table testtable from both nodes although I 
specified the "execute only on" -option. Is there something I'm missing 
or is there a bug?

Regards

MP

From ssinger_pg at sympatico.ca  Sat Mar 17 05:43:14 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Sat Mar 17 05:43:20 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <45FBB4B9.30501@haisuli.net>
References: <45FBB4B9.30501@haisuli.net>
Message-ID: <Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>

On Sat, 17 Mar 2007, Mikko Partio wrote:

Your getting this because your EVENT_NODE is 1 but you only want to execute 
the script on 2.

Slonik probably should have a check to see if you have specified an only 
excecute different than your event node and just submit the script into the 
queue at that stage.

Another option is to require the event node be equal to the only 
execute node.



> Hi,
>
> slonik's EXECUTE SCRIPT -documentation says that:
>
> EXECUTE ONLY ON = ival
>
>   (Optional) The ID of the only node to actually execute the script.
>   This option causes the script to be propagated by all nodes but
>   executed only by one. The default is to execute the script on all
>   nodes that are subscribed to the set.
>
>
> In my experience this property is not working correctly, and here's the proof 
> ("tiuhti" is origin and "viuhti" subscriber):
>
> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h tiuhti
> CREATE TABLE
> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h viuhti
> CREATE TABLE
>
> slony1@tiuhti:~$ cat drop_table_testtable.sql
> DROP TABLE testtable;
>
> slony1@tiuhti:~$ cat droptest.slonik
> #!/usr/bin/slonik
>
> CLUSTER NAME=climate;
>
> NODE 1 ADMIN CONNINFO = 'dbname=cldb host=tiuhti user=slony1';
> NODE 2 ADMIN CONNINFO = 'dbname=cldb host=viuhti user=slony1';
>
> EXECUTE SCRIPT (
>       SET ID = 1,
>       FILENAME = '/home/slony1/drop_table_testtable.sql',
>       EVENT NODE = 1,
>       EXECUTE ONLY ON = 2
> );
>
> slony1@tiuhti:~$ slonik droptest.slonik
> DDL script consisting of 1 SQL statements
> DDL Statement 0: (0,21) [DROP TABLE testtable;]
> Submit DDL Event to subscribers...
> DDL on origin - PGRES_TUPLES_OK
>
> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h viuhti
> Did not find any relation named "testtable".
>
> This is what I expected, but
>
> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h tiuhti
> Did not find any relation named "testtable".
>
> Wooah - the script dropped table testtable from both nodes although I 
> specified the "execute only on" -option. Is there something I'm missing or is 
> there a bug?
>
> Regards
>
> MP
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From david at fetter.org  Sat Mar 17 07:34:19 2007
From: david at fetter.org (David Fetter)
Date: Sat Mar 17 07:34:27 2007
Subject: [Slony1-general] EXECUTE SCRIPT and Combinations of DDL and DML
In-Reply-To: <5a0a9d6f0703161711i4b442e3bj41b40df9c2822a8e@mail.gmail.com>
References: <20070316125646.GA29668@fetter.org>
	<5a0a9d6f0703161711i4b442e3bj41b40df9c2822a8e@mail.gmail.com>
Message-ID: <20070317143419.GC21950@fetter.org>

On Fri, Mar 16, 2007 at 05:11:15PM -0700, Andrew Hammond wrote:
> On 3/16/07, David Fetter <david@fetter.org> wrote:
> >Folks,
> >
> >When managing database changes on a single database, I frequently do
> >things like:
> >
> >BEGIN;
> >CREATE TABLE ...;
> either slonik execute script or via psql against all members in the cluster
> 
> slonik create new set
> slonik add table,
> slonik subscribe set as necessary

Is there a wrapper for the "add a table to a replication set"
operation?  It seems this would be a common use case.

> >COPY ... ; -- Populate the new table
> 
> Against the origin, as you'd expect.

OK

> >ALTER TABLE ... ADD COLUMN ... USING ... ; -- From previous statements
> >ALTER TABLE ...;
> 
> slonik execute script,

Right.

> >DROP TABLE ....;
> 
> slonik drop set,

Really?!?  I thought it would be a SET DROP TABLE.

> slonik execute script or via psql on all nodes
> 
> >COMMIT;
> >
> >This way, only whole schema changes are visible.
> 
> I am not aware of any way to achieve this in a single transaction.

This strikes me as a major lack.  I discussed this with Jan yesterday,
and he told me that it would be possible to send the script through an
EXECUTE SCRIPT, but that it would be a Good Idea(TM) to schedule some
down time for write operations.  I didn't ask about the effects going
forward on the set, though.

> >How do people approach this with Slony?
> 
> I recently submitted an RFC to both this list and pgsql-general for a
> tool we're building here that's intended to address automation of this
> kind of multi-phased upgrade.
> 
> http://archives.postgresql.org/pgsql-general/2007-03/msg00272.php
> 
> There has been further thinking about this tool locally so that
> posting is a bit out of date. If you're interested. I need to clean up
> the wiki page that hosts the design anyway. I'll put it back on the
> list if you're intereested. We're already  into coding at this point.
> I intend to release the tool once it's got the basic functionality
> implemented.

Interesting :)

Could you publish the updates, and what you have so far?  This looks
like a good candidate for inclusion the Slony-I tool kit. :)

Cheers,
David.
-- 
David Fetter <david@fetter.org> http://fetter.org/
phone: +1 415 235 3778        AIM: dfetter666
                              Skype: davidfetter

Remember to vote!
Consider donating to PostgreSQL: http://www.postgresql.org/about/donate
From ssinger_pg at sympatico.ca  Sat Mar 17 10:06:54 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Sat Mar 17 10:07:04 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>
	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
Message-ID: <Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>

On Sat, 17 Mar 2007, Steve Singer wrote:

The attached patch against 1.2 should fix this bug by not
executing your DDL on the event node.

If your DDL has errors then you won't find out about it 
until slon tries to execute it on the other node (slonik won't give you an 
error but making changes just on your subscribers can have the potential 
for all sorts of trouble if your not careful)



> On Sat, 17 Mar 2007, Mikko Partio wrote:
>
> Your getting this because your EVENT_NODE is 1 but you only want to execute 
> the script on 2.
>
> Slonik probably should have a check to see if you have specified an only 
> excecute different than your event node and just submit the script into the 
> queue at that stage.
>
> Another option is to require the event node be equal to the only execute 
> node.
>
>
>
>> Hi,
>> 
>> slonik's EXECUTE SCRIPT -documentation says that:
>> 
>> EXECUTE ONLY ON = ival
>> 
>>   (Optional) The ID of the only node to actually execute the script.
>>   This option causes the script to be propagated by all nodes but
>>   executed only by one. The default is to execute the script on all
>>   nodes that are subscribed to the set.
>> 
>> 
>> In my experience this property is not working correctly, and here's the 
>> proof ("tiuhti" is origin and "viuhti" subscriber):
>> 
>> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h 
>> tiuhti
>> CREATE TABLE
>> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h 
>> viuhti
>> CREATE TABLE
>> 
>> slony1@tiuhti:~$ cat drop_table_testtable.sql
>> DROP TABLE testtable;
>> 
>> slony1@tiuhti:~$ cat droptest.slonik
>> #!/usr/bin/slonik
>> 
>> CLUSTER NAME=climate;
>> 
>> NODE 1 ADMIN CONNINFO = 'dbname=cldb host=tiuhti user=slony1';
>> NODE 2 ADMIN CONNINFO = 'dbname=cldb host=viuhti user=slony1';
>> 
>> EXECUTE SCRIPT (
>>       SET ID = 1,
>>       FILENAME = '/home/slony1/drop_table_testtable.sql',
>>       EVENT NODE = 1,
>>       EXECUTE ONLY ON = 2
>> );
>> 
>> slony1@tiuhti:~$ slonik droptest.slonik
>> DDL script consisting of 1 SQL statements
>> DDL Statement 0: (0,21) [DROP TABLE testtable;]
>> Submit DDL Event to subscribers...
>> DDL on origin - PGRES_TUPLES_OK
>> 
>> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h viuhti
>> Did not find any relation named "testtable".
>> 
>> This is what I expected, but
>> 
>> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h tiuhti
>> Did not find any relation named "testtable".
>> 
>> Wooah - the script dropped table testtable from both nodes although I 
>> specified the "execute only on" -option. Is there something I'm missing or 
>> is there a bug?
>> 
>> Regards
>> 
>> MP
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>> 
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
Index: slonik/slonik.c
===================================================================
RCS file: /slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.67.2.6
diff -c -w -r1.67.2.6 slonik.c
*** slonik/slonik.c	15 Mar 2007 18:52:02 -0000	1.67.2.6
--- slonik/slonik.c	17 Mar 2007 16:30:55 -0000
***************
*** 3916,3921 ****
--- 3916,3926 ----
  		}
  		strncpy(dest, dstring_data(&script) + startpos, endpos-startpos);
  		dest[STMTS[stmtno]-startpos] = 0;
+ 
+ 		if(stmt->only_on_node==-1 ||
+ 		   stmt->only_on_node ==stmt->ev_origin) 
+ 		  {
+ 
  		  slon_mkquery(&query, dest);
  		  printf("DDL Statement %d: (%d,%d) [%s]\n", stmtno, startpos, endpos, dest);
  		  free(dest);
***************
*** 3933,3938 ****
--- 3938,3955 ----
  		    }
  		  /* rstat = PQresultStatus(res); */
  		  /* printf ("Success - %s\n", PQresStatus(rstat)); */
+ 
+ 		  }
+ 		else {
+ 
+ 		    /**
+ 		     * Do not execute
+ 		     */
+ 		    printf("Skipping EXECUTE SCRIPT on node %d only for %d\n",
+ 			   stmt->only_on_node, stmt->ev_origin);
+ 		}
+ 
+ 
  	}
  	
  	printf("Submit DDL Event to subscribers...\n");
From dun at haisuli.net  Sat Mar 17 10:08:11 2007
From: dun at haisuli.net (Mikko Partio)
Date: Sat Mar 17 10:08:22 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>
	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
Message-ID: <45FC207B.8030608@haisuli.net>

Steve Singer wrote:
>
>
> Slonik probably should have a check to see if you have specified an 
> only excecute different than your event node and just submit the 
> script into the queue at that stage.
>
Yes this is what I would expect after reading the documentation. I 
suggest that either the functionality is changed according to your 
description or the documentation is updated.

Regards

MP
From dun at haisuli.net  Sat Mar 17 10:11:16 2007
From: dun at haisuli.net (Mikko Partio)
Date: Sat Mar 17 10:11:24 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>
	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
Message-ID: <45FC2134.80601@haisuli.net>

Steve Singer wrote:
> On Sat, 17 Mar 2007, Steve Singer wrote:
>
> The attached patch against 1.2 should fix this bug by not
> executing your DDL on the event node.
>
> If your DDL has errors then you won't find out about it until slon 
> tries to execute it on the other node (slonik won't give you an error 
> but making changes just on your subscribers can have the potential for 
> all sorts of trouble if your not careful)
>
Thanks for your quick response. I will try out this patch asap.

Regards

MP
From jeff at frostconsultingllc.com  Sat Mar 17 10:24:45 2007
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Sat Mar 17 10:24:57 2007
Subject: [Slony1-general] EXECUTE SCRIPT and Combinations of DDL and DML
In-Reply-To: <20070316125646.GA29668@fetter.org>
References: <20070316125646.GA29668@fetter.org>
Message-ID: <Pine.LNX.4.64.0703171017030.5621@discord.home.frostconsultingllc.com>

On Fri, 16 Mar 2007, David Fetter wrote:

> Folks,
>
> When managing database changes on a single database, I frequently do
> things like:
>
> BEGIN;
> CREATE TABLE ...;
> COPY ... ; -- Populate the new table
> ALTER TABLE ... ADD COLUMN ... USING ... ; -- From previous statements
> ALTER TABLE ...;
> DROP TABLE ....;
> ...
> COMMIT;
>
> This way, only whole schema changes are visible.
>
> How do people approach this with Slony?

David,

Anything which is executed inside an EXECUTE SCRIPT is done as a single 
transaction.

Note that if you DROP a table, you'll need to have dropped it from replication 
prior to the execute script or you'll cause yourself some grief.

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From andrew.george.hammond at gmail.com  Sun Mar 18 13:52:45 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Sun Mar 18 13:52:53 2007
Subject: [Slony1-general] Re: Slony info - pg user
In-Reply-To: <5c8d96530703171043w56ebb371v66f35b5b49fd867b@mail.gmail.com>
References: <5c8d96530703171043w56ebb371v66f35b5b49fd867b@mail.gmail.com>
Message-ID: <5a0a9d6f0703181352y1036be14h226eed4110310d06@mail.gmail.com>

1) Join the mailing list. It's relatively low traffic and an excellent
resource for more complicated questions. As you have demonstrated by
finding my name in the archives, it has the added benefit that the
answer to your question may in the future help another person. If you
want personalized help, there are a number of highly qualified
consultants who would be very happy for the work.

2) For simpler questions (like the below), you probably want to hop
onto the IRC channel (as documented on the site)

3) I don't use windows, so I'm a poor choice to ask (again, the
mailing list would be a much better place to get in touch with windows
users). I seem to recall someone posting windows binaries. If I wanted
to run slony on windows, I would first find those binaries and see if
they included any windows specific documentation.

Once you have the binaries and have figured out how to run slons the
windows way, the standard documentation should be fine from then on.

Andrew


On 3/17/07, Valeriano Cossu <valeriano.cossu@gmail.com> wrote:
> Hi,
>
> sorry I've found your address on the Slony list.
>
> Can you help me? I want to use Slony on PostgreSQL on a windows system,
> you know some document or web site that explain how to set up Slony On
> Windows ? And some example?
>
>
> Thanks,
>
> Valeriano Cossu
> mobile: (+39) 3462187419
From mlists at rp-online.de  Sun Mar 18 14:17:14 2007
From: mlists at rp-online.de (Thomas Pundt)
Date: Sun Mar 18 14:17:29 2007
Subject: [Slony1-general] Slony-I Releases 1.1.8, 1.2.8
In-Reply-To: <606490n5ta.fsf@dba2.int.libertyrms.com>
References: <606490n5ta.fsf@dba2.int.libertyrms.com>
Message-ID: <45FDAC5A.9090102@rp-online.de>

Hi,

Christopher Browne schrieb:
> Listed below are the release notes for these releases, now available
> at main.slony.info...  =

> =

> I ran 1.2.8 thru tests on each major versions > 7.3 of PostgreSQL, so
> we're definitely not missing any version tests there.  =


there's a glitch in 1.2.8 in slony1_funcs.sql:upgradeSchema():
version numbers 1.1.7 and 1.1.8 in if-clause are improperly quoted, the
function exits with an error. A patch is attached.

Ciao,
Thomas
-------------- next part --------------
--- /usr/src/slony1-1.2.8/src/backend/slony1_funcs.sql	2007-03-16 15:43:00.=
000000000 +0100
+++ slony1_funcs.sql	2007-03-18 21:42:02.557918463 +0100
@@ -5806,7 +5806,7 @@
 	-- ----
 	-- Changes for 1.2
 	-- ----
-	if p_old IN (''1.0.2'', ''1.0.5'', ''1.0.6'', ''1.1.0'', ''1.1.1'', ''1.1=
.2'', ''1.1.3'',''1.1.5'', ''1.1.6'', "1.1.7", "1.1.8") then
+	if p_old IN (''1.0.2'', ''1.0.5'', ''1.0.6'', ''1.1.0'', ''1.1.1'', ''1.1=
.2'', ''1.1.3'',''1.1.5'', ''1.1.6'', ''1.1.7'', ''1.1.8'') then
 		-- Add new table sl_registry
 		execute ''create table @NAMESPACE@.sl_registry (
 						reg_key			text primary key,
From rod at iol.ie  Sun Mar 18 14:59:46 2007
From: rod at iol.ie (Raymond O'Donnell)
Date: Sun Mar 18 14:59:56 2007
Subject: [Slony1-general] Re: Slony info - pg user
In-Reply-To: <5a0a9d6f0703181352y1036be14h226eed4110310d06@mail.gmail.com>
References: <5c8d96530703171043w56ebb371v66f35b5b49fd867b@mail.gmail.com>
	<5a0a9d6f0703181352y1036be14h226eed4110310d06@mail.gmail.com>
Message-ID: <45FDB652.1000601@iol.ie>

On 18/03/2007 20:52, Andrew Hammond wrote:

> users). I seem to recall someone posting windows binaries. If I wanted
> to run slony on windows, I would first find those binaries and see if
> they included any windows specific documentation.

The PgInstaller for Windows will install Slony also. There was one 
gotcha regarding the naming of one of the script files, which I posted 
about some time back; hopefully that's been fixed.

> On 3/17/07, Valeriano Cossu <valeriano.cossu@gmail.com> wrote:
>> Can you help me? I want to use Slony on PostgreSQL on a windows system,

I have a small Slony setup on Windows (one master, one slave). I used 
the PostgreSQL installer to install, and had no difficulty with the 
exception of the problem above, which was easily fixed - if you run into 
it I'll post in greater detail.

The one major difference from the Linux version is that, instead of a 
separate slon daemon for each node in the cluster, you register a single 
slon service and then issue a slon --addengine command for each node. 
This is all described in the docs.

Ray.

---------------------------------------------------------------
Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
rod@iol.ie
---------------------------------------------------------------
From andrew at ca.afilias.info  Mon Mar 19 01:28:46 2007
From: andrew at ca.afilias.info (Andrew Sullivan)
Date: Mon Mar 19 01:29:20 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>
	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
Message-ID: <20070319082845.GB18136@afilias.info>

On Sat, Mar 17, 2007 at 01:06:54PM -0400, Steve Singer wrote:
> 
> If your DDL has errors then you won't find out about it 
> until slon tries to execute it on the other node (slonik won't give you an 
> error but making changes just on your subscribers can have the potential 
> for all sorts of trouble if your not careful)

I think this is the reason it didn't work as expected -- I think
someone raised the objection about what sort of mess this could make.
That doesn't mean it's not useful.

This piece of functionality seems to me to be exactly the sort of area
where using savepoints on the remote note might be a good idea, but
I'm worried about the potential for really nasty locks.  I think in
the medium term, we probably need a complete proposal on how to
improve EXECUTE SCRIPT's safety, or some sort of comprehensive
discussion about the various locks that are being taken and the like.
I'm slightly worried that some of the design in that area is happening
a little _ad hoc_, and I think a more complete idea of what the
trade-offs are, and which ones we can countenance, would be really
nice to have.

A



-- 
Andrew Sullivan                         204-4141 Yonge Street
Afilias Canada                        Toronto, Ontario Canada
<andrew@ca.afilias.info>                              M2P 2A8
jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
From andrew.george.hammond at gmail.com  Mon Mar 19 13:17:36 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Mon Mar 19 13:17:44 2007
Subject: [Slony1-general] EXECUTE SCRIPT and Combinations of DDL and DML
In-Reply-To: <20070317143419.GC21950@fetter.org>
References: <20070316125646.GA29668@fetter.org>
	<5a0a9d6f0703161711i4b442e3bj41b40df9c2822a8e@mail.gmail.com>
	<20070317143419.GC21950@fetter.org>
Message-ID: <5a0a9d6f0703191317k185c85ddk7b20561208fecaea@mail.gmail.com>

On 3/17/07, David Fetter <david@fetter.org> wrote:
> On Fri, Mar 16, 2007 at 05:11:15PM -0700, Andrew Hammond wrote:
> > On 3/16/07, David Fetter <david@fetter.org> wrote:
> > >Folks,
> > >
> > >When managing database changes on a single database, I frequently do
> > >things like:
> > >
> > >BEGIN;
> > >CREATE TABLE ...;
> > either slonik execute script or via psql against all members in the cluster
> >
> > slonik create new set
> > slonik add table,
> > slonik subscribe set as necessary
>
> Is there a wrapper for the "add a table to a replication set"
> operation?  It seems this would be a common use case.
>
> > >COPY ... ; -- Populate the new table
> >
> > Against the origin, as you'd expect.
>
> OK
>
> > >ALTER TABLE ... ADD COLUMN ... USING ... ; -- From previous statements
> > >ALTER TABLE ...;
> >
> > slonik execute script,
>
> Right.
>
> > >DROP TABLE ....;
> >
> > slonik drop set,
>
> Really?!?  I thought it would be a SET DROP TABLE.

Sure, if you want to. But then you still should probably deal with the
temporary set you created to do this. If you're planning to just dump
it anyway, why bother removing the table first?

> > slonik execute script or via psql on all nodes
> >
> > >COMMIT;
> > >
> > >This way, only whole schema changes are visible.
> >
> > I am not aware of any way to achieve this in a single transaction.
>
> This strikes me as a major lack.  I discussed this with Jan yesterday,
> and he told me that it would be possible to send the script through an
> EXECUTE SCRIPT, but that it would be a Good Idea(TM) to schedule some
> down time for write operations.  I didn't ask about the effects going
> forward on the set, though.

Yes, in retrospect, you certainly could create your working table,
copy into it, do your alters then drop the working table all in a
single execute script. That'd be a very clever approach, and you
wouldn't have to bother making a new set or replicating the working
table. Neat.

> > >How do people approach this with Slony?
> >
> > I recently submitted an RFC to both this list and pgsql-general for a
> > tool we're building here that's intended to address automation of this
> > kind of multi-phased upgrade.
> >
> > http://archives.postgresql.org/pgsql-general/2007-03/msg00272.php
> >
> > There has been further thinking about this tool locally so that
> > posting is a bit out of date. If you're interested. I need to clean up
> > the wiki page that hosts the design anyway. I'll put it back on the
> > list if you're intereested. We're already  into coding at this point.
> > I intend to release the tool once it's got the basic functionality
> > implemented.
>
> Interesting :)
>
> Could you publish the updates, and what you have so far?  This looks
> like a good candidate for inclusion the Slony-I tool kit. :)

I've been working on polishing up the wiki page and will post it again
shortly. I'll CC you when I do.

Andrew
From ssinger_pg at sympatico.ca  Mon Mar 19 17:48:15 2007
From: ssinger_pg at sympatico.ca (ssinger_pg@sympatico.ca)
Date: Mon Mar 19 17:48:29 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <20070319082845.GB18136@afilias.info>
References: <45FBB4B9.30501@haisuli.net>
	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
	<20070319082845.GB18136@afilias.info>
Message-ID: <Pine.LNX.4.62.0703191939160.3038@mini.atlantida.localdomain>

On Mon, 19 Mar 2007, Andrew Sullivan wrote:

> This piece of functionality seems to me to be exactly the sort of area
> where using savepoints on the remote note might be a good idea, but
> I'm worried about the potential for really nasty locks.  I think in
> the medium term, we probably need a complete proposal on how to
> improve EXECUTE SCRIPT's safety, or some sort of comprehensive
> discussion about the various locks that are being taken and the like.
> I'm slightly worried that some of the design in that area is happening
> a little _ad hoc_, and I think a more complete idea of what the
> trade-offs are, and which ones we can countenance, would be really
> nice to have.

In this case are we better off instead having slonik connect to the 
subscriber that the script is destined for and applying it there directly? 
For example if only_on is specified to be a subscriber then allow the event 
node to  be a subscriber as well?  You should be able to get this behavior 
today by creating an empty set on your subscriber/target where that 
subscriber is the origin to that set.   There are certain use-cases that 
this approach would disallow (Cases where slonik can't connect directly to 
the subscriber, if in the future we had a all_nodes_except_origin option to 
execute script; but I'm not sure if these use cases are realistic enough to 
be concerned about)

Once you start performing execute scripts only on some nodes then your data 
and/or schema won't be the same everywhere in the cluster and in any 
master/slave replication system if you don't do this very carefully your 
going to get burned.

Maybe we should start with trying to decide what the purpose of execute 
script is.

The main uses for execute script that I see are:

1. DDL changes
2. If you want to change a run a query that changes data without having to 
replicate the results statement by statement.  Ie something like 'INSERT 
INTO x ...SELECT FROM y'
3. You want to fire off a stored procedure on all nodes that might have side 
effects that need to executed on all nodes

Feel free to add what I've missed.


The manual says, "Executes a script containing arbitrary SQL statements on 
all nodes that are subscribed to a set at a common controlled point within 
the replication transaction stream"

It's that 'arbitrary' part that makes this very hard.

At this stage in the discussion I think we want to review the thread on 2pc 
from last June.
http://lists.slony.info/pipermail/slony1-general/2006-July/004609.html

Nothing has really changed since then. In some circumstances a global outage 
is acceptable so you can run the script on all nodes before committing to any 
of them.  In other environments this isn't always an option 
(log-shipping among others) and there wil always be 'some' risk of things 
failing on a subscriber.  The more your subscriber is different than your 
provider the more this risk goes up.

Steve


>
>
> -- 
> Andrew Sullivan                         204-4141 Yonge Street
> Afilias Canada                        Toronto, Ontario Canada
> <andrew@ca.afilias.info>                              M2P 2A8
> jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From JanWieck at Yahoo.com  Mon Mar 19 20:34:14 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Mar 19 20:34:31 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <20070319082845.GB18136@afilias.info>
References: <45FBB4B9.30501@haisuli.net>	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
	<20070319082845.GB18136@afilias.info>
Message-ID: <45FF5636.2050909@Yahoo.com>

On 3/19/2007 4:28 AM, Andrew Sullivan wrote:
> On Sat, Mar 17, 2007 at 01:06:54PM -0400, Steve Singer wrote:
>> 
>> If your DDL has errors then you won't find out about it 
>> until slon tries to execute it on the other node (slonik won't give you an 
>> error but making changes just on your subscribers can have the potential 
>> for all sorts of trouble if your not careful)
> 
> I think this is the reason it didn't work as expected -- I think
> someone raised the objection about what sort of mess this could make.
> That doesn't mean it's not useful.
> 
> This piece of functionality seems to me to be exactly the sort of area
> where using savepoints on the remote note might be a good idea, but
> I'm worried about the potential for really nasty locks.  I think in
> the medium term, we probably need a complete proposal on how to
> improve EXECUTE SCRIPT's safety, or some sort of comprehensive
> discussion about the various locks that are being taken and the like.
> I'm slightly worried that some of the design in that area is happening
> a little _ad hoc_, and I think a more complete idea of what the
> trade-offs are, and which ones we can countenance, would be really
> nice to have.

You can always roll the dice with a left or a right twist ... so 
sometimes you want the thing to halt on error, sometimes you just want 
it to skip errors. The "halt on error" implies some sort of "until" 
thing ... yet to be defined. Anyhow, this means that EXECUTE SCRIPT 
basically needs to perform the script itself in a subtransaction and 
that another option (parameter) should tell if the outer transaction 
(confirming the event) shall commit regardless or not.

This brings back the "cancel event" discussion, we had on IRC a while 
back. I don't really like "cancel event" by itself because the event 
might have been successfully executed on some nodes prior to being 
canceled, so this opens another can of worms. But I would certainly 
agree to the usefulness of some "cancel event X on node Y on error" 
feature. Not that this would be easy to implement, since it somehow 
implies that the node can see the future of its own event queue. Or, it 
would require another Slony table "sl_events_to_ignorei_on_error".


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From JanWieck at Yahoo.com  Mon Mar 19 20:41:11 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Mar 19 20:41:28 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <Pine.LNX.4.62.0703191939160.3038@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>	<20070319082845.GB18136@afilias.info>
	<Pine.LNX.4.62.0703191939160.3038@mini.atlantida.localdomain>
Message-ID: <45FF57D7.9080206@Yahoo.com>

On 3/19/2007 8:48 PM, ssinger_pg@sympatico.ca wrote:
> On Mon, 19 Mar 2007, Andrew Sullivan wrote:
> 
>> This piece of functionality seems to me to be exactly the sort of area
>> where using savepoints on the remote note might be a good idea, but
>> I'm worried about the potential for really nasty locks.  I think in
>> the medium term, we probably need a complete proposal on how to
>> improve EXECUTE SCRIPT's safety, or some sort of comprehensive
>> discussion about the various locks that are being taken and the like.
>> I'm slightly worried that some of the design in that area is happening
>> a little _ad hoc_, and I think a more complete idea of what the
>> trade-offs are, and which ones we can countenance, would be really
>> nice to have.
> 
> In this case are we better off instead having slonik connect to the 
> subscriber that the script is destined for and applying it there directly? 

The original idea to ship the DDL script inside of the event stream from 
the origin was that to ensure that the DDL is executed at the same 
logical point in time (with respect to changes to the data) on all 
subscribers, as it happened on the origin. This of course is pretty 
pointless in the case of "execute only on", because it implies that the 
time, when (or even if) these changes have been applied to the origin is 
unknown to Slony.

Yes, in the case of "execute only on" we would be better off if slonik 
would operate directly on that node and that node alone.


Jan



> For example if only_on is specified to be a subscriber then allow the event 
> node to  be a subscriber as well?  You should be able to get this behavior 
> today by creating an empty set on your subscriber/target where that 
> subscriber is the origin to that set.   There are certain use-cases that 
> this approach would disallow (Cases where slonik can't connect directly to 
> the subscriber, if in the future we had a all_nodes_except_origin option to 
> execute script; but I'm not sure if these use cases are realistic enough to 
> be concerned about)
> 
> Once you start performing execute scripts only on some nodes then your data 
> and/or schema won't be the same everywhere in the cluster and in any 
> master/slave replication system if you don't do this very carefully your 
> going to get burned.
> 
> Maybe we should start with trying to decide what the purpose of execute 
> script is.
> 
> The main uses for execute script that I see are:
> 
> 1. DDL changes
> 2. If you want to change a run a query that changes data without having to 
> replicate the results statement by statement.  Ie something like 'INSERT 
> INTO x ...SELECT FROM y'
> 3. You want to fire off a stored procedure on all nodes that might have side 
> effects that need to executed on all nodes
> 
> Feel free to add what I've missed.
> 
> 
> The manual says, "Executes a script containing arbitrary SQL statements on 
> all nodes that are subscribed to a set at a common controlled point within 
> the replication transaction stream"
> 
> It's that 'arbitrary' part that makes this very hard.
> 
> At this stage in the discussion I think we want to review the thread on 2pc 
> from last June.
> http://lists.slony.info/pipermail/slony1-general/2006-July/004609.html
> 
> Nothing has really changed since then. In some circumstances a global outage 
> is acceptable so you can run the script on all nodes before committing to any 
> of them.  In other environments this isn't always an option 
> (log-shipping among others) and there wil always be 'some' risk of things 
> failing on a subscriber.  The more your subscriber is different than your 
> provider the more this risk goes up.
> 
> Steve
> 
> 
>>
>>
>> -- 
>> Andrew Sullivan                         204-4141 Yonge Street
>> Afilias Canada                        Toronto, Ontario Canada
>> <andrew@ca.afilias.info>                              M2P 2A8
>> jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From dun at haisuli.net  Mon Mar 19 22:42:50 2007
From: dun at haisuli.net (Mikko Partio)
Date: Mon Mar 19 22:43:14 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <Pine.LNX.4.62.0703191939160.3038@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>	<20070319082845.GB18136@afilias.info>
	<Pine.LNX.4.62.0703191939160.3038@mini.atlantida.localdomain>
Message-ID: <45FF745A.3070105@haisuli.net>

ssinger_pg@sympatico.ca wrote:
>
> Once you start performing execute scripts only on some nodes then your 
> data and/or schema won't be the same everywhere in the cluster and in 
> any master/slave replication system if you don't do this very 
> carefully your going to get burned.
>
> Maybe we should start with trying to decide what the purpose of 
> execute script is.

My original purpose why I used EXECUTE ONLY ON was similar to this case 
by Jeff Frost:

http://lists.slony.info/pipermail/slony1-general/2006-November/005216.html

To summarize, I had a non-replicated table which had a foreign key to a 
replicated table on a subscriber node. When trying to drop the 
non-replicated table, I run into problems with Slony's "catalogue 
fiddling", and EXECUTE ONLY ON was the only (reasonable) way to fix 
things. Fortunately I had up-to-date backups when I discovered that the 
table was dropped at both nodes.. :)

Regards,

MP

From andrew.george.hammond at gmail.com  Tue Mar 20 10:44:06 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue Mar 20 10:44:15 2007
Subject: [Slony1-general] RFC tool to support development / operations work
	with slony replicated databases, second revision.
Message-ID: <5a0a9d6f0703201044q3ef310cax2e7c2bfd3af43cbc@mail.gmail.com>

I pasted this out of our wiki, so the formatting may be off. Comments,
as always, are appreciated.

Andrew


Current Approach

A common problem in the database world is handling revisions to the
database that go with revisions in the software running against this
database. Currently our method is to include upgrade.sql and
downgrade.sql scripts with each software release. These are
automatically applied by the packaging system.

Problem Statement

This will fail when we start using slony since we need to handle DML
differently from DDL and DCL. We also need a way to apply slonik
scripts. Ordering matters in the application of these scripts.

After talking about it for a while, we agreed that developers want a
way to apply their updates without stepping on each other's toes while
in the process of developing and testing their work.

Design

Interface

uply [-f target] [-i] [-U pguser] [-h pghost] [-p pgport] [-d
pgdatabase] [--cluster clustername]
dply [-f target] [-i] [-U pguser] [-h pghost] [-p pgport] [-d
pgdatabase] [--cluster clustername]

uply forces an upgrade application of plys. dply forces a downgrade
application of plys.

-f    Optional Defaults to the current working directory. Specifies
the target intended to be upgraded or downgraded to. This may be
either the full or relative path. This may be either a directory or a
file.

-i    Optional Interactive mode. If set, ask questions, otherwise
assume that the user "knows what she is doing".

-U -h -p -d    Optional As for psql and other PostgreSQL command line
utilities.

--cluster    Optional Defaults to the database name. Specifies the
name of the slony cluster to work with. This should have a one-letter
short form that conforms with other similar tools. Gotta figure out
what those are though...

--trust-me-i-know-what-im-doing    Optional and not recommended. When
applying DDL plys, skip the per-node BEGIN; ... ROLLBACK pre-EXECUTE
validation test.

Limitations

    * When using this to manage a replicated database, it is assumed
that the connection information presented is for the origin database.
We should probably verify this upon connection.
    * Since we're using a python connector which is based on libqp, we
will auto-magically respect the standard postgres environment
variables including the .pgpass file for handling passwords.
    * We are not trying to deal cleverly with databases with more than
one slony replication cluster in them.
    * We are not going to deal with the case where various sets have
different origins.
    * We assume that this is run off the same machine that is
currently running the slons. We can connect to every database in the
cluster.
    * Aside from generating the slonik preamble, we are not going to
try and auto-generate slonik scripts that do anything more complicated
than EXECUTE SCRIPT. At least not initially. Maybe we can get more
clever later?
    * We will not (yet) try to be clever about detecting changes to
plys. Alfred floated the idea of using the SVN id tag to detect if a
file had been changed since it was last applied and then forcing a
downgrade/upgrade cycle. That seems like a lot of code for a corner
case. Alfred and Long agreed that it's probably a good idea to create
a convention instead. Do not edit files after they're committed unless
it will cause in-efficiencies in the application to the production
database. Instead, create a new file. If you are forced to edit a
committed file, then email the dev list.
    * Along the lines of not being clever, we assume there is only one
set and that's the one we're supposed to do stuff with. If there is
more than one set floating around, if there's a set 1, we pay
attention to that. Otherwise, we throw an error?
    * We will not assume the existence of a node 1. The whole point of
increasing availability by replicating is that we don't have to rely
on the existence of a given database.

Data Structure

Each release will include a directory that has the same name as the
full release tag. Every file in this directory is a ply (we'd call it
a patch but patches already have a very clear definition that doesn't
quite match, similarly script and file are just to vague). Each
directory must contain all the plys to be applied for the version it
represents. The release may include directories of plys from prior
releases in the same parent directory. The plys may have an arbitrary
name, but must end with a suffix of either dml.sql, ddl.sql, dcl.sql
or slonik. Ply names should incorporate the bug number they're
addressing. Ply names should somewhat describe what they do.

    * /my/base/directory
          o 3.10.0
                + create_foo_23451.ddl.sql
                + populate_foo_23451.dml.sql
                + alter_bar_add_column_reference_foo_23451.ddl.sql
                + update_bar_reference_foo_23451.dml.sql
                + alter_bar_column_not_null_23451.ddl.sql
                + subscribe_foo_23451.slonik
                + cleanup_some_data_migration_stuff_23451.ddl.sql
                + fix_bug_24341.ddl.sql -- these are poorly chosen
names, but hey, it's an example...
                + fix_bug_24341.dml.sql
                + fix_bug_24341.slonik
                + drop_broken_node_30031.slonik
          o 3.10.1
                + another_table_29341.ddl.sql

Inside the script, we add some semantics to what are usually comments.
An example is probably the best way to show this.

-- alter_bar_column_not_null_23451.ddl.sql
-- Witty comment about why this column needs to be not null.
--dep update_bar_reference_foo_23451.dml.sql
ALTER TABLE bar ALTER COLUMN foo_id DROP NOT NULL;
--upgrade
ALTER TABLE bar ALTER COLUMN foo_id SET NOT NULL;

At the top of the script, before any line that isn't either a comment
or whitespace, you can have zero or more comments of the form --dep
<filename> in SQL or #dep <filename> in slonik scripts. This is how
you define which other files the current file depends on. All files
for a given version depend on all files of all previous versions.
Filenames are all characters following the space after def until the
end of the line, not including any whitespace at the tail of the line.
Don't use filenames that end with whitespace, it's annoying.

I don't see any need to get even more restrictive and disallow
whitespace in filenames. This would allow brief comments on the same
line. More involved comments will take more than just a single line
anyway. Thoughtful selection of filenames should eliminate the need
for brief comments and tab eliminates the annoyance of typing them.

The other additional semantic is the --upgrade or #upgrade tag. This
defines when the downgrade section ends and the upgrade begins.

Finally, we need to add a table in the database which lists files applied.

CREATE SCHEMA _plyers;
ALTER SCHEMA _plyers OWNER TO pgsql;

CREATE TABLE _plyers.ply
(   release text  -- reference dbinfo.version
,   name text
,   uplied_on timestamptz default now()
,   primary key (release, name)
);
ALTER TABLE _plyers.ply OWNER TO pgsql;

Algorithm

Determining the Target

The parameter passed to -f must be either a relative or absolute path
to either a ply or a directory. Obviously, it's an error to pass -f
something that doesn't exist.

A target must consist of a version tag and may include a ply. If there
is no -f parameter, then the cwd is assumed to be the parameter. If
the parameter is a directory, then the name of that directory is
inspected. If it looks like \d+\.\d+\.\d+ (ie 3.10.0 or 3.10.1 or
3.10.2) then this is the target version tag. If it doesn't match this
pattern, then look for sub-directories within this directory which do
match this pattern. If no sub-directories that match the pattern are
found, then fail. In the event of an uply, the highest (sorted by
dotted numeric, not alphabetically) found is the target version tag.
For dply, the target will be the version immediately preceding the
current version (as obtained from the database). This does not support
alpha/beta style tags since there is no reasonable way of defining the
application order.

If the -f parameter is a file then the version tag must be the name of
the directory in which that file resides.

Deciding Between Upgrade and Downgrade

If the binary is called by the uply name, we are upgrading. If the
binary is called by the dply name, we are downgrading.

The current version and file set are obtained from the database. The
current version is the largest _plyers.ply.revision (sorted by dotted
numeric, not alphabetically). If the target version is higher than our
current version, we had better be uplying. If the target is a lower
version than our current version, we had better be dplying.
agh hmmm, it looks like our schema kind of sucks. Maybe we need a
column each for major_vision, minor_vision and patch_level?

If we're upgrading and there is no filename involved then we need to
uply all the plys necessary to achieve the target version (including
all the plys for that version). If we're downgrading, then we need to
dply all the plys necessary to achieve the target version (do not
downgrade any of the patches for that version).

When file names are involved then an upgrade means to apply all the
plys necessary to achieve the version prior to the target version,
plus any files from the target version upon which the target ply
depends (recursively) followed by the target ply itself, of course.
For a downgrade the opposite effect is desired: downgrade all the
versions greater than the target version, then recursively dply any
ply which depends on the target ply followed by dplying the target
ply.

Neither upgrading nor downgrading a ply will have any effect on other
plys unless there is an explicit dependency defined between that ply
and the ply being upgraded or downgraded. Dependencies are processed
recursively.

The process of upgrading or downgrading is incremental by patch
number, then by minor version number, finally by major version number.

For example, to upgrade from an existing version to a new version, the
update tool checks to see what the current version of the database is
as well as seeing what plys have been applied. If there are more plys
to be applied for the current version, it applies them. It then looks
for the next reversion up by incrementing the patch number. If that
doesn't exist, increments the minor version and checks again. If that
doesn't exist then set the minor version to zero and increments the
major version. If that doesn't exist then we're done. Downgrades are
the same except that they decrement instead of increment.

As each upgrade file is applied, the file name is inserted into the
_plyers.ply table. Downgrades delete the file name out of the
_plyers.ply table once the downgrade has been applied.

agh hmm, we could instead have a dply_on timestamptz column that's
null by default. Then we could track both uplys and dplys over time.
Dunno if there's much real value in this though. YAGNI?

Applying Different Types of Changes

The method of application for updates varies depending on if the
database is replicated or not. It also varies depending on the nature
of the update as determined by it's suffix. A database is assumed to
be replicated if it has a _cluster schema.

dml
    This is the easiest category of changes. The update tool needs
only to connect to the origin, issue a BEGIN statement, send all the
DML statements from the ply and a COMMIT if they succeed. Otherwise it
should issue a ROLLBACK and abort the upgrade/downgrade process with
an appropriate error message.

ddl / dcl
    These changes need to be applied globally to the cluster. The
update tool must connect to all the databases in the cluster, issue a
BEGIN statement, run the DDL / DCL script and see if it completes,
finally issue a ROLLBACK. This is to verify that it can reasonably be
expected to succeed on all the nodes in the cluster. Abort with a
suitable error message if the script doesn't apply to all members in
the cluster. We might want to be able to skip the verification step by
having a --trust-me-i-know-what-im-doing parameter. Once the scripts
are verified, they are applied via slonik execute. The slonik preamble
(cluster name, and connection info for all nodes) should be drawn from
the slonik schema in the origin database.
    Long said DDL has two cases: 1. alter subscribed tables which
needs to run with "execute script"; 2. create new tables,
modify/create stored procedures which need to be executed directly on
all databases. We need to distinct those two cases with different
suffixes.
      agh: Why? While I agree that ALTERs need to be handled by
EXECUTE SCRIPT, and obviously so to TRUNCATEs. However, I don't see
any reason not to use EXECUTE SCRIPT to handle other DDL statements as
well.
      Long See http://cbbrowne.com/info/addthings.html for details. I
tried to add a new by execute script. It never adds new table to
slaves.
      agh: Yeah, the locking actually is a pretty good reason not to
do it this way when we don't have to. I guess we need to have three
categories of SQL. DML (applies only to master server), DDL (that is
applied to each server directly) and DDL (that gets applied through
slonik EXECUTE SCRIPT). That suggests to me that ddl / dml tags aren't
quite what we're looking for. Any suggestions for an alternative
convention?
      agh: I think we might have to pay some locking tax. Consider if
we create a new table that REFERENCEs a table which is already under
replication. This will create triggers on the replicated table,
however slony does some [clever stuff] with triggers. We certainly
don't want to step on slony's toes with regards to trigger management.
We could specify that REFERENCE constraints must always be added by
ALTER scripts and that CREATEs are never allowed to contain
REFERENCEs. However this still seems kinda awkward. The more I think
about it, the less happy I am with trying to make a distinction here.
Since we should only really be uplying or dplying during a maintenance
window, I think that we should just pay the lock tax. If at some time
we actually need a way to avoid locking tax, that's when we should add
that capability. YANGI.

      slonik
          These are intended to manipulate the cluster directly. For
example, creating a new set, adding some tables and sequences and then
subscribing a bunch of nodes to it. The update tool will generate the
preamble for this.

Implementation protocols/rules

   1. The main set number is 1.
   2. Temporary file name will be
/tmp/ddl-pid-timestamp-plyname(including extension). This file must be
removed after successful execution. Unless of execution fails in which
case we want to leave it around to facilitate debugging.
   3. Given that that file is not intended for concurrency control, do
we want one? Seems to me there's no reason to allow two of these
processes to run together.
      agh: Unless of course they're running against different
databases / clusters / whatever. If we're gonna have a lock file, then
it should be specific to the cluster we're working on. Since this can
be run over the network from any arbitrary client, the Right Way to
lock it might be inside the database. Do we want to bother?

   4. Temporary set (to hold new tables) number is arbitrary, but
should probably be based on the PID.
From sven at dmv.com  Wed Mar 21 07:39:10 2007
From: sven at dmv.com (Sven Willenberger)
Date: Wed Mar 21 07:24:39 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <45FBB4B9.30501@haisuli.net>
References: <45FBB4B9.30501@haisuli.net>
Message-ID: <1174487950.24477.3.camel@lanshark.dmv.com>

On Sat, 2007-03-17 at 11:28 +0200, Mikko Partio wrote:
> Hi,
> 
> slonik's EXECUTE SCRIPT -documentation says that:
> 
> EXECUTE ONLY ON = ival
> 
>     (Optional) The ID of the only node to actually execute the script.
>     This option causes the script to be propagated by all nodes but
>     executed only by one. The default is to execute the script on all
>     nodes that are subscribed to the set.
> 
> 
> In my experience this property is not working correctly, and here's the 
> proof ("tiuhti" is origin and "viuhti" subscriber):
> 
> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h tiuhti
> CREATE TABLE
> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h viuhti
> CREATE TABLE
> 
> slony1@tiuhti:~$ cat drop_table_testtable.sql
> DROP TABLE testtable;
> 
> slony1@tiuhti:~$ cat droptest.slonik
> #!/usr/bin/slonik
> 
> CLUSTER NAME=climate;
> 
> NODE 1 ADMIN CONNINFO = 'dbname=cldb host=tiuhti user=slony1';
> NODE 2 ADMIN CONNINFO = 'dbname=cldb host=viuhti user=slony1';
> 
> EXECUTE SCRIPT (
>         SET ID = 1,
>         FILENAME = '/home/slony1/drop_table_testtable.sql',
>         EVENT NODE = 1,
>         EXECUTE ONLY ON = 2
> );
> 
> slony1@tiuhti:~$ slonik droptest.slonik
> DDL script consisting of 1 SQL statements
> DDL Statement 0: (0,21) [DROP TABLE testtable;]
> Submit DDL Event to subscribers...
> DDL on origin - PGRES_TUPLES_OK
> 
> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h viuhti
> Did not find any relation named "testtable".
> 
> This is what I expected, but
> 
> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h tiuhti
> Did not find any relation named "testtable".
> 
> Wooah - the script dropped table testtable from both nodes although I 
> specified the "execute only on" -option. Is there something I'm missing 
> or is there a bug?
> 
> Regards
> 
> MP

The other issue with DDL scripts is that they do not honor SET
subscription members; certain portions of the sequence of events during
a DDL execute attempt actions on nodes not part of the affected SET.

http://lists.slony.info/pipermail/slony1-general/2007-January/005687.html

If we are examining the execute only portion of the code, perhaps we
could also check the SET parameters ...

From warren.little at meridiascapital.com  Wed Mar 21 08:19:04 2007
From: warren.little at meridiascapital.com (Warren Little)
Date: Wed Mar 21 08:19:09 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
Message-ID: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>

Hello,
I need to get data migrated from a pg8.1 cluster to a pg8.2 cluster  
and was wondering how well slony will accomplish this task.
The database size is about 250GB, so doing a dump and restore will  
take longer than the maintenance window for system downtime.
I have read several posts on the postgres mailing lists that people  
recommend using slony to do major version upgrades, so I thought to  
give it a try.
One of the concerns I have is regarding the limitation that slony  
does not support large objects, is this referring to the postgres  
LargeObject type?
ie are bytea fields of significant size replicated?

Also, is there a guide to configuring Slony-I on an active system, or  
is the procedure the same (aside for the standard "backup all data  
before you begin")

thanks

Warren Little
Chief Technology Officer
Meridias Capital Inc
ph 866.369.7763



-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070321/cc5391f7/attachment.htm
From cbbrowne at ca.afilias.info  Wed Mar 21 08:25:01 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar 21 08:25:10 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
	(Warren Little's message of "Wed, 21 Mar 2007 09:19:04 -0600")
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
Message-ID: <60wt1ak4aa.fsf@dba2.int.libertyrms.com>

Warren Little <warren.little@meridiascapital.com> writes:
> I need to get data migrated from a pg8.1 cluster to a pg8.2 cluster and was wondering how well slony will
> accomplish this task.
>
> The database size is about 250GB, so doing a dump and restore will take longer than the maintenance window for
> system downtime.
>
> I have read several posts on the postgres mailing lists that people recommend using slony to do major version
> upgrades, so I thought to give it a try.
>
> One of the concerns I have is regarding the limitation that slony does not support large objects, is this
> referring to the postgres LargeObject type?
>
> ie are bytea fields of significant size replicated?

Your interpretation is correct:

 - The nonsupported thing is the LOB type; it isn't replicable because
   triggers do not fire when LOBs are modified

 - BYTEA columns are replicated fine, and people have replicated
   Rather Large chunks of data in BYTEA columns.

   We, for instance, have been replicating our RT/3 database using
   Slony-I; on occasion users attach painfully large documents to
   tickets.  We've had documents (and hence tuples with BYTEA columns)
   of >50MB in size, and that has worked.

> Also, is there a guide to configuring Slony-I on an active system,
> or is the procedure the same (aside for the standard "backup all
> data before you begin")?

Yeah, the procedure is pretty well the usual...
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From shoaibmir at gmail.com  Wed Mar 21 08:25:25 2007
From: shoaibmir at gmail.com (Shoaib Mir)
Date: Wed Mar 21 08:25:30 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
Message-ID: <bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>

In order to do the same you can use pg_migrator (
http://pgfoundry.org/projects/pg-migrator/) as well. It will take a LOT less
time then normal backup/restore.

--
Shoaib Mir
EnterpriseDB (www.enterprisedb.com)

On 3/21/07, Warren Little <warren.little@meridiascapital.com> wrote:
>
> Hello,
> I need to get data migrated from a pg8.1 cluster to a pg8.2 cluster and
> was wondering how well slony will accomplish this task.
> The database size is about 250GB, so doing a dump and restore will take
> longer than the maintenance window for system downtime.
> I have read several posts on the postgres mailing lists that people
> recommend using slony to do major version upgrades, so I thought to give =
it
> a try.
> One of the concerns I have is regarding the limitation that slony does not
> support large objects, is this referring to the postgres LargeObject type?
> ie are bytea fields of significant size replicated?
>
> Also, is there a guide to configuring Slony-I on an active system, or is
> the procedure the same (aside for the standard "backup all data before you
> begin")
>
> thanks
>
> Warren Little
> Chief Technology Officer
> Meridias Capital Inc
> ph 866.369.7763
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070321/=
50c6d1e4/attachment.htm
From andreas at kostyrka.org  Wed Mar 21 08:30:46 2007
From: andreas at kostyrka.org (Andreas Kostyrka)
Date: Wed Mar 21 08:29:35 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
Message-ID: <20070321153043.GB4625@andi-lap.la.revver.com>

* Warren Little <warren.little@meridiascapital.com> [070321 16:23]:
>    Hello,
>    I need to get data migrated from a pg8.1 cluster to a pg8.2 cluster and
>    was wondering how well slony will accomplish this task.
>    The database size is about 250GB, so doing a dump and restore will take
>    longer than the maintenance window for system downtime.
>    I have read several posts on the postgres mailing lists that people
>    recommend using slony to do major version upgrades, so I thought to give
>    it a try.

It works in practice well. Furthermore you should consider it, or
something else to have a hotstandby backup of your database anyway.
Another IMHO important benefit is that such a cluster, even if unused
by the application, can be used to do pg_dump style backups without
putting disc IO pressure onto the master server.

>    One of the concerns I have is regarding the limitation that slony does not
>    support large objects, is this referring to the postgres LargeObject type?
This refers to the pseudo file large objects, not to anything stored
directly in tables.
>    ie are bytea fields of significant size replicated?
yes.
>    Also, is there a guide to configuring Slony-I on an active system, or is
>    the procedure the same (aside for the standard "backup all data before you
>    begin")
Well, if it's a mission critical system, you should consider hiring a
consultant to walk you through in detail or do it himself. It's not
exactly a lengthy procedure, so it shouldn't cost you to much.

OTOH, the procedure is basically as documented. Personally, I've found
that the documentation somehow manages to be correct and complete, and
yet it makes one sweet the first times one does some operation ;)

Andreas
From warren.little at meridiascapital.com  Wed Mar 21 09:36:34 2007
From: warren.little at meridiascapital.com (Warren Little)
Date: Wed Mar 21 09:36:49 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
	<bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>
Message-ID: <33F44DEE-3284-4E52-BBA2-9E2D810895FB@meridiascapital.com>

Part I left off is I will also be moving the cluster to another  
server, so I need something that replicates over the network over time.
Nice to know such a tool is being developed  for future upgrades.

thx

On Mar 21, 2007, at 9:25 AM, Shoaib Mir wrote:

> In order to do the same you can use pg_migrator (http:// 
> pgfoundry.org/projects/pg-migrator/) as well. It will take a LOT  
> less time then normal backup/restore.
>
> --
> Shoaib Mir
> EnterpriseDB (www.enterprisedb.com)
>
> On 3/21/07, Warren Little < warren.little@meridiascapital.com> wrote:
> Hello,
> I need to get data migrated from a pg8.1 cluster to a pg8.2 cluster  
> and was wondering how well slony will accomplish this task.
> The database size is about 250GB, so doing a dump and restore will  
> take longer than the maintenance window for system downtime.
> I have read several posts on the postgres mailing lists that people  
> recommend using slony to do major version upgrades, so I thought to  
> give it a try.
> One of the concerns I have is regarding the limitation that slony  
> does not support large objects, is this referring to the postgres  
> LargeObject type?
> ie are bytea fields of significant size replicated?
>
> Also, is there a guide to configuring Slony-I on an active system,  
> or is the procedure the same (aside for the standard "backup all  
> data before you begin")
>
> thanks
>
> Warren Little
> Chief Technology Officer
> Meridias Capital Inc
> ph 866.369.7763
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>

Warren Little
Chief Technology Officer
Meridias Capital Inc
ph 866.369.7763



-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070321/c546d71a/attachment-0001.htm
From htomeh at firstam.com  Wed Mar 21 10:15:43 2007
From: htomeh at firstam.com (Tomeh, Husam)
Date: Wed Mar 21 10:16:03 2007
Subject: [Slony1-general] using slony to migrate large database from
	pg8.1 to pg8.2
In-Reply-To: <bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
	<bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>
Message-ID: <F1B0F9305B343E43A1C3EECE48B853D595B039@CITGSNA01SXCH02.ana.firstamdata.com>

Good to know that such tool is available now. Hopefully it'll be
integrated to the core distribution of postgres. 
Is the tool capable of migrating earlier version such as 7.4.5 and 8.0
to the latest ?
 
Thanks,
 
--
 Husam

________________________________

From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Shoaib Mir
Sent: Wednesday, March 21, 2007 8:25 AM
To: Warren Little
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] using slony to migrate large database from
pg8.1 to pg8.2


In order to do the same you can use pg_migrator
(http://pgfoundry.org/projects/pg-migrator/) as well. It will take a LOT
less time then normal backup/restore. 

--
Shoaib Mir
EnterpriseDB (www.enterprisedb.com)


On 3/21/07, Warren Little < warren.little@meridiascapital.com
<mailto:warren.little@meridiascapital.com> > wrote: 

	Hello,
	I need to get data migrated from a pg8.1 cluster to a pg8.2
cluster and was wondering how well slony will accomplish this task.
	The database size is about 250GB, so doing a dump and restore
will take longer than the maintenance window for system downtime. 
	I have read several posts on the postgres mailing lists that
people recommend using slony to do major version upgrades, so I thought
to give it a try.
	One of the concerns I have is regarding the limitation that
slony does not support large objects, is this referring to the postgres
LargeObject type? 
	ie are bytea fields of significant size replicated?

	Also, is there a guide to configuring Slony-I on an active
system, or is the procedure the same (aside for the standard "backup all
data before you begin")  

	thanks
	
	
	
	Warren Little
	Chief Technology Officer
	Meridias Capital Inc
	ph 866.369.7763




	_______________________________________________
	Slony1-general mailing list
	Slony1-general@lists.slony.info
	http://lists.slony.info/mailman/listinfo/slony1-general
	
	



**********************************************************************
This message contains confidential information intended only for the use of the addressee(s) named above and may contain information that is legally privileged.  If you are not the addressee, or the person responsible for delivering it to the addressee, you are hereby notified that reading, disseminating, distributing or copying this message is strictly prohibited.  If you have received this message by mistake, please immediately notify us by replying to the message and delete the original message immediately thereafter.

Thank you.

                                   FADLD Tag
**********************************************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070321/f9be9ee9/attachment.htm
From decibel at decibel.org  Wed Mar 21 14:00:43 2007
From: decibel at decibel.org (Jim C. Nasby)
Date: Wed Mar 21 14:01:01 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <F1B0F9305B343E43A1C3EECE48B853D595B039@CITGSNA01SXCH02.ana.firstamdata.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
	<bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>
	<F1B0F9305B343E43A1C3EECE48B853D595B039@CITGSNA01SXCH02.ana.firstamdata.com>
Message-ID: <20070321210043.GC12826@decibel.org>

AFAIK the tool only handles 8.1-8.2 right now. It also won't
automatically handle any tables that would need the physical files
re-written; in 8.1-8.2 that means any tables that have any internet
address data types (inet, etc).

On Wed, Mar 21, 2007 at 10:15:43AM -0700, Tomeh, Husam wrote:
> Good to know that such tool is available now. Hopefully it'll be
> integrated to the core distribution of postgres. 
> Is the tool capable of migrating earlier version such as 7.4.5 and 8.0
> to the latest ?
>  
> Thanks,
>  
> --
>  Husam
> 
> ________________________________
> 
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Shoaib Mir
> Sent: Wednesday, March 21, 2007 8:25 AM
> To: Warren Little
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] using slony to migrate large database from
> pg8.1 to pg8.2
> 
> 
> In order to do the same you can use pg_migrator
> (http://pgfoundry.org/projects/pg-migrator/) as well. It will take a LOT
> less time then normal backup/restore. 
> 
> --
> Shoaib Mir
> EnterpriseDB (www.enterprisedb.com)
> 
> 
> On 3/21/07, Warren Little < warren.little@meridiascapital.com
> <mailto:warren.little@meridiascapital.com> > wrote: 
> 
> 	Hello,
> 	I need to get data migrated from a pg8.1 cluster to a pg8.2
> cluster and was wondering how well slony will accomplish this task.
> 	The database size is about 250GB, so doing a dump and restore
> will take longer than the maintenance window for system downtime. 
> 	I have read several posts on the postgres mailing lists that
> people recommend using slony to do major version upgrades, so I thought
> to give it a try.
> 	One of the concerns I have is regarding the limitation that
> slony does not support large objects, is this referring to the postgres
> LargeObject type? 
> 	ie are bytea fields of significant size replicated?
> 
> 	Also, is there a guide to configuring Slony-I on an active
> system, or is the procedure the same (aside for the standard "backup all
> data before you begin")  
> 
> 	thanks
> 	
> 	
> 	
> 	Warren Little
> 	Chief Technology Officer
> 	Meridias Capital Inc
> 	ph 866.369.7763
> 
> 
> 
> 
> 	_______________________________________________
> 	Slony1-general mailing list
> 	Slony1-general@lists.slony.info
> 	http://lists.slony.info/mailman/listinfo/slony1-general
> 	
> 	
> 
> 
> 
> **********************************************************************
> This message contains confidential information intended only for the use of the addressee(s) named above and may contain information that is legally privileged.  If you are not the addressee, or the person responsible for delivering it to the addressee, you are hereby notified that reading, disseminating, distributing or copying this message is strictly prohibited.  If you have received this message by mistake, please immediately notify us by replying to the message and delete the original message immediately thereafter.
> 
> Thank you.
> 
>                                    FADLD Tag
> **********************************************************************

> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Jim C. Nasby, Database Architect                decibel@decibel.org 
Give your computer some brain candy! www.distributed.net Team #1828

Windows: "Where do you want to go today?"
Linux: "Where do you want to go tomorrow?"
FreeBSD: "Are you guys coming, or what?"
From andrew.george.hammond at gmail.com  Wed Mar 21 14:59:06 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Wed Mar 21 14:59:16 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
Message-ID: <5a0a9d6f0703211459p1685ca97xc060b8cb6277fa0@mail.gmail.com>

On 3/21/07, Warren Little <warren.little@meridiascapital.com> wrote:

> Also, is there a guide to configuring Slony-I on an active system, or is the
> procedure the same (aside for the standard "backup all data before you
> begin")

You may want to schedule a brief maintenance window when initially
adding tables since it requires an exclusive lock. Which is part of
the standard procedures, however I thought I'd mention it specifically
to make sure you don't miss it.

Andrew
From ajs at crankycanuck.ca  Wed Mar 21 17:49:53 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Mar 21 17:50:16 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <F1B0F9305B343E43A1C3EECE48B853D595B039@CITGSNA01SXCH02.ana.firstamdata.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
	<bf54be870703210825x1b51f410p60c3252c57c13f55@mail.gmail.com>
	<F1B0F9305B343E43A1C3EECE48B853D595B039@CITGSNA01SXCH02.ana.firstamdata.com>
Message-ID: <20070322004953.GA29677@phlogiston.dyndns.org>

On Wed, Mar 21, 2007 at 10:15:43AM -0700, Tomeh, Husam wrote:

> Hopefully it'll be integrated to the core distribution of postgres.

_Please_, not this chestnut again.  What is this "core distribution
of Postgres" that people keep talking about?  This is a PostgreSQL
community project, supported with PostgreSQL community resources
(pgfoundry).  It is an important tool, but there is no reason
whatever to "integrate" it with the "core distribution".

Nobody thinks vi should be part of the kernel, just because vi is on
every usable UNIX in the world (pick some other "basic" program if
you like.  Perhaps mkdir?).  It's a separate program.  It happens
to be shipped together in the same (perhaps virtual) box, but that
doesn't make it any less of a part of the working system.  

A



-- 
Andrew Sullivan  | ajs@crankycanuck.ca
When my information changes, I alter my conclusions.  What do you do sir?
		--attr. John Maynard Keynes
From htomeh at firstam.com  Wed Mar 21 18:34:53 2007
From: htomeh at firstam.com (Tomeh, Husam)
Date: Wed Mar 21 18:35:26 2007
Subject: [Slony1-general] using slony to migrate large database from
	pg8.1 to pg8.2
Message-ID: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>

 I strongly disagree with you. Any decent RDBMS such as Postgres should and I think will have eventually a decent upgrade and migration tool rather than using the conventional time-consuming dump and reload. What if your db is in terabyte size like many enterprise-level ones nowadays!  I think this an essential tool for enterprise postgres implementations. 
   
-- 
       Husam

-----Original Message-----
From: "Andrew Sullivan" <ajs@crankycanuck.ca>
To: "slony1-general@lists.slony.info" <slony1-general@lists.slony.info>
Sent: 21-Mar-07 5:50 PM
Subject: Re: [Slony1-general] using slony to migrate large database from pg8.1 to pg8.2

On Wed, Mar 21, 2007 at 10:15:43AM -0700, Tomeh, Husam wrote:

> Hopefully it'll be integrated to the core distribution of postgres.

_Please_, not this chestnut again.  What is this "core distribution
of Postgres" that people keep talking about?  This is a PostgreSQL
community project, supported with PostgreSQL community resources
(pgfoundry).  It is an important tool, but there is no reason
whatever to "integrate" it with the "core distribution".

Nobody thinks vi should be part of the kernel, just because vi is on
every usable UNIX in the world (pick some other "basic" program if
you like.  Perhaps mkdir?).  It's a separate program.  It happens
to be shipped together in the same (perhaps virtual) box, but that
doesn't make it any less of a part of the working system.  

A



-- 
Andrew Sullivan  | ajs@crankycanuck.ca
When my information changes, I alter my conclusions.  What do you do sir?
		--attr. John Maynard Keynes
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general
**********************************************************************
This message contains confidential information intended only for the use of the addressee(s) named above and may contain information that is legally privileged.  If you are not the addressee, or the person responsible for delivering it to the addressee, you are hereby notified that reading, disseminating, distributing or copying this message is strictly prohibited.  If you have received this message by mistake, please immediately notify us by replying to the message and delete the original message immediately thereafter.

Thank you.

                                   FADLD Tag
**********************************************************************

From ajs at crankycanuck.ca  Wed Mar 21 18:40:36 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Mar 21 18:40:50 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
References: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
Message-ID: <20070322014036.GD29677@phlogiston.dyndns.org>

On Wed, Mar 21, 2007 at 06:34:53PM -0700, Tomeh, Husam wrote:

>  I strongly disagree with you. Any decent RDBMS such as Postgres
>  should and I think will have eventually a decent upgrade and
>  migration tool rather than using the conventional time-consuming
>  dump and reload. What if your db is in terabyte size like many
>  enterprise-level ones nowadays!  I think this an essential tool
>  for enterprise postgres implementations.

This has exactly nothing to do with where the source code for the
tool is available.  Every decent UNIX has a vi editor.  The source
repositories are nevertheless unlikely to be the same.

The difference in this area between commercial tools (which have to
deliver you a complete package that appears to you to be one thing,
even if the components are actually developed by people who report to
different bosses) and free software like PostgreSQL is that in free
software projects, you get to see the plumbing of how those tools fit
together.  Things are not more or less supported because of which
source repository they live.  They're more or less supported on the
basis of community involvement.

A


-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The whole tendency of modern prose is away from concreteness.
		--George Orwell
From cbbrowne at ca.afilias.info  Wed Mar 21 18:44:14 2007
From: cbbrowne at ca.afilias.info (cbbrowne@ca.afilias.info)
Date: Wed Mar 21 18:44:25 2007
Subject: [Slony1-general] using slony to migrate large database from 
	pg8.1 to pg8.2
In-Reply-To: <5a0a9d6f0703211459p1685ca97xc060b8cb6277fa0@mail.gmail.com>
References: <8B2CE960-6B88-4874-BF07-74BA13228330@meridiascapital.com>
	<5a0a9d6f0703211459p1685ca97xc060b8cb6277fa0@mail.gmail.com>
Message-ID: <61518.64.229.230.128.1174527854.squirrel@look.libertyrms.info>

> On 3/21/07, Warren Little <warren.little@meridiascapital.com> wrote:
>
>> Also, is there a guide to configuring Slony-I on an active system, or is
>> the
>> procedure the same (aside for the standard "backup all data before you
>> begin")
>
> You may want to schedule a brief maintenance window when initially
> adding tables since it requires an exclusive lock. Which is part of
> the standard procedures, however I thought I'd mention it specifically
> to make sure you don't miss it.

There's a difference between the locking for this and other locking; the
SET ADD TABLE action requires a brief exclusive lock to attach the
replication trigger to each table.

Unlike behaviour on subscribers, where there winds up being one grand
transaction requiring exclusive locks on ALL the replicated tables, all at
once, the actions initially on the master are (hopefully) brief, and go
individually, table by table.

If you have processes on the master database that hold lots of locks, this
can bite you.  If you don't have heavy locking going on there, SET ADD
TABLE may be able to slip through fairly trouble-free alongside the other
traffic.

We've frequently been able to add new tables (not yet used by the
application) on the fly while application users are connected.  Tables
that are thus far unused will be pretty easy on locking.  It's less clear
what'll happen with busy tables that are widely used.

In such a case, it's entirely possible for there to be a few seconds of
pause as the request for the exclusive lock blocks everything that comes
in behind it, but still has to wait until all the requests that came in
before it clear their way through.  Whether that's milliseconds or seconds
or more depends on the usage pattern...

From cbbrowne at ca.afilias.info  Wed Mar 21 19:22:28 2007
From: cbbrowne at ca.afilias.info (cbbrowne@ca.afilias.info)
Date: Wed Mar 21 19:22:39 2007
Subject: [Slony1-general] using slony to migrate large database from 
	pg8.1 to pg8.2
In-Reply-To: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
References: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
Message-ID: <61222.64.229.230.128.1174530148.squirrel@look.libertyrms.info>

>  I strongly disagree with you. Any decent RDBMS such as Postgres should
> and I think will have eventually a decent upgrade and migration tool
> rather than using the conventional time-consuming dump and reload. What
> if your db is in terabyte size like many enterprise-level ones nowadays!
> I think this an essential tool for enterprise postgres implementations.

I agree in part...

Yes, I think there should be a more direct migration mechanism, in the
long term.  I understand some people at Sun may be starting to work on
this.

What you have today, here, is Slony-I; that's the present workaround.

Bashing the options that you *do* have when they're not as nice as what we
might like to someday have is in poor taste particularly when talking with
the people that invested in creating what *is* available.

If these things are, as you say, "essential," a natural next question is:
What are you doing to contribute to making them available?  Wishing
migration tools into existence doesn't happen; it requires a lot of work.

If they're so essential, in your view, then can we assume you'll be
contributing time and/or money to building upgrade capabilities into
PostgreSQL 8.4?  Now's the time to start the effort...

From ssinger_pg at sympatico.ca  Wed Mar 21 20:34:02 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Wed Mar 21 20:34:16 2007
Subject: [Slony1-general] Execute script and "execute only on"
In-Reply-To: <1174487950.24477.3.camel@lanshark.dmv.com>
References: <45FBB4B9.30501@haisuli.net>
	<1174487950.24477.3.camel@lanshark.dmv.com>
Message-ID: <Pine.LNX.4.62.0703212309510.3038@mini.atlantida.localdomain>

On Wed, 21 Mar 2007, Sven Willenberger wrote:

> The other issue with DDL scripts is that they do not honor SET
> subscription members; certain portions of the sequence of events during
> a DDL execute attempt actions on nodes not part of the affected SET.
>
> http://lists.slony.info/pipermail/slony1-general/2007-January/005687.html
>
> If we are examining the execute only portion of the code, perhaps we
> could also check the SET parameters ...
>

Glancing through the code it looks like ddlscript_int had this filtering in 
1.1 branch so that it didn't execute the script on a node that wasn't 
subscribed to the set.

In 1.2 the script gets parsed into individual statements and parsed by 
remote_worker which doesn't seem to consider if the node is a subscriber to 
the set or not.  Was this just an oversight or did someone have a reason for 
requiring the execute script to run on non-subscriber nodes?

If we can't think of a reason for only executing the script on nodes that 
subscribe to the set, then I will work on a replacement patch to get slonik 
to connect directly to the target when using only_on_node in the next few 
days.


Steve

> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From htomeh at firstam.com  Wed Mar 21 20:44:28 2007
From: htomeh at firstam.com (Tomeh, Husam)
Date: Wed Mar 21 20:45:13 2007
Subject: [Slony1-general] using slony to migrate large database from
	pg8.1 to pg8.2
Message-ID: <181201c76c34$72ea2be8$3a5811ac@ana.firstamdata.com>

I was not bashing anyone. Bringing suggestions or ideas is a starter. Please don't take thbigs so personally.

   -- 
       Husam

-----Original Message-----
From: "cbbrowne@ca.afilias.info" <cbbrowne@ca.afilias.info>
To: "Tomeh, Husam" <htomeh@firstam.com>
Cc: "slony1-general@lists.slony.info" <slony1-general@lists.slony.info>
Sent: 21-Mar-07 7:22 PM
Subject: RE: [Slony1-general] using slony to migrate large database from pg8.1 to pg8.2

>  I strongly disagree with you. Any decent RDBMS such as Postgres should
> and I think will have eventually a decent upgrade and migration tool
> rather than using the conventional time-consuming dump and reload. What
> if your db is in terabyte size like many enterprise-level ones nowadays!
> I think this an essential tool for enterprise postgres implementations.

I agree in part...

Yes, I think there should be a more direct migration mechanism, in the
long term.  I understand some people at Sun may be starting to work on
this.

What you have today, here, is Slony-I; that's the present workaround.

Bashing the options that you *do* have when they're not as nice as what we
might like to someday have is in poor taste particularly when talking with
the people that invested in creating what *is* available.

If these things are, as you say, "essential," a natural next question is:
What are you doing to contribute to making them available?  Wishing
migration tools into existence doesn't happen; it requires a lot of work.

If they're so essential, in your view, then can we assume you'll be
contributing time and/or money to building upgrade capabilities into
PostgreSQL 8.4?  Now's the time to start the effort...
**********************************************************************
This message contains confidential information intended only for the use of the addressee(s) named above and may contain information that is legally privileged.  If you are not the addressee, or the person responsible for delivering it to the addressee, you are hereby notified that reading, disseminating, distributing or copying this message is strictly prohibited.  If you have received this message by mistake, please immediately notify us by replying to the message and delete the original message immediately thereafter.

Thank you.

                                   FADLD Tag
**********************************************************************

From andreas at kostyrka.org  Thu Mar 22 01:36:30 2007
From: andreas at kostyrka.org (Andreas Kostyrka)
Date: Thu Mar 22 01:35:25 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
References: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
Message-ID: <20070322083630.GB4439@andi-lap.la.revver.com>

* Tomeh, Husam <htomeh@firstam.com> [070322 09:07]:
>  I strongly disagree with you. Any decent RDBMS such as Postgres should and I think will have eventually a decent upgrade and migration tool rather than using the conventional time-consuming dump and reload. What if your db is in terabyte size like many enterprise-level ones nowadays!  I think this an essential tool for enterprise postgres implementations.
I strongly disagree with you too :)

Well, most production setups should have some form of replication,
because hardware can fail.

Basically, you need to deal with the hardware failure case anyway, or
you can accept the dump&reload waiting time anyway (because that's
what you would be doing after a hardware failure). Now some nice
replication packages like slony allow for easy upgrading.
Plus as an addon, you get an exercise in switchover handling, that you
can plan ahead.

Andreas

>    
> -- 
>        Husam
> 
> -----Original Message-----
> From: "Andrew Sullivan" <ajs@crankycanuck.ca>
> To: "slony1-general@lists.slony.info" <slony1-general@lists.slony.info>
> Sent: 21-Mar-07 5:50 PM
> Subject: Re: [Slony1-general] using slony to migrate large database from pg8.1 to pg8.2
> 
> On Wed, Mar 21, 2007 at 10:15:43AM -0700, Tomeh, Husam wrote:
> 
> > Hopefully it'll be integrated to the core distribution of postgres.
> 
> _Please_, not this chestnut again.  What is this "core distribution
> of Postgres" that people keep talking about?  This is a PostgreSQL
> community project, supported with PostgreSQL community resources
> (pgfoundry).  It is an important tool, but there is no reason
> whatever to "integrate" it with the "core distribution".
> 
> Nobody thinks vi should be part of the kernel, just because vi is on
> every usable UNIX in the world (pick some other "basic" program if
> you like.  Perhaps mkdir?).  It's a separate program.  It happens
> to be shipped together in the same (perhaps virtual) box, but that
> doesn't make it any less of a part of the working system.  
> 
> A
> 
> 
> 
> -- 
> Andrew Sullivan  | ajs@crankycanuck.ca
> When my information changes, I alter my conclusions.  What do you do sir?
> 		--attr. John Maynard Keynes
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> **********************************************************************
> This message contains confidential information intended only for the use of the addressee(s) named above and may contain information that is legally privileged.  If you are not the addressee, or the person responsible for delivering it to the addressee, you are hereby notified that reading, disseminating, distributing or copying this message is strictly prohibited.  If you have received this message by mistake, please immediately notify us by replying to the message and delete the original message immediately thereafter.
> 
> Thank you.
> 
>                                    FADLD Tag
> **********************************************************************
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
From cbbrowne at ca.afilias.info  Thu Mar 22 09:04:57 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 22 09:05:04 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06DF40F5@blackex02.detica.com>
	(Victoria Parsons's message of "Thu, 22 Mar 2007 15:03:34 -0000")
References: <E0A702B010CA5F499BAAD4C2A2FF739F06DF40F5@blackex02.detica.com>
Message-ID: <604podjmc6.fsf@dba2.int.libertyrms.com>

"Victoria Parsons" <victoria.parsons@streamshield.com> writes:
> Has anyone else found that they need a vacuum full to keep their
> replication, and/or general postgres use up to speed? Have I
> mis-understood the use of vacuum full, and does it do more than just
> recover disk space? Next time it goes wrong I will do vacuum full on
> a table at a time to see if I can narrow down the culprit.

We *never* run a VACUUM FULL on a whole database; just occasionally on
some very carefully selected table.

Definitely you should narrow it down, and run VACUUM FULL ANALYZE on
each table to see where you get a huge reclaiming of space.  By all
means, report back on your findings, that may help us help you, and
help others, as well...
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From decibel at decibel.org  Thu Mar 22 09:09:18 2007
From: decibel at decibel.org (Jim C. Nasby)
Date: Thu Mar 22 09:09:31 2007
Subject: [Slony1-general] using slony to migrate large database from pg8.1
	to pg8.2
In-Reply-To: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
References: <17f001c76c22$52b44134$3a5811ac@ana.firstamdata.com>
Message-ID: <20070322160917.GB12826@decibel.org>

On Wed, Mar 21, 2007 at 06:34:53PM -0700, Tomeh, Husam wrote:
>  I strongly disagree with you. Any decent RDBMS such as Postgres should and I think will have eventually a decent upgrade and migration tool rather than using the conventional time-consuming dump and reload. What if your db is in terabyte size like many enterprise-level ones nowadays!  I think this an essential tool for enterprise postgres implementations. 

http://pgfoundry.org/projects/pg-migrator/

And yes, that should probably be in core eventually.
-- 
Jim C. Nasby, Database Architect                decibel@decibel.org 
Give your computer some brain candy! www.distributed.net Team #1828

Windows: "Where do you want to go today?"
Linux: "Where do you want to go tomorrow?"
FreeBSD: "Are you guys coming, or what?"
From victoria.parsons at streamshield.com  Thu Mar 22 09:48:07 2007
From: victoria.parsons at streamshield.com (Victoria Parsons)
Date: Thu Mar 22 09:48:24 2007
Subject: [Slony1-general] Vacuum full required?
Message-ID: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4535@blackex02.detica.com>


The problem is on pg_listener table. I found the same problem on my test
3 node set up.

I printed the table contents, then did a vacuum full. I then waited 5
minutes, print the table contents and do a vacuum full again.

The table details have not changed, all the same slony pids. We have no
listens on public tables on this particular database so its easy to see
what is going on. To the very best of my knowledge there have been no
inserts and deletes to this table since the last print, 5 minutes ago.

I paste the vacuum results below. I've never read the vacuum verbose
reports before so any guidance on what to look for would be appreciated.
The lines that appear to be of interest are

INFO:  "pg_listener": found 1036 removable, 12 nonremovable row versions
in 16 pages
INFO:  "pg_listener": moved 5 row versions, truncated 16 to 1 pages

I can wait 5 minutes and repeat and another 16-18 pages of data will
again be truncated into 1 page. Where is al this extra data coming from.
I don't want to jump slony but it seems much worse on the 11 node
system, which is where the problem first came to light.

Vicki


sss=# vacuum full verbose pg_listener;
INFO:  vacuuming "pg_catalog.pg_listener"
INFO:  "pg_listener": found 1036 removable, 12 nonremovable row versions
in 16 pages
DETAIL:  0 dead row versions cannot be removed yet.
Nonremovable row versions range from 96 to 96 bytes long.
There were 0 unused item pointers.
Total free space (including removable row versions) is 125408 bytes.
10 pages are or will become empty, including 0 at the end of the table.
16 pages containing 125408 free bytes are potential move destinations.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
INFO:  "pg_listener": moved 5 row versions, truncated 16 to 1 pages
DETAIL:  CPU 0.00s/0.00u sec elapsed 0.07 sec.
VACUUM




-----Original Message-----
From: Christopher Browne [mailto:cbbrowne@ca.afilias.info] 
Sent: 22 March 2007 16:05
To: Victoria Parsons
Cc: slony
Subject: Re: [Slony1-general] Vacuum full required?

"Victoria Parsons" <victoria.parsons@streamshield.com> writes:
> Has anyone else found that they need a vacuum full to keep their
> replication, and/or general postgres use up to speed? Have I
> mis-understood the use of vacuum full, and does it do more than just
> recover disk space? Next time it goes wrong I will do vacuum full on
> a table at a time to see if I can narrow down the culprit.

We *never* run a VACUUM FULL on a whole database; just occasionally on
some very carefully selected table.

Definitely you should narrow it down, and run VACUUM FULL ANALYZE on
each table to see where you get a huge reclaiming of space.  By all
means, report back on your findings, that may help us help you, and
help others, as well...
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)


This message should be regarded as confidential. If you have received this 
email in error please notify the sender and destroy it immediately.
Statements of intent shall only become binding when confirmed in hard copy 
by an authorized signatory.

From jks at selectacast.net  Thu Mar 22 10:14:26 2007
From: jks at selectacast.net (Joseph Shraibman)
Date: Thu Mar 22 10:14:44 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4535@blackex02.detica.com>
References: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4535@blackex02.detica.com>
Message-ID: <4602B972.50601@selectacast.net>

You need to run select pg_relation_size('tablename') to see how much 
space vacuum full is reclaiming.

Also remember that vacuum full does not reclaim space in the indexes. 
If you don't REINDEX you might end up with indexes that are larger than 
the table itself.

Victoria Parsons wrote:
> The problem is on pg_listener table. I found the same problem on my test
> 3 node set up.
> 
> I printed the table contents, then did a vacuum full. I then waited 5
> minutes, print the table contents and do a vacuum full again.
> 
> The table details have not changed, all the same slony pids. We have no
> listens on public tables on this particular database so its easy to see
> what is going on. To the very best of my knowledge there have been no
> inserts and deletes to this table since the last print, 5 minutes ago.
> 
> I paste the vacuum results below. I've never read the vacuum verbose
> reports before so any guidance on what to look for would be appreciated.
> The lines that appear to be of interest are
> 
> INFO:  "pg_listener": found 1036 removable, 12 nonremovable row versions
> in 16 pages
> INFO:  "pg_listener": moved 5 row versions, truncated 16 to 1 pages
> 
> I can wait 5 minutes and repeat and another 16-18 pages of data will
> again be truncated into 1 page. Where is al this extra data coming from.
> I don't want to jump slony but it seems much worse on the 11 node
> system, which is where the problem first came to light.
> 
> Vicki
> 
> 
> sss=# vacuum full verbose pg_listener;
> INFO:  vacuuming "pg_catalog.pg_listener"
> INFO:  "pg_listener": found 1036 removable, 12 nonremovable row versions
> in 16 pages
> DETAIL:  0 dead row versions cannot be removed yet.
> Nonremovable row versions range from 96 to 96 bytes long.
> There were 0 unused item pointers.
> Total free space (including removable row versions) is 125408 bytes.
> 10 pages are or will become empty, including 0 at the end of the table.
> 16 pages containing 125408 free bytes are potential move destinations.
> CPU 0.00s/0.00u sec elapsed 0.00 sec.
> INFO:  "pg_listener": moved 5 row versions, truncated 16 to 1 pages
> DETAIL:  CPU 0.00s/0.00u sec elapsed 0.07 sec.
> VACUUM
> 
> 
> 
> 
> -----Original Message-----
> From: Christopher Browne [mailto:cbbrowne@ca.afilias.info] 
> Sent: 22 March 2007 16:05
> To: Victoria Parsons
> Cc: slony
> Subject: Re: [Slony1-general] Vacuum full required?
> 
> "Victoria Parsons" <victoria.parsons@streamshield.com> writes:
>> Has anyone else found that they need a vacuum full to keep their
>> replication, and/or general postgres use up to speed? Have I
>> mis-understood the use of vacuum full, and does it do more than just
>> recover disk space? Next time it goes wrong I will do vacuum full on
>> a table at a time to see if I can narrow down the culprit.
> 
> We *never* run a VACUUM FULL on a whole database; just occasionally on
> some very carefully selected table.
> 
> Definitely you should narrow it down, and run VACUUM FULL ANALYZE on
> each table to see where you get a huge reclaiming of space.  By all
> means, report back on your findings, that may help us help you, and
> help others, as well...
From victoria.parsons at streamshield.com  Thu Mar 22 10:36:13 2007
From: victoria.parsons at streamshield.com (Victoria Parsons)
Date: Thu Mar 22 10:36:23 2007
Subject: [Slony1-general] Vacuum full required?
Message-ID: <E0A702B010CA5F499BAAD4C2A2FF739F06DF471F@blackex02.detica.com>


pg_listener does not have any indexes, so no re-indexing to be done.

I cannot find the function you use pg_relation_size, but I can see the
rel_pages in pg_class stays at 1 for 'pg_listener' even though every
vacuum claims to be reducing the pages from some large number to 1. Is
the vacuum output lying to me? The utput below says it is reducing pages
from 97 to 1 but the data in pg_class shows it was only ever 1 page of
data.




sss=# select pg_relation_size('pg_listener');
ERROR:  function pg_relation_size("unknown") does not exist
HINT:  No function matches the given name and argument types. You may
need to add explicit type casts.

sss=# select * from pg_class where relname='pg_listener';
   relname   | relnamespace | reltype | relowner | relam | relfilenode |
relpages | reltuples | reltoastrelid | reltoastidxid | relhasindex |
relisshared | relkind | relnatts | relchecks | reltriggers | relukeys |
relfkeys | relrefs | relhasoids | relhaspkey | relhasrules |
relhassubclass |    relacl
-------------+--------------+---------+----------+-------+-------------+
----------+-----------+---------------+---------------+-------------+---
----------+---------+----------+-----------+-------------+----------+---
-------+---------+------------+------------+-------------+--------------
--+---------------
 pg_listener |           11 |   16415 |        1 |     0 |       16414 |
1 |        26 |             0 |             0 | f           | f
| r       |        3 |         0 |           0 |        0 |        0 |
0 | f          | f          | t           | f              |
{=r/postgres}
(1 row)

sss=# select * from pg_listener order by listenerpid;
         relname         | listenerpid | notification
-------------------------+-------------+--------------
 _sss_repcluster_Node_3  |        5637 |            0
 _sss_repcluster_Confirm |        5637 |            0
 _sss_repcluster_Event   |        5637 |            0
 _sss_repcluster_Node_3  |        5644 |            0
 _sss_repcluster_Node_2  |        7730 |            0
 _sss_repcluster_Confirm |        7730 |            0
 _sss_repcluster_Event   |        7730 |            0
 _sss_repcluster_Node_2  |        7732 |            0
 _sss_repcluster_Restart |        7866 |            0
 _sss_repcluster_Event   |        7866 |            0
 _sss_repcluster_Node_1  |        7873 |            0
 _sss_repcluster_Node_1  |        7874 |            0
(12 rows)

sss=# vacuum full verbose pg_listener;
INFO:  vacuuming "pg_catalog.pg_listener"
INFO:  "pg_listener": found 7630 removable, 12 nonremovable row versions
in 97 pages
DETAIL:  0 dead row versions cannot be removed yet.
Nonremovable row versions range from 96 to 96 bytes long.
There were 0 unused item pointers.
Total free space (including removable row versions) is 760964 bytes.
91 pages are or will become empty, including 0 at the end of the table.
97 pages containing 760964 free bytes are potential move destinations.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
INFO:  "pg_listener": moved 5 row versions, truncated 97 to 1 pages
DETAIL:  CPU 0.00s/0.00u sec elapsed 0.16 sec.
VACUUM

sss=# select * from pg_class where relname='pg_listener';
   relname   | relnamespace | reltype | relowner | relam | relfilenode |
relpages | reltuples | reltoastrelid | reltoastidxid | relhasindex |
relisshared | relkind | relnatts | relchecks | reltriggers | relukeys |
relfkeys | relrefs | relhasoids | relhaspkey | relhasrules |
relhassubclass |    relacl
-------------+--------------+---------+----------+-------+-------------+
----------+-----------+---------------+---------------+-------------+---
----------+---------+----------+-----------+-------------+----------+---
-------+---------+------------+------------+-------------+--------------
--+---------------
 pg_listener |           11 |   16415 |        1 |     0 |       16414 |
1 |        12 |             0 |             0 | f           | f
| r       |        3 |         0 |           0 |        0 |        0 |
0 | f          | f          | t           | f              |
{=r/postgres}
(1 row)





-----Original Message-----
From: Joseph Shraibman [mailto:jks@selectacast.net] 
Sent: 22 March 2007 17:14
To: Victoria Parsons
Cc: slony
Subject: Re: [Slony1-general] Vacuum full required?

You need to run select pg_relation_size('tablename') to see how much 
space vacuum full is reclaiming.

Also remember that vacuum full does not reclaim space in the indexes. 
If you don't REINDEX you might end up with indexes that are larger than 
the table itself.

Victoria Parsons wrote:
> The problem is on pg_listener table. I found the same problem on my
test
> 3 node set up.
> 
> I printed the table contents, then did a vacuum full. I then waited 5
> minutes, print the table contents and do a vacuum full again.
> 
> The table details have not changed, all the same slony pids. We have
no
> listens on public tables on this particular database so its easy to
see
> what is going on. To the very best of my knowledge there have been no
> inserts and deletes to this table since the last print, 5 minutes ago.
> 
> I paste the vacuum results below. I've never read the vacuum verbose
> reports before so any guidance on what to look for would be
appreciated.
> The lines that appear to be of interest are
> 
> INFO:  "pg_listener": found 1036 removable, 12 nonremovable row
versions
> in 16 pages
> INFO:  "pg_listener": moved 5 row versions, truncated 16 to 1 pages
> 
> I can wait 5 minutes and repeat and another 16-18 pages of data will
> again be truncated into 1 page. Where is al this extra data coming
from.
> I don't want to jump slony but it seems much worse on the 11 node
> system, which is where the problem first came to light.
> 
> Vicki
> 
> 
> sss=# vacuum full verbose pg_listener;
> INFO:  vacuuming "pg_catalog.pg_listener"
> INFO:  "pg_listener": found 1036 removable, 12 nonremovable row
versions
> in 16 pages
> DETAIL:  0 dead row versions cannot be removed yet.
> Nonremovable row versions range from 96 to 96 bytes long.
> There were 0 unused item pointers.
> Total free space (including removable row versions) is 125408 bytes.
> 10 pages are or will become empty, including 0 at the end of the
table.
> 16 pages containing 125408 free bytes are potential move destinations.
> CPU 0.00s/0.00u sec elapsed 0.00 sec.
> INFO:  "pg_listener": moved 5 row versions, truncated 16 to 1 pages
> DETAIL:  CPU 0.00s/0.00u sec elapsed 0.07 sec.
> VACUUM
> 
> 
> 
> 
> -----Original Message-----
> From: Christopher Browne [mailto:cbbrowne@ca.afilias.info] 
> Sent: 22 March 2007 16:05
> To: Victoria Parsons
> Cc: slony
> Subject: Re: [Slony1-general] Vacuum full required?
> 
> "Victoria Parsons" <victoria.parsons@streamshield.com> writes:
>> Has anyone else found that they need a vacuum full to keep their
>> replication, and/or general postgres use up to speed? Have I
>> mis-understood the use of vacuum full, and does it do more than just
>> recover disk space? Next time it goes wrong I will do vacuum full on
>> a table at a time to see if I can narrow down the culprit.
> 
> We *never* run a VACUUM FULL on a whole database; just occasionally on
> some very carefully selected table.
> 
> Definitely you should narrow it down, and run VACUUM FULL ANALYZE on
> each table to see where you get a huge reclaiming of space.  By all
> means, report back on your findings, that may help us help you, and
> help others, as well...


This message should be regarded as confidential. If you have received this 
email in error please notify the sender and destroy it immediately.
Statements of intent shall only become binding when confirmed in hard copy 
by an authorized signatory.

From cbbrowne at ca.afilias.info  Thu Mar 22 12:28:03 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 22 12:28:16 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <4602B972.50601@selectacast.net> (Joseph Shraibman's message of
	"Thu, 22 Mar 2007 13:14:26 -0400")
References: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4535@blackex02.detica.com>
	<4602B972.50601@selectacast.net>
Message-ID: <60r6rhhyd8.fsf@dba2.int.libertyrms.com>

Joseph Shraibman <jks@selectacast.net> writes:
> You need to run select pg_relation_size('tablename') to see how much
> space vacuum full is reclaiming.
>
> Also remember that vacuum full does not reclaim space in the
> indexes. If you don't REINDEX you might end up with indexes that are
> larger than the table itself.

That's actually not a problem in this case; pg_listener doesn't have
any indices, so there's nothing to bloat.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Thu Mar 22 12:31:02 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 22 12:31:08 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06DF471F@blackex02.detica.com>
	(Victoria Parsons's message of "Thu, 22 Mar 2007 17:36:13 -0000")
References: <E0A702B010CA5F499BAAD4C2A2FF739F06DF471F@blackex02.detica.com>
Message-ID: <60mz25hy89.fsf@dba2.int.libertyrms.com>

"Victoria Parsons" <victoria.parsons@streamshield.com> writes:
> pg_listener does not have any indexes, so no re-indexing to be done.

Right; that's good news.

> I cannot find the function you use pg_relation_size, but I can see the
> rel_pages in pg_class stays at 1 for 'pg_listener' even though every
> vacuum claims to be reducing the pages from some large number to 1. Is
> the vacuum output lying to me? The utput below says it is reducing pages
> from 97 to 1 but the data in pg_class shows it was only ever 1 page of
> data.
> sss=# select * from pg_listener order by listenerpid;
>          relname         | listenerpid | notification
> -------------------------+-------------+--------------
>  _sss_repcluster_Node_3  |        5637 |            0
>  _sss_repcluster_Confirm |        5637 |            0
>  _sss_repcluster_Event   |        5637 |            0
>  _sss_repcluster_Node_3  |        5644 |            0
>  _sss_repcluster_Node_2  |        7730 |            0
>  _sss_repcluster_Confirm |        7730 |            0
>  _sss_repcluster_Event   |        7730 |            0
>  _sss_repcluster_Node_2  |        7732 |            0
>  _sss_repcluster_Restart |        7866 |            0
>  _sss_repcluster_Event   |        7866 |            0
>  _sss_repcluster_Node_1  |        7873 |            0
>  _sss_repcluster_Node_1  |        7874 |            0
> (12 rows)

Unfortunately, it's common that whenever an event goes out to indicate
(for instance) a new SYNC, all of the tuples associated with the node
get updated.  The more nodes there are, the more "listening" that
happens.

In Slony-I 1.2, there is an attempt to generate less dead tuples in
pg_listener; things should improve there.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From jks at selectacast.net  Thu Mar 22 12:44:35 2007
From: jks at selectacast.net (Joseph S)
Date: Thu Mar 22 12:44:50 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06DF471F@blackex02.detica.com>
References: <E0A702B010CA5F499BAAD4C2A2FF739F06DF471F@blackex02.detica.com>
Message-ID: <4602DCA3.4040704@selectacast.net>

Victoria Parsons wrote:
> pg_listener does not have any indexes, so no re-indexing to be done.
> 
> I cannot find the function you use pg_relation_size, but I can see the

What version of pg are you using?  In older versions it was in the 
contrib directory.  See 
http://www.postgresql.org/docs/8.2/static/functions-admin.html
From victoria.parsons at streamshield.com  Fri Mar 23 03:29:59 2007
From: victoria.parsons at streamshield.com (Victoria Parsons)
Date: Fri Mar 23 03:30:24 2007
Subject: [Slony1-general] Vacuum full required?
Message-ID: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4DD7@blackex02.detica.com>


Thanks for your help. Knowing where the problem lies and that there is
nothing we can do about it before a slony upgrade has at least stopped
our need for investigation into other causes.

Because we are not ready to upgrade slony at the moment we will solve
this with a vacuum full of pg_listener for each database once an hour.
That seems to be keeping our replication under control.

Thanks for you help, I would not have known what was causing the table
bloat without you. I am surprised this has not got anyone else running
with a large number of nodes, hopefully this thread will help anyone
suffering in the future.

Cheers,
Vicki



-----Original Message-----
From: Christopher Browne [mailto:cbbrowne@ca.afilias.info] 
Sent: 22 March 2007 19:31
To: Victoria Parsons
Cc: Joseph Shraibman; slony
Subject: Re: [Slony1-general] Vacuum full required?

"Victoria Parsons" <victoria.parsons@streamshield.com> writes:
> pg_listener does not have any indexes, so no re-indexing to be done.

Right; that's good news.

> I cannot find the function you use pg_relation_size, but I can see the
> rel_pages in pg_class stays at 1 for 'pg_listener' even though every
> vacuum claims to be reducing the pages from some large number to 1. Is
> the vacuum output lying to me? The utput below says it is reducing
pages
> from 97 to 1 but the data in pg_class shows it was only ever 1 page of
> data.
> sss=# select * from pg_listener order by listenerpid;
>          relname         | listenerpid | notification
> -------------------------+-------------+--------------
>  _sss_repcluster_Node_3  |        5637 |            0
>  _sss_repcluster_Confirm |        5637 |            0
>  _sss_repcluster_Event   |        5637 |            0
>  _sss_repcluster_Node_3  |        5644 |            0
>  _sss_repcluster_Node_2  |        7730 |            0
>  _sss_repcluster_Confirm |        7730 |            0
>  _sss_repcluster_Event   |        7730 |            0
>  _sss_repcluster_Node_2  |        7732 |            0
>  _sss_repcluster_Restart |        7866 |            0
>  _sss_repcluster_Event   |        7866 |            0
>  _sss_repcluster_Node_1  |        7873 |            0
>  _sss_repcluster_Node_1  |        7874 |            0
> (12 rows)

Unfortunately, it's common that whenever an event goes out to indicate
(for instance) a new SYNC, all of the tuples associated with the node
get updated.  The more nodes there are, the more "listening" that
happens.

In Slony-I 1.2, there is an attempt to generate less dead tuples in
pg_listener; things should improve there.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)


This message should be regarded as confidential. If you have received this 
email in error please notify the sender and destroy it immediately.
Statements of intent shall only become binding when confirmed in hard copy 
by an authorized signatory.

From wmoran at collaborativefusion.com  Fri Mar 23 08:05:26 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Fri Mar 23 08:05:33 2007
Subject: [Slony1-general] Unexpected problems with Slony config
Message-ID: <20070323110526.b7487afa.wmoran@collaborativefusion.com>


Hit two things recently that were pretty non-POLA from my perspective.
I haven't seen these documented anywhere ...

First, it appears that when installing Slony, you _must_ have a node 1.
We were planning to use "serial #" style node IDs that conveyed some
meaning about what the nodes do (actually, node names would be even
better, but that's a wish).  When trying to create a new cluster without
defining a node 1, Slony gives an error that it couldn't find conninfo
for node 1.  It would appear as if a dependency on node 1 is hardcoded
somewhere.

The second thing that surprised me is an odd limit on the size of the node
ID.  As I already mentioned, we were trying to use node IDs that contain
some intelligence (serial # style), so our node IDs were around 20000000.
I didn't expect that to be a problem, since it's certainly small enough
to fit in an int, but slonik gave a rather cryptic error:
<stdin>:974: PGRES_FATAL_ERROR select "_clustername".initializeLocalNode(20000000, 'Local Slave node'); select "_clustername".enableNode_int(20000000);  - ERROR:  bigint out of range
CONTEXT:  SQL statement "SELECT  setval('"_clustername".sl_rowid_seq',  $1 ::int8 * '1000000000000000'::int8)"
PL/pgSQL function "initializelocalnode" line 26 at perform

Which makes it appear as if the node ID is being multiplied by one
quadrillion before attempting to stuff it into a BIGINT.  when I set
the node ID down to single-digits, the error stopped.

-- 
Bill Moran
Collaborative Fusion Inc.
From lavalamp at spiritual-machines.org  Fri Mar 23 09:28:21 2007
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Fri Mar 23 09:28:39 2007
Subject: [Slony1-general] Unexpected problems with Slony config
In-Reply-To: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
Message-ID: <20070323121026.A82515@arbitor.digitalfreaks.org>

> some intelligence (serial # style), so our node IDs were around 
> 20000000.

Looks like a unique ID collision avoidance technique.  Presumably you 
can't really put lot of restraints on Slony tables themselves and 
guarantee synchronicity (did i really just say that?)

# select * from _cores_minnesota_2.sl_rowid_seq  ;
  sequence_name |    last_value    | increment_by |      max_value      | 
min_value | cache_value | log_cnt | is_cycled | is_called

---------------+------------------+--------------+---------------------+---

sl_rowid_seq  | 2000000000000000 |            1 | 9223372036854775807 | 
1 |           1 |       0 | f         | t

(1 row)


Well, an 8 digit number going by your scheme at max: 99999999 * 
1000000000000000 is well > max(int8) which is 9223372036854775808

$ echo 1000000000000000 | wc -c
       17

Where I work, that value seems completely arbitrary (Chris?) so feel free 
to change it.  Also, where I'm at, I use a similar scheme for mapping 
POSIX UIDs into Windows NT UUIDs in Samba/LDAP.

The assumption here is very small Node IDs, which is bollocks if you're 
running more than one slon(8) instance on a machine your logs will fill 
with many unreadable messages from different slon(8) PIDs that do not have 
unique node IDs.

~BAS

---------

$ more src/backend/slony1_funcs.sql
[...snip[
create or replace function @NAMESPACE@.initializeLocalNode (int4, text)
returns int4
as '
declare
         p_local_node_id         alias for $1;
         p_comment                       alias for $2;
         v_old_node_id           int4;
         v_first_log_no          int4;
         v_event_seq                     int8;
begin
         -- ----
         -- Grab the central configuration lock
         -- ----
         lock table @NAMESPACE@.sl_config_lock;

         -- ----
         -- Make sure this node is uninitialized or got reset
         -- ----
         select last_value::int4 into v_old_node_id from 
@NAMESPACE@.sl_local_node_id;
         if v_old_node_id != -1 then
                 raise exception ''Slony-I: This node is already 
initialized'';
         end if;

         -- ----
         -- Set sl_local_node_id to the requested value and add our
         -- own system to sl_node.
         -- ----
         perform setval(''@NAMESPACE@.sl_local_node_id'', p_local_node_id);
         perform setval(''@NAMESPACE@.sl_rowid_seq'',
                         p_local_node_id::int8 * 
''1000000000000000''::int8);
         perform @NAMESPACE@.storeNode_int (p_local_node_id, p_comment, 
false);

         return p_local_node_id;
end;
' language plpgsql;

comment on function @NAMESPACE@.initializeLocalNode (int4, text) is
   'no_id - Node ID #
no_comment - Human-oriented comment


> I didn't expect that to be a problem, since it's certainly small enough
> to fit in an int, but slonik gave a rather cryptic error:
> <stdin>:974: PGRES_FATAL_ERROR select "_clustername".initializeLocalNode(20000000, 'Local Slave node'); select "_clustername".enableNode_int(20000000);  - ERROR:  bigint out of range
> CONTEXT:  SQL statement "SELECT  setval('"_clustername".sl_rowid_seq',  $1 ::int8 * '1000000000000000'::int8)"
> PL/pgSQL function "initializelocalnode" line 26 at perform
>
> Which makes it appear as if the node ID is being multiplied by one
> quadrillion before attempting to stuff it into a BIGINT.  when I set
> the node ID down to single-digits, the error stopped.
>
> -- 
> Bill Moran
> Collaborative Fusion Inc.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

l8*
 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
 	       http://www.spiritual-machines.org/

"...from back in the heady days when "helpdesk" meant nothing, "diskquota"
meant everything, and lives could be bought and sold for a couple of pages
of laser printout - and frequently were."
From ssinger_pg at sympatico.ca  Sat Mar 24 15:31:51 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Sat Mar 24 15:32:06 2007
Subject: [Slony1-patches] Re: [Slony1-general] Execute script and "execute
	only on"
In-Reply-To: <Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
References: <45FBB4B9.30501@haisuli.net>
	<Pine.LNX.4.62.0703170838470.3246@mini.atlantida.localdomain>
	<Pine.LNX.4.62.0703171232030.3246@mini.atlantida.localdomain>
Message-ID: <Pine.LNX.4.62.0703241820090.2846@mini.atlantida.localdomain>

On Sat, 17 Mar 2007, Steve Singer wrote:

The attached patch replaces the one I sent last week.  This version has the 
only_on execute script executed directly against the subscriber node by 
slonik as discussed on slony1-general.

Also per the discussion on general this patch also includes a fix so that 
subscribers that do not subscribe to the set a script is submitted against 
do not receive the script (As was the behaviour in the 1.1 series)

These patches are all against the 1.2 branch.


Steve

> On Sat, 17 Mar 2007, Steve Singer wrote:
>
> The attached patch against 1.2 should fix this bug by not
> executing your DDL on the event node.
>
> If your DDL has errors then you won't find out about it until slon tries to 
> execute it on the other node (slonik won't give you an error but making 
> changes just on your subscribers can have the potential for all sorts of 
> trouble if your not careful)
>
>
>
>> On Sat, 17 Mar 2007, Mikko Partio wrote:
>> 
>> Your getting this because your EVENT_NODE is 1 but you only want to execute 
>> the script on 2.
>> 
>> Slonik probably should have a check to see if you have specified an only 
>> excecute different than your event node and just submit the script into the 
>> queue at that stage.
>> 
>> Another option is to require the event node be equal to the only execute 
>> node.
>> 
>> 
>> 
>>> Hi,
>>> 
>>> slonik's EXECUTE SCRIPT -documentation says that:
>>> 
>>> EXECUTE ONLY ON = ival
>>> 
>>>   (Optional) The ID of the only node to actually execute the script.
>>>   This option causes the script to be propagated by all nodes but
>>>   executed only by one. The default is to execute the script on all
>>>   nodes that are subscribed to the set.
>>> 
>>> 
>>> In my experience this property is not working correctly, and here's the 
>>> proof ("tiuhti" is origin and "viuhti" subscriber):
>>> 
>>> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h 
>>> tiuhti
>>> CREATE TABLE
>>> slony1@tiuhti:~$ psql -d cldb -c "CREATE TABLE testtable (id int)" -h 
>>> viuhti
>>> CREATE TABLE
>>> 
>>> slony1@tiuhti:~$ cat drop_table_testtable.sql
>>> DROP TABLE testtable;
>>> 
>>> slony1@tiuhti:~$ cat droptest.slonik
>>> #!/usr/bin/slonik
>>> 
>>> CLUSTER NAME=climate;
>>> 
>>> NODE 1 ADMIN CONNINFO = 'dbname=cldb host=tiuhti user=slony1';
>>> NODE 2 ADMIN CONNINFO = 'dbname=cldb host=viuhti user=slony1';
>>> 
>>> EXECUTE SCRIPT (
>>>       SET ID = 1,
>>>       FILENAME = '/home/slony1/drop_table_testtable.sql',
>>>       EVENT NODE = 1,
>>>       EXECUTE ONLY ON = 2
>>> );
>>> 
>>> slony1@tiuhti:~$ slonik droptest.slonik
>>> DDL script consisting of 1 SQL statements
>>> DDL Statement 0: (0,21) [DROP TABLE testtable;]
>>> Submit DDL Event to subscribers...
>>> DDL on origin - PGRES_TUPLES_OK
>>> 
>>> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h viuhti
>>> Did not find any relation named "testtable".
>>> 
>>> This is what I expected, but
>>> 
>>> slony1@tiuhti:~$ psql -d cldb -c "\d testtable" -h tiuhti
>>> Did not find any relation named "testtable".
>>> 
>>> Wooah - the script dropped table testtable from both nodes although I 
>>> specified the "execute only on" -option. Is there something I'm missing or 
>>> is there a bug?
>>> 
>>> Regards
>>> 
>>> MP
>>> 
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general@lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>> 
>
-------------- next part --------------
Index: src/backend/slony1_funcs.sql
===================================================================
RCS file: /slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.98.2.13
diff -c -r1.98.2.13 slony1_funcs.sql
*** src/backend/slony1_funcs.sql	22 Mar 2007 20:41:27 -0000	1.98.2.13
--- src/backend/slony1_funcs.sql	24 Mar 2007 22:24:19 -0000
***************
*** 3683,3690 ****
--- 3683,3693 ----
  	-- ----
  	lock table @NAMESPACE@.sl_config_lock;
  
+ 	
  	-- ----
  	-- Check that the set exists and originates here
+ 	-- unless only_on_node was specified (then it can be applied to
+ 	-- that node because that is what the user wanted)
  	-- ----
  	select set_origin into v_set_origin
  			from @NAMESPACE@.sl_set
***************
*** 3693,3699 ****
  	if not found then
  		raise exception ''Slony-I: set % not found'', p_set_id;
  	end if;
! 	if v_set_origin <> @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@'') then
  		raise exception ''Slony-I: set % does not originate on local node'',
  				p_set_id;
  	end if;
--- 3696,3703 ----
  	if not found then
  		raise exception ''Slony-I: set % not found'', p_set_id;
  	end if;
! 	if v_set_origin <> @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@'') AND
! 	p_only_on_node=-1 then
  		raise exception ''Slony-I: set % does not originate on local node'',
  				p_set_id;
  	end if;
***************
*** 5904,5907 ****
  to specify fields for the passed-in tab_id.  
  
  In PG versions > 7.3, this looks like (field1,field2,...fieldn)';
- 
--- 5908,5910 ----
Index: src/slon/remote_worker.c
===================================================================
RCS file: /slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.124.2.12
diff -c -r1.124.2.12 remote_worker.c
*** src/slon/remote_worker.c	6 Mar 2007 18:47:45 -0000	1.124.2.12
--- src/slon/remote_worker.c	24 Mar 2007 22:24:22 -0000
***************
*** 268,273 ****
--- 268,276 ----
  
  static void compress_actionseq(const char *ssy_actionseq, SlonDString * action_subquery);
  
+ static int process_ddl_script(SlonWorkMsg_event * event,SlonNode * node,
+ 							  PGconn * local_dbconn, char * seqbuf );
+ static int check_set_subscriber(int set_id, int node_id,PGconn * local_dbconn);
  
  /* ----------
   * slon_remoteWorkerThread
***************
*** 1339,1439 ****
  			}
  			else if (strcmp(event->ev_type, "DDL_SCRIPT") == 0)
  			{
! 				int			ddl_setid = (int)strtol(event->ev_data1, NULL, 10);
! 				char	   *ddl_script = event->ev_data2;
! 				int			ddl_only_on_node = (int)strtol(event->ev_data3, NULL, 10);
! 				int num_statements = -1, stmtno;
! 
! 				PGresult *res;
! 				ExecStatusType rstat;
! 
! 
! 				slon_appendquery(&query1,
! 						 "select %s.ddlScript_prepare_int(%d, %d); ",
! 						 rtcfg_namespace,
! 						 ddl_setid, ddl_only_on_node);
! 
! 				if (query_execute(node, local_dbconn, &query1) < 0) {
! 						slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL preparation failed - set %d - only on node %\n",
! 							 node->no_id, ddl_setid, ddl_only_on_node);
! 						slon_retry();
! 				}
! 
! 				num_statements = scan_for_statements (ddl_script);
! 				slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL request with %d statements\n",
! 					 node->no_id, num_statements);
! 				if ((num_statements < 0) || (num_statements >= MAXSTATEMENTS)) {
! 					slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL had invalid number of statements - %d\n", 
! 						 node->no_id, num_statements);
! 					slon_retry();
! 				}
! 				
! 				for (stmtno=0; stmtno < num_statements;  stmtno++) {
! 					int startpos, endpos;
! 					char *dest;
! 					if (stmtno == 0)
! 						startpos = 0;
! 					else
! 						startpos = STMTS[stmtno-1];
! 
! 					endpos = STMTS[stmtno];
! 					dest = (char *) malloc (endpos - startpos + 1);
! 					if (dest == 0) {
! 						slon_log(SLON_ERROR, "remoteWorkerThread_%d: malloc() failure in DDL_SCRIPT - could not allocate %d bytes of memory\n", 
! 							 node->no_id, endpos - startpos + 1);
! 						slon_retry();
! 					}
! 					strncpy(dest, ddl_script + startpos, endpos-startpos);
! 					dest[STMTS[stmtno]-startpos] = 0;
! 					slon_mkquery(&query1, dest);
! 					slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL Statement %d: [%s]\n", 
! 						 node->no_id, stmtno, dest);						 
! 					free(dest);
! 
! 					res = PQexec(local_dbconn, dstring_data(&query1));
! 
! 					if (PQresultStatus(res) != PGRES_COMMAND_OK && 
! 					    PQresultStatus(res) != PGRES_TUPLES_OK &&
! 					    PQresultStatus(res) != PGRES_EMPTY_QUERY)
! 					{
! 						rstat = PQresultStatus(res);
! 						slon_log(SLON_ERROR, "DDL Statement failed - %s\n", PQresStatus(rstat));
! 						dstring_free(&query1);
! 						slon_retry();
! 					}
! 					rstat = PQresultStatus(res);
! 					slon_log (SLON_CONFIG, "DDL success - %s\n", PQresStatus(rstat));
! 				}
! 	
! 				slon_mkquery(&query1, "select %s.ddlScript_complete_int(%d, %d); ", 
! 					     rtcfg_namespace,
! 					     ddl_setid,
! 					     ddl_only_on_node);
! 
! 				/* DDL_SCRIPT needs to be turned into a log shipping script */
! 				/* Note that the issue about parsing that mandates breaking 
! 				   up compound statements into
! 				   individually-processed statements does not apply to log
! 				   shipping as psql parses and processes each statement
! 				   individually */
! 
! 				if (archive_dir)
! 				{
! 					if ((ddl_only_on_node < 1) || (ddl_only_on_node == rtcfg_nodeid))
! 					{
! 
! 						if (archive_open(node, seqbuf) < 0)
! 							slon_retry();
! 						if (archive_tracking(node, rtcfg_namespace, 
! 								ddl_setid, seqbuf, seqbuf, 
! 								event->ev_timestamp_c) < 0)
! 							slon_retry();
! 						if (archive_append_str(node, ddl_script) < 0)
! 							slon_retry();
! 						if (archive_close(node) < 0)
! 							slon_retry();
! 					}
! 				}
  			}
  			else if (strcmp(event->ev_type, "RESET_CONFIG") == 0)
  			{
--- 1342,1348 ----
  			}
  			else if (strcmp(event->ev_type, "DDL_SCRIPT") == 0)
  			{
! 				process_ddl_script(event,node,local_dbconn,seqbuf);
  			}
  			else if (strcmp(event->ev_type, "RESET_CONFIG") == 0)
  			{
***************
*** 6097,6099 ****
--- 6006,6175 ----
  	}
  	slon_log(SLON_DEBUG4, " compressed actionseq subquery... %s\n", dstring_data(action_subquery));
  }
+ 
+ 
+ /**
+  *
+  * Process a ddl_script command.
+  */
+ static int process_ddl_script(SlonWorkMsg_event * event,SlonNode * node,
+ 							  PGconn * local_dbconn,
+ 							  char * seqbuf) 
+ {
+ 	int			ddl_setid = (int)strtol(event->ev_data1, NULL, 10);
+ 	char	   *ddl_script = event->ev_data2;
+ 	int			ddl_only_on_node = (int)strtol(event->ev_data3, NULL, 10);
+ 	int num_statements = -1, stmtno;
+ 	int node_in_set;
+ 	int localNodeId;
+ 	PGresult *res;
+ 	ExecStatusType rstat;
+ 	SlonDString query1;
+ 
+ 	
+ 
+ 	dstring_init(&query1);
+ 	/**
+ 	 * Check to make sure this node is part of the set
+ 	 */
+ 	slon_log(SLON_INFO, "Checking local node id");
+ 	localNodeId = db_getLocalNodeId(local_dbconn);
+ 	slon_log(SLON_INFO,"Found local node id");
+ 	node_in_set = check_set_subscriber(ddl_setid,localNodeId,local_dbconn);
+ 	
+ 	if(!node_in_set) {
+ 		/**
+ 		 *
+ 		 * Node is not part of the set.  
+ 		 * Do not forward teh DDL to the node,
+ 		 * nor should it be included in the log for log-shipping.
+ 		 */
+ 		slon_log(SLON_INFO,"Not forwarding DDL to node %d for set %d\n",
+ 				 node->no_id,ddl_setid);
+ 		
+ 	}
+ 	else 
+ 	{
+ 		slon_appendquery(&query1,
+ 						 "select %s.ddlScript_prepare_int(%d, %d); ",
+ 						 rtcfg_namespace,
+ 						 ddl_setid, ddl_only_on_node);
+ 		
+ 		if (query_execute(node, local_dbconn, &query1) < 0) {
+ 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL preparation failed - set %d - only on node %\n",
+ 					 node->no_id, ddl_setid, ddl_only_on_node);			
+ 			slon_retry();
+ 		}
+ 		
+ 		num_statements = scan_for_statements (ddl_script);
+ 		slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL request with %d statements\n",
+ 				 node->no_id, num_statements);
+ 		if ((num_statements < 0) || (num_statements >= MAXSTATEMENTS)) {
+ 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL had invalid number of statements - %d\n", 
+ 					 node->no_id, num_statements);
+ 			slon_retry();
+ 		}
+ 		
+ 		for (stmtno=0; stmtno < num_statements;  stmtno++) {
+ 			int startpos, endpos;
+ 			char *dest;
+ 			if (stmtno == 0)
+ 				startpos = 0;
+ 			else
+ 				startpos = STMTS[stmtno-1];
+ 			
+ 			endpos = STMTS[stmtno];
+ 			dest = (char *) malloc (endpos - startpos + 1);
+ 			if (dest == 0) {
+ 				slon_log(SLON_ERROR, "remoteWorkerThread_%d: malloc() failure in DDL_SCRIPT - could not allocate %d bytes of memory\n", 
+ 						 node->no_id, endpos - startpos + 1);
+ 				slon_retry();
+ 			}
+ 			strncpy(dest, ddl_script + startpos, endpos-startpos);
+ 			dest[STMTS[stmtno]-startpos] = 0;
+ 			slon_mkquery(&query1, dest);
+ 			slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL Statement %d: [%s]\n", 
+ 					 node->no_id, stmtno, dest);						 
+ 			free(dest);
+ 			
+ 			res = PQexec(local_dbconn, dstring_data(&query1));
+ 			
+ 			if (PQresultStatus(res) != PGRES_COMMAND_OK && 
+ 				PQresultStatus(res) != PGRES_TUPLES_OK &&
+ 				PQresultStatus(res) != PGRES_EMPTY_QUERY)
+ 				{
+ 					rstat = PQresultStatus(res);
+ 					slon_log(SLON_ERROR, "DDL Statement failed - %s\n", PQresStatus(rstat));
+ 					dstring_free(&query1);
+ 					slon_retry();
+ 				}
+ 			rstat = PQresultStatus(res);
+ 			slon_log (SLON_CONFIG, "DDL success - %s\n", PQresStatus(rstat));
+ 		}
+ 		
+ 		slon_mkquery(&query1, "select %s.ddlScript_complete_int(%d, %d); ", 
+ 					 rtcfg_namespace,
+ 					 ddl_setid,
+ 					 ddl_only_on_node);
+ 		
+ 		/* DDL_SCRIPT needs to be turned into a log shipping script */
+ 		/* Note that the issue about parsing that mandates breaking 
+ 		   up compound statements into
+ 		   individually-processed statements does not apply to log
+ 		   shipping as psql parses and processes each statement
+ 		   individually */
+ 		
+ 		if (archive_dir)
+ 			{
+ 				if ((ddl_only_on_node < 1) || (ddl_only_on_node == rtcfg_nodeid))
+ 					{
+ 						
+ 						if (archive_open(node, seqbuf) < 0)
+ 							slon_retry();
+ 						if (archive_tracking(node, rtcfg_namespace, 
+ 											 ddl_setid, seqbuf, seqbuf, 
+ 											 event->ev_timestamp_c) < 0)
+ 							slon_retry();
+ 						if (archive_append_str(node, ddl_script) < 0)
+ 							slon_retry();
+ 						if (archive_close(node) < 0)
+ 							slon_retry();
+ 					}
+ 			}
+ 	}/*else node a subscriber */
+ 	
+ 	dstring_free(&query1);
+ 
+ }
+ 
+ /**
+  * Checks to see if the node specified is a member of the set.
+  *
+  */
+ static int check_set_subscriber(int set_id, int node_id,PGconn * local_dbconn) 
+ {
+   
+   
+   SlonDString query1;
+   PGresult* res;
+   dstring_init(&query1);
+ 
+   slon_appendquery(&query1,"select 1 from %s.sl_subscribe WHERE sub_set=%d AND sub_receiver=%d for update"
+ 	       ,rtcfg_namespace,set_id,node_id);
+   res = PQexec(local_dbconn,dstring_data(&query1));
+   if(PQresultStatus(res)!=PGRES_TUPLES_OK) {
+     slon_log(SLON_ERROR,"remoteWorkerThread_%d: DDL preperation can not check set membership"
+ 	     ,node_id);
+ 	dstring_free(&query1);
+     slon_retry();
+   }
+   dstring_free(&query1);
+   if(PQntuples(res)==0) {
+     PQclear(res);
+     return 0;
+   }
+   PQclear(res);
+   return 1;
+ 
+ 
+ }
Index: src/slonik/slonik.c
===================================================================
RCS file: /slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.67.2.6
diff -c -r1.67.2.6 slonik.c
*** src/slonik/slonik.c	15 Mar 2007 18:52:02 -0000	1.67.2.6
--- src/slonik/slonik.c	24 Mar 2007 22:24:24 -0000
***************
*** 3849,3861 ****
  	PGresult *res;
  	ExecStatusType rstat;
  
  #define PARMCOUNT 1  
  
          const char *params[PARMCOUNT];
          int paramlens[PARMCOUNT];
          int paramfmts[PARMCOUNT];
  
! 	adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->ev_origin);
  	if (adminfo1 == NULL)
  		return -1;
  
--- 3849,3865 ----
  	PGresult *res;
  	ExecStatusType rstat;
  
+ 
  #define PARMCOUNT 1  
  
          const char *params[PARMCOUNT];
          int paramlens[PARMCOUNT];
          int paramfmts[PARMCOUNT];
  
! 	if(stmt->only_on_node>-1) {
! 		adminfo1 = get_active_adminfo((SlonikStmt*) stmt,stmt->only_on_node);
! 	}
! 	  adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->ev_origin);
  	if (adminfo1 == NULL)
  		return -1;
  
***************
*** 3935,3967 ****
  		/* printf ("Success - %s\n", PQresStatus(rstat)); */
  	}
  	
! 	printf("Submit DDL Event to subscribers...\n");
! 
! 	slon_mkquery(&query, "select \"_%s\".ddlScript_complete(%d, $1::text, %d); ", 
! 		     stmt->hdr.script->clustername,
! 		     stmt->ddl_setid,  
! 		     stmt->only_on_node);
! 
! 	paramlens[PARMCOUNT-1] = 0;
! 	paramfmts[PARMCOUNT-1] = 0;
! 	params[PARMCOUNT-1] = dstring_data(&script);
! 
! 	res = PQexecParams(adminfo1->dbconn, dstring_data(&query), PARMCOUNT,
! 			   NULL, params, paramlens, paramfmts, 0);
! 	
! 	if (PQresultStatus(res) != PGRES_COMMAND_OK && 
! 	    PQresultStatus(res) != PGRES_TUPLES_OK &&
! 	    PQresultStatus(res) != PGRES_EMPTY_QUERY)
! 	{
! 		rstat = PQresultStatus(res);
! 		printf("Event submission for DDL failed - %s\n", PQresStatus(rstat));
! 		dstring_free(&query);
! 		return -1;
! 	} else {
! 		rstat = PQresultStatus(res);
! 		printf ("DDL on origin - %s\n", PQresStatus(rstat));
! 	}
! 	
  	dstring_free(&script);
  	dstring_free(&query);
  	return 0;
--- 3939,3972 ----
  		/* printf ("Success - %s\n", PQresStatus(rstat)); */
  	}
  	
! 	if(stmt->only_on_node==-1) {
! 		printf("Submit DDL Event to subscribers...\n");
! 		
! 		slon_mkquery(&query, "select \"_%s\".ddlScript_complete(%d, $1::text, %d); ", 
! 					 stmt->hdr.script->clustername,
! 					 stmt->ddl_setid,  
! 					 stmt->only_on_node);
! 		
! 		paramlens[PARMCOUNT-1] = 0;
! 		paramfmts[PARMCOUNT-1] = 0;
! 		params[PARMCOUNT-1] = dstring_data(&script);
! 		
! 		res = PQexecParams(adminfo1->dbconn, dstring_data(&query), PARMCOUNT,
! 						   NULL, params, paramlens, paramfmts, 0);
! 		
! 		if (PQresultStatus(res) != PGRES_COMMAND_OK && 
! 			PQresultStatus(res) != PGRES_TUPLES_OK &&
! 			PQresultStatus(res) != PGRES_EMPTY_QUERY)
! 			{
! 				rstat = PQresultStatus(res);
! 				printf("Event submission for DDL failed - %s\n", PQresStatus(rstat));
! 				dstring_free(&query);
! 				return -1;
! 			} else {
! 				rstat = PQresultStatus(res);
! 				printf ("DDL on origin - %s\n", PQresStatus(rstat));
! 			}
! 	}/*only on node*/
  	dstring_free(&script);
  	dstring_free(&query);
  	return 0;
From andrew at ca.afilias.info  Mon Mar 26 04:37:59 2007
From: andrew at ca.afilias.info (Andrew Sullivan)
Date: Mon Mar 26 04:38:20 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4DD7@blackex02.detica.com>
References: <E0A702B010CA5F499BAAD4C2A2FF739F06DF4DD7@blackex02.detica.com>
Message-ID: <20070326113759.GA4526@afilias.info>

On Fri, Mar 23, 2007 at 10:29:59AM -0000, Victoria Parsons wrote:

> Because we are not ready to upgrade slony at the moment we will solve
> this with a vacuum full of pg_listener for each database once an hour.
> That seems to be keeping our replication under control.

You ought to be able to cause the same effect with a plain vacuum,
maybe more often than once an hour.  

Remember that VACUUM FULL takes a write lock on the table, so you're
going to block things while you do this.

Just as important, VACUUM FULL collapses the table, meaning that the
very next write on the table will require an I/O operation to extend
the table _before_ it can write in the table.  A regular VACUUM will
merely mark the relevant areas as eligible spots for writing.  That
means that the next write operation that comes alone can re-use that
free space, which actually means less I/O and therefore better
performance.

Note that we have actually had tables in some of our databases where
we were, historically, vacuuming every 5 minutes.  The key is to watch
your I/O and table growth.  There's a sweet spot where you vacuum that
often, the table remains managable in size, and the I/O is as low as
it can get.

A


-- 
Andrew Sullivan                         204-4141 Yonge Street
Afilias Canada                        Toronto, Ontario Canada
<andrew@ca.afilias.info>                              M2P 2A8
jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
From andrew.george.hammond at gmail.com  Mon Mar 26 16:13:16 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Mon Mar 26 16:13:25 2007
Subject: [Slony1-general] Slony-I Releases 1.2.9 (and 1.1.9)
Message-ID: <5a0a9d6f0703261613s1ac37bebx8e60e6442014ca0e@mail.gmail.com>

>From the website,

Slony-I Release - 1.2.9

We are pleased to announce the release of version 1.2.9, which may be
found here. There is also a Documentation tarball .

This release fixes several problems that were found in the 1.2 stream:

    * Reverted change that tried to support elderly apache rotatelogs
    * Added a patch file to apply if you need to support elderly
apache rotatelogs (tools/altperl/old-apache-rotatelogs.patch)
    * Bug in UPDATE FUNCTIONS - wrong quoting in plpgsql function
    * Add a regression test that runs UPDATE FUNCTIONS to ensure that
it at least has no syntax errors

Chris Browne 2007-03-22



I didn't see an announcement on -general, but it appears that 1.*.9
kind of broke out Thursday night after knocking over few developers
and the odd innocent by-standing DBA.

Andrew


---------- Forwarded message ----------
From: Chris Browne <cbbrowne@lists.slony.info>
Date: Mar 22, 2007 1:41 PM
Subject: [Slony1-commit] slony1-engine config.h.in RELEASE-1.2.9
postgresql-slony1-engine.spec.in
To: slony1-commit@lists.slony.info


Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv24016

Modified Files:
      Tag: REL_1_2_STABLE
        config.h.in postgresql-slony1-engine.spec.in
Added Files:
      Tag: REL_1_2_STABLE
        RELEASE-1.2.9
Log Message:
1.  Update to 1.2.9
2.  Add 1.1.9 to set of versions for UPDATE FUNCTIONS
3.  Add latest release notes to RPM spec file


Index: postgresql-slony1-engine.spec.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/postgresql-slony1-engine.spec.in,v
retrieving revision 1.31.2.7
retrieving revision 1.31.2.8
diff -C2 -d -r1.31.2.7 -r1.31.2.8
*** postgresql-slony1-engine.spec.in    7 Mar 2007 23:03:49 -0000       1.31.2.7
--- postgresql-slony1-engine.spec.in    22 Mar 2007 20:41:27 -0000      1.31.2.8
***************
*** 109,113 ****
  %files
  %defattr(-,root,root,-)
! %doc COPYRIGHT UPGRADING HISTORY-1.1 INSTALL SAMPLE RELEASE-1.2.1
RELEASE-1.2.2 RELEASE-1.2.5 RELEASE-1.2.6 RELEASE-1.2.7
  %{_bindir}/*
  %{_libdir}/pgsql/slony1_funcs.so
--- 109,113 ----
  %files
  %defattr(-,root,root,-)
! %doc COPYRIGHT UPGRADING HISTORY-1.1 INSTALL SAMPLE RELEASE-1.2.1
RELEASE-1.2.2 RELEASE-1.2.5 RELEASE-1.2.6 RELEASE-1.2.7 RELEASE-1.2.8
RELEASE-1.2.9
  %{_bindir}/*
  %{_libdir}/pgsql/slony1_funcs.so
***************
*** 127,130 ****
--- 127,133 ----

  %changelog
+ * Wed Mar 22 2007 Christopher Browne <cbbrowne@ca.afilias.info>
+ - Added more recent release notes
+
  * Wed Mar 7 2007 Christopher Browne <cbbrowne@ca.afilias.info>
  - Added more recent release notes

Index: config.h.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/config.h.in,v
retrieving revision 1.17.2.5
retrieving revision 1.17.2.6
diff -C2 -d -r1.17.2.5 -r1.17.2.6
*** config.h.in 14 Mar 2007 16:24:30 -0000      1.17.2.5
--- config.h.in 22 Mar 2007 20:41:27 -0000      1.17.2.6
***************
*** 13,18 ****
  #define SLONY_I_CONFIG_H

! #define SLONY_I_VERSION_STRING        "1.2.8"
! #define SLONY_I_VERSION_STRING_DEC 1,2,8

  #ifndef PG_VERSION_MAJOR
--- 13,18 ----
  #define SLONY_I_CONFIG_H

! #define SLONY_I_VERSION_STRING        "1.2.9"
! #define SLONY_I_VERSION_STRING_DEC 1,2,9

  #ifndef PG_VERSION_MAJOR

--- NEW FILE: RELEASE-1.2.9 ---
$Id: RELEASE-1.2.9,v 1.1.2.1 2007-03-22 20:41:27 cbbrowne Exp $

- Reverted change that tried to support elderly apache rotatelogs

- Added a patch file to apply if you need to support elderly apache
rotatelogs (tools/altperl/old-apache-rotatelogs.patch)

- Bug in UPDATE FUNCTIONS - wrong quoting in plpgsql function

- Add a regression test that runs UPDATE FUNCTIONS to ensure that it
at least has no syntax errors

_______________________________________________
Slony1-commit mailing list
Slony1-commit@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-commit
From andrew.george.hammond at gmail.com  Mon Mar 26 17:09:21 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Mon Mar 26 17:09:31 2007
Subject: [Slony1-general] missed to create tables on slave
In-Reply-To: <45FAA835.7000407@collax.com>
References: <45FAA835.7000407@collax.com>
Message-ID: <5a0a9d6f0703261709o3b0b40dk29fda9be9bcb0a6d@mail.gmail.com>

On 3/16/07, Tilman Baumann <tilman.baumann@collax.com> wrote:
> Hi,
>
> in a inattentive moment i addes tome table to the replacation without
> creating the tables on the slave fist.
> I used the slony helpers in pgadmin3.
> I added the new tables to a new replication set. After that i merged the
> new set with my previous one. And than it struk me that i missed a step.
> I guess if i had not merged the sets. I could remove the slave note from
> that set, create the tables and add ist again.

Possibly, but you'd probably have to do some surgery to get rid of the
subscribe event (and probably subsequence SYNCs) that will never
actually succeed. At which point you may as well drop the cluster and
start over again.

> Can anyone please give me a direction what i could do to bring the
> synchronisation in 'sync' again?
> Settiung the slave up new would be a ok option. The main thing is that
> it syncs all tables after that. :)

Well, it's gonna be hard for that to succeed since you'll never be
able to get past the subscribe event.

> Are there some tricks to tell slony to have a look for not yet synced
> tables?

No, slony is not that user-friendly yet. I'm sure that there'd be some
interest in figuring out how to make it more user-friendly if you had
a well thought out, well researched proposal along those lines. And
some willingness to contribute code to the effort.

Andrew
From victoria.parsons at streamshield.com  Tue Mar 27 01:09:55 2007
From: victoria.parsons at streamshield.com (Victoria Parsons)
Date: Tue Mar 27 01:10:20 2007
Subject: [Slony1-general] Vacuum full required?
Message-ID: <E0A702B010CA5F499BAAD4C2A2FF739F06EA0C43@blackex02.detica.com>

Hi Andrew,

My first reason for this query is because vacuum full is sorting out the
problem, where a regular vacuum wouldn't. I'm still not sure why, but it
is definitely the case that performance does not improve following a
vacuum, or vacuum analyze, but increases dramatically after a vacuum
full. My best reasoning is that the table changes so frequently that the
old entries end up spread all around the disk so it takes a long time to
read from the table because it is reading from lots of different disk
areas. The vacuum full cleans all these old spread out entries, so
speeding up reads. To be honest though, I don't need to know why it
makes a difference, just that vacuum full is what works.

We have found we need to perform this vacuum full on pg_listener once
every 10 minutes, instead of once an hour, but doing that our
performance problem is fixed. I am a bit worried about locks, but my
test system has now been running with the vacuum full (only on
pg_listener) every 2 minutes for 3 days, and nothing has reported an
error. A block causing a slight delay is much better than a constant
performance problem.

Vicki

 

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Andrew
Sullivan
Sent: 26 March 2007 12:38
To: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Vacuum full required?

On Fri, Mar 23, 2007 at 10:29:59AM -0000, Victoria Parsons wrote:

> Because we are not ready to upgrade slony at the moment we will solve
> this with a vacuum full of pg_listener for each database once an hour.
> That seems to be keeping our replication under control.

You ought to be able to cause the same effect with a plain vacuum,
maybe more often than once an hour.  

Remember that VACUUM FULL takes a write lock on the table, so you're
going to block things while you do this.

Just as important, VACUUM FULL collapses the table, meaning that the
very next write on the table will require an I/O operation to extend
the table _before_ it can write in the table.  A regular VACUUM will
merely mark the relevant areas as eligible spots for writing.  That
means that the next write operation that comes alone can re-use that
free space, which actually means less I/O and therefore better
performance.

Note that we have actually had tables in some of our databases where
we were, historically, vacuuming every 5 minutes.  The key is to watch
your I/O and table growth.  There's a sweet spot where you vacuum that
often, the table remains managable in size, and the I/O is as low as
it can get.

A


-- 
Andrew Sullivan                         204-4141 Yonge Street
Afilias Canada                        Toronto, Ontario Canada
<andrew@ca.afilias.info>                              M2P 2A8
jabber: ajsaf@jabber.org                 +1 416 646 3304 x4110
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


This message should be regarded as confidential. If you have received this 
email in error please notify the sender and destroy it immediately.
Statements of intent shall only become binding when confirmed in hard copy 
by an authorized signatory.

From tilman.baumann at collax.com  Tue Mar 27 02:11:57 2007
From: tilman.baumann at collax.com (Tilman Baumann)
Date: Tue Mar 27 02:18:10 2007
Subject: [Slony1-general] missed to create tables on slave
In-Reply-To: <5a0a9d6f0703261709o3b0b40dk29fda9be9bcb0a6d@mail.gmail.com>
References: <45FAA835.7000407@collax.com>
	<5a0a9d6f0703261709o3b0b40dk29fda9be9bcb0a6d@mail.gmail.com>
Message-ID: <C1B69675-90D5-4097-9452-FA25A6CF25AD@collax.com>

Thank you for your answer.
Sorry, i forgot to tell the list. I circumvented the problem by  
uninstalling and re-initializing and subscribing the slave. Seems to  
work fine.


Am 27.03.2007 um 02:09 schrieb Andrew Hammond:

> On 3/16/07, Tilman Baumann <tilman.baumann@collax.com> wrote:
>> Hi,
>>
>> in a inattentive moment i addes tome table to the replacation without
>> creating the tables on the slave fist.
>> I used the slony helpers in pgadmin3.
>> I added the new tables to a new replication set. After that i  
>> merged the
>> new set with my previous one. And than it struk me that i missed a  
>> step.
>> I guess if i had not merged the sets. I could remove the slave  
>> note from
>> that set, create the tables and add ist again.
>
> Possibly, but you'd probably have to do some surgery to get rid of the
> subscribe event (and probably subsequence SYNCs) that will never
> actually succeed. At which point you may as well drop the cluster and
> start over again.
>
>> Can anyone please give me a direction what i could do to bring the
>> synchronisation in 'sync' again?
>> Settiung the slave up new would be a ok option. The main thing is  
>> that
>> it syncs all tables after that. :)
>
> Well, it's gonna be hard for that to succeed since you'll never be
> able to get past the subscribe event.
>
>> Are there some tricks to tell slony to have a look for not yet synced
>> tables?
>
> No, slony is not that user-friendly yet. I'm sure that there'd be some
> interest in figuring out how to make it more user-friendly if you had
> a well thought out, well researched proposal along those lines. And
> some willingness to contribute code to the effort.
>
> Andrew

From ahodgson at simkin.ca  Tue Mar 27 09:49:43 2007
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Tue Mar 27 09:49:51 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
Message-ID: <200703270949.43495@hal.medialogik.com>

I recently upgraded to Slony 1.2, specifically 1.2.8.  1.1.5 was working 
fine, but I'm getting ready for 8.2, etc, and it seemed like 1.2 had 
settled down bug-wise. 

I am now having a fairly severe problem with DDL changes.

I have a relatively complex replication environment.   In the cluster that 
is having problems, I have the following:

Node 1 - master for sets 1,2,3
Node 2 - subscribes to sets 1,2,3 with forward = yes
Nodes 3-10 - subscribe to sets 1,2 from node 2
Nodes 11-12 - subscribe to sets 2,3 from node 2

I'm not sure how much of that is relevant.  I believe the issue is simply 
due to having multiple replication sets with different subscribers.

The problem that I am having is that DDL changes issues with:

  EXECUTE SCRIPT (SET ID = 1, FILENAME = 'ddl.sql', EVENT NODE = 1);

are failing on nodes 11 and 12 with errors like:

09:27:49.827654', '3676500785', '3676500786', '', 'DDL_SCRIPT', '1', 'alter 
table ppc_provider add column active boolean not null default true;
@400000004609486420df8cdc ', '-1'); insert into "_web_cluster".sl_confirm       
(con_origin, con_received, con_seqno, con_timestamp)    values (1, 
11, '147115', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:  
Slony-I: alterTableForReplication(): Table "public"."geo_combined_data" is 
already in altered state
@400000004609486420dfac1c CONTEXT:  SQL 
statement "SELECT  "_web_cluster".alterTableForReplication( $1 )"
@400000004609486420dfbbbc PL/pgSQL function "ddlscript_complete_int" line 11 
at perform

... as in the following log:

@40000000460948641ba439ac 2007-03-27 09:37:46 PDT DEBUG2 
remoteListenThread_2: queue event 1,147115 DDL_SCRIPT
@40000000460948641bad5d84 2007-03-27 09:37:46 PDT DEBUG4 
remoteWorkerThread_1: update provider configuration
@40000000460948641baf844c 2007-03-27 09:37:46 PDT DEBUG1 
remoteWorkerThread_1: helper thread for provider 2 created
@40000000460948641bb0ebac 2007-03-27 09:37:46 PDT DEBUG4 
remoteWorkerThread_1: added active set 2 to provider 2
@40000000460948641bb131fc 2007-03-27 09:37:46 PDT DEBUG4 
remoteWorkerThread_1: added active set 3 to provider 2
@40000000460948641bb1707c 2007-03-27 09:37:46 PDT DEBUG2 
remoteWorkerThread_1: Received event 1,147115 DDL_SCRIPT
@40000000460948641bb2ace4 2007-03-27 09:37:46 PDT DEBUG4 
remoteHelperThread_1_2: waiting for work
@40000000460948641c405e54 2007-03-27 09:37:46 PDT CONFIG 
remoteWorkerThread_1: DDL request with 1 statements
@40000000460948641c4121a4 2007-03-27 09:37:46 PDT CONFIG 
remoteWorkerThread_1: DDL Statement 0: [alter table ppc_provider add column 
active boolean not null default true;]
@40000000460948641c57e5c4 2007-03-27 09:37:46 PDT DEBUG4 version 
for "dbname=Hitfarm user=slony host=node11.mydomain.com" is 80108
@40000000460948641dfdc5c4 2007-03-27 09:37:46 PDT DEBUG4 
remoteWorkerThread_7: update provider configuration
@40000000460948641e218a04 2007-03-27 09:37:46 PDT DEBUG4 version 
for "dbname=Hitfarm user=slony host=node11.mydomain.com" is 80108
@40000000460948641e6c4ea4 2007-03-27 09:37:46 PDT DEBUG4 
remoteWorkerThread_8: update provider configuration
@40000000460948641f25e65c 2007-03-27 09:37:46 PDT CONFIG DDL success - 
PGRES_COMMAND_OK
@400000004609486420df36ec 2007-03-27 09:37:46 PDT ERROR  
remoteWorkerThread_1: "select "_web_cluster".ddlScript_complete_int(1, -1); 
notify "_web_cluster_Event"; notify "_web_cluster_Confirm"; insert 
into "_web_cluster".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    ) 
values ('1', '147115', '2007-03-27 
09:27:49.827654', '3676500785', '3676500786', '', 'DDL_SCRIPT', '1', 'alter 
table table_in_set_1 add column active boolean not null default true;
@400000004609486420df8cdc ', '-1'); insert into "_web_cluster".sl_confirm       
(con_origin, con_received, con_seqno, con_timestamp)    values (1, 
11, '147115', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:  
Slony-I: alterTableForReplication(): Table "public"."geo_combined_data" is 
already in altered state
@400000004609486420dfac1c CONTEXT:  SQL 
statement "SELECT  "_web_cluster".alterTableForReplication( $1 )"
@400000004609486420dfbbbc PL/pgSQL function "ddlscript_complete_int" line 11 
at perform
@400000004609486420dffa3c 2007-03-27 09:37:46 PDT DEBUG2 slon_retry() from 
pid=11132
@400000004609486420e07f0c 2007-03-27 09:37:46 PDT DEBUG1 slon: retry 
requested
@400000004609486420e0fff4 2007-03-27 09:37:46 PDT DEBUG2 slon: notify worker 
process to shutdown


"public"."geo_combined_data", the table giving the error, is in set 2.  The 
DDL change is only to set 1.  The node that is failing is not subscribed to 
set 1, but is subscribed to sets 2 and 3.  I rebuilt node 11 from scratch a 
few minutes ago to see if it was due to any legacy issues, but it seems to 
recur quite predictably.

The events are processing correctly on node 2, which is subscribed to all 3 
sets, and also any node subscribed to set 1 and 2 but not set 3.

I would appreciate any assistance resolving this.   Using Slony 1.1, these 
changes would not have even replicated out to these nodes (I had a separate 
script to update them), but now it's just killing my replication until I 
can delete the DDL events and process them manually.

-- 
"Bad laws are the worst sort of tyranny." -- Edmund Burke (1729-1797)
From andrew.george.hammond at gmail.com  Tue Mar 27 10:42:11 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue Mar 27 10:42:16 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06EA0C43@blackex02.detica.com>
References: <E0A702B010CA5F499BAAD4C2A2FF739F06EA0C43@blackex02.detica.com>
Message-ID: <5a0a9d6f0703271042h3349ceeoc393aed8349ab465@mail.gmail.com>

On 3/27/07, Victoria Parsons <victoria.parsons@streamshield.com> wrote:

> I am a bit worried about locks, but my
> test system has now been running with the vacuum full (only on
> pg_listener) every 2 minutes for 3 days, and nothing has reported an
> error.

Assuming your application makes no use of LISTEN/NOTIFY, then locking
pg_listener with a vacuum full should not interfere with anything
except the slons.

Andrew
From andrew.george.hammond at gmail.com  Tue Mar 27 11:27:50 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue Mar 27 11:27:59 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <200703270949.43495@hal.medialogik.com>
References: <200703270949.43495@hal.medialogik.com>
Message-ID: <5a0a9d6f0703271127p6de3f082wd265066a8d1f7a5a@mail.gmail.com>

On 3/27/07, Alan Hodgson <ahodgson@simkin.ca> wrote:
> I recently upgraded to Slony 1.2, specifically 1.2.8.  1.1.5 was working
> fine, but I'm getting ready for 8.2, etc, and it seemed like 1.2 had
> settled down bug-wise.
>
> I am now having a fairly severe problem with DDL changes.
>
> I have a relatively complex replication environment.   In the cluster that
> is having problems, I have the following:
>
> Node 1 - master for sets 1,2,3
> Node 2 - subscribes to sets 1,2,3 with forward = yes
> Nodes 3-10 - subscribe to sets 1,2 from node 2
> Nodes 11-12 - subscribe to sets 2,3 from node 2
>
> I'm not sure how much of that is relevant.  I believe the issue is simply
> due to having multiple replication sets with different subscribers.
>
> The problem that I am having is that DDL changes issues with:
>
>   EXECUTE SCRIPT (SET ID = 1, FILENAME = 'ddl.sql', EVENT NODE = 1);
>
> are failing on nodes 11 and 12 with errors like:
>
> 09:27:49.827654', '3676500785', '3676500786', '', 'DDL_SCRIPT', '1', 'alter
> table ppc_provider add column active boolean not null default true;
> @400000004609486420df8cdc ', '-1'); insert into "_web_cluster".sl_confirm
> (con_origin, con_received, con_seqno, con_timestamp)    values (1,
> 11, '147115', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:
> Slony-I: alterTableForReplication(): Table "public"."geo_combined_data" is
> already in altered state
> @400000004609486420dfac1c CONTEXT:  SQL
> statement "SELECT  "_web_cluster".alterTableForReplication( $1 )"
> @400000004609486420dfbbbc PL/pgSQL function "ddlscript_complete_int" line 11
> at perform
>
> ... as in the following log:
>
> @40000000460948641ba439ac 2007-03-27 09:37:46 PDT DEBUG2
> remoteListenThread_2: queue event 1,147115 DDL_SCRIPT
> @40000000460948641bad5d84 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_1: update provider configuration
> @40000000460948641baf844c 2007-03-27 09:37:46 PDT DEBUG1
> remoteWorkerThread_1: helper thread for provider 2 created
> @40000000460948641bb0ebac 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_1: added active set 2 to provider 2
> @40000000460948641bb131fc 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_1: added active set 3 to provider 2
> @40000000460948641bb1707c 2007-03-27 09:37:46 PDT DEBUG2
> remoteWorkerThread_1: Received event 1,147115 DDL_SCRIPT
> @40000000460948641bb2ace4 2007-03-27 09:37:46 PDT DEBUG4
> remoteHelperThread_1_2: waiting for work
> @40000000460948641c405e54 2007-03-27 09:37:46 PDT CONFIG
> remoteWorkerThread_1: DDL request with 1 statements
> @40000000460948641c4121a4 2007-03-27 09:37:46 PDT CONFIG
> remoteWorkerThread_1: DDL Statement 0: [alter table ppc_provider add column
> active boolean not null default true;]
> @40000000460948641c57e5c4 2007-03-27 09:37:46 PDT DEBUG4 version
> for "dbname=Hitfarm user=slony host=node11.mydomain.com" is 80108
> @40000000460948641dfdc5c4 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_7: update provider configuration
> @40000000460948641e218a04 2007-03-27 09:37:46 PDT DEBUG4 version
> for "dbname=Hitfarm user=slony host=node11.mydomain.com" is 80108
> @40000000460948641e6c4ea4 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_8: update provider configuration
> @40000000460948641f25e65c 2007-03-27 09:37:46 PDT CONFIG DDL success -
> PGRES_COMMAND_OK
> @400000004609486420df36ec 2007-03-27 09:37:46 PDT ERROR
> remoteWorkerThread_1: "select "_web_cluster".ddlScript_complete_int(1, -1);
> notify "_web_cluster_Event"; notify "_web_cluster_Confirm"; insert
> into "_web_cluster".sl_event     (ev_origin, ev_seqno, ev_timestamp,
> ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    )
> values ('1', '147115', '2007-03-27
> 09:27:49.827654', '3676500785', '3676500786', '', 'DDL_SCRIPT', '1', 'alter
> table table_in_set_1 add column active boolean not null default true;
> @400000004609486420df8cdc ', '-1'); insert into "_web_cluster".sl_confirm
> (con_origin, con_received, con_seqno, con_timestamp)    values (1,
> 11, '147115', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:
> Slony-I: alterTableForReplication(): Table "public"."geo_combined_data" is
> already in altered state
> @400000004609486420dfac1c CONTEXT:  SQL
> statement "SELECT  "_web_cluster".alterTableForReplication( $1 )"
> @400000004609486420dfbbbc PL/pgSQL function "ddlscript_complete_int" line 11
> at perform
> @400000004609486420dffa3c 2007-03-27 09:37:46 PDT DEBUG2 slon_retry() from
> pid=11132
> @400000004609486420e07f0c 2007-03-27 09:37:46 PDT DEBUG1 slon: retry
> requested
> @400000004609486420e0fff4 2007-03-27 09:37:46 PDT DEBUG2 slon: notify worker
> process to shutdown
>
>
> "public"."geo_combined_data", the table giving the error, is in set 2.  The
> DDL change is only to set 1.  The node that is failing is not subscribed to
> set 1, but is subscribed to sets 2 and 3.  I rebuilt node 11 from scratch a
> few minutes ago to see if it was due to any legacy issues, but it seems to
> recur quite predictably.
>
> The events are processing correctly on node 2, which is subscribed to all 3
> sets, and also any node subscribed to set 1 and 2 but not set 3.
>
> I would appreciate any assistance resolving this.   Using Slony 1.1, these
> changes would not have even replicated out to these nodes (I had a separate
> script to update them), but now it's just killing my replication until I
> can delete the DDL events and process them manually.

This is a known issue. The solution, such as it is, is to maintain the
entire schema for all sets on all nodes. You obviously can leave the
tables / sequences empty and un-replicated. Be aware that this is a
good way to create lots of constraint annoyances.

Andrew
From ahodgson at simkin.ca  Tue Mar 27 12:09:03 2007
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Tue Mar 27 12:09:09 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <5a0a9d6f0703271127p6de3f082wd265066a8d1f7a5a@mail.gmail.com>
References: <200703270949.43495@hal.medialogik.com>
	<5a0a9d6f0703271127p6de3f082wd265066a8d1f7a5a@mail.gmail.com>
Message-ID: <200703271209.03128@hal.medialogik.com>

On Tuesday 27 March 2007 11:27, "Andrew Hammond" 
<andrew.george.hammond@gmail.com> wrote:
> This is a known issue. The solution, such as it is, is to maintain the
> entire schema for all sets on all nodes. You obviously can leave the
> tables / sequences empty and un-replicated. Be aware that this is a
> good way to create lots of constraint annoyances.

I guess I didn't explicitly say that, but the entire schema exists and is 
consistent on all nodes.   I want the DDL the execute on these nodes.  This 
error is not due to a schema difference.

-- 
"Pulling together is the aim of despotism and tyranny. Free men pull in 
all kinds of directions." -- Terry Pratchett

From cbbrowne at ca.afilias.info  Tue Mar 27 13:51:32 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Mar 27 13:51:44 2007
Subject: [Slony1-general] Vacuum full required?
In-Reply-To: <E0A702B010CA5F499BAAD4C2A2FF739F06EA0C43@blackex02.detica.com>
	(Victoria Parsons's message of "Tue, 27 Mar 2007 09:09:55 +0100")
References: <E0A702B010CA5F499BAAD4C2A2FF739F06EA0C43@blackex02.detica.com>
Message-ID: <60slbqo1ez.fsf@dba2.int.libertyrms.com>

"Victoria Parsons" <victoria.parsons@streamshield.com> writes:
> We have found we need to perform this vacuum full on pg_listener once
> every 10 minutes, instead of once an hour, but doing that our
> performance problem is fixed. I am a bit worried about locks, but my
> test system has now been running with the vacuum full (only on
> pg_listener) every 2 minutes for 3 days, and nothing has reported an
> error. A block causing a slight delay is much better than a constant
> performance problem.

I don't see this being a particularly "awful" application of VACUUM
FULL.

VACUUM FULL on pg_listener will cause a bit of deferral of replication
activity, but what's good is that pg_listener _isn't_ accessed by the
triggers that are put on the tables, so the REALLY VITAL concern,
namely that we don't want to stop the user's application, is met.
This VACUUM FULL doesn't prevent the user's application from modifying
the application tables and seeing those updates get queued in
sl_log_[12].

I daresay that in my tenure in DB Operations, I have *never* scheduled
a VACUUM FULL of any table (outside of maybe a situation of doing so
by hand during an outage).  Certainly I've never scheduled VACUUM FULL
in a cron...

But on the other hand, I do not see a reason to be overly paranoid
about doing so to the table pg_catalog.pg_listener.
-- 
(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From wmoran at collaborativefusion.com  Tue Mar 27 14:27:42 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue Mar 27 14:27:53 2007
Subject: node ID limitations (was Re: [Slony1-general] Unexpected problems
	with Slony config)
In-Reply-To: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
Message-ID: <20070327172742.35176dc6.wmoran@collaborativefusion.com>

In response to Bill Moran <wmoran@collaborativefusion.com>:

> The second thing that surprised me is an odd limit on the size of the node
> ID.  As I already mentioned, we were trying to use node IDs that contain
> some intelligence (serial # style), so our node IDs were around 20000000.
> I didn't expect that to be a problem, since it's certainly small enough
> to fit in an int, but slonik gave a rather cryptic error:
> <stdin>:974: PGRES_FATAL_ERROR select "_clustername".initializeLocalNode(20000000, 'Local Slave node'); select "_clustername".enableNode_int(20000000);  - ERROR:  bigint out of range
> CONTEXT:  SQL statement "SELECT  setval('"_clustername".sl_rowid_seq',  $1 ::int8 * '1000000000000000'::int8)"
> PL/pgSQL function "initializelocalnode" line 26 at perform
> 
> Which makes it appear as if the node ID is being multiplied by one
> quadrillion before attempting to stuff it into a BIGINT.  when I set
> the node ID down to single-digits, the error stopped.

It appears as if this is happening to guarantee unique IDs for
Slony-generated primary keys across all nodes.

The thing is that it's only needed for tables that don't already have
primary keys, and in that case it's rather arbitrary.  It's making the
assumption that all the slony-generated primary keys on this node will
never exceed 10^15.  

I'm kind up in the air about this.  I would like to put together a patch
that will support longer node IDs, but 10^15 seems arbitrary, and anything
I'd change it to would be arbitrary as well.  The painful thing is that in
a database where all tables have primary keys, it's not needed anyway.

I expect that not a lot of people are having trouble with this, or there
would be more discussion about it up till now, but I'm curious if
anyone has ideas to contribute.

-- 
Bill Moran
Collaborative Fusion Inc.
From lavalamp at spiritual-machines.org  Tue Mar 27 14:28:12 2007
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Tue Mar 27 14:28:28 2007
Subject: [Slony1-general] Assuming node id = 1 (STORE NODE, etc.)
Message-ID: <20070327170402.C6132@arbitor.digitalfreaks.org>

In Two places:

line 334 src/slonik/slonik.c:

                         case STMT_STORE_NODE:
                                 {
                                         SlonikStmt_store_node *stmt =
                                         (SlonikStmt_store_node *) hdr;

                                         if (stmt->ev_origin < 0)
                                         {
                                                 stmt->ev_origin = 1;
                                         }

and line 611 src/slonik/parser.y:

stmt_store_node         : lno K_STORE K_NODE option_list
                                         {
                                  SlonikStmt_store_node *new;
                     statement_option opt[] = {
                                     STMT_OPTION_INT( O_ID, -1 ),
                                     STMT_OPTION_STR( O_COMMENT, NULL ),
                                     STMT_OPTION_YN( O_SPOOLNODE, 0 ),
                                     STMT_OPTION_INT( O_EVENT_NODE, 1 ),
                                     STMT_OPTION_END
                                  };



This works for 99.9% of situations, but it may be more pragmatic to use 
the node ID of the least number value instead of static "1".  E.g., 
default in the .y to "-1", and in the .c, to check for "-1" or 
...undefined in conf = default (-1), thus check some algorithm 
getLowestDefinedNodeID();

The work-around for this is to explicitly declare:
  ... "event node = [some_node_id_other_than_1_here]"

Which is also really inconsistent because in many other commands this 
would be referred to as "node id"; that's a separate issue.

Is this feature request/ticket worthy?  Also, are we presently using gborg 
or pgfoundry for issue tracking?

TIA, ~BAS

--


l8*
 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
 	       http://www.spiritual-machines.org/

"...from back in the heady days when "helpdesk" meant nothing, "diskquota"
meant everything, and lives could be bought and sold for a couple of pages
of laser printout - and frequently were."
From andrew.george.hammond at gmail.com  Tue Mar 27 16:38:31 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue Mar 27 16:38:41 2007
Subject: node ID limitations (was Re: [Slony1-general] Unexpected problems
	with Slony config)
In-Reply-To: <20070327172742.35176dc6.wmoran@collaborativefusion.com>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
	<20070327172742.35176dc6.wmoran@collaborativefusion.com>
Message-ID: <5a0a9d6f0703271638m7f36087v8564f088e48afd50@mail.gmail.com>

On 3/27/07, Bill Moran <wmoran@collaborativefusion.com> wrote:
> In response to Bill Moran <wmoran@collaborativefusion.com>:
>
> > The second thing that surprised me is an odd limit on the size of the node
> > ID.  As I already mentioned, we were trying to use node IDs that contain
> > some intelligence (serial # style), so our node IDs were around 20000000.
> > I didn't expect that to be a problem, since it's certainly small enough
> > to fit in an int, but slonik gave a rather cryptic error:
> > <stdin>:974: PGRES_FATAL_ERROR select "_clustername".initializeLocalNode(20000000, 'Local Slave node'); select "_clustername".enableNode_int(20000000);  - ERROR:  bigint out of range
> > CONTEXT:  SQL statement "SELECT  setval('"_clustername".sl_rowid_seq',  $1 ::int8 * '1000000000000000'::int8)"
> > PL/pgSQL function "initializelocalnode" line 26 at perform
> >
> > Which makes it appear as if the node ID is being multiplied by one
> > quadrillion before attempting to stuff it into a BIGINT.  when I set
> > the node ID down to single-digits, the error stopped.
>
> It appears as if this is happening to guarantee unique IDs for
> Slony-generated primary keys across all nodes.
>
> The thing is that it's only needed for tables that don't already have
> primary keys, and in that case it's rather arbitrary.  It's making the
> assumption that all the slony-generated primary keys on this node will
> never exceed 10^15.
>
> I'm kind up in the air about this.  I would like to put together a patch
> that will support longer node IDs, but 10^15 seems arbitrary, and anything
> I'd change it to would be arbitrary as well.  The painful thing is that in
> a database where all tables have primary keys, it's not needed anyway.
>
> I expect that not a lot of people are having trouble with this, or there
> would be more discussion about it up till now, but I'm curious if
> anyone has ideas to contribute.

Considering that people seem to agree that slony generated pkeys are
more of a wart than a feature, why not simply deprecate the feature
and remove it?

Andrew
From ssinger_pg at sympatico.ca  Tue Mar 27 17:45:04 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Tue Mar 27 17:45:20 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <200703270949.43495@hal.medialogik.com>
References: <200703270949.43495@hal.medialogik.com>
Message-ID: <Pine.LNX.4.62.0703272037290.2859@mini.atlantida.localdomain>

On Tue, 27 Mar 2007, Alan Hodgson wrote:

I don't know what workarounds are going to be options for you compared with 
running a patched version of slony but if you 
need a fix right away you could consider patching your version of slony.

The patched attached to 
http://lists.slony.info/pipermail/slony1-patches/2007-March/000005.html

should address the problem your describing.  I assume it will be 
included with the next 1.2.x release.

Steve


> I recently upgraded to Slony 1.2, specifically 1.2.8.  1.1.5 was working
> fine, but I'm getting ready for 8.2, etc, and it seemed like 1.2 had
> settled down bug-wise.
>
> I am now having a fairly severe problem with DDL changes.
>
> I have a relatively complex replication environment.   In the cluster that
> is having problems, I have the following:
>
> Node 1 - master for sets 1,2,3
> Node 2 - subscribes to sets 1,2,3 with forward = yes
> Nodes 3-10 - subscribe to sets 1,2 from node 2
> Nodes 11-12 - subscribe to sets 2,3 from node 2
>
> I'm not sure how much of that is relevant.  I believe the issue is simply
> due to having multiple replication sets with different subscribers.
>
> The problem that I am having is that DDL changes issues with:
>
>  EXECUTE SCRIPT (SET ID = 1, FILENAME = 'ddl.sql', EVENT NODE = 1);
>
> are failing on nodes 11 and 12 with errors like:
>
> 09:27:49.827654', '3676500785', '3676500786', '', 'DDL_SCRIPT', '1', 'alter
> table ppc_provider add column active boolean not null default true;
> @400000004609486420df8cdc ', '-1'); insert into "_web_cluster".sl_confirm
> (con_origin, con_received, con_seqno, con_timestamp)    values (1,
> 11, '147115', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:
> Slony-I: alterTableForReplication(): Table "public"."geo_combined_data" is
> already in altered state
> @400000004609486420dfac1c CONTEXT:  SQL
> statement "SELECT  "_web_cluster".alterTableForReplication( $1 )"
> @400000004609486420dfbbbc PL/pgSQL function "ddlscript_complete_int" line 11
> at perform
>
> ... as in the following log:
>
> @40000000460948641ba439ac 2007-03-27 09:37:46 PDT DEBUG2
> remoteListenThread_2: queue event 1,147115 DDL_SCRIPT
> @40000000460948641bad5d84 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_1: update provider configuration
> @40000000460948641baf844c 2007-03-27 09:37:46 PDT DEBUG1
> remoteWorkerThread_1: helper thread for provider 2 created
> @40000000460948641bb0ebac 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_1: added active set 2 to provider 2
> @40000000460948641bb131fc 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_1: added active set 3 to provider 2
> @40000000460948641bb1707c 2007-03-27 09:37:46 PDT DEBUG2
> remoteWorkerThread_1: Received event 1,147115 DDL_SCRIPT
> @40000000460948641bb2ace4 2007-03-27 09:37:46 PDT DEBUG4
> remoteHelperThread_1_2: waiting for work
> @40000000460948641c405e54 2007-03-27 09:37:46 PDT CONFIG
> remoteWorkerThread_1: DDL request with 1 statements
> @40000000460948641c4121a4 2007-03-27 09:37:46 PDT CONFIG
> remoteWorkerThread_1: DDL Statement 0: [alter table ppc_provider add column
> active boolean not null default true;]
> @40000000460948641c57e5c4 2007-03-27 09:37:46 PDT DEBUG4 version
> for "dbname=Hitfarm user=slony host=node11.mydomain.com" is 80108
> @40000000460948641dfdc5c4 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_7: update provider configuration
> @40000000460948641e218a04 2007-03-27 09:37:46 PDT DEBUG4 version
> for "dbname=Hitfarm user=slony host=node11.mydomain.com" is 80108
> @40000000460948641e6c4ea4 2007-03-27 09:37:46 PDT DEBUG4
> remoteWorkerThread_8: update provider configuration
> @40000000460948641f25e65c 2007-03-27 09:37:46 PDT CONFIG DDL success -
> PGRES_COMMAND_OK
> @400000004609486420df36ec 2007-03-27 09:37:46 PDT ERROR
> remoteWorkerThread_1: "select "_web_cluster".ddlScript_complete_int(1, -1);
> notify "_web_cluster_Event"; notify "_web_cluster_Confirm"; insert
> into "_web_cluster".sl_event     (ev_origin, ev_seqno, ev_timestamp,
> ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    )
> values ('1', '147115', '2007-03-27
> 09:27:49.827654', '3676500785', '3676500786', '', 'DDL_SCRIPT', '1', 'alter
> table table_in_set_1 add column active boolean not null default true;
> @400000004609486420df8cdc ', '-1'); insert into "_web_cluster".sl_confirm
> (con_origin, con_received, con_seqno, con_timestamp)    values (1,
> 11, '147115', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:
> Slony-I: alterTableForReplication(): Table "public"."geo_combined_data" is
> already in altered state
> @400000004609486420dfac1c CONTEXT:  SQL
> statement "SELECT  "_web_cluster".alterTableForReplication( $1 )"
> @400000004609486420dfbbbc PL/pgSQL function "ddlscript_complete_int" line 11
> at perform
> @400000004609486420dffa3c 2007-03-27 09:37:46 PDT DEBUG2 slon_retry() from
> pid=11132
> @400000004609486420e07f0c 2007-03-27 09:37:46 PDT DEBUG1 slon: retry
> requested
> @400000004609486420e0fff4 2007-03-27 09:37:46 PDT DEBUG2 slon: notify worker
> process to shutdown
>
>
> "public"."geo_combined_data", the table giving the error, is in set 2.  The
> DDL change is only to set 1.  The node that is failing is not subscribed to
> set 1, but is subscribed to sets 2 and 3.  I rebuilt node 11 from scratch a
> few minutes ago to see if it was due to any legacy issues, but it seems to
> recur quite predictably.
>
> The events are processing correctly on node 2, which is subscribed to all 3
> sets, and also any node subscribed to set 1 and 2 but not set 3.
>
> I would appreciate any assistance resolving this.   Using Slony 1.1, these
> changes would not have even replicated out to these nodes (I had a separate
> script to update them), but now it's just killing my replication until I
> can delete the DDL events and process them manually.
>
> -- 
> "Bad laws are the worst sort of tyranny." -- Edmund Burke (1729-1797)
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From wmoran at collaborativefusion.com  Wed Mar 28 05:31:48 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed Mar 28 05:31:59 2007
Subject: Slony-generated primary keys (was Re: node ID limitations (was Re:
	[Slony1-general] Unexpected problems with Slony config))
In-Reply-To: <5a0a9d6f0703271638m7f36087v8564f088e48afd50@mail.gmail.com>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
	<20070327172742.35176dc6.wmoran@collaborativefusion.com>
	<5a0a9d6f0703271638m7f36087v8564f088e48afd50@mail.gmail.com>
Message-ID: <20070328083148.9fda6a80.wmoran@collaborativefusion.com>

In response to "Andrew Hammond" <andrew.george.hammond@gmail.com>:

> On 3/27/07, Bill Moran <wmoran@collaborativefusion.com> wrote:
> > In response to Bill Moran <wmoran@collaborativefusion.com>:
> >
> > > The second thing that surprised me is an odd limit on the size of the node
> > > ID.  As I already mentioned, we were trying to use node IDs that contain
> > > some intelligence (serial # style), so our node IDs were around 20000000.
> > > I didn't expect that to be a problem, since it's certainly small enough
> > > to fit in an int, but slonik gave a rather cryptic error:
> > > <stdin>:974: PGRES_FATAL_ERROR select "_clustername".initializeLocalNode(20000000, 'Local Slave node'); select "_clustername".enableNode_int(20000000);  - ERROR:  bigint out of range
> > > CONTEXT:  SQL statement "SELECT  setval('"_clustername".sl_rowid_seq',  $1 ::int8 * '1000000000000000'::int8)"
> > > PL/pgSQL function "initializelocalnode" line 26 at perform
> > >
> > > Which makes it appear as if the node ID is being multiplied by one
> > > quadrillion before attempting to stuff it into a BIGINT.  when I set
> > > the node ID down to single-digits, the error stopped.
> >
> > It appears as if this is happening to guarantee unique IDs for
> > Slony-generated primary keys across all nodes.
> >
> > The thing is that it's only needed for tables that don't already have
> > primary keys, and in that case it's rather arbitrary.  It's making the
> > assumption that all the slony-generated primary keys on this node will
> > never exceed 10^15.
> >
> > I'm kind up in the air about this.  I would like to put together a patch
> > that will support longer node IDs, but 10^15 seems arbitrary, and anything
> > I'd change it to would be arbitrary as well.  The painful thing is that in
> > a database where all tables have primary keys, it's not needed anyway.
> >
> > I expect that not a lot of people are having trouble with this, or there
> > would be more discussion about it up till now, but I'm curious if
> > anyone has ideas to contribute.
> 
> Considering that people seem to agree that slony generated pkeys are
> more of a wart than a feature, why not simply deprecate the feature
> and remove it?

I tend to lean that way, since I've been making it a point to ensure that
all our tables have primary keys.  I believe it's not a big deal to
create them for Slony if they don't exist, and I'd rather intelligently
select a primary key than have Slony add an "arbitrary" bigint field.
Removing that capability would certainly simply the slony code.

However, I'm hoping to come up with an improvement that the community will
accept so it goes back into the tree.  My thought is that the capability
wouldn't be there if people weren't using it, so yanking it could cause
some upset.

-- 
Bill Moran
Collaborative Fusion Inc.
From nagy at ecircle-ag.com  Wed Mar 28 05:52:09 2007
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Mar 28 05:52:39 2007
Subject: Slony-generated primary keys (was Re: node ID limitations (was
	Re: [Slony1-general] Unexpected problems with Slony config))
In-Reply-To: <20070328083148.9fda6a80.wmoran@collaborativefusion.com>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
	<20070327172742.35176dc6.wmoran@collaborativefusion.com>
	<5a0a9d6f0703271638m7f36087v8564f088e48afd50@mail.gmail.com>
	<20070328083148.9fda6a80.wmoran@collaborativefusion.com>
Message-ID: <1175086329.15684.36.camel@coppola.muc.ecircle.de>

Hi all,

> > Considering that people seem to agree that slony generated pkeys are
> > more of a wart than a feature, why not simply deprecate the feature
> > and remove it?
> 
> I tend to lean that way, since I've been making it a point to ensure that
> all our tables have primary keys.  I believe it's not a big deal to
> create them for Slony if they don't exist, and I'd rather intelligently
> select a primary key than have Slony add an "arbitrary" bigint field.
> Removing that capability would certainly simply the slony code.
> 
> However, I'm hoping to come up with an improvement that the community will
> accept so it goes back into the tree.  My thought is that the capability
> wouldn't be there if people weren't using it, so yanking it could cause
> some upset.

We do use here this feature (letting slony add the "arbitrary" PKs). 
It has a couple of advantages over maintaining it in our own schema.

There are tables where having a PK simply does NOT make sense. Most of
our tables do have PKs, and I fitted some of them which did not have
with new ones specifically after we started to use slony (I even had to
split tables in different sub-tables for this, but it was actually
making sense), but there are a couple where I simply couldn't justify
adding a PK...

This combined with the fact that not all our data bases are replicated
via slony but they share the same schema results in me being happy if
slony takes care of adding the PK which is totally irrelevant to our
application and it only suits slony... with the added benefit that it
gets deleted once slony is uninstalled from the DB.

Now if slony will not do that anymore on its own, it could be
acceptable, if only there would be some nice GUI admin interface which I
can tell "please replicate all my tables and do whatever it is needed to
do so"... and get some warnings if there are BLOBs, missing PKs, etc.
(yes I know about pgadmin and I'm using it but it's not really working
well if you have 200 tables to replicate).

Cheers,
Csaba.


From lavalamp at spiritual-machines.org  Wed Mar 28 07:49:41 2007
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Wed Mar 28 07:49:59 2007
Subject: Slony-generated primary keys (was Re: node ID limitations (was
	Re: [Slony1-general] Unexpected problems with Slony config))
In-Reply-To: <1175086329.15684.36.camel@coppola.muc.ecircle.de>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
	<20070327172742.35176dc6.wmoran@collaborativefusion.com>
	<5a0a9d6f0703271638m7f36087v8564f088e48afd50@mail.gmail.com>
	<20070328083148.9fda6a80.wmoran@collaborativefusion.com>
	<1175086329.15684.36.camel@coppola.muc.ecircle.de>
Message-ID: <20070328104907.F6132@arbitor.digitalfreaks.org>



http://gborg.postgresql.org/project/slony1/bugs/bugupdate.php?1658

Used gborg instead of pgfoundry since I don't see tickets on the later and 
no one else reponded.

~BAS


On Wed, 28 Mar 2007, Csaba Nagy wrote:

> Hi all,
>
>>> Considering that people seem to agree that slony generated pkeys are
>>> more of a wart than a feature, why not simply deprecate the feature
>>> and remove it?
>>
>> I tend to lean that way, since I've been making it a point to ensure that
>> all our tables have primary keys.  I believe it's not a big deal to
>> create them for Slony if they don't exist, and I'd rather intelligently
>> select a primary key than have Slony add an "arbitrary" bigint field.
>> Removing that capability would certainly simply the slony code.
>>
>> However, I'm hoping to come up with an improvement that the community will
>> accept so it goes back into the tree.  My thought is that the capability
>> wouldn't be there if people weren't using it, so yanking it could cause
>> some upset.
>
> We do use here this feature (letting slony add the "arbitrary" PKs).
> It has a couple of advantages over maintaining it in our own schema.
>
> There are tables where having a PK simply does NOT make sense. Most of
> our tables do have PKs, and I fitted some of them which did not have
> with new ones specifically after we started to use slony (I even had to
> split tables in different sub-tables for this, but it was actually
> making sense), but there are a couple where I simply couldn't justify
> adding a PK...
>
> This combined with the fact that not all our data bases are replicated
> via slony but they share the same schema results in me being happy if
> slony takes care of adding the PK which is totally irrelevant to our
> application and it only suits slony... with the added benefit that it
> gets deleted once slony is uninstalled from the DB.
>
> Now if slony will not do that anymore on its own, it could be
> acceptable, if only there would be some nice GUI admin interface which I
> can tell "please replicate all my tables and do whatever it is needed to
> do so"... and get some warnings if there are BLOBs, missing PKs, etc.
> (yes I know about pgadmin and I'm using it but it's not really working
> well if you have 200 tables to replicate).
>
> Cheers,
> Csaba.
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

l8*
 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
 	       http://www.spiritual-machines.org/

"...from back in the heady days when "helpdesk" meant nothing, "diskquota"
meant everything, and lives could be bought and sold for a couple of pages
of laser printout - and frequently were."
From ahodgson at simkin.ca  Wed Mar 28 13:02:28 2007
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Wed Mar 28 13:02:37 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <Pine.LNX.4.62.0703272037290.2859@mini.atlantida.localdomain>
References: <200703270949.43495@hal.medialogik.com>
	<Pine.LNX.4.62.0703272037290.2859@mini.atlantida.localdomain>
Message-ID: <200703281302.28372@hal.medialogik.com>

On Tuesday 27 March 2007 17:45, Steve Singer <ssinger_pg@sympatico.ca> 
wrote:
> On Tue, 27 Mar 2007, Alan Hodgson wrote:
>
> I don't know what workarounds are going to be options for you compared
> with running a patched version of slony but if you
> need a fix right away you could consider patching your version of slony.
>
> The patched attached to
> http://lists.slony.info/pipermail/slony1-patches/2007-March/000005.html
>
> should address the problem your describing.  I assume it will be
> included with the next 1.2.x release.
>

Thank you for that. I will test it and consider running with it for now.  
However, I actually think the intended behaviour for 1.2 is correct - 
DDL scripts should execute on all nodes (working around the 1.1 
behaviour was also quite tedious).  It just isn't working correctly ... 
and I can't see why.  

I was poking around the slon code last night, and it really looks like 
it should be altering all the same tables before and after the DDL 
script is run, but the error I'm getting seems to imply that either 
alterTableRestore is not being run on some tables or  
alterTableForReplication is somehow being run twice.

And actually, now that I think about it, neither should be run on tables 
that aren't actually replicated on this node.  Hrmm ... I can probably 
manage to find and fix that.

-- 
"Government big enough to supply everything you need is big enough to
take everything you have ... the course of history shows that as a
government grows, liberty decreases." -- Thomas Jefferson
From andrew.george.hammond at gmail.com  Wed Mar 28 14:01:12 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Wed Mar 28 14:01:24 2007
Subject: [Slony1-general] Re: updated slon-mkservice (and logrep-mkservice
	too)
In-Reply-To: <5a0a9d6f0703161158u63edd393rbbef3389ddd31793@mail.gmail.com>
References: <5a0a9d6f0703161158u63edd393rbbef3389ddd31793@mail.gmail.com>
Message-ID: <5a0a9d6f0703281401k52bee618md8cf6b60c0f34ed8@mail.gmail.com>

On 3/16/07, Andrew Hammond <andrew.george.hammond@gmail.com> wrote:
> On 3/9/07, Andrew Hammond <andrew.george.hammond@gmail.com> wrote:
> > On 3/9/07, Vivek Khera <vivek@khera.org> wrote:
> > >
> > > On Mar 9, 2007, at 12:59 PM, Andrew Hammond wrote:
> > >
> > > > I'll give a +1 on using daemontools to run the slons. It's far clea=
ner
> > > > and easier than anything else I've tried. I've also put some more w=
ork
> > > > into polishing the slon-mkservice.sh script, which I would be happy=
 to
> > > > contribute back if there's any interest.
> > >
> > > absolutely!
> >
> > Ok, I'll spit-n-polish then email my version to you.
>
> Attached. Feedback is very welcome. Yes, I realize it creates
> annoyingly verbose directory names for the services.
>
> logrep is a tool intended to allow efficient real-time filtering of
> log files so that you can pull semantically related information from a
> highly verbose log (ie DEBUG2) and put it all in one place. This is
> very handy if you want to implement automated monitoring scripts for
> stuff like nagios. It relies on the tail -F functionality (tested on
> FreeBSD, OSX, Linux).
>
> I've included a multilog filter for use with logrep that plucks out
> everything related to subscribe as an example. I'll eventually get
> around to writing a nagios monitor that does something clever with
> this log output (like say warn when a slon is doing the initial copy
> so you know not to clobber it). I'd love to see other filters that
> pull out other interesting stuff, like specific error conditions, for
> example.

And now with the debugging and other cleanups...


Andrew
-------------- next part --------------
A non-text attachment was scrubbed...
Name: logrep-mkservice.sh
Type: application/x-sh
Size: 4014 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20070328=
/6bb6e302/logrep-mkservice-0001.sh
-------------- next part --------------
A non-text attachment was scrubbed...
Name: slon-mkservice.sh
Type: application/x-sh
Size: 13273 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20070328=
/6bb6e302/slon-mkservice-0001.sh
From ssinger_pg at sympatico.ca  Wed Mar 28 18:50:21 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Wed Mar 28 18:50:37 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <200703281302.28372@hal.medialogik.com>
References: <200703270949.43495@hal.medialogik.com>
	<Pine.LNX.4.62.0703272037290.2859@mini.atlantida.localdomain>
	<200703281302.28372@hal.medialogik.com>
Message-ID: <Pine.LNX.4.62.0703282127350.2859@mini.atlantida.localdomain>

On Wed, 28 Mar 2007, Alan Hodgson wrote:

>
> Thank you for that. I will test it and consider running with it for now.
> However, I actually think the intended behaviour for 1.2 is correct -
> DDL scripts should execute on all nodes (working around the 1.1
> behaviour was also quite tedious).  It just isn't working correctly ...
> and I can't see why.


I don't think a behaviour change was ever 'intended' for 1.2 (someone 
correct me if I'm wrong) I think it was un unnotice concequence of other 
changes.

For a lot of situtations I think the 1.1.x behaviour is preferrable.  Having 
an execute script run on nodes that don't have it as part of the set will 
mean that you must keep all tables definitions on all nodes. Sometimes thats 
useful to do but othertimes you might not want to do that.  EXECUTE SCRIPT 
can be used for more than just sending DDL around.  There are times when 
people find it useful to put queries that perform INSERT or UPDATE 
operations in an EXECUTE SCRIPT and this type of thing will probably not 
behave as desired if it gets executed on the slaves.

If you want to run your execute script on all nodes you just have to create 
an extra replication set that all nodes subscribe to.  The set doesn't even 
need to have any tables you could then submit your execute script against 
that set and it will execute everywhere.  I don't think I've tried this but 
I don't see why it wouldn't work.  If you try this with 1.2.8 do you still 
get your problems running alterTableForReplication on the slaves? (If so 
then there might be another problem)



Steve

From ahodgson at simkin.ca  Thu Mar 29 08:40:21 2007
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Thu Mar 29 08:40:28 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <Pine.LNX.4.62.0703282127350.2859@mini.atlantida.localdomain>
References: <200703270949.43495@hal.medialogik.com>
	<200703281302.28372@hal.medialogik.com>
	<Pine.LNX.4.62.0703282127350.2859@mini.atlantida.localdomain>
Message-ID: <200703290840.22063@hal.medialogik.com>

On Wednesday 28 March 2007 18:50, Steve Singer <ssinger_pg@sympatico.ca> 
wrote:
> If you want to run your execute script on all nodes you just have to
> create an extra replication set that all nodes subscribe to.  The set
> doesn't even need to have any tables you could then submit your execute
> script against that set and it will execute everywhere.  I don't think
> I've tried this but I don't see why it wouldn't work.  If you try this
> with 1.2.8 do you still get your problems running
> alterTableForReplication on the slaves? (If so then there might be
> another problem)

I'm pretty sure it would still cause a problem.  I've tracked the problem 
down to the following; ddlScript_prepare_int only executes on the node if 
it is subscribed to the set, but ddlScript_complete_int runs on all nodes.  
And both run against every table in sl_table when they do run, which 
appears to be fundamentally broken, because some of those tables may not 
need to be modified on a particular node if it doesn't subscribe to their 
set.  

In the particular problem I posted about, I think what is happening is that 
ddlScript_prepare_int is not running, because the node is not subscribed to 
the set, but the DDL change is being applied anyway and then 
ddlScript_complete_int is running and trying to modify tables back for 
replication that weren't changed by ddlScript_prepare_int, because it 
didn't run.  Does that sound correct?

I can certainly fix the ddlScript_prepare_int and  ddlScript_complete_int 
problems.  Probably in conjunction with your remote_worker patch, that 
would fix the problem completely and also make it so that the changes don't 
execute at all on nodes not subscribed to the set (which I guess I agree 
sounds reasonable).  I'm hoping to get some time this weekend to build a 
better test cluster to try the changes out on.


-- 
Canadian workers spend more of their day working to pay taxes than they do 
to feed, clothe, and house their families.  Why keep voting for politicians
who want to spend even more?

From ahodgson at simkin.ca  Thu Mar 29 14:46:39 2007
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Thu Mar 29 14:46:48 2007
Subject: [Slony1-general] Problem with 1.2.8 and EXECUTE SCRIPT with
	multiple sets
In-Reply-To: <Pine.LNX.4.62.0703272037290.2859@mini.atlantida.localdomain>
References: <200703270949.43495@hal.medialogik.com>
	<Pine.LNX.4.62.0703272037290.2859@mini.atlantida.localdomain>
Message-ID: <200703291446.39520@hal.medialogik.com>

On Tuesday 27 March 2007 17:45, Steve Singer <ssinger_pg@sympatico.ca> 
wrote:
> I don't know what workarounds are going to be options for you compared
> with running a patched version of slony but if you
> need a fix right away you could consider patching your version of slony.
>
> The patched attached to
> http://lists.slony.info/pipermail/slony1-patches/2007-March/000005.html
>
> should address the problem your describing.  I assume it will be
> included with the next 1.2.x release.

The patch referenced above, with the very small fix I posted earlier 
to -patches, completely fixes this problem.  Hopefully it will get reviewed 
and make its way into 1.2.10.

As I suspected, my problem was due to ddlScript_prepare_int not running 
alterTableRestore against tables (because it checked for set membership), 
but ddlScript_complete_int trying to run alterTableForReplication (because 
it doesn't). However, I misunderstood what should be the normal contents of 
sl_table. As it turns out, there is no need for changes to the SQL 
functions if used with Steve's patch. There are some checks in 
ddlScript_prepare_int that are made redundant and could be removed, but 
they no longer hurt anything.

Steve - thank you very much for your help.

-- 
Opportunity is missed by most people because it is dressed in overalls and
looks like work. - Thomas Edison
From cbbrowne at ca.afilias.info  Thu Mar 29 15:24:16 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 29 15:24:40 2007
Subject: [Slony1-general] Re: node ID limitations
In-Reply-To: <20070327172742.35176dc6.wmoran@collaborativefusion.com> (Bill
	Moran's message of "Tue, 27 Mar 2007 17:27:42 -0400")
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
	<20070327172742.35176dc6.wmoran@collaborativefusion.com>
Message-ID: <604po3ofhr.fsf@dba2.int.libertyrms.com>

Bill Moran <wmoran@collaborativefusion.com> writes:
> I'm kind up in the air about this.  I would like to put together a patch
> that will support longer node IDs, but 10^15 seems arbitrary, and anything
> I'd change it to would be arbitrary as well.  The painful thing is that in
> a database where all tables have primary keys, it's not needed anyway.
>
> I expect that not a lot of people are having trouble with this, or there
> would be more discussion about it up till now, but I'm curious if
> anyone has ideas to contribute.

I hadn't noticed this particular issue before this thread pointed it
out.

My reaction is that I *really* dislike TABLE ADD KEY.  It has several
ways (aside from this way that I was unaware of) that it can "bite"
you, and is a sign of poor schema design.

To that end, one of my tasks of this afternoon was, in fact, setting
up a request in our ticketing system to "seek out and destroy" the
(happily fairly few) instances where we have such usage.

Removing TABLE ADD KEY from 1.2 would be unreasonable, but it seems to
me that we might consider deprecating or it (or worse) in 1.3.
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From darcyb at commandprompt.com  Thu Mar 29 15:45:34 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Thu Mar 29 15:45:57 2007
Subject: [Slony1-general] Re: node ID limitations
In-Reply-To: <604po3ofhr.fsf@dba2.int.libertyrms.com>
References: <20070323110526.b7487afa.wmoran@collaborativefusion.com>
	<20070327172742.35176dc6.wmoran@collaborativefusion.com>
	<604po3ofhr.fsf@dba2.int.libertyrms.com>
Message-ID: <200703291545.35016.darcyb@commandprompt.com>

On March 29, 2007 03:24 pm, Christopher Browne wrote:
> Bill Moran <wmoran@collaborativefusion.com> writes:
> > I'm kind up in the air about this.  I would like to put together a patch
> > that will support longer node IDs, but 10^15 seems arbitrary, and
> > anything I'd change it to would be arbitrary as well.  The painful thing
> > is that in a database where all tables have primary keys, it's not needed
> > anyway.
> >
> > I expect that not a lot of people are having trouble with this, or there
> > would be more discussion about it up till now, but I'm curious if
> > anyone has ideas to contribute.
>
> I hadn't noticed this particular issue before this thread pointed it
> out.
>
> My reaction is that I *really* dislike TABLE ADD KEY.  It has several
> ways (aside from this way that I was unaware of) that it can "bite"
> you, and is a sign of poor schema design.
>
> To that end, one of my tasks of this afternoon was, in fact, setting
> up a request in our ticketing system to "seek out and destroy" the
> (happily fairly few) instances where we have such usage.
>
> Removing TABLE ADD KEY from 1.2 would be unreasonable, but it seems to
> me that we might consider deprecating or it (or worse) in 1.3.

In 1.3 I'd vote that we remove the ability to table add key (but support it as 
already present during an upgrade)  but whine loudly during the upgrade 
functions() saying please fix this.  and whine to the slon logs every vacuum 
cycle of the same thing.. 

Then we can drop it all together in a later 1.3, or 1.4


-- 
Darcy Buskermolen
Command Prompt, Inc.
Sales/Support: +1.503.667.4564 || 24x7/Emergency: +1.800.492.2240
PostgreSQL solutions since 1997
http://www.commandprompt.com/
From ahi_public at immerman.org  Thu Mar 29 16:50:45 2007
From: ahi_public at immerman.org (Andrew Immerman)
Date: Thu Mar 29 16:50:27 2007
Subject: [Slony1-general] PostgreSQL/Slony-I: Purging Queued Replication
	Events
Message-ID: <460C50D5.9040508@immerman.org>

Hello,

I'm working on a fairly substantial database with several large tables
(seven orders of magnitude each) replicated.  For the sake of this
description, consider one database master and one slave where table B
references table A with ON DELETE CASCADE specified.  In the course of a
normal day, say 100,000 records are INSERTed/UPDATEd and 90,000 records
are DELETEd from A.  Replication typically works without issue.

Recently, we executed DELETE FROM A WHERE ... on the master.  As
expected, this purged nearly 75% of the recordsets in tables A and B.
Our slave has been unable to maintain synchronization.  The system
completes a nightly VACUUM ANALYZE job on all objects -- this has been
disabled from before the massive DELETE.  Taking the advice of this
list, we attempted a REINDEX of sl_log_1/2, which was unable to complete
without disrupting other services -- eventually the REINDEX was killed.
 sl_log_1/2 currently have 0 and 50M records, respectively; and,
sl_log_2 is growing.

Our current theory is that sl_log_1/2 have become unreasonably
large/bloated.  Our slaves initiate FETCH 100 FROM LOG that execute for
30+ minutes and must occasionally be canceled to prevent service
disruptions.

As tables A and B no longer require replication, we wish to disable
replication of those tables and purge the queued events for those
tables.  To do this, can we do something akin to the following?

1. Disable Slony on both master and slave,

2. Reconfigure Slony to not replicate A or B,

3. On the master: DELETE FROM sl_log_1/2 WHERE sl_table = # AND
log_cmdtype = 'D'

4. On the master: VACUUM FULL ANALYZE sl_log_1/2, REINDEX TABLE
sl_log_1/2, ANALYZE sl_log_1/2,

5. Re-enable replication, and

6. On the slave: DELETE FROM A, VACUUM FULL ANALYZE A/B.

We currently run Linux v2.6.18, PostgreSQL v8.1.4, and Slony-I v1.2.2 on
both master and slave.

Thank you in advance to any assistance you can provide,

-Andrew
From mlists at rp-online.de  Fri Mar 30 05:37:47 2007
From: mlists at rp-online.de (Thomas Pundt)
Date: Fri Mar 30 05:35:18 2007
Subject: [Slony1-general] Database Schema Changes (DDL) not working on
	third node
In-Reply-To: <007601c772c0$01983cb0$0b00a8c0@mpsro.dom>
References: <007601c772c0$01983cb0$0b00a8c0@mpsro.dom>
Message-ID: <200703301437.47230.mlists@rp-online.de>

Hi,

On Friday 30 March 2007 13:38, Forum wrote:
| I have the following scenario:
| DB1 -> is replicated to DB2 - set called replic_back - one server
| DB2 -> is replicated ahead to DB3 - set called replic_loco. - second server
|
| Db2 and db3 are on the same machine(second server).
|
| Every time I make a schema change(alter table etc etc) in DB1 which is
| successfully done in DB2 the replication replic_loco stops to work. The
| schema change does not reach DB3. In the logs I didn't find anything
| suspicios. If I make to replic_loco a unsubscribe and subscribe the data's
| are actually refreshed, but the replication still does not work.

you _have to_ make DDL changes using the slonik command "execute script()"
(see slony documentation for details) - otherwise the replication most 
definitely breaks (as you've discovered), because the Slony trigger signatures
don't match your table layout anymore.

Schema changes can't be automatically distributed because you can't put 
triggers on catalog tables (which would be necessary).

Ciao,
Thomas

-- 
Thomas Pundt <thomas.pundt@rp-online.de> ---- http://rp-online.de/ ----
From frum at ar-sd.net  Fri Mar 30 06:02:32 2007
From: frum at ar-sd.net (Forum)
Date: Fri Mar 30 06:00:21 2007
Subject: [Slony1-general] Database Schema Changes (DDL) not working on
	third node
In-Reply-To: <200703301437.47230.mlists@rp-online.de>
Message-ID: <007b01c772cb$ae0502c0$0b00a8c0@mpsro.dom>

Hi, 

I do the schema change with "execute script()", maybe I have something
misconfigured. 

I tried with this example:

REPLICATION 1: DB a to DB b

cluster name=replic_loco1;
NODE 1 ADMIN CONNINFO = 'dbname=a host=127.0.0.1 user=slony port=5432'; NODE
2 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 user=slony port=5432';

init cluster (id=1, comment='replic_loco Master Node'); store node (id=2,
comment='replic_loco Subscriber Node 1');

STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=a host=127.0.0.1 user=slony
port=5432'); STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=b
host=127.0.0.1 user=slony port=5432');

create set (id=11, origin=1, comment='replic_loco Tables and Sequences');
set add table (id=1, set id=11, origin=1, fully qualified name='public.t1',
comment='replic_loco table public.t1');

subscribe set (id=11, provider=1, receiver=2, forward=yes);

And replication 2: DB b to DB c

cluster name=replic_loco2;
NODE 1 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 user=slony port=5432'; NODE
2 ADMIN CONNINFO = 'dbname=c host=127.0.0.1 user=slony port=5432';

init cluster (id=1, comment='replic_loco Master Node'); store node (id=2,
comment='replic_loco Subscriber Node 1');

STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=b host=127.0.0.1 user=slony
port=5432'); STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=c
host=127.0.0.1 user=slony port=5432');

create set (id=21, origin=1, comment='replic_loco Tables and Sequences');
set add table (id=1, set id=21, origin=1, fully qualified name='public.t1',
comment='replic_loco table public.t1');

subscribe set (id=21, provider=1, receiver=2, forward=yes);



If I change the schema of a, the replication from b to c is stopped. From a
to b works well. 

The "execute script()" does not reach the C table....

I have a feeling that I misconfigured something...............


Regards,
Andy. 

> -----Original Message-----
> From: Thomas Pundt [mailto:mlists@rp-online.de] 
> Sent: Friday, March 30, 2007 3:38 PM
> To: slony1-general@lists.slony.info; Forum
> Subject: Re: [Slony1-general] Database Schema Changes (DDL) 
> not working on third node
> 
> Hi,
> 
> On Friday 30 March 2007 13:38, Forum wrote:
> | I have the following scenario:
> | DB1 -> is replicated to DB2 - set called replic_back - one server
> | DB2 -> is replicated ahead to DB3 - set called replic_loco. 
> - second 
> | server
> |
> | Db2 and db3 are on the same machine(second server).
> |
> | Every time I make a schema change(alter table etc etc) in 
> DB1 which is 
> | successfully done in DB2 the replication replic_loco stops to work. 
> | The schema change does not reach DB3. In the logs I didn't find 
> | anything suspicios. If I make to replic_loco a unsubscribe and 
> | subscribe the data's are actually refreshed, but the 
> replication still does not work.
> 
> you _have to_ make DDL changes using the slonik command 
> "execute script()"
> (see slony documentation for details) - otherwise the 
> replication most definitely breaks (as you've discovered), 
> because the Slony trigger signatures don't match your table 
> layout anymore.
> 
> Schema changes can't be automatically distributed because you 
> can't put triggers on catalog tables (which would be necessary).
> 
> Ciao,
> Thomas
> 
> --
> Thomas Pundt <thomas.pundt@rp-online.de> ---- 
> http://rp-online.de/ ----
> 
> 

From mlists at rp-online.de  Fri Mar 30 06:24:49 2007
From: mlists at rp-online.de (Thomas Pundt)
Date: Fri Mar 30 06:22:17 2007
Subject: [Slony1-general] Database Schema Changes (DDL) not working on
	third node
In-Reply-To: <007b01c772cb$ae0502c0$0b00a8c0@mpsro.dom>
References: <007b01c772cb$ae0502c0$0b00a8c0@mpsro.dom>
Message-ID: <200703301524.49176.mlists@rp-online.de>

Hi,

On Friday 30 March 2007 15:02, Forum wrote:
| I do the schema change with "execute script()", maybe I have something
| misconfigured.

looks suspiciously as if, yes.

| I tried with this example:
|
| REPLICATION 1: DB a to DB b
|
| cluster name=replic_loco1;
| NODE 1 ADMIN CONNINFO = 'dbname=a host=127.0.0.1 user=slony port=5432';
| NODE 2 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 user=slony port=5432';
|
| init cluster (id=1, comment='replic_loco Master Node'); 
| store node (id=2, comment='replic_loco Subscriber Node 1');
|
| STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=a host=127.0.0.1 user=slony 
port=5432'); 
| STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=b host=127.0.0.1 user=slony 
port=5432');
|
| create set (id=11, origin=1, comment='replic_loco Tables and Sequences');
| set add table (id=1, set id=11, origin=1, fully qualified name='public.t1',
| comment='replic_loco table public.t1');
|
| subscribe set (id=11, provider=1, receiver=2, forward=yes);
|
| And replication 2: DB b to DB c
|
| cluster name=replic_loco2;
| NODE 1 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 user=slony port=5432';
| NODE 2 ADMIN CONNINFO = 'dbname=c host=127.0.0.1 user=slony port=5432';
|
| init cluster (id=1, comment='replic_loco Master Node'); 
| store node (id=2, comment='replic_loco Subscriber Node 1');
|
| STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=b host=127.0.0.1 user=slony 
port=5432'); 
| STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=c host=127.0.0.1 user=slony 
port=5432');
|
| create set (id=21, origin=1, comment='replic_loco Tables and Sequences');
| set add table (id=1, set id=21, origin=1, fully qualified name='public.t1',
| comment='replic_loco table public.t1');
|
| subscribe set (id=21, provider=1, receiver=2, forward=yes);

you mean, you have created two clusters for replication that replicate the
same tables? IIRC that won't work.

If all you want is replicating a number of tables to two (slave) nodes, then
you need to create these nodes _in the same cluster_. Simply use something 
like

  store node (id = 3, comment = 'Node 3');

to add a new node to your existing cluster and 

  store path (server=1, client=3, conninfo=...);
  store path (server=3, client=1, conninfo=...);

to store the communication paths. A

  subscribe set (id=1, provider=1, receiver=3, forward=yes);

would start replicating your set to the new node.

Ciao,
Thomas

-- 
Thomas Pundt <thomas.pundt@rp-online.de> ---- http://rp-online.de/ ----
From frum at ar-sd.net  Fri Mar 30 08:08:24 2007
From: frum at ar-sd.net (Forum)
Date: Fri Mar 30 08:06:18 2007
Subject: [Slony1-general] Database Schema Changes (DDL) not working on
	third node
In-Reply-To: <200703301524.49176.mlists@rp-online.de>
Message-ID: <000001c772dd$439e86b0$0b00a8c0@mpsro.dom>

In the real scenarion:

DB a is fully replicated into b. 

And some tables from b are replicated into c. 


Here is the place that fails.... 

I think I am making a confusion about how to configure this.

Should I do only a new set for the second replication (b->c)??

Andy. 

> -----Original Message-----
> From: Thomas Pundt [mailto:mlists@rp-online.de] 
> Sent: Friday, March 30, 2007 4:25 PM
> To: slony1-general@lists.slony.info; Forum
> Subject: Re: [Slony1-general] Database Schema Changes (DDL) 
> not working on third node
> 
> Hi,
> 
> On Friday 30 March 2007 15:02, Forum wrote:
> | I do the schema change with "execute script()", maybe I 
> have something 
> | misconfigured.
> 
> looks suspiciously as if, yes.
> 
> | I tried with this example:
> |
> | REPLICATION 1: DB a to DB b
> |
> | cluster name=replic_loco1;
> | NODE 1 ADMIN CONNINFO = 'dbname=a host=127.0.0.1 user=slony 
> | port=5432'; NODE 2 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 
> | user=slony port=5432';
> |
> | init cluster (id=1, comment='replic_loco Master Node'); store node 
> | (id=2, comment='replic_loco Subscriber Node 1');
> |
> | STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=a host=127.0.0.1 
> | user=slony
> port=5432'); 
> | STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=b host=127.0.0.1 
> | user=slony
> port=5432');
> |
> | create set (id=11, origin=1, comment='replic_loco Tables and 
> | Sequences'); set add table (id=1, set id=11, origin=1, 
> fully qualified 
> | name='public.t1', comment='replic_loco table public.t1');
> |
> | subscribe set (id=11, provider=1, receiver=2, forward=yes);
> |
> | And replication 2: DB b to DB c
> |
> | cluster name=replic_loco2;
> | NODE 1 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 user=slony 
> | port=5432'; NODE 2 ADMIN CONNINFO = 'dbname=c host=127.0.0.1 
> | user=slony port=5432';
> |
> | init cluster (id=1, comment='replic_loco Master Node'); store node 
> | (id=2, comment='replic_loco Subscriber Node 1');
> |
> | STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=b host=127.0.0.1 
> | user=slony
> port=5432'); 
> | STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=c host=127.0.0.1 
> | user=slony
> port=5432');
> |
> | create set (id=21, origin=1, comment='replic_loco Tables and 
> | Sequences'); set add table (id=1, set id=21, origin=1, 
> fully qualified 
> | name='public.t1', comment='replic_loco table public.t1');
> |
> | subscribe set (id=21, provider=1, receiver=2, forward=yes);
> 
> you mean, you have created two clusters for replication that 
> replicate the same tables? IIRC that won't work.
> 
> If all you want is replicating a number of tables to two 
> (slave) nodes, then you need to create these nodes _in the 
> same cluster_. Simply use something like
> 
>   store node (id = 3, comment = 'Node 3');
> 
> to add a new node to your existing cluster and 
> 
>   store path (server=1, client=3, conninfo=...);
>   store path (server=3, client=1, conninfo=...);
> 
> to store the communication paths. A
> 
>   subscribe set (id=1, provider=1, receiver=3, forward=yes);
> 
> would start replicating your set to the new node.
> 
> Ciao,
> Thomas
> 
> --
> Thomas Pundt <thomas.pundt@rp-online.de> ---- 
> http://rp-online.de/ ----
> 
> 

From ajs at crankycanuck.ca  Fri Mar 30 10:17:58 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Mar 30 10:18:27 2007
Subject: [Slony1-general] Database Schema Changes (DDL) not working on
	third node
In-Reply-To: <000001c772dd$439e86b0$0b00a8c0@mpsro.dom>
References: <200703301524.49176.mlists@rp-online.de>
	<000001c772dd$439e86b0$0b00a8c0@mpsro.dom>
Message-ID: <20070330171758.GA22149@phlogiston.dyndns.org>

Ah.  That won't work.  What you need is to break the b->c tables into
a separate set, and then subscribe c only to that set, using b as
the provider.

A

On Fri, Mar 30, 2007 at 06:08:24PM +0300, Forum wrote:
> In the real scenarion:
> 
> DB a is fully replicated into b. 
> 
> And some tables from b are replicated into c. 
> 
> 
> Here is the place that fails.... 
> 
> I think I am making a confusion about how to configure this.
> 
> Should I do only a new set for the second replication (b->c)??
> 
> Andy. 
> 
> > -----Original Message-----
> > From: Thomas Pundt [mailto:mlists@rp-online.de] 
> > Sent: Friday, March 30, 2007 4:25 PM
> > To: slony1-general@lists.slony.info; Forum
> > Subject: Re: [Slony1-general] Database Schema Changes (DDL) 
> > not working on third node
> > 
> > Hi,
> > 
> > On Friday 30 March 2007 15:02, Forum wrote:
> > | I do the schema change with "execute script()", maybe I 
> > have something 
> > | misconfigured.
> > 
> > looks suspiciously as if, yes.
> > 
> > | I tried with this example:
> > |
> > | REPLICATION 1: DB a to DB b
> > |
> > | cluster name=replic_loco1;
> > | NODE 1 ADMIN CONNINFO = 'dbname=a host=127.0.0.1 user=slony 
> > | port=5432'; NODE 2 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 
> > | user=slony port=5432';
> > |
> > | init cluster (id=1, comment='replic_loco Master Node'); store node 
> > | (id=2, comment='replic_loco Subscriber Node 1');
> > |
> > | STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=a host=127.0.0.1 
> > | user=slony
> > port=5432'); 
> > | STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=b host=127.0.0.1 
> > | user=slony
> > port=5432');
> > |
> > | create set (id=11, origin=1, comment='replic_loco Tables and 
> > | Sequences'); set add table (id=1, set id=11, origin=1, 
> > fully qualified 
> > | name='public.t1', comment='replic_loco table public.t1');
> > |
> > | subscribe set (id=11, provider=1, receiver=2, forward=yes);
> > |
> > | And replication 2: DB b to DB c
> > |
> > | cluster name=replic_loco2;
> > | NODE 1 ADMIN CONNINFO = 'dbname=b host=127.0.0.1 user=slony 
> > | port=5432'; NODE 2 ADMIN CONNINFO = 'dbname=c host=127.0.0.1 
> > | user=slony port=5432';
> > |
> > | init cluster (id=1, comment='replic_loco Master Node'); store node 
> > | (id=2, comment='replic_loco Subscriber Node 1');
> > |
> > | STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=b host=127.0.0.1 
> > | user=slony
> > port=5432'); 
> > | STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=c host=127.0.0.1 
> > | user=slony
> > port=5432');
> > |
> > | create set (id=21, origin=1, comment='replic_loco Tables and 
> > | Sequences'); set add table (id=1, set id=21, origin=1, 
> > fully qualified 
> > | name='public.t1', comment='replic_loco table public.t1');
> > |
> > | subscribe set (id=21, provider=1, receiver=2, forward=yes);
> > 
> > you mean, you have created two clusters for replication that 
> > replicate the same tables? IIRC that won't work.
> > 
> > If all you want is replicating a number of tables to two 
> > (slave) nodes, then you need to create these nodes _in the 
> > same cluster_. Simply use something like
> > 
> >   store node (id = 3, comment = 'Node 3');
> > 
> > to add a new node to your existing cluster and 
> > 
> >   store path (server=1, client=3, conninfo=...);
> >   store path (server=3, client=1, conninfo=...);
> > 
> > to store the communication paths. A
> > 
> >   subscribe set (id=1, provider=1, receiver=3, forward=yes);
> > 
> > would start replicating your set to the new node.
> > 
> > Ciao,
> > Thomas
> > 
> > --
> > Thomas Pundt <thomas.pundt@rp-online.de> ---- 
> > http://rp-online.de/ ----
> > 
> > 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
"The year's penultimate month" is not in truth a good way of saying
November.
		--H.W. Fowler
From cbbrowne at ca.afilias.info  Fri Mar 30 14:36:50 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar 30 14:37:03 2007
Subject: [Slony1-general] PostgreSQL/Slony-I: Purging Queued Replication
	Events
In-Reply-To: <460C50D5.9040508@immerman.org> (Andrew Immerman's message of
	"Thu, 29 Mar 2007 16:50:45 -0700")
References: <460C50D5.9040508@immerman.org>
Message-ID: <60zm5umn0t.fsf@dba2.int.libertyrms.com>

Andrew Immerman <ahi_public@immerman.org> writes:

> Hello,
>
> I'm working on a fairly substantial database with several large tables
> (seven orders of magnitude each) replicated.  For the sake of this
> description, consider one database master and one slave where table B
> references table A with ON DELETE CASCADE specified.  In the course of a
> normal day, say 100,000 records are INSERTed/UPDATEd and 90,000 records
> are DELETEd from A.  Replication typically works without issue.
>
> Recently, we executed DELETE FROM A WHERE ... on the master.  As
> expected, this purged nearly 75% of the recordsets in tables A and B.
> Our slave has been unable to maintain synchronization.  The system
> completes a nightly VACUUM ANALYZE job on all objects -- this has been
> disabled from before the massive DELETE.  Taking the advice of this
> list, we attempted a REINDEX of sl_log_1/2, which was unable to complete
> without disrupting other services -- eventually the REINDEX was killed.
>  sl_log_1/2 currently have 0 and 50M records, respectively; and,
> sl_log_2 is growing.
>
> Our current theory is that sl_log_1/2 have become unreasonably
> large/bloated.  Our slaves initiate FETCH 100 FROM LOG that execute for
> 30+ minutes and must occasionally be canceled to prevent service
> disruptions.
>
> As tables A and B no longer require replication, we wish to disable
> replication of those tables and purge the queued events for those
> tables.  To do this, can we do something akin to the following?
>
> 1. Disable Slony on both master and slave,

I presume you mean that you'll stop the slon processes for a little
while.  And that step #5 is that you restart the slons.  Seems right.

> 2. Reconfigure Slony to not replicate A or B,

> 3. On the master: DELETE FROM sl_log_1/2 WHERE sl_table = # AND
> log_cmdtype = 'D'

Hmm.  I think I'd want to do this in something of reverse order.

That is...

- On master: delete from sl_log_* where sl_table = [table ID#]

- *Then*, on each node: delete from sl_table where tab_id = [table ID#].

- Third:  on each node: select _slony_cluster.alterTableRestore([table ID#])

I don't think you can have the "and log_cmdtype = 'D' clause; that
would leave the table partly in replication and partly not.

If you want to get fresh data onto the subscriber, you're going to
need to do a pg_dump -t on the table; you can't safely expect to drop
the table out solely for the purposes of the big delete.

> 4. On the master: VACUUM FULL ANALYZE sl_log_1/2, REINDEX TABLE
> sl_log_1/2, ANALYZE sl_log_1/2,

In principle, you can handle this in one swell foop via:
   cluster [some index name] on sl_log_1;
   cluster [some index name] on sl_log_2;

That does the cleanup and reindex all in one.  It's not MVCC-safe, but
the slons are down, so there's nothing during this maintenance for
them to see, and they're the only apps querying sl_log_[12].

And actually, this step isn't particularly vital.  The log tables will
have gaping empty spaces, but the indexes should allow access to be
reasonably efficient short term even though they're bloated.  And when
Slony-I rotates thru the log tables, it'll do a TRUNCATE and properly
scrub them out over the next few days.

> 5. Re-enable replication, and

e.g. - restart slons...  Yeah, that seems appropriate.

> 6. On the slave: DELETE FROM A, VACUUM FULL ANALYZE A/B.

If you're deleting ALL the data, I'd use TRUNCATE, as that leaves the
table scrubbed clean.

> We currently run Linux v2.6.18, PostgreSQL v8.1.4, and Slony-I v1.2.2 on
> both master and slave.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
