From tmblue at gmail.com  Thu Aug 13 15:51:31 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 13 Aug 2015 15:51:31 -0700
Subject: [Slony1-general] Slony multiple locations,
	questions concerning node ID's
Message-ID: <CAEaSS0YmauW0FBmYARGwAKrDjX+3pTMyxCuxqNLRjoDRMB7eeg@mail.gmail.com>

Been a bit, but I'm back with a question.

I'm looking at doing a migration from one site to another and eventually
have a hot warm site.

Right now I have a 5 box slony cluster at site A

Site A: Node 1 and 2 can switch/fail, nodes 3-5 are partial and can't
become master.

Node 1: MASTER
Node 2: SLAVE
Node 3: QUERYSLAVE1
Node 4: QUERYSLAVE2
Node 5: QUERYSLAVE3

I have a new 5 node cluster in Site B.

Site B:  This is our default turn up, so they have the same NODE ID as Site
A..

Node 1: MASTER
Node 2: SLAVE
Node 3: QUERYSLAVE1
Node 4: QUERYSLAVE2
Node 5: QUERYSLAVE3

I was really interested in Node 2 of Site A replicating to Node 1 of Site
B, and then just let the other boxen in site B take their information from
Node 1 of the same site.

 I think there is going to be some confusion with NODE ID's. I believe I
can tell Site A that the Master of Site B is actually node 10 <whatever>
and get it replicating and be able to drop NODE 10 if I needed from site A.
However if I end up switching to the Master in Site B,  It won't know how
to tell SITE A to kiss off,  as the NODE's in it's config will be
1,2,3,4,5, which are the same as Site A, so there would be no real way for
me to clean up and or remove SITE A devices, drop node using NODE ID would
be no beuno.

So I guess basically, do I have to have unique NODE ID's (think I do, but
if not??). It will just require some rework for every site I bring up,
 this means I have a few different configurations and or scripts to use
depending on what site the cluster is in.

Is this sort of clear, murky yet you have the general idea, or holy carp
Tory what are you asking?

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150813/03237a07/attachment.htm 

From tmblue at gmail.com  Fri Aug 14 14:16:52 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 14 Aug 2015 14:16:52 -0700
Subject: [Slony1-general] Slony multiple locations,
	questions concerning node ID's
In-Reply-To: <CAEaSS0YmauW0FBmYARGwAKrDjX+3pTMyxCuxqNLRjoDRMB7eeg@mail.gmail.com>
References: <CAEaSS0YmauW0FBmYARGwAKrDjX+3pTMyxCuxqNLRjoDRMB7eeg@mail.gmail.com>
Message-ID: <CAEaSS0ZFkj6NWEZskgjAq8g5gabHoQF6EtRLXZb19_jxjmnAuA@mail.gmail.com>

On Thu, Aug 13, 2015 at 3:51 PM, Tory M Blue <tmblue at gmail.com> wrote:

>
> Been a bit, but I'm back with a question.
>
> I'm looking at doing a migration from one site to another and eventually
> have a hot warm site.
>
> Right now I have a 5 box slony cluster at site A
>
> Site A: Node 1 and 2 can switch/fail, nodes 3-5 are partial and can't
> become master.
>
> Node 1: MASTER
> Node 2: SLAVE
> Node 3: QUERYSLAVE1
> Node 4: QUERYSLAVE2
> Node 5: QUERYSLAVE3
>
> I have a new 5 node cluster in Site B.
>
> Site B:  This is our default turn up, so they have the same NODE ID as
> Site A..
>
> Node 1: MASTER
> Node 2: SLAVE
> Node 3: QUERYSLAVE1
> Node 4: QUERYSLAVE2
> Node 5: QUERYSLAVE3
>
> I was really interested in Node 2 of Site A replicating to Node 1 of Site
> B, and then just let the other boxen in site B take their information from
> Node 1 of the same site.
>
>  I think there is going to be some confusion with NODE ID's. I believe I
> can tell Site A that the Master of Site B is actually node 10 <whatever>
> and get it replicating and be able to drop NODE 10 if I needed from site A.
> However if I end up switching to the Master in Site B,  It won't know how
> to tell SITE A to kiss off,  as the NODE's in it's config will be
> 1,2,3,4,5, which are the same as Site A, so there would be no real way for
> me to clean up and or remove SITE A devices, drop node using NODE ID would
> be no beuno.
>
> So I guess basically, do I have to have unique NODE ID's (think I do, but
> if not??). It will just require some rework for every site I bring up,
>  this means I have a few different configurations and or scripts to use
> depending on what site the cluster is in.
>
> Is this sort of clear, murky yet you have the general idea, or holy carp
> Tory what are you asking?
>
> Thanks
> Tory
>


Okay so someone pinged me off list and stated yes NODE ID's have to be
unique, okay figured but that's good to know.

Now I'm wondering how to best setup the replication.

If Site A has

(Nodes 1-5)
MASTER - Replicates to the 4 servers below
SLAVE
QUERYSLAVE1,2,3

Site B has similar

(Nodes 11-15)
DRMASTER - Replicates to the 4 servers below.
DRSLAVE
DRQUERYSLAVE1,2,3

I was just thinking I would add DRMASTER as a slave off of SLAVE. Since
SLAVE has forward = yes, it could easily forward the data on to DRMASTER
and DRMASTER could in turn send all that DATA down it's pipe to
DRSLAVE/DRQUERYS*

I've run into a couple of issues and that is that it seems that all nodes
in site A become aware of DRMASTER from site B, and not sure I want that
and/or if it's possible to get around that.

The paths should be in site A

Master <-> Slave (set 1-3)
Master <-> Queryslave* (set 1)
Slave <-> Queryslave* (set 1) (due to switchover possibility).

paths in site B

DrMaster <-> DRSlave (set 1-3)
DrMaster <-> DRQueryslave* (set 1)
DrSlave <-> DRQueryslave* (set 1) (due to switchover possibility).


Then I would add a new path

Slave <-> DrMaster (set 1-3)

I'm wondering if I'm looking at this right. These are 2 separate clusters
but the data has to be in sync and I'm looking to use this as a site
migration tool.

BTW we have been using slony for over 9 years ,so I'm not totally new to
it, but it's always been isolated into it's own cluster (prod, staging, Qa,
Dev etc), now I'm looking to use it as a migration mechanism between 2
clusters at differing locations.

Thanks and sorry for being dense
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150814/79f9d8ae/attachment.htm 

From steve at ssinger.info  Fri Aug 14 19:05:08 2015
From: steve at ssinger.info (Steve Singer)
Date: Fri, 14 Aug 2015 22:05:08 -0400
Subject: [Slony1-general] Slony multiple locations,
 questions concerning node ID's
In-Reply-To: <CAEaSS0ZFkj6NWEZskgjAq8g5gabHoQF6EtRLXZb19_jxjmnAuA@mail.gmail.com>
References: <CAEaSS0YmauW0FBmYARGwAKrDjX+3pTMyxCuxqNLRjoDRMB7eeg@mail.gmail.com>
	<CAEaSS0ZFkj6NWEZskgjAq8g5gabHoQF6EtRLXZb19_jxjmnAuA@mail.gmail.com>
Message-ID: <BLU436-SMTP35004927BAA37C7198DF62DC7B0@phx.gbl>

On Fri, 14 Aug 2015, Tory M Blue wrote:

> Okay so someone pinged me off list and stated yes NODE ID's have to be unique, okay figured but that's good to know.
> 
> Now I'm wondering how to best setup the replication.
> 
> If Site A has?
> 
> (Nodes 1-5)
> MASTER - Replicates to the 4 servers below
> SLAVE
> QUERYSLAVE1,2,3
> 
> Site B has similar
> 
> (Nodes 11-15)
> DRMASTER - Replicates to the 4 servers below.
> DRSLAVE
> DRQUERYSLAVE1,2,3
> 
> I was just thinking I would add DRMASTER as a slave off of SLAVE. Since SLAVE has forward = yes, it could easily
> forward the data on to DRMASTER and DRMASTER could in turn send all that DATA down it's pipe to DRSLAVE/DRQUERYS*
> 
> I've run into a couple of issues and that is that it seems that all nodes in site A become aware of DRMASTER from
> site B, and not sure I want that and/or if it's possible to get around that. ?

I think you need to be a bit more careful about your terminology and set + 
node numbering, I am not exactly sure what your describing.

All nodes in a cluster need to be aware of all other nodes.  If your going 
to have 1 slony cluster that spans both sites you need to have unique node 
numbers and all nodes need to be 'aware' of other nodes.  All nodes in your 
cluster need to confirm all events from all other nodes, even if the node 
isn't subscribed to anything from some other node.  It's a bit chatty but I 
don't think this is going to be a problem for 12 nodes.

What I don't understand though is what you mean below by '2 separate 
clusters' and that your re-using 'set 1', 'set 1' can only have 1 node of an 
origin.

Is what you really mean something like this

Site 1
Node 11: Master for set 1.  Replica for set 2
Node 12: replica for set 1
Node 1[3,4,5]: Query slave replicas for set 1

Site 2:
Node 21: Master for set 2, replica for set 1
Node 22: Replica for set 2
Node 2[3,4,5]: Query slave replicas for set 2

In the above setup there can't be any overlap between the tables in set 1 
and the tables in set 2.  What I describe above is a single slony cluster.

If what you really means is having 2 slony clusters that are somehow 
connected then we aren't talking about the same thing.  If you can make 
that work (and I think some people on the list have done that type of thing) 
then you would need some nodes to be part of both clusters.


> 
> The paths should be in site A
> 
> Master <-> Slave (set 1-3)
> Master <-> Queryslave* (set 1)
> Slave <-> Queryslave* (set 1) (due to switchover possibility).
> 
> paths in site B
> 
> DrMaster <-> DRSlave (set 1-3)
> DrMaster <-> DRQueryslave* (set 1)
> DrSlave <-> DRQueryslave* (set 1) (due to switchover possibility).
> 
> 
> Then I would add a new path
> 
> Slave <-> DrMaster (set 1-3)
> 
> I'm wondering if I'm looking at this right. These are 2 separate clusters but the data has to be in sync and I'm
> looking to use this as a site migration tool.
> 
> BTW we have been using slony for over 9 years ,so I'm not totally new to it, but it's always been isolated into it's
> own cluster (prod, staging, Qa, Dev etc), now I'm looking to use it as a migration mechanism between 2 clusters at
> differing locations.
> 
> Thanks and sorry for being dense
> Tory
> 
> 
> 
>

From tmblue at gmail.com  Fri Aug 14 22:02:23 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 14 Aug 2015 22:02:23 -0700
Subject: [Slony1-general] Slony multiple locations,
 questions concerning node ID's
In-Reply-To: <BLU436-SMTP35004927BAA37C7198DF62DC7B0@phx.gbl>
References: <CAEaSS0YmauW0FBmYARGwAKrDjX+3pTMyxCuxqNLRjoDRMB7eeg@mail.gmail.com>
	<CAEaSS0ZFkj6NWEZskgjAq8g5gabHoQF6EtRLXZb19_jxjmnAuA@mail.gmail.com>
	<BLU436-SMTP35004927BAA37C7198DF62DC7B0@phx.gbl>
Message-ID: <CAEaSS0ZM9ncBYwQB13r5TEoYw40czvkNCtYAoeozUMfA3gPN5g@mail.gmail.com>

On Fri, Aug 14, 2015 at 7:05 PM, Steve Singer <steve at ssinger.info> wrote:

> On Fri, 14 Aug 2015, Tory M Blue wrote:
>
> Okay so someone pinged me off list and stated yes NODE ID's have to be
>> unique, okay figured but that's good to know.
>>
>> Now I'm wondering how to best setup the replication.
>>
>> If Site A has
>>
>> (Nodes 1-5)
>> MASTER - Replicates to the 4 servers below
>> SLAVE
>> QUERYSLAVE1,2,3
>>
>> Site B has similar
>>
>> (Nodes 11-15)
>> DRMASTER - Replicates to the 4 servers below.
>> DRSLAVE
>> DRQUERYSLAVE1,2,3
>>
>> I was just thinking I would add DRMASTER as a slave off of SLAVE. Since
>> SLAVE has forward = yes, it could easily
>> forward the data on to DRMASTER and DRMASTER could in turn send all that
>> DATA down it's pipe to DRSLAVE/DRQUERYS*
>>
>> I've run into a couple of issues and that is that it seems that all nodes
>> in site A become aware of DRMASTER from
>> site B, and not sure I want that and/or if it's possible to get around
>> that.
>>
>
> I think you need to be a bit more careful about your terminology and set +
> node numbering, I am not exactly sure what your describing.
>
> There is no doubt, and I appreciate you attempting :)



> All nodes in a cluster need to be aware of all other nodes.  If your going
> to have 1 slony cluster that spans both sites you need to have unique node
> numbers and all nodes need to be 'aware' of other nodes.  All nodes in your
> cluster need to confirm all events from all other nodes, even if the node
> isn't subscribed to anything from some other node.  It's a bit chatty but I
> don't think this is going to be a problem for 12 nodes.
>
> What I don't understand though is what you mean below by '2 separate
> clusters' and that your re-using 'set 1', 'set 1' can only have 1 node of
> an origin.
>
> Is what you really mean something like this
>
> Site 1
> Node 11: Master for set 1.  Replica for set 2
> Node 12: replica for set 1
> Node 1[3,4,5]: Query slave replicas for set 1
>
> Site 2:
> Node 21: Master for set 2, replica for set 1
> Node 22: Replica for set 2
> Node 2[3,4,5]: Query slave replicas for set 2
>
> In the above setup there can't be any overlap between the tables in set 1
> and the tables in set 2.  What I describe above is a single slony cluster.
>
> If what you really means is having 2 slony clusters that are somehow
> connected then we aren't talking about the same thing.  If you can make
> that work (and I think some people on the list have done that type of
> thing) then you would need some nodes to be part of both clusters.
>
>
>>
Thanks again Steve. I am not trying to establish a 10+ node cluster. What I
am trying to do is establish a relationship between 2 clusters.

Set 1-3 are unique sets tied to a single Origin yes, but they are available
to read from the slaves.

What i'm trying to do , figure out (without the required vocabulary
obviously), is to in fact create a fail over scenario

There is a slon document that uses this image.

http://slony.info/documentation/concepts.html



However in my failed description, I was trying to free up the origin from
having to talk to all 10 nodes, but instead offload some of that chatter
from the Slave to site B's "would be Master and let site B's ("Master)
handle the replication to nodes 12-15  (there would never be a need to
switchover from site A  to any other node in Site B (but node 11).

So Site A would be aware of Nodes 1-5 (1 being the insert origin) and node
11 (which is in Site B configured as a slave).  Site B nodes would only
know that they are talking to node 11 as their origin, other than node 11,
node 12-15 would only know about Node 11-15 and nothing about Nodes 1-5.

UUGH improper terminology I'm sure.. It's probably more frustrating trying
to decipher more than it is me trying to explain.

If I had 2 circles the only box that would touch both circles would be node
11, where node 1-5 are in their own  bubble, and nodes 12-15 in their own.
with the exception of node 11 being in both circles.

man.. hopefully I'm doing somewhat of a better job explaining.
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150814/59714a43/attachment.htm 

From steve at ssinger.info  Sat Aug 15 09:41:33 2015
From: steve at ssinger.info (Steve Singer)
Date: Sat, 15 Aug 2015 12:41:33 -0400
Subject: [Slony1-general] Slony multiple locations,
 questions concerning node ID's
In-Reply-To: <CAEaSS0ZM9ncBYwQB13r5TEoYw40czvkNCtYAoeozUMfA3gPN5g@mail.gmail.com>
References: <CAEaSS0YmauW0FBmYARGwAKrDjX+3pTMyxCuxqNLRjoDRMB7eeg@mail.gmail.com>
	<CAEaSS0ZFkj6NWEZskgjAq8g5gabHoQF6EtRLXZb19_jxjmnAuA@mail.gmail.com>
	<BLU436-SMTP35004927BAA37C7198DF62DC7B0@phx.gbl>
	<CAEaSS0ZM9ncBYwQB13r5TEoYw40czvkNCtYAoeozUMfA3gPN5g@mail.gmail.com>
Message-ID: <BLU437-SMTP79A3C9FAE122EFFC1C5D0DC7B0@phx.gbl>

On Fri, 14 Aug 2015, Tory M Blue wrote:

> Thanks again Steve. I am not trying to establish a 10+ node cluster. What I am trying to do is establish a
> relationship between 2 clusters. ?
>

Slony doesn't have any direct support for relationships between two slony 
clusters.  The failover command requires that all nodes be part of the same 
cluster.   A slony cluster can span as many networks and data centers as you 
want (at some point you will start to hit performance limitations).

> Set 1-3 are unique sets tied to a single Origin yes, but they are available to read from the slaves.
> 
> What i'm trying to do , figure out (without the required vocabulary obviously), is to in fact create a fail over
> scenario
> 
> There is a slon document that uses this image.
> 
> http://slony.info/documentation/concepts.html
> 
> [complexenv.png]
> 
> However in my failed description, I was trying to free up the origin from having to talk to all 10 nodes, but instead
> offload some of that chatter from the Slave to site B's "would be Master and let site B's ("Master) handle the
> replication to nodes 12-15 ?(there would never be a need to switchover from site A ?to any other node in Site B (but
> node 11).
> 
> So Site A would be aware of Nodes 1-5 (1 being the insert origin) and node 11 (which is in Site B configured as a
> slave).? Site B nodes would only know that they are talking to node 11 as their origin, other than node 11, node
> 12-15 would only know about Node 11-15 and nothing about Nodes 1-5.
> 
> UUGH improper terminology I'm sure.. It's probably more frustrating trying to decipher more than it is me trying to
> explain.
> 
> If I had 2 circles the only box that would touch both circles would be node 11, where node 1-5 are in their own
> ?bubble, and nodes 12-15 in their own. with the exception of node 11 being in both circles.
>

You can have 1 slony cluster with a configuration like

  11--->12
  |
  ----->13
  |
  ------>14
  |
  ------>15
  |
  |
  |
  V
  21---->22
  |
  ------>23
  |
  ------>24
  |
  ------>25


What I describe above is a common slony configuration for cascading sets 
from a single origin to replicas in a different data center.




  > man.. hopefully I'm doing somewhat of a better job explaining.



> Tory
> 
>

From davecramer at gmail.com  Sun Aug 23 14:56:34 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Sun, 23 Aug 2015 17:56:34 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
Message-ID: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>

Drop node always wants to connect to the node ??

Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150823/fa309d83/attachment.htm 

From mike.james at clutch.com  Mon Aug 24 06:19:47 2015
From: mike.james at clutch.com (Mike James)
Date: Mon, 24 Aug 2015 09:19:47 -0400
Subject: [Slony1-general] Slony replication failed invalid memory alloc
	request size
Message-ID: <CADeyJsUPMC3GogRi=FjxZcg7f1pb=uKL02Z8rocT2Bs-dAWAbQ@mail.gmail.com>

We have a slony slave in AWS. During an Amazon maintenance, the instance
was rebooted and now the daemon won't start. The log file has this error
message:


2015-08-24 06:50:03 UTC INFO   remoteWorkerThread_1: syncing set 1 with 102
table(s) from provider 1
2015-08-24 06:50:33 UTC ERROR  remoteWorkerThread_1_1: error at end of COPY
IN: ERROR:  invalid memory alloc request size 1970234207
CONTEXT:  COPY sl_log_1, line 97033:

I'm not sure whether this is a Postgres error or a slony error. We're
running Postgres 9.3.5 and slony 2.2.1. Any help much appreciated.

Mike
  [image: Clutch Holdings, LLC] <http://www.clutch.com> Mike James |
Manager of Infrastructure
267.419.6400, ext 204 | mike.james at clutch.com201 S Maple St. | Suite 250 |
Ambler, PA 19002
Clutch.com <http://www.clutch.com> | Twitter
<https://twitter.com/clutchsuccess> | LinkedIn
<https://www.linkedin.com/company/2837209> | YouTube
<https://www.youtube.com/user/clutchsuccess> | Clutch Support Center
<http://clientsupport.clutch.com/> The only end to end consumer management
platform that empowers consumer-focused businesses to identify, target,
message, and engage their best customers.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/fa458215/attachment.htm 

From ssinger at ca.afilias.info  Mon Aug 24 07:26:03 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 10:26:03 -0400
Subject: [Slony1-general] Slony replication failed invalid memory alloc
 request size
In-Reply-To: <CADeyJsUPMC3GogRi=FjxZcg7f1pb=uKL02Z8rocT2Bs-dAWAbQ@mail.gmail.com>
References: <CADeyJsUPMC3GogRi=FjxZcg7f1pb=uKL02Z8rocT2Bs-dAWAbQ@mail.gmail.com>
Message-ID: <55DB297B.6040507@ca.afilias.info>

On 08/24/2015 09:19 AM, Mike James wrote:
> We have a slony slave in AWS. During an Amazon maintenance, the instance
> was rebooted and now the daemon won't start. The log file has this error
> message:
>
>
> 2015-08-24 06:50:03 UTC INFO   remoteWorkerThread_1: syncing set 1 with
> 102 table(s) from provider 1
> 2015-08-24 06:50:33 UTC ERROR  remoteWorkerThread_1_1: error at end of
> COPY IN: ERROR:  invalid memory alloc request size 1970234207
> CONTEXT:  COPY sl_log_1, line 97033:
>
> I'm not sure whether this is a Postgres error or a slony error. We're
> running Postgres 9.3.5 and slony 2.2.1. Any help much appreciated.


Try upgrading to slony 2.2.4

This MIGHT be bug http://www.slony.info/bugzilla/show_bug.cgi?id=327

Memory corruption bugs sometimes manifest themselves with slightly 
different symptoms.


>
> Mike
> Clutch Holdings, LLC <http://www.clutch.com>		Mike James | Manager of
> Infrastructure
> 267.419.6400, ext 204 | mike.james at clutch.com <mailto:mike.james at clutch.com>
> 201 S Maple St. | Suite 250 | Ambler, PA 19002
> Clutch.com <http://www.clutch.com> | Twitter
> <https://twitter.com/clutchsuccess> | LinkedIn
> <https://www.linkedin.com/company/2837209> | YouTube
> <https://www.youtube.com/user/clutchsuccess> | Clutch Support Center
> <http://clientsupport.clutch.com/>
>
> The only end to end consumer management platform that empowers
> consumer-focused businesses to identify, target, message, and engage
> their best customers.
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From mike.james at clutch.com  Mon Aug 24 08:16:58 2015
From: mike.james at clutch.com (Mike James)
Date: Mon, 24 Aug 2015 11:16:58 -0400
Subject: [Slony1-general] Slony replication failed invalid memory alloc
 request size
In-Reply-To: <55DB297B.6040507@ca.afilias.info>
References: <CADeyJsUPMC3GogRi=FjxZcg7f1pb=uKL02Z8rocT2Bs-dAWAbQ@mail.gmail.com>
	<55DB297B.6040507@ca.afilias.info>
Message-ID: <CADeyJsU6Q8KXQV8kJ9kT3jP8u=ZtaAJXhk_+CHvEa6WTY_GFJQ@mail.gmail.com>

I *think* the problem is that Amazon rebooted our instance and it was not a
clean restart. In my mind, it's possible memory got corrupted.

The postgresql.log file shows this:

2015-08-24 06:48:31 UTC ERROR:  invalid memory alloc request size 1970234207
2015-08-24 06:48:31 UTC CONTEXT:  COPY sl_log_1, line 97033: "1 310825709
    20      120889769       public  customers       U       11
 {first_name,ABE,last_name,VIGODA,address_1,"1313 MO..."
2015-08-24 06:48:31 UTC STATEMENT:  COPY "_clients_cluster"."sl_log_1" (
log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname,
log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN

  [image: Clutch Holdings, LLC] <http://www.clutch.com> Mike James |
Manager of Infrastructure
267.419.6400, ext 204 | mike.james at clutch.com201 S Maple St. | Suite 250 |
Ambler, PA 19002
Clutch.com <http://www.clutch.com> | Twitter
<https://twitter.com/clutchsuccess> | LinkedIn
<https://www.linkedin.com/company/2837209> | YouTube
<https://www.youtube.com/user/clutchsuccess> | Clutch Support Center
<http://clientsupport.clutch.com/> The only end to end consumer management
platform that empowers consumer-focused businesses to identify, target,
message, and engage their best customers.

On Mon, Aug 24, 2015 at 10:26 AM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 08/24/2015 09:19 AM, Mike James wrote:
>
>> We have a slony slave in AWS. During an Amazon maintenance, the instance
>> was rebooted and now the daemon won't start. The log file has this error
>> message:
>>
>>
>> 2015-08-24 06:50:03 UTC INFO   remoteWorkerThread_1: syncing set 1 with
>> 102 table(s) from provider 1
>> 2015-08-24 06:50:33 UTC ERROR  remoteWorkerThread_1_1: error at end of
>> COPY IN: ERROR:  invalid memory alloc request size 1970234207
>> CONTEXT:  COPY sl_log_1, line 97033:
>>
>> I'm not sure whether this is a Postgres error or a slony error. We're
>> running Postgres 9.3.5 and slony 2.2.1. Any help much appreciated.
>>
>
>
> Try upgrading to slony 2.2.4
>
> This MIGHT be bug http://www.slony.info/bugzilla/show_bug.cgi?id=327
>
> Memory corruption bugs sometimes manifest themselves with slightly
> different symptoms.
>
>
>
>> Mike
>> Clutch Holdings, LLC <http://www.clutch.com>            Mike James |
>> Manager of
>> Infrastructure
>> 267.419.6400, ext 204 | mike.james at clutch.com <mailto:
>> mike.james at clutch.com>
>> 201 S Maple St. | Suite 250 | Ambler, PA 19002
>> Clutch.com <http://www.clutch.com> | Twitter
>> <https://twitter.com/clutchsuccess> | LinkedIn
>> <https://www.linkedin.com/company/2837209> | YouTube
>> <https://www.youtube.com/user/clutchsuccess> | Clutch Support Center
>> <http://clientsupport.clutch.com/>
>>
>> The only end to end consumer management platform that empowers
>> consumer-focused businesses to identify, target, message, and engage
>> their best customers.
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/91ff7c37/attachment.htm 

From greg at endpoint.com  Mon Aug 24 11:19:30 2015
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Mon, 24 Aug 2015 14:19:30 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
Message-ID: <20150824181929.GQ21370@broken.home>

On Sun, Aug 23, 2015 at 05:56:34PM -0400, Dave Cramer wrote:
> Drop node always wants to connect to the node ??

Yes: it needs to clean up all the Slony pieces (especially triggers) 
on the tables in the node in question. This is done by having drop 
node call uninstall node.

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: Digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150824/fd129e68/attachment-0001.pgp 

From davecramer at gmail.com  Mon Aug 24 11:20:30 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 14:20:30 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <20150824181929.GQ21370@broken.home>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
Message-ID: <CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>

Yes, but the node is no longer physically there ??
Even worse it is a provider for another node

Dave Cramer

On 24 August 2015 at 14:19, Greg Sabino Mullane <greg at endpoint.com> wrote:

> On Sun, Aug 23, 2015 at 05:56:34PM -0400, Dave Cramer wrote:
> > Drop node always wants to connect to the node ??
>
> Yes: it needs to clean up all the Slony pieces (especially triggers)
> on the tables in the node in question. This is done by having drop
> node call uninstall node.
>
> --
> Greg Sabino Mullane greg at endpoint.com
> End Point Corporation
> PGP Key: 0x14964AC8
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/2dcfa7ef/attachment.htm 

From ssinger at ca.afilias.info  Mon Aug 24 11:52:32 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 14:52:32 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
Message-ID: <55DB67F0.9010902@ca.afilias.info>

On 08/24/2015 02:20 PM, Dave Cramer wrote:
> Yes, but the node is no longer physically there ??
> Even worse it is a provider for another node
>

I think you can leave out the admin conninfo for the node being dropped 
and slonik will skip the uinstall step.

(This is from memory,so I might be confused)

You can use the resubscribe command to point the other nodes at a new 
provider.


> Dave Cramer
>
> On 24 August 2015 at 14:19, Greg Sabino Mullane <greg at endpoint.com
> <mailto:greg at endpoint.com>> wrote:
>
>     On Sun, Aug 23, 2015 at 05:56:34PM -0400, Dave Cramer wrote:
>     > Drop node always wants to connect to the node ??
>
>     Yes: it needs to clean up all the Slony pieces (especially triggers)
>     on the tables in the node in question. This is done by having drop
>     node call uninstall node.
>
>     --
>     Greg Sabino Mullane greg at endpoint.com <mailto:greg at endpoint.com>
>     End Point Corporation
>     PGP Key: 0x14964AC8
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From scott.marlowe at gmail.com  Mon Aug 24 12:38:47 2015
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Mon, 24 Aug 2015 13:38:47 -0600
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB67F0.9010902@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
Message-ID: <CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>

Note that the node will still show up in sl_nodes and sl_status for a
while, until slony does a cleanup event / log switch (can't remember
which right now). This is normal. Don't freak out.

From davecramer at gmail.com  Mon Aug 24 13:12:54 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 16:12:54 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
Message-ID: <CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>

I am currently waiting on a resubscribe event. How does slony get this
event through all the backlog events ?

Dave Cramer

On 24 August 2015 at 15:38, Scott Marlowe <scott.marlowe at gmail.com> wrote:

> Note that the node will still show up in sl_nodes and sl_status for a
> while, until slony does a cleanup event / log switch (can't remember
> which right now). This is normal. Don't freak out.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/63b5d10e/attachment.htm 

From ssinger at ca.afilias.info  Mon Aug 24 13:57:07 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 16:57:07 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
Message-ID: <55DB8523.3050600@ca.afilias.info>

On 08/24/2015 04:12 PM, Dave Cramer wrote:
> I am currently waiting on a resubscribe event. How does slony get this
> event through all the backlog events ?
>

Usually pretty quickly.
Slonik waits for the provider to be caught up before submitting the event.

The question is, why is your provider behind? Is it getting caught up?


> Dave Cramer
>
> On 24 August 2015 at 15:38, Scott Marlowe <scott.marlowe at gmail.com
> <mailto:scott.marlowe at gmail.com>> wrote:
>
>     Note that the node will still show up in sl_nodes and sl_status for a
>     while, until slony does a cleanup event / log switch (can't remember
>     which right now). This is normal. Don't freak out.
>
>


From davecramer at gmail.com  Mon Aug 24 13:58:56 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 16:58:56 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB8523.3050600@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
Message-ID: <CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>

On 24 August 2015 at 16:57, Steve Singer <ssinger at ca.afilias.info> wrote:

> On 08/24/2015 04:12 PM, Dave Cramer wrote:
>
>> I am currently waiting on a resubscribe event. How does slony get this
>> event through all the backlog events ?
>>
>>
> Usually pretty quickly.
> Slonik waits for the provider to be caught up before submitting the event.
>
> The question is, why is your provider behind? Is it getting caught up?
>

I presumed it was behind because one node was missing and this was blocking
replication.

Note the nodes are chained 1->2, 2->3, 3-4. 3 is the one that failed

>
>
> Dave Cramer
>>
>> On 24 August 2015 at 15:38, Scott Marlowe <scott.marlowe at gmail.com
>> <mailto:scott.marlowe at gmail.com>> wrote:
>>
>>     Note that the node will still show up in sl_nodes and sl_status for a
>>     while, until slony does a cleanup event / log switch (can't remember
>>     which right now). This is normal. Don't freak out.
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/86dcd694/attachment.htm 

From ssinger at ca.afilias.info  Mon Aug 24 14:03:28 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 17:03:28 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
Message-ID: <55DB86A0.3010508@ca.afilias.info>

On 08/24/2015 04:58 PM, Dave Cramer wrote:
>
> On 24 August 2015 at 16:57, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 08/24/2015 04:12 PM, Dave Cramer wrote:
>
>         I am currently waiting on a resubscribe event. How does slony
>         get this
>         event through all the backlog events ?
>
>
>     Usually pretty quickly.
>     Slonik waits for the provider to be caught up before submitting the
>     event.
>
>     The question is, why is your provider behind? Is it getting caught up?
>
>
> I presumed it was behind because one node was missing and this was
> blocking replication.
>
> Note the nodes are chained 1->2, 2->3, 3-4. 3 is the one that failed
>


and a slonik script like
---------------
cluster name=mycluster;
node 1 admin conninfo='...'
node 4 admin conninfo='...'

resubscribe node(origin=1,provider=1,receiver=4);
----------

is waiting?

If so it should tell you from which node it is waiting for events from.




>
>
>         Dave Cramer
>
>         On 24 August 2015 at 15:38, Scott Marlowe
>         <scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>         <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>> wrote:
>
>              Note that the node will still show up in sl_nodes and
>         sl_status for a
>              while, until slony does a cleanup event / log switch (can't
>         remember
>              which right now). This is normal. Don't freak out.
>
>
>
>


From davecramer at gmail.com  Mon Aug 24 14:06:54 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 17:06:54 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB86A0.3010508@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
Message-ID: <CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>

On 24 August 2015 at 17:03, Steve Singer <ssinger at ca.afilias.info> wrote:

> On 08/24/2015 04:58 PM, Dave Cramer wrote:
>
>>
>> On 24 August 2015 at 16:57, Steve Singer <ssinger at ca.afilias.info
>> <mailto:ssinger at ca.afilias.info>> wrote:
>>
>>     On 08/24/2015 04:12 PM, Dave Cramer wrote:
>>
>>         I am currently waiting on a resubscribe event. How does slony
>>         get this
>>         event through all the backlog events ?
>>
>>
>>     Usually pretty quickly.
>>     Slonik waits for the provider to be caught up before submitting the
>>     event.
>>
>>     The question is, why is your provider behind? Is it getting caught up?
>>
>>
>> I presumed it was behind because one node was missing and this was
>> blocking replication.
>>
>> Note the nodes are chained 1->2, 2->3, 3-4. 3 is the one that failed
>>
>>
>
> and a slonik script like
> ---------------
> cluster name=mycluster;
> node 1 admin conninfo='...'
> node 4 admin conninfo='...'
>
> resubscribe node(origin=1,provider=1,receiver=4);
> ----------
>
> is waiting?
>
> If so it should tell you from which node it is waiting for events from.
>

Close enough. I went from 2 to 4 and this is the output

waiting for events  (4,5002587214) only at (4,5002579907) to be confirmed
on node 2


>
>
>
>
>
>>
>>         Dave Cramer
>>
>>         On 24 August 2015 at 15:38, Scott Marlowe
>>         <scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>>         <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>> wrote:
>>
>>              Note that the node will still show up in sl_nodes and
>>         sl_status for a
>>              while, until slony does a cleanup event / log switch (can't
>>         remember
>>              which right now). This is normal. Don't freak out.
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/f8127613/attachment.htm 

From glynastill at yahoo.co.uk  Mon Aug 24 14:09:59 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 24 Aug 2015 21:09:59 +0000 (UTC)
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB67F0.9010902@ca.afilias.info>
References: <55DB67F0.9010902@ca.afilias.info>
Message-ID: <1365936625.1698.1440450599052.JavaMail.yahoo@mail.yahoo.com>





----- Original Message -----
> From: Steve Singer <ssinger at ca.afilias.info>
> To: Dave Cramer <davecramer at gmail.com>; Greg Sabino Mullane <greg at endpoint.com>
> Cc: slony <slony1-general at lists.slony.info>
> Sent: Monday, 24 August 2015, 19:52
> Subject: Re: [Slony1-general] Dropping a node when the node is gone ?
> 
> On 08/24/2015 02:20 PM, Dave Cramer wrote:
>>  Yes, but the node is no longer physically there ??
>>  Even worse it is a provider for another node
>> 
> 
> I think you can leave out the admin conninfo for the node being dropped 
> and slonik will skip the uinstall step.
> 
> (This is from memory,so I might be confused)

>> You can use the resubscribe command to point the other nodes at a new 
> provider.
> 
> 


AFAIK that is the advice you gave me a while ago, and I can confirm it works as I tested it in various scenarios.   I recall having to resubscribe the downstream nodes to another provider first though.

From ssinger at ca.afilias.info  Mon Aug 24 14:13:07 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 17:13:07 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
Message-ID: <55DB88E3.5090400@ca.afilias.info>

On 08/24/2015 05:06 PM, Dave Cramer wrote:
>

>
>
>     and a slonik script like
>     ---------------
>     cluster name=mycluster;
>     node 1 admin conninfo='...'
>     node 4 admin conninfo='...'
>
>     resubscribe node(origin=1,provider=1,receiver=4);
>     ----------
>
>     is waiting?
>
>     If so it should tell you from which node it is waiting for events from.
>
>
> Close enough. I went from 2 to 4 and this is the output
>
> waiting for events  (4,5002587214) only at (4,5002579907) to be
> confirmed on node 2
>

are there paths between node 2 and 4?


>
>
>
>
>
>
>                  Dave Cramer
>
>                  On 24 August 2015 at 15:38, Scott Marlowe
>                  <scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com> <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>> wrote:
>
>                       Note that the node will still show up in sl_nodes and
>                  sl_status for a
>                       while, until slony does a cleanup event / log
>         switch (can't
>                  remember
>                       which right now). This is normal. Don't freak out.
>
>
>
>
>
>


From davecramer at gmail.com  Mon Aug 24 14:17:18 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 17:17:18 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB88E3.5090400@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
Message-ID: <CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>

On 24 August 2015 at 17:13, Steve Singer <ssinger at ca.afilias.info> wrote:

> On 08/24/2015 05:06 PM, Dave Cramer wrote:
>
>>
>>
>
>>
>>     and a slonik script like
>>     ---------------
>>     cluster name=mycluster;
>>     node 1 admin conninfo='...'
>>     node 4 admin conninfo='...'
>>
>>     resubscribe node(origin=1,provider=1,receiver=4);
>>     ----------
>>
>>     is waiting?
>>
>>     If so it should tell you from which node it is waiting for events
>> from.
>>
>>
>> Close enough. I went from 2 to 4 and this is the output
>>
>> waiting for events  (4,5002587214) only at (4,5002579907) to be
>> confirmed on node 2
>>
>>
> are there paths between node 2 and 4?
>
> There are but I thought I would try your suggestion which evokes a
different error message

waiting for events  (2,5003485579) only at (2,5003478340), (4,5002587214)
only at (4,5002579907) to be confirmed on node 1



>
>
>
>>
>>
>>
>>
>>
>>                  Dave Cramer
>>
>>                  On 24 August 2015 at 15:38, Scott Marlowe
>>                  <scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com> <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>> wrote:
>>
>>                       Note that the node will still show up in sl_nodes
>> and
>>                  sl_status for a
>>                       while, until slony does a cleanup event / log
>>         switch (can't
>>                  remember
>>                       which right now). This is normal. Don't freak out.
>>
>>
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/90cd66e1/attachment.htm 

From ssinger at ca.afilias.info  Mon Aug 24 14:23:23 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 17:23:23 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
	<CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
Message-ID: <55DB8B4B.7020101@ca.afilias.info>

On 08/24/2015 05:17 PM, Dave Cramer wrote:
>
> On 24 August 2015 at 17:13, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 08/24/2015 05:06 PM, Dave Cramer wrote:
>
>
>
>
>
>              and a slonik script like
>              ---------------
>              cluster name=mycluster;
>              node 1 admin conninfo='...'
>              node 4 admin conninfo='...'
>
>              resubscribe node(origin=1,provider=1,receiver=4);
>              ----------
>
>              is waiting?
>
>              If so it should tell you from which node it is waiting for
>         events from.
>
>
>         Close enough. I went from 2 to 4 and this is the output
>
>         waiting for events  (4,5002587214 <tel:5002587214>) only at
>         (4,5002579907 <tel:5002579907>) to be
>         confirmed on node 2
>
>


Now I assume the  "only at (2,5003478340) number is staying the same and 
it isn't going up. IF it is then you actually have progress being made 
but I find that unlikely if node 2 isn't the origin of any sets to node 
1(you would be caught up quickly).


Another option would be

   cluster name=mycluster;
   node 1 admin conninfo='...'
   node 2 admin conninfo='..
   node 3 admin conninfo='..
   node 4 admin conninfo='..
   failover(id=3, backup node = 2);


Per the failover documentation

Nodes that are forwarding providers can also be passed to the failover 
command as a failed node. The failover process will redirect the 
subscriptions from these nodes to the backup node.



>     are there paths between node 2 and 4?
>
> There are but I thought I would try your suggestion which evokes a
> different error message
>
> waiting for events  (2,5003485579) only at (2,5003478340),
> (4,5002587214) only at (4,5002579907) to be confirmed on node 1
>
>
>
>
>
>
>
>
>
>                           Dave Cramer
>
>                           On 24 August 2015 at 15:38, Scott Marlowe
>                           <scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>         <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>> wrote:
>
>                                Note that the node will still show up in
>         sl_nodes and
>                           sl_status for a
>                                while, until slony does a cleanup event / log
>                  switch (can't
>                           remember
>                                which right now). This is normal. Don't
>         freak out.
>
>
>
>
>
>
>
>


From davecramer at gmail.com  Mon Aug 24 14:29:42 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 17:29:42 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB8B4B.7020101@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
	<CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
	<55DB8B4B.7020101@ca.afilias.info>
Message-ID: <CADK3HHLdtOv0zE+Evhr0+XZ3Qj+7oPkyATpuOcTHYFJwyQwg8w@mail.gmail.com>

On 24 August 2015 at 17:23, Steve Singer <ssinger at ca.afilias.info> wrote:

> On 08/24/2015 05:17 PM, Dave Cramer wrote:
>
>>
>> On 24 August 2015 at 17:13, Steve Singer <ssinger at ca.afilias.info
>> <mailto:ssinger at ca.afilias.info>> wrote:
>>
>>     On 08/24/2015 05:06 PM, Dave Cramer wrote:
>>
>>
>>
>>
>>
>>              and a slonik script like
>>              ---------------
>>              cluster name=mycluster;
>>              node 1 admin conninfo='...'
>>              node 4 admin conninfo='...'
>>
>>              resubscribe node(origin=1,provider=1,receiver=4);
>>              ----------
>>
>>              is waiting?
>>
>>              If so it should tell you from which node it is waiting for
>>         events from.
>>
>>
>>         Close enough. I went from 2 to 4 and this is the output
>>
>>         waiting for events  (4,5002587214 <tel:5002587214>) only at
>>         (4,5002579907 <tel:5002579907>) to be
>>         confirmed on node 2
>>
>>
>>
>
> Now I assume the  "only at (2,5003478340) number is staying the same and
> it isn't going up. IF it is then you actually have progress being made but
> I find that unlikely if node 2 isn't the origin of any sets to node 1(you
> would be caught up quickly).
>
>
> Another option would be
>
>   cluster name=mycluster;
>   node 1 admin conninfo='...'
>   node 2 admin conninfo='..
>   node 3 admin conninfo='..
>   node 4 admin conninfo='..
>   failover(id=3, backup node = 2);
>
>
> Per the failover documentation
>
> Nodes that are forwarding providers can also be passed to the failover
> command as a failed node. The failover process will redirect the
> subscriptions from these nodes to the backup node.


failover provided no feedback and doesn't appear to have done anything.

>
>
>
>
>     are there paths between node 2 and 4?
>>
>> There are but I thought I would try your suggestion which evokes a
>> different error message
>>
>> waiting for events  (2,5003485579) only at (2,5003478340),
>> (4,5002587214) only at (4,5002579907) to be confirmed on node 1
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>                           Dave Cramer
>>
>>                           On 24 August 2015 at 15:38, Scott Marlowe
>>                           <scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>         <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>> wrote:
>>
>>                                Note that the node will still show up in
>>         sl_nodes and
>>                           sl_status for a
>>                                while, until slony does a cleanup event /
>> log
>>                  switch (can't
>>                           remember
>>                                which right now). This is normal. Don't
>>         freak out.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/f93f1868/attachment-0001.htm 

From ssinger at ca.afilias.info  Mon Aug 24 14:35:36 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 17:35:36 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHLdtOv0zE+Evhr0+XZ3Qj+7oPkyATpuOcTHYFJwyQwg8w@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
	<CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
	<55DB8B4B.7020101@ca.afilias.info>
	<CADK3HHLdtOv0zE+Evhr0+XZ3Qj+7oPkyATpuOcTHYFJwyQwg8w@mail.gmail.com>
Message-ID: <55DB8E28.8090903@ca.afilias.info>

On 08/24/2015 05:29 PM, Dave Cramer wrote:
>

>
>
>
>     Now I assume the  "only at (2,5003478340 <tel:5003478340>) number is
>     staying the same and it isn't going up. IF it is then you actually
>     have progress being made but I find that unlikely if node 2 isn't
>     the origin of any sets to node 1(you would be caught up quickly).
>
>
>     Another option would be
>
>        cluster name=mycluster;
>        node 1 admin conninfo='...'
>        node 2 admin conninfo='..
>        node 3 admin conninfo='..
>        node 4 admin conninfo='..
>        failover(id=3, backup node = 2);
>
>
>     Per the failover documentation
>
>     Nodes that are forwarding providers can also be passed to the
>     failover command as a failed node. The failover process will
>     redirect the subscriptions from these nodes to the backup node.
>
>
> failover provided no feedback and doesn't appear to have done anything.

Most slonik commands don't produce feedback when they work.

select * FROM _mycluster.sl_subscribe
on your origin;

Does it show node 3 as a provider?

If not you can then drop node 3.

--
cluster name=mycluster;
node 1 admin conninfo=''
node 2 admin conninfo=''
node 4 admin conninfo=''
drop node (id=3, event node=1);


>
>
>
>
>
>              are there paths between node 2 and 4?
>
>         There are but I thought I would try your suggestion which evokes a
>         different error message
>
>         waiting for events  (2,5003485579 <tel:5003485579>) only at
>         (2,5003478340 <tel:5003478340>),
>         (4,5002587214 <tel:5002587214>) only at (4,5002579907
>         <tel:5002579907>) to be confirmed on node 1
>
>
>
>
>
>
>
>
>
>                                    Dave Cramer
>
>                                    On 24 August 2015 at 15:38, Scott Marlowe
>                                    <scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com> <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>
>                                    <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                                    <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>>> wrote:
>
>                                         Note that the node will still
>         show up in
>                  sl_nodes and
>                                    sl_status for a
>                                         while, until slony does a
>         cleanup event / log
>                           switch (can't
>                                    remember
>                                         which right now). This is
>         normal. Don't
>                  freak out.
>
>
>
>
>
>
>
>
>
>



From davecramer at gmail.com  Mon Aug 24 14:40:08 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 17:40:08 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB8E28.8090903@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
	<CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
	<55DB8B4B.7020101@ca.afilias.info>
	<CADK3HHLdtOv0zE+Evhr0+XZ3Qj+7oPkyATpuOcTHYFJwyQwg8w@mail.gmail.com>
	<55DB8E28.8090903@ca.afilias.info>
Message-ID: <CADK3HHJfKHbGQ4bms0gdHUazsZ4DUs+mdjKNKmtS4V4Lvy7ATA@mail.gmail.com>

On 24 August 2015 at 17:35, Steve Singer <ssinger at ca.afilias.info> wrote:

> On 08/24/2015 05:29 PM, Dave Cramer wrote:
>
>>
>>
>
>>
>>
>>     Now I assume the  "only at (2,5003478340 <tel:5003478340>) number is
>>     staying the same and it isn't going up. IF it is then you actually
>>     have progress being made but I find that unlikely if node 2 isn't
>>     the origin of any sets to node 1(you would be caught up quickly).
>>
>>
>>     Another option would be
>>
>>        cluster name=mycluster;
>>        node 1 admin conninfo='...'
>>        node 2 admin conninfo='..
>>        node 3 admin conninfo='..
>>        node 4 admin conninfo='..
>>        failover(id=3, backup node = 2);
>>
>>
>>     Per the failover documentation
>>
>>     Nodes that are forwarding providers can also be passed to the
>>     failover command as a failed node. The failover process will
>>     redirect the subscriptions from these nodes to the backup node.
>>
>>
>> failover provided no feedback and doesn't appear to have done anything.
>>
>
> Most slonik commands don't produce feedback when they work.
>
> select * FROM _mycluster.sl_subscribe
> on your origin;
>
> Does it show node 3 as a provider?
>

It is still there. FWIW, this is  2.2.2-1 was there a fix for this
recently?

>
> If not you can then drop node 3.
>
> --
> cluster name=mycluster;
> node 1 admin conninfo=''
> node 2 admin conninfo=''
> node 4 admin conninfo=''
> drop node (id=3, event node=1);
>
>
>
>>
>>
>>
>>
>>              are there paths between node 2 and 4?
>>
>>         There are but I thought I would try your suggestion which evokes a
>>         different error message
>>
>>         waiting for events  (2,5003485579 <tel:5003485579>) only at
>>         (2,5003478340 <tel:5003478340>),
>>         (4,5002587214 <tel:5002587214>) only at (4,5002579907
>>         <tel:5002579907>) to be confirmed on node 1
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>                                    Dave Cramer
>>
>>                                    On 24 August 2015 at 15:38, Scott
>> Marlowe
>>                                    <scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com> <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>
>>                                    <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                                    <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>>> wrote:
>>
>>                                         Note that the node will still
>>         show up in
>>                  sl_nodes and
>>                                    sl_status for a
>>                                         while, until slony does a
>>         cleanup event / log
>>                           switch (can't
>>                                    remember
>>                                         which right now). This is
>>         normal. Don't
>>                  freak out.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/422a5745/attachment.htm 

From ssinger at ca.afilias.info  Mon Aug 24 14:42:48 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 24 Aug 2015 17:42:48 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <CADK3HHJfKHbGQ4bms0gdHUazsZ4DUs+mdjKNKmtS4V4Lvy7ATA@mail.gmail.com>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
	<CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
	<55DB8B4B.7020101@ca.afilias.info>
	<CADK3HHLdtOv0zE+Evhr0+XZ3Qj+7oPkyATpuOcTHYFJwyQwg8w@mail.gmail.com>
	<55DB8E28.8090903@ca.afilias.info>
	<CADK3HHJfKHbGQ4bms0gdHUazsZ4DUs+mdjKNKmtS4V4Lvy7ATA@mail.gmail.com>
Message-ID: <55DB8FD8.6090903@ca.afilias.info>

On 08/24/2015 05:40 PM, Dave Cramer wrote:
>
> On 24 August 2015 at 17:35, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 08/24/2015 05:29 PM, Dave Cramer wrote:
>
>
>
>
>
>
>              Now I assume the  "only at (2,5003478340 <tel:5003478340>
>         <tel:5003478340 <tel:5003478340>>) number is
>              staying the same and it isn't going up. IF it is then you
>         actually
>              have progress being made but I find that unlikely if node 2
>         isn't
>              the origin of any sets to node 1(you would be caught up
>         quickly).
>
>
>              Another option would be
>
>                 cluster name=mycluster;
>                 node 1 admin conninfo='...'
>                 node 2 admin conninfo='..
>                 node 3 admin conninfo='..
>                 node 4 admin conninfo='..
>                 failover(id=3, backup node = 2);
>
>
>              Per the failover documentation
>
>              Nodes that are forwarding providers can also be passed to the
>              failover command as a failed node. The failover process will
>              redirect the subscriptions from these nodes to the backup node.
>
>
>         failover provided no feedback and doesn't appear to have done
>         anything.
>
>
>     Most slonik commands don't produce feedback when they work.
>
>     select * FROM _mycluster.sl_subscribe
>     on your origin;
>
>     Does it show node 3 as a provider?
>
>
> It is still there. FWIW, this is  2.2.2-1 was there a fix for this
> recently?
>

Sounds similar to bug 342
http://www.slony.info/bugzilla/show_bug.cgi?id=342

That was fixed in 2.2.3


>
>     If not you can then drop node 3.
>
>     --
>     cluster name=mycluster;
>     node 1 admin conninfo=''
>     node 2 admin conninfo=''
>     node 4 admin conninfo=''
>     drop node (id=3, event node=1);
>
>
>
>
>
>
>
>                       are there paths between node 2 and 4?
>
>                  There are but I thought I would try your suggestion
>         which evokes a
>                  different error message
>
>                  waiting for events  (2,5003485579 <tel:5003485579>
>         <tel:5003485579 <tel:5003485579>>) only at
>                  (2,5003478340 <tel:5003478340> <tel:5003478340
>         <tel:5003478340>>),
>                  (4,5002587214 <tel:5002587214> <tel:5002587214
>         <tel:5002587214>>) only at (4,5002579907 <tel:5002579907>
>                  <tel:5002579907 <tel:5002579907>>) to be confirmed on
>         node 1
>
>
>
>
>
>
>
>
>
>
>                                             Dave Cramer
>
>                                             On 24 August 2015 at 15:38,
>         Scott Marlowe
>                                             <scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                                    <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>         <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                                    <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>>
>
>           <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                                    <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>
>
>           <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>
>                                    <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>
>                           <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>
>                  <mailto:scott.marlowe at gmail.com
>         <mailto:scott.marlowe at gmail.com>>>>>>> wrote:
>
>                                                  Note that the node will
>         still
>                  show up in
>                           sl_nodes and
>                                             sl_status for a
>                                                  while, until slony does a
>                  cleanup event / log
>                                    switch (can't
>                                             remember
>                                                  which right now). This is
>                  normal. Don't
>                           freak out.
>
>
>
>
>
>
>
>
>
>
>
>
>


From davecramer at gmail.com  Mon Aug 24 14:44:47 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 24 Aug 2015 17:44:47 -0400
Subject: [Slony1-general] Dropping a node when the node is gone ?
In-Reply-To: <55DB8FD8.6090903@ca.afilias.info>
References: <CADK3HH+QJ=83oML4Qw=mTLL=Ha9FG-0NqLGv6oDrKmATp82EFQ@mail.gmail.com>
	<20150824181929.GQ21370@broken.home>
	<CADK3HHJ1t50Y6kVCQNdw26XZB5M6xYO8p=3q5bbEQbbizAR7Mg@mail.gmail.com>
	<55DB67F0.9010902@ca.afilias.info>
	<CAOR=d=2HphVpmZcdjOmZFAushP14N+QjVJEXTvjpBP0NcFrzkw@mail.gmail.com>
	<CADK3HHKyShgUV4eV=Gg8squ6AXAsDBz=bveee5n5Y1kC4-tPOg@mail.gmail.com>
	<55DB8523.3050600@ca.afilias.info>
	<CADK3HHJoJM71MQo8YHYf_dPEuBZ8iZGor+B6qs6-PHW6AXcdYg@mail.gmail.com>
	<55DB86A0.3010508@ca.afilias.info>
	<CADK3HHL=-_b6zk2oto+G-BoMzpioeuw4-94pUX6OnQ0k-BC+7w@mail.gmail.com>
	<55DB88E3.5090400@ca.afilias.info>
	<CADK3HHKrA7=o5OOMM6RiTcVjn3xNUhJs=KRZTrdC5xjcCnxWyQ@mail.gmail.com>
	<55DB8B4B.7020101@ca.afilias.info>
	<CADK3HHLdtOv0zE+Evhr0+XZ3Qj+7oPkyATpuOcTHYFJwyQwg8w@mail.gmail.com>
	<55DB8E28.8090903@ca.afilias.info>
	<CADK3HHJfKHbGQ4bms0gdHUazsZ4DUs+mdjKNKmtS4V4Lvy7ATA@mail.gmail.com>
	<55DB8FD8.6090903@ca.afilias.info>
Message-ID: <CADK3HHKg-81TLLTKjRSsGJD0NSvexV9ZJJva8x8y2779fcc0Vg@mail.gmail.com>

Dave Cramer

On 24 August 2015 at 17:42, Steve Singer <ssinger at ca.afilias.info> wrote:

> On 08/24/2015 05:40 PM, Dave Cramer wrote:
>
>>
>> On 24 August 2015 at 17:35, Steve Singer <ssinger at ca.afilias.info
>> <mailto:ssinger at ca.afilias.info>> wrote:
>>
>>     On 08/24/2015 05:29 PM, Dave Cramer wrote:
>>
>>
>>
>>
>>
>>
>>              Now I assume the  "only at (2,5003478340 <tel:5003478340>
>>         <tel:5003478340 <tel:5003478340>>) number is
>>
>>              staying the same and it isn't going up. IF it is then you
>>         actually
>>              have progress being made but I find that unlikely if node 2
>>         isn't
>>              the origin of any sets to node 1(you would be caught up
>>         quickly).
>>
>>
>>              Another option would be
>>
>>                 cluster name=mycluster;
>>                 node 1 admin conninfo='...'
>>                 node 2 admin conninfo='..
>>                 node 3 admin conninfo='..
>>                 node 4 admin conninfo='..
>>                 failover(id=3, backup node = 2);
>>
>>
>>              Per the failover documentation
>>
>>              Nodes that are forwarding providers can also be passed to the
>>              failover command as a failed node. The failover process will
>>              redirect the subscriptions from these nodes to the backup
>> node.
>>
>>
>>         failover provided no feedback and doesn't appear to have done
>>         anything.
>>
>>
>>     Most slonik commands don't produce feedback when they work.
>>
>>     select * FROM _mycluster.sl_subscribe
>>     on your origin;
>>
>>     Does it show node 3 as a provider?
>>
>>
>> It is still there. FWIW, this is  2.2.2-1 was there a fix for this
>> recently?
>>
>>
> Sounds similar to bug 342
> http://www.slony.info/bugzilla/show_bug.cgi?id=342
>
> That was fixed in 2.2.3
>
> Sounds like it. I really didn't want to have to upgrade this mess to fix
another mess.....

Thanks for your help, Sorry for not brining up the version earlier.

Dave

>
>
>>     If not you can then drop node 3.
>>
>>     --
>>     cluster name=mycluster;
>>     node 1 admin conninfo=''
>>     node 2 admin conninfo=''
>>     node 4 admin conninfo=''
>>     drop node (id=3, event node=1);
>>
>>
>>
>>
>>
>>
>>
>>                       are there paths between node 2 and 4?
>>
>>                  There are but I thought I would try your suggestion
>>         which evokes a
>>                  different error message
>>
>>                  waiting for events  (2,5003485579 <tel:5003485579>
>>         <tel:5003485579 <tel:5003485579>>) only at
>>                  (2,5003478340 <tel:5003478340> <tel:5003478340
>>         <tel:5003478340>>),
>>                  (4,5002587214 <tel:5002587214> <tel:5002587214
>>         <tel:5002587214>>) only at (4,5002579907 <tel:5002579907>
>>                  <tel:5002579907 <tel:5002579907>>) to be confirmed on
>>
>>         node 1
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>                                             Dave Cramer
>>
>>                                             On 24 August 2015 at 15:38,
>>         Scott Marlowe
>>                                             <scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                                    <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>         <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                                    <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>>
>>
>>           <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com
>> >
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                                    <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>
>>
>>           <mailto:scott.marlowe at gmail.com <mailto:scott.marlowe at gmail.com
>> >
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>
>>                                    <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>
>>                           <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>
>>                  <mailto:scott.marlowe at gmail.com
>>         <mailto:scott.marlowe at gmail.com>>>>>>> wrote:
>>
>>                                                  Note that the node will
>>         still
>>                  show up in
>>                           sl_nodes and
>>                                             sl_status for a
>>                                                  while, until slony does a
>>                  cleanup event / log
>>                                    switch (can't
>>                                             remember
>>                                                  which right now). This is
>>                  normal. Don't
>>                           freak out.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150824/ceb52eb5/attachment.htm 

