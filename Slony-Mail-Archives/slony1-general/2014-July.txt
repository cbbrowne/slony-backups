From ssinger at ca.afilias.info  Tue Jul  1 10:37:33 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 01 Jul 2014 13:37:33 -0400
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>	<53AAE50E.7030101@ca.afilias.info>	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>	<53AAE9CC.6020706@ca.afilias.info>	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>	<53AB741F.907@ca.afilias.info>	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>
	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>
Message-ID: <53B2F1DD.9000400@ca.afilias.info>

On 06/30/2014 01:05 PM, Soni M wrote:
> it seems a slony process that has <IDLE> in transaction for many times.
> the client address and the user are identical to slony slave.
>
>

Which version of slony are you on?



> On Mon, Jun 30, 2014 at 11:54 PM, Soni M <diptatapa at gmail.com
> <mailto:diptatapa at gmail.com>> wrote:
>
>     On Thu, Jun 26, 2014 at 8:15 AM, Steve Singer
>     <ssinger at ca.afilias.info <mailto:ssinger at ca.afilias.info>> wrote:
>
>
>
>         Which transactions are locking sl_log_2 when slony is in that state?
>
>         The slony log trigger should only be adding rows to sl_log_1 in
>         this state.   If this isn't the case then there is a problem.
>
>         The problem with waiting for the lock is other transactions will
>         the block and queue up behind the cleanup thread/transaction.
>
>
>
>     I saw this query from slony slave :
>
>     fetch 500 from LOG;
>
>     but another time it is
>
>     <IDLE> in transaction
>
>     that has lock on sl_log_2. The <IDLE> in transaction appear much
>     more often.
>
>     --
>     Regards,
>
>     Soni Maula Harriz
>
>
>
>
> --
> Regards,
>
> Soni Maula Harriz


From diptatapa at gmail.com  Tue Jul  1 17:24:21 2014
From: diptatapa at gmail.com (Soni M)
Date: Wed, 2 Jul 2014 07:24:21 +0700
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <53B2F1DD.9000400@ca.afilias.info>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>
	<53AAE50E.7030101@ca.afilias.info>
	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>
	<53AAE9CC.6020706@ca.afilias.info>
	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>
	<53AB741F.907@ca.afilias.info>
	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>
	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>
	<53B2F1DD.9000400@ca.afilias.info>
Message-ID: <CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>

Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from Ubuntu Package
Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems build from
source.
DB size 1.5 TB, Master utilize relative high number of temp tables and
relative High number of locks on busy time

DB=# select mode, count(*) from pg_locks group by mode;
           mode           | count
--------------------------+--------
 ExclusiveLock            |    112
 RowShareLock             |     37
 AccessExclusiveLock      | 208577
 RowExclusiveLock         |  33087
 ShareUpdateExclusiveLock |      5
 ShareLock                |  54906
 AccessShareLock          |  93607
(7 rows)

but the slave is relative low on load.

this is the connection from slave that open from May 25th
postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08 postgres:
slony_user dbname master_ip_address(58554) idle

On slave, this connection seems always keep on idle status, but on master,
this connection often in "<IDLE> in transaction" status for some minutes
and hold lock on sl_log_2 while transaction are filling up sl_log_1.

Such condition usually happen on server high load time, but on low load it
sometimes happen, but not many and does not affect on replication lag.

On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 06/30/2014 01:05 PM, Soni M wrote:
>
>> it seems a slony process that has <IDLE> in transaction for many times.
>> the client address and the user are identical to slony slave.
>>
>>
>>
> Which version of slony are you on?
>
>>
>>
>


-- 
Regards,

Soni Maula Harriz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140702/7bc31293/attachment.htm 

From ssinger at ca.afilias.info  Wed Jul  2 06:33:34 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 02 Jul 2014 09:33:34 -0400
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>	<53AAE50E.7030101@ca.afilias.info>	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>	<53AAE9CC.6020706@ca.afilias.info>	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>	<53AB741F.907@ca.afilias.info>	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>	<53B2F1DD.9000400@ca.afilias.info>
	<CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>
Message-ID: <53B40A2E.4010100@ca.afilias.info>

On 07/01/2014 08:24 PM, Soni M wrote:


What is
select last_value from sl_log_status (it is a sequence) on the master 
when you see those "NOTICE:  Slony-I: could not lock sl_log_2 - sl_log_2 
not truncated" messages?

The sl_log_status value SHOULD be controlling both which (if any) tables 
the cleanup thread tries to lock/truncate and which of the log tables 
slon selects from.  It should be doing it such a way that the cleanup 
thread will never try to lock a table that slon is going to select from. 
Of course, there could be a bug here.

Also you should be aware of bug 167 (
http://www.slony.info/bugzilla/show_bug.cgi?id=167) which exists in 
slony 2.0.x where if replication is behind (your sl_log tables grow 
large) then selecting from sl_log becomes a full table scan making 
replication very slow.  Bug 167 was fixed in 2.1







>
> Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from Ubuntu Package
> Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems build from
> source.
> DB size 1.5 TB, Master utilize relative high number of temp tables and
> relative High number of locks on busy time
>
> DB=# select mode, count(*) from pg_locks group by mode;
>             mode           | count
> --------------------------+--------
>   ExclusiveLock            |    112
>   RowShareLock             |     37
>   AccessExclusiveLock      | 208577
>   RowExclusiveLock         |  33087
>   ShareUpdateExclusiveLock |      5
>   ShareLock                |  54906
>   AccessShareLock          |  93607
> (7 rows)
> but the slave is relative low on load.
>
> this is the connection from slave that open from May 25th
> postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08
> postgres: slony_user dbname master_ip_address(58554) idle
>
> On slave, this connection seems always keep on idle status, but on
> master, this connection often in "<IDLE> in transaction" status for some
> minutes and hold lock on sl_log_2 while transaction are filling up sl_log_1.
>
> Such condition usually happen on server high load time, but on low load
> it sometimes happen, but not many and does not affect on replication lag.
>
> On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 06/30/2014 01:05 PM, Soni M wrote:
>
>         it seems a slony process that has <IDLE> in transaction for many
>         times.
>         the client address and the user are identical to slony slave.
>
>
>
>     Which version of slony are you on?
>
>
>
>
>
> --
> Regards,
>
> Soni Maula Harriz


From ssinger at ca.afilias.info  Wed Jul  2 09:02:22 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 02 Jul 2014 12:02:22 -0400
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>	<53AAE50E.7030101@ca.afilias.info>	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>	<53AAE9CC.6020706@ca.afilias.info>	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>	<53AB741F.907@ca.afilias.info>	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>	<53B2F1DD.9000400@ca.afilias.info>
	<CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>
Message-ID: <53B42D0E.9060603@ca.afilias.info>

On 07/01/2014 08:24 PM, Soni M wrote:
>
> Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from Ubuntu Package
> Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems build from


I should also point out that slony 2.0.x does not properly support PG 
9.1 or higher.  You should use slony 2.1.x or 2.2.x with PG 9.1+.

If you are seeing a lot of transactions being aborted due to 
serialization conflicts then this is because of the issues with PG 9.1 
and slony 2.0 (though it doesn't sound like that is causing this 
particular issue)



> source.
> DB size 1.5 TB, Master utilize relative high number of temp tables and
> relative High number of locks on busy time
>
> DB=# select mode, count(*) from pg_locks group by mode;
>             mode           | count
> --------------------------+--------
>   ExclusiveLock            |    112
>   RowShareLock             |     37
>   AccessExclusiveLock      | 208577
>   RowExclusiveLock         |  33087
>   ShareUpdateExclusiveLock |      5
>   ShareLock                |  54906
>   AccessShareLock          |  93607
> (7 rows)
> but the slave is relative low on load.
>
> this is the connection from slave that open from May 25th
> postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08
> postgres: slony_user dbname master_ip_address(58554) idle
>
> On slave, this connection seems always keep on idle status, but on
> master, this connection often in "<IDLE> in transaction" status for some
> minutes and hold lock on sl_log_2 while transaction are filling up sl_log_1.
>
> Such condition usually happen on server high load time, but on low load
> it sometimes happen, but not many and does not affect on replication lag.
>
> On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 06/30/2014 01:05 PM, Soni M wrote:
>
>         it seems a slony process that has <IDLE> in transaction for many
>         times.
>         the client address and the user are identical to slony slave.
>
>
>
>     Which version of slony are you on?
>
>
>
>
>
> --
> Regards,
>
> Soni Maula Harriz


From ssinger at ca.afilias.info  Mon Jul  7 18:54:31 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 07 Jul 2014 21:54:31 -0400
Subject: [Slony1-general] Slony 2.2.3 released
Message-ID: <53BB4F57.4030907@ca.afilias.info>

The Slony team is pleased to announce Slony 2.2.3 the next minor release 
of the Slony 2.2.x series

Slony 2.2.3 includes the following changes

  - Bug 338 - Have ddlScript return a bigint instead of a integer
  - fixing  Deadlock with application during minor version slony upgrade
  - Bug 342 FAILOVER fixes for some multi-node configurations
  - Remove HAVE_POSIX_SIGNALS from config.h
  - Bug 344 Do not abort reading slon config values when an unrecognized 
one is encountered


Slony 2.2.3 can be downloaded from from the following URL

http://main.slony.info/downloads/2.2/source/slony1-2.2.3.tar.bz2


From diptatapa at gmail.com  Tue Jul  8 08:06:10 2014
From: diptatapa at gmail.com (Soni M)
Date: Tue, 8 Jul 2014 22:06:10 +0700
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <53B42D0E.9060603@ca.afilias.info>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>
	<53AAE50E.7030101@ca.afilias.info>
	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>
	<53AAE9CC.6020706@ca.afilias.info>
	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>
	<53AB741F.907@ca.afilias.info>
	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>
	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>
	<53B2F1DD.9000400@ca.afilias.info>
	<CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>
	<53B42D0E.9060603@ca.afilias.info>
Message-ID: <CAAMgDXneq4LxiL=WwZL8=5SKp98EX3G_VuQ0yi=_j+aPsy7H3Q@mail.gmail.com>

Sorry for late response,
currently :
NOTICE:  Slony-I: could not lock sl_log_1 - sl_log_1 not truncated
dbname=# select last_value from sl_log_status;
 last_value
------------
          3
(1 row)

That's what it should be, right?

But now, i see application transaction holding locks on sl_log_1, some
select, insert and update transaction and also the <IDLE> in transaction
just like before. But the row numbers of sl_log_1 not increasing anyway.
The row num is increasing in sl_log_2. Is that normal?

The bug on 2.0.x as You said does happen in our environment, and it happens
when the slave falls further behind the master. I should plan to move to
2.1 or 2.2.



On Wed, Jul 2, 2014 at 11:02 PM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 07/01/2014 08:24 PM, Soni M wrote:
>
>>
>> Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from Ubuntu Package
>> Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems build from
>>
>
>
> I should also point out that slony 2.0.x does not properly support PG 9.1
> or higher.  You should use slony 2.1.x or 2.2.x with PG 9.1+.
>
> If you are seeing a lot of transactions being aborted due to serialization
> conflicts then this is because of the issues with PG 9.1 and slony 2.0
> (though it doesn't sound like that is causing this particular issue)
>
>
>
>  source.
>> DB size 1.5 TB, Master utilize relative high number of temp tables and
>> relative High number of locks on busy time
>>
>> DB=# select mode, count(*) from pg_locks group by mode;
>>             mode           | count
>> --------------------------+--------
>>   ExclusiveLock            |    112
>>   RowShareLock             |     37
>>   AccessExclusiveLock      | 208577
>>   RowExclusiveLock         |  33087
>>   ShareUpdateExclusiveLock |      5
>>   ShareLock                |  54906
>>   AccessShareLock          |  93607
>> (7 rows)
>> but the slave is relative low on load.
>>
>> this is the connection from slave that open from May 25th
>> postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08
>> postgres: slony_user dbname master_ip_address(58554) idle
>>
>> On slave, this connection seems always keep on idle status, but on
>> master, this connection often in "<IDLE> in transaction" status for some
>> minutes and hold lock on sl_log_2 while transaction are filling up
>> sl_log_1.
>>
>> Such condition usually happen on server high load time, but on low load
>> it sometimes happen, but not many and does not affect on replication lag.
>>
>> On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer <ssinger at ca.afilias.info
>> <mailto:ssinger at ca.afilias.info>> wrote:
>>
>>     On 06/30/2014 01:05 PM, Soni M wrote:
>>
>>         it seems a slony process that has <IDLE> in transaction for many
>>         times.
>>         the client address and the user are identical to slony slave.
>>
>>
>>
>>     Which version of slony are you on?
>>
>>
>>
>>
>>
>> --
>> Regards,
>>
>> Soni Maula Harriz
>>
>
>


-- 
Regards,

Soni Maula Harriz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140708/224b365c/attachment.htm 

From jan at wi3ck.info  Tue Jul  8 08:14:39 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Tue, 8 Jul 2014 11:14:39 -0400
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <CAAMgDXneq4LxiL=WwZL8=5SKp98EX3G_VuQ0yi=_j+aPsy7H3Q@mail.gmail.com>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>
	<53AAE50E.7030101@ca.afilias.info>
	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>
	<53AAE9CC.6020706@ca.afilias.info>
	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>
	<53AB741F.907@ca.afilias.info>
	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>
	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>
	<53B2F1DD.9000400@ca.afilias.info>
	<CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>
	<53B42D0E.9060603@ca.afilias.info>
	<CAAMgDXneq4LxiL=WwZL8=5SKp98EX3G_VuQ0yi=_j+aPsy7H3Q@mail.gmail.com>
Message-ID: <CAGBW59cga+1Qj=MmRY00N5QSakokHs1KEFk12ZH=_O9JBUpnZg@mail.gmail.com>

Why and what application is holding locks on the Slony log tables?

Jan

--
Jan Wieck
Senior Software Engineer
http://slony.info
On Jul 8, 2014 11:06 AM, "Soni M" <diptatapa at gmail.com> wrote:

> Sorry for late response,
> currently :
> NOTICE:  Slony-I: could not lock sl_log_1 - sl_log_1 not truncated
> dbname=# select last_value from sl_log_status;
>  last_value
> ------------
>           3
> (1 row)
>
> That's what it should be, right?
>
> But now, i see application transaction holding locks on sl_log_1, some
> select, insert and update transaction and also the <IDLE> in transaction
> just like before. But the row numbers of sl_log_1 not increasing anyway.
> The row num is increasing in sl_log_2. Is that normal?
>
> The bug on 2.0.x as You said does happen in our environment, and it
> happens when the slave falls further behind the master. I should plan to
> move to 2.1 or 2.2.
>
>
>
> On Wed, Jul 2, 2014 at 11:02 PM, Steve Singer <ssinger at ca.afilias.info>
> wrote:
>
>> On 07/01/2014 08:24 PM, Soni M wrote:
>>
>>>
>>> Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from Ubuntu
>>> Package
>>> Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems build from
>>>
>>
>>
>> I should also point out that slony 2.0.x does not properly support PG 9.1
>> or higher.  You should use slony 2.1.x or 2.2.x with PG 9.1+.
>>
>> If you are seeing a lot of transactions being aborted due to
>> serialization conflicts then this is because of the issues with PG 9.1 and
>> slony 2.0 (though it doesn't sound like that is causing this particular
>> issue)
>>
>>
>>
>>  source.
>>> DB size 1.5 TB, Master utilize relative high number of temp tables and
>>> relative High number of locks on busy time
>>>
>>> DB=# select mode, count(*) from pg_locks group by mode;
>>>             mode           | count
>>> --------------------------+--------
>>>   ExclusiveLock            |    112
>>>   RowShareLock             |     37
>>>   AccessExclusiveLock      | 208577
>>>   RowExclusiveLock         |  33087
>>>   ShareUpdateExclusiveLock |      5
>>>   ShareLock                |  54906
>>>   AccessShareLock          |  93607
>>> (7 rows)
>>> but the slave is relative low on load.
>>>
>>> this is the connection from slave that open from May 25th
>>> postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08
>>> postgres: slony_user dbname master_ip_address(58554) idle
>>>
>>> On slave, this connection seems always keep on idle status, but on
>>> master, this connection often in "<IDLE> in transaction" status for some
>>> minutes and hold lock on sl_log_2 while transaction are filling up
>>> sl_log_1.
>>>
>>> Such condition usually happen on server high load time, but on low load
>>> it sometimes happen, but not many and does not affect on replication lag.
>>>
>>> On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer <ssinger at ca.afilias.info
>>> <mailto:ssinger at ca.afilias.info>> wrote:
>>>
>>>     On 06/30/2014 01:05 PM, Soni M wrote:
>>>
>>>         it seems a slony process that has <IDLE> in transaction for many
>>>         times.
>>>         the client address and the user are identical to slony slave.
>>>
>>>
>>>
>>>     Which version of slony are you on?
>>>
>>>
>>>
>>>
>>>
>>> --
>>> Regards,
>>>
>>> Soni Maula Harriz
>>>
>>
>>
>
>
> --
> Regards,
>
> Soni Maula Harriz
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140708/52223a67/attachment.htm 

From ssinger at ca.afilias.info  Wed Jul  9 19:40:45 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 09 Jul 2014 22:40:45 -0400
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <CAGBW59cga+1Qj=MmRY00N5QSakokHs1KEFk12ZH=_O9JBUpnZg@mail.gmail.com>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>	<53AAE50E.7030101@ca.afilias.info>	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>	<53AAE9CC.6020706@ca.afilias.info>	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>	<53AB741F.907@ca.afilias.info>	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>	<53B2F1DD.9000400@ca.afilias.info>	<CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>	<53B42D0E.9060603@ca.afilias.info>	<CAAMgDXneq4LxiL=WwZL8=5SKp98EX3G_VuQ0yi=_j+aPsy7H3Q@mail.gmail.com>
	<CAGBW59cga+1Qj=MmRY00N5QSakokHs1KEFk12ZH=_O9JBUpnZg@mail.gmail.com>
Message-ID: <53BDFD2D.7010402@ca.afilias.info>

On 07/08/2014 11:14 AM, Jan Wieck wrote:
> Why and what application is holding locks on the Slony log tables?
>

His application will hold locks on the sl_log tables by virtue of them 
inserting data into replicated tables.

The application performs an insert => call the log trigger => inserts 
into sl_log_X which will aquire a non exclusive lock for the duration of 
the transaction.

You can't tell from sl_log if the 'log trigger' got the lock or if the 
application directly used the log tables.

My reading of the code in logswitch_finish for current_status=3 is that 
it tries to get the exclusive lock before determining if the table is 
empty or not.   Does this mean if the slon remoteHelper thread is 
pulling data from the log tables (since with log state=3 it queries both 
tables) it will stop the logswitch from going ahead?



> Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>
> On Jul 8, 2014 11:06 AM, "Soni M" <diptatapa at gmail.com
> <mailto:diptatapa at gmail.com>> wrote:
>
>     Sorry for late response,
>     currently :
>     NOTICE:  Slony-I: could not lock sl_log_1 - sl_log_1 not truncated
>     dbname=# select last_value from sl_log_status;
>       last_value
>     ------------
>                3
>     (1 row)
>
>     That's what it should be, right?
>
>     But now, i see application transaction holding locks on sl_log_1,
>     some select, insert and update transaction and also the <IDLE> in
>     transaction just like before. But the row numbers of sl_log_1 not
>     increasing anyway. The row num is increasing in sl_log_2. Is that
>     normal?
>
>     The bug on 2.0.x as You said does happen in our environment, and it
>     happens when the slave falls further behind the master. I should
>     plan to move to 2.1 or 2.2.
>
>
>
>     On Wed, Jul 2, 2014 at 11:02 PM, Steve Singer
>     <ssinger at ca.afilias.info <mailto:ssinger at ca.afilias.info>> wrote:
>
>         On 07/01/2014 08:24 PM, Soni M wrote:
>
>
>             Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from
>             Ubuntu Package
>             Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems
>             build from
>
>
>
>         I should also point out that slony 2.0.x does not properly
>         support PG 9.1 or higher.  You should use slony 2.1.x or 2.2.x
>         with PG 9.1+.
>
>         If you are seeing a lot of transactions being aborted due to
>         serialization conflicts then this is because of the issues with
>         PG 9.1 and slony 2.0 (though it doesn't sound like that is
>         causing this particular issue)
>
>
>
>             source.
>             DB size 1.5 TB, Master utilize relative high number of temp
>             tables and
>             relative High number of locks on busy time
>
>             DB=# select mode, count(*) from pg_locks group by mode;
>                          mode           | count
>             --------------------------+---__-----
>                ExclusiveLock            |    112
>                RowShareLock             |     37
>                AccessExclusiveLock      | 208577
>                RowExclusiveLock         |  33087
>                ShareUpdateExclusiveLock |      5
>                ShareLock                |  54906
>                AccessShareLock          |  93607
>             (7 rows)
>             but the slave is relative low on load.
>
>             this is the connection from slave that open from May 25th
>             postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08
>             postgres: slony_user dbname master_ip_address(58554) idle
>
>             On slave, this connection seems always keep on idle status,
>             but on
>             master, this connection often in "<IDLE> in transaction"
>             status for some
>             minutes and hold lock on sl_log_2 while transaction are
>             filling up sl_log_1.
>
>             Such condition usually happen on server high load time, but
>             on low load
>             it sometimes happen, but not many and does not affect on
>             replication lag.
>
>             On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer
>             <ssinger at ca.afilias.info <mailto:ssinger at ca.afilias.info>
>             <mailto:ssinger at ca.afilias.__info
>             <mailto:ssinger at ca.afilias.info>>> wrote:
>
>                  On 06/30/2014 01:05 PM, Soni M wrote:
>
>                      it seems a slony process that has <IDLE> in
>             transaction for many
>                      times.
>                      the client address and the user are identical to
>             slony slave.
>
>
>
>                  Which version of slony are you on?
>
>
>
>
>
>             --
>             Regards,
>
>             Soni Maula Harriz
>
>
>
>
>
>     --
>     Regards,
>
>     Soni Maula Harriz
>
>     _______________________________________________
>     Slony1-general mailing list
>     Slony1-general at lists.slony.info <mailto:Slony1-general at lists.slony.info>
>     http://lists.slony.info/mailman/listinfo/slony1-general
>


From ssinger at ca.afilias.info  Wed Jul  9 19:43:22 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 09 Jul 2014 22:43:22 -0400
Subject: [Slony1-general] manually delete sl_log_x table
In-Reply-To: <CAAMgDXneq4LxiL=WwZL8=5SKp98EX3G_VuQ0yi=_j+aPsy7H3Q@mail.gmail.com>
References: <CAAMgDX=KUx8Hty2WJXARWSTtKsnKP2Je=f0VVVcP8hwZyaAuHA@mail.gmail.com>	<53AAE50E.7030101@ca.afilias.info>	<CAAMgDXmMB9gJrxgVuBRnvKMewShy2xiV0kygE2Rv+0JK9pLPrA@mail.gmail.com>	<53AAE9CC.6020706@ca.afilias.info>	<CAAMgDXnpdJYESt3zBpVi8tCe-qJgXSM+e=7xiQXzUxKTDwz9kQ@mail.gmail.com>	<53AB741F.907@ca.afilias.info>	<CAAMgDXmWR_pNdm1cwmXy+TWKnitPki+A_S1gKxfsnpzmz1gKFw@mail.gmail.com>	<CAAMgDX=dXYbK_5X+7wbapx-=Kc8cH9b+VjX7Jc6XcPZDqL4iHQ@mail.gmail.com>	<53B2F1DD.9000400@ca.afilias.info>	<CAAMgDXkuAUo-96hamWtE_DiBEbkG1=0OUA9wmovYxMb1Omheqw@mail.gmail.com>	<53B42D0E.9060603@ca.afilias.info>
	<CAAMgDXneq4LxiL=WwZL8=5SKp98EX3G_VuQ0yi=_j+aPsy7H3Q@mail.gmail.com>
Message-ID: <53BDFDCA.7020309@ca.afilias.info>

On 07/08/2014 11:06 AM, Soni M wrote:
> Sorry for late response,
> currently :
> NOTICE:  Slony-I: could not lock sl_log_1 - sl_log_1 not truncated
> dbname=# select last_value from sl_log_status;
>   last_value
> ------------
>            3
> (1 row)
>
> That's what it should be, right?
>
> But now, i see application transaction holding locks on sl_log_1, some
> select, insert and update transaction and also the <IDLE> in transaction
> just like before. But the row numbers of sl_log_1 not increasing anyway.
> The row num is increasing in sl_log_2. Is that normal?
>

Yes in log switch state 3 new rows get inserted into sl_log_2, and the 
slon tries to pull data from both tables.  When all the rows in sl_log_1 
have been replicated (and slon can get that exclusive lock) it truncates 
sl_log_1 and switches the log status to 1.


> The bug on 2.0.x as You said does happen in our environment, and it
> happens when the slave falls further behind the master. I should plan to
> move to 2.1 or 2.2.

I recommend 2.2


>
>
>
> On Wed, Jul 2, 2014 at 11:02 PM, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 07/01/2014 08:24 PM, Soni M wrote:
>
>
>         Master : Ubuntu 10.04 LTS, Postgre 9.1.13, Slony 2.0.7 from
>         Ubuntu Package
>         Slave : Ubuntu 10.04 LTS, Postgres 9.1.13, Slony 2.0.7 seems
>         build from
>
>
>
>     I should also point out that slony 2.0.x does not properly support
>     PG 9.1 or higher.  You should use slony 2.1.x or 2.2.x with PG 9.1+.
>
>     If you are seeing a lot of transactions being aborted due to
>     serialization conflicts then this is because of the issues with PG
>     9.1 and slony 2.0 (though it doesn't sound like that is causing this
>     particular issue)
>
>
>
>         source.
>         DB size 1.5 TB, Master utilize relative high number of temp
>         tables and
>         relative High number of locks on busy time
>
>         DB=# select mode, count(*) from pg_locks group by mode;
>                      mode           | count
>         --------------------------+---__-----
>            ExclusiveLock            |    112
>            RowShareLock             |     37
>            AccessExclusiveLock      | 208577
>            RowExclusiveLock         |  33087
>            ShareUpdateExclusiveLock |      5
>            ShareLock                |  54906
>            AccessShareLock          |  93607
>         (7 rows)
>         but the slave is relative low on load.
>
>         this is the connection from slave that open from May 25th
>         postgres 14090  0.1  1.1 4512644 230760 ?      Ss   May25  59:08
>         postgres: slony_user dbname master_ip_address(58554) idle
>
>         On slave, this connection seems always keep on idle status, but on
>         master, this connection often in "<IDLE> in transaction" status
>         for some
>         minutes and hold lock on sl_log_2 while transaction are filling
>         up sl_log_1.
>
>         Such condition usually happen on server high load time, but on
>         low load
>         it sometimes happen, but not many and does not affect on
>         replication lag.
>
>         On Wed, Jul 2, 2014 at 12:37 AM, Steve Singer
>         <ssinger at ca.afilias.info <mailto:ssinger at ca.afilias.info>
>         <mailto:ssinger at ca.afilias.__info
>         <mailto:ssinger at ca.afilias.info>>> wrote:
>
>              On 06/30/2014 01:05 PM, Soni M wrote:
>
>                  it seems a slony process that has <IDLE> in transaction
>         for many
>                  times.
>                  the client address and the user are identical to slony
>         slave.
>
>
>
>              Which version of slony are you on?
>
>
>
>
>
>         --
>         Regards,
>
>         Soni Maula Harriz
>
>
>
>
>
> --
> Regards,
>
> Soni Maula Harriz


From davecramer at gmail.com  Mon Jul 14 06:16:21 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 14 Jul 2014 09:16:21 -0400
Subject: [Slony1-general] Slony over very long distances
Message-ID: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>

How well does this work. Does anyone have some real world experience with
this ?


Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140714/2fea9384/attachment.html 

From ajs at crankycanuck.ca  Mon Jul 14 06:29:57 2014
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon, 14 Jul 2014 09:29:57 -0400
Subject: [Slony1-general] Slony over very long distances
In-Reply-To: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>
References: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>
Message-ID: <A6D9C59F-F1F7-4CB1-A008-4C1E24B8ED61@crankycanuck.ca>

I don't think "distance" is the problem.  "High rtt links" might be one, though: you can have cases where you time out or never catch up.  

-- 
Andrew Sullivan 
Please excuse my clumbsy thums. 

> On Jul 14, 2014, at 9:16, Dave Cramer <davecramer at gmail.com> wrote:
> 
> 
> How well does this work. Does anyone have some real world experience with this ?
> 
> 
> Dave Cramer
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From davecramer at gmail.com  Mon Jul 14 06:35:15 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Mon, 14 Jul 2014 09:35:15 -0400
Subject: [Slony1-general] Slony over very long distances
In-Reply-To: <A6D9C59F-F1F7-4CB1-A008-4C1E24B8ED61@crankycanuck.ca>
References: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>
	<A6D9C59F-F1F7-4CB1-A008-4C1E24B8ED61@crankycanuck.ca>
Message-ID: <CADK3HHLSbfbaLi=XyEO3cku1M=h_ydYPbnH4CeOm4pu78i_DQQ@mail.gmail.com>

Well distance pretty much translates into high rtt. I am talking about
Toronto to Mumbai or the equivalent

Dave Cramer


On 14 July 2014 09:29, Andrew Sullivan <ajs at crankycanuck.ca> wrote:

> I don't think "distance" is the problem.  "High rtt links" might be one,
> though: you can have cases where you time out or never catch up.
>
> --
> Andrew Sullivan
> Please excuse my clumbsy thums.
>
> > On Jul 14, 2014, at 9:16, Dave Cramer <davecramer at gmail.com> wrote:
> >
> >
> > How well does this work. Does anyone have some real world experience
> with this ?
> >
> >
> > Dave Cramer
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140714/a3c560f8/attachment.htm 

From wmoran at potentialtech.com  Mon Jul 14 06:56:44 2014
From: wmoran at potentialtech.com (Bill Moran)
Date: Mon, 14 Jul 2014 09:56:44 -0400
Subject: [Slony1-general] Slony over very long distances
In-Reply-To: <CADK3HHLSbfbaLi=XyEO3cku1M=h_ydYPbnH4CeOm4pu78i_DQQ@mail.gmail.com>
References: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>
	<A6D9C59F-F1F7-4CB1-A008-4C1E24B8ED61@crankycanuck.ca>
	<CADK3HHLSbfbaLi=XyEO3cku1M=h_ydYPbnH4CeOm4pu78i_DQQ@mail.gmail.com>
Message-ID: <20140714095644.cd55c53ea64a113a1f89a95e@potentialtech.com>

On Mon, 14 Jul 2014 09:35:15 -0400 Dave Cramer <davecramer at gmail.com> wrote:

> Well distance pretty much translates into high rtt. I am talking about
> Toronto to Mumbai or the equivalent

In my previous job I was involved in lots of Slony replication across the
contenental U.S.  (From east to west coast).  The RTT was seldom a problem.
We did have difficulty occasionally when spikes of activity would back
replication up for a day or more, but these were the exceptions, not the
rule, and they almost always recovered.

That only gives you so much info, though.  Your success/failure is going
to be a factor of how much replication traffic you actually generate, what
kind of actual bandwidth you're able to secure between your two sites, and
how much you can tolerate occasional replication lag if there are spikes of
activity or network problems.

> Dave Cramer
> 
> 
> On 14 July 2014 09:29, Andrew Sullivan <ajs at crankycanuck.ca> wrote:
> 
> > I don't think "distance" is the problem.  "High rtt links" might be one,
> > though: you can have cases where you time out or never catch up.
> >
> > --
> > Andrew Sullivan
> > Please excuse my clumbsy thums.
> >
> > > On Jul 14, 2014, at 9:16, Dave Cramer <davecramer at gmail.com> wrote:
> > >
> > >
> > > How well does this work. Does anyone have some real world experience
> > with this ?
> > >
> > >
> > > Dave Cramer
> > > _______________________________________________
> > > Slony1-general mailing list
> > > Slony1-general at lists.slony.info
> > > http://lists.slony.info/mailman/listinfo/slony1-general
> >


-- 
Bill Moran <wmoran at potentialtech.com>

From jan at wi3ck.info  Mon Jul 14 07:45:25 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Mon, 14 Jul 2014 10:45:25 -0400
Subject: [Slony1-general] Slony over very long distances
In-Reply-To: <CADK3HHLSbfbaLi=XyEO3cku1M=h_ydYPbnH4CeOm4pu78i_DQQ@mail.gmail.com>
References: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>	<A6D9C59F-F1F7-4CB1-A008-4C1E24B8ED61@crankycanuck.ca>
	<CADK3HHLSbfbaLi=XyEO3cku1M=h_ydYPbnH4CeOm4pu78i_DQQ@mail.gmail.com>
Message-ID: <53C3ED05.1020206@wi3ck.info>

On 07/14/14 09:35, Dave Cramer wrote:
> Well distance pretty much translates into high rtt. I am talking about
> Toronto to Mumbai or the equivalent

There was a substantial change in communication in version 2.2 where the 
data is pulled via the COPY protocol now as opposed to using a cursor 
and small FETCH operations. This reduces the number of round trips 
required especially when there is a backlog and the system starts 
grouping events together.

You can relatively easy reproduce a WAN scenario in the laboratory using 
WANem, whe Wide Area Network Emulator.

http://wanem.sourceforge.net/

The thing comes as a Knoppix image that is rather easy to set up in a 
virtual machine and use as a router between vlans. You can impose 
bandwidth limits, packet delay (round trip), packet loss, packet 
corruption, packet duplication, packet reordering, idle disconnects, 
random disconnects ... you get the picture.

In my tests a cluster based on 2.2 is mostly affected by bandwidth and 
packet loss due to the delays that can introduce, leading to unused 
bandwidth. High round trip times (delay) are not much of an issue.


Regards,
Jan




>
> Dave Cramer
>
>
> On 14 July 2014 09:29, Andrew Sullivan <ajs at crankycanuck.ca
> <mailto:ajs at crankycanuck.ca>> wrote:
>
>     I don't think "distance" is the problem.  "High rtt links" might be
>     one, though: you can have cases where you time out or never catch up.
>
>     --
>     Andrew Sullivan
>     Please excuse my clumbsy thums.
>
>      > On Jul 14, 2014, at 9:16, Dave Cramer <davecramer at gmail.com
>     <mailto:davecramer at gmail.com>> wrote:
>      >
>      >
>      > How well does this work. Does anyone have some real world
>     experience with this ?
>      >
>      >
>      > Dave Cramer
>      > _______________________________________________
>      > Slony1-general mailing list
>      > Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>      > http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From cbbrowne at afilias.info  Mon Jul 14 08:03:33 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Mon, 14 Jul 2014 11:03:33 -0400
Subject: [Slony1-general] Slony over very long distances
In-Reply-To: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>
References: <CADK3HHK0MFTyYZ05LZkivg4yJDLOEEx_DYA9FEABb+V-17CAmg@mail.gmail.com>
Message-ID: <CANfbgba=MdSMUWzEZ9Pqso2W3nZxZfQ+j7ofAb-vyvbg47hyLQ@mail.gmail.com>

We have had the Asia-to-North-America case; it was fine, so long as we had
sufficient bandwidth.

Latency wasn't great, but Slony actually copes with that quite well.

I think we had more troubles with reliability of links (e.g. - they were
passing through enough intermediaries that occasionally connections would
get deranged) than anything else.

Pre-2.2, it was crucially important for the slon servicing each node to be
local, otherwise the round trip times between slon and database would be
troublesome.  The shift to using streaming (e.g. - copying sl_log_{1,2}
around via COPY)  gets rid of that particular round trip issue.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140714/d57b9ebd/attachment.htm 

From davecramer at gmail.com  Wed Jul 16 09:24:19 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Wed, 16 Jul 2014 12:24:19 -0400
Subject: [Slony1-general] How to get database profiling with slony
Message-ID: <CADK3HHK=sck2Jf-jm=9pAEpLbgSOEL-6cHp0nckECD5vVuKqPw@mail.gmail.com>

I'm looking to figure out the read/write/transactions per day net of slony.

Log grepping is not an option as the app generates too many logs

Any ideas ?

Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140716/ba97aba4/attachment.htm 

From cbbrowne at afilias.info  Wed Jul 16 09:53:42 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 16 Jul 2014 12:53:42 -0400
Subject: [Slony1-general] How to get database profiling with slony
In-Reply-To: <CADK3HHK=sck2Jf-jm=9pAEpLbgSOEL-6cHp0nckECD5vVuKqPw@mail.gmail.com>
References: <CADK3HHK=sck2Jf-jm=9pAEpLbgSOEL-6cHp0nckECD5vVuKqPw@mail.gmail.com>
Message-ID: <CANfbgba=h8bdk3HStFtCYgoBnTkQstJt-Tpez6EPTKKLgyCgoQ@mail.gmail.com>

Hmm.  I'd thought that there were I/U/D stats captured in the logs (which,
I observe, you think isn't entirely suitable).

A look at HEAD is showing that there's a struct in src/slon/remote_worker.c
called PerfMon that seems like it's intended for this sort of thing.

Sadly, the code for this seems to have experienced a bit of bit-rot,
notably in that the struct includes some "Big Tuple" stats that are
obsolete as of Slony 2.2.

Further, it only does *any* reporting at logging level DEBUG1, and you'd
need for it to *always* report at that level to get any data, again, in the
logs, which you weren't keen on.  (And I'll accept that; it's not nice for
it to need to be captured that way.)

There's something not quite right about this activity being solely
considered as "debugging" activity that shouldn't be reported unless you're
"debugging something."

It seems to warrant a bit of brainstorming.

I imagine it a neat idea to capture some data about levels of activity in a
Slony table, thus, at the end of a SYNC, we might write out a tuple with
some of this data into a wee table, and aggregate it somewhere before the
cleanup thread cleans up the "detailed" data.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140716/1a23064a/attachment.htm 

From davecramer at gmail.com  Wed Jul 16 10:14:18 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Wed, 16 Jul 2014 13:14:18 -0400
Subject: [Slony1-general] How to get database profiling with slony
In-Reply-To: <CANfbgba=h8bdk3HStFtCYgoBnTkQstJt-Tpez6EPTKKLgyCgoQ@mail.gmail.com>
References: <CADK3HHK=sck2Jf-jm=9pAEpLbgSOEL-6cHp0nckECD5vVuKqPw@mail.gmail.com>
	<CANfbgba=h8bdk3HStFtCYgoBnTkQstJt-Tpez6EPTKKLgyCgoQ@mail.gmail.com>
Message-ID: <CADK3HHKN8oE=M7q8g7umt=nQ26Q0Huyfm0F4Wri4NrP81K-e1w@mail.gmail.com>

Not sure I was clear enough. The goal is to find out what the read/write/tx
profile of the application is as if slony wasn't there.

If your intent was to suggest that if we aggregated slony stats so that we
could subsequently use them to get the net statistics then yes, this would
work

Dave Cramer


On 16 July 2014 12:53, Christopher Browne <cbbrowne at afilias.info> wrote:

> Hmm.  I'd thought that there were I/U/D stats captured in the logs (which,
> I observe, you think isn't entirely suitable).
>
> A look at HEAD is showing that there's a struct in
> src/slon/remote_worker.c called PerfMon that seems like it's intended for
> this sort of thing.
>
> Sadly, the code for this seems to have experienced a bit of bit-rot,
> notably in that the struct includes some "Big Tuple" stats that are
> obsolete as of Slony 2.2.
>
> Further, it only does *any* reporting at logging level DEBUG1, and you'd
> need for it to *always* report at that level to get any data, again, in the
> logs, which you weren't keen on.  (And I'll accept that; it's not nice for
> it to need to be captured that way.)
>
> There's something not quite right about this activity being solely
> considered as "debugging" activity that shouldn't be reported unless you're
> "debugging something."
>
> It seems to warrant a bit of brainstorming.
>
> I imagine it a neat idea to capture some data about levels of activity in
> a Slony table, thus, at the end of a SYNC, we might write out a tuple with
> some of this data into a wee table, and aggregate it somewhere before the
> cleanup thread cleans up the "detailed" data.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140716/5d6e1a27/attachment.htm 

From cbbrowne at afilias.info  Wed Jul 16 11:52:14 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 16 Jul 2014 14:52:14 -0400
Subject: [Slony1-general] How to get database profiling with slony
In-Reply-To: <CADK3HHKN8oE=M7q8g7umt=nQ26Q0Huyfm0F4Wri4NrP81K-e1w@mail.gmail.com>
References: <CADK3HHK=sck2Jf-jm=9pAEpLbgSOEL-6cHp0nckECD5vVuKqPw@mail.gmail.com>
	<CANfbgba=h8bdk3HStFtCYgoBnTkQstJt-Tpez6EPTKKLgyCgoQ@mail.gmail.com>
	<CADK3HHKN8oE=M7q8g7umt=nQ26Q0Huyfm0F4Wri4NrP81K-e1w@mail.gmail.com>
Message-ID: <CANfbgbbmougM1r_W-h_BYYNt-5LJbMFfHDgKMegDPcGcr7L6mA@mail.gmail.com>

On Wed, Jul 16, 2014 at 1:14 PM, Dave Cramer <davecramer at gmail.com> wrote:

> Not sure I was clear enough. The goal is to find out what the
> read/write/tx profile of the application is as if slony wasn't there.
>

OK, that's a useful clarification.

That makes things a bit harder, as Slony does a fair bit of activity (e.g.
- queries to manage events, queries to determine what data to replicate,
 cleanup of old data) that would also need to be accounted for.

I'm not quite sure how to account for that load.


> If your intent was to suggest that if we aggregated slony stats so that we
> could subsequently use them to get the net statistics then yes, this would
> work
>

Cool, sounds like it's a broadly helpful thing that should be helpful for
your case, and, I'd hope, others.

BTW, Jan has had some thoughts about trying to run the cleanup more often
on the basis that if the cleanup frequency is higher than the Postgres
checkpoint frequency, we might be able to avoid pushing sl_log_* to disk,
which would mean that the cost of replication turns out to be lower than
people were thinking.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140716/7ef88fb7/attachment.htm 

From jan at wi3ck.info  Thu Jul 17 09:07:40 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 17 Jul 2014 12:07:40 -0400
Subject: [Slony1-general] How to get database profiling with slony
In-Reply-To: <CANfbgbbmougM1r_W-h_BYYNt-5LJbMFfHDgKMegDPcGcr7L6mA@mail.gmail.com>
References: <CADK3HHK=sck2Jf-jm=9pAEpLbgSOEL-6cHp0nckECD5vVuKqPw@mail.gmail.com>	<CANfbgba=h8bdk3HStFtCYgoBnTkQstJt-Tpez6EPTKKLgyCgoQ@mail.gmail.com>	<CADK3HHKN8oE=M7q8g7umt=nQ26Q0Huyfm0F4Wri4NrP81K-e1w@mail.gmail.com>
	<CANfbgbbmougM1r_W-h_BYYNt-5LJbMFfHDgKMegDPcGcr7L6mA@mail.gmail.com>
Message-ID: <53C7F4CC.4060406@wi3ck.info>

On 07/16/14 14:52, Christopher Browne wrote:
> On Wed, Jul 16, 2014 at 1:14 PM, Dave Cramer <davecramer at gmail.com
> <mailto:davecramer at gmail.com>> wrote:
>
>     Not sure I was clear enough. The goal is to find out what the
>     read/write/tx profile of the application is as if slony wasn't there.
>
>
> OK, that's a useful clarification.
>
> That makes things a bit harder, as Slony does a fair bit of activity
> (e.g. - queries to manage events, queries to determine what data to
> replicate,  cleanup of old data) that would also need to be accounted for.
>
> I'm not quite sure how to account for that load.
>
>     If your intent was to suggest that if we aggregated slony stats so
>     that we could subsequently use them to get the net statistics then
>     yes, this would work
>
>
> Cool, sounds like it's a broadly helpful thing that should be helpful
> for your case, and, I'd hope, others.
>
> BTW, Jan has had some thoughts about trying to run the cleanup more
> often on the basis that if the cleanup frequency is higher than the
> Postgres checkpoint frequency, we might be able to avoid pushing
> sl_log_* to disk, which would mean that the cost of replication turns
> out to be lower than people were thinking.

Correct. Not only lower in volume, but it would turn out that most of 
the writes added by Slony go to WAL, which are sequential writes and 
thus can often be included in just a slightly larger IO. So that would 
be a double win.


Jan


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From Ger.Timmens at adyen.com  Thu Jul 17 14:45:25 2014
From: Ger.Timmens at adyen.com (Ger Timmens)
Date: Thu, 17 Jul 2014 23:45:25 +0200
Subject: [Slony1-general] slony 2.2.3 experiences
In-Reply-To: <mailman.1.1405623601.5222.slony1-general@lists.slony.info>
References: <mailman.1.1405623601.5222.slony1-general@lists.slony.info>
Message-ID: <53C843F5.8020405@adyen.com>

Hi all,

After upgrading our environment from slony 2.1.4 to slony 2.2.3
we see 'a lot' more slony connections both on master, forwarders
as subscribers.

E.g. on the master we see approx 120 more 'idle' slony connections.
This connections start around 50, groing to 120 over time
(restarting slons, will bring it down and connections will increase again).

This is probably related to the changed failover logic in slony 2.2.x
over 2.1.x where each forwarding note should know about each
other node ?

If so, is there a way to go back to the 2.1.x behaviour (is it
configurable) ?

Thanks,

Ger Timmens

From jan at wi3ck.info  Thu Jul 17 14:58:57 2014
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 17 Jul 2014 17:58:57 -0400
Subject: [Slony1-general] slony 2.2.3 experiences
In-Reply-To: <53C843F5.8020405@adyen.com>
References: <mailman.1.1405623601.5222.slony1-general@lists.slony.info>
	<53C843F5.8020405@adyen.com>
Message-ID: <53C84721.1040704@wi3ck.info>

On 07/17/14 17:45, Ger Timmens wrote:
> Hi all,
>
> After upgrading our environment from slony 2.1.4 to slony 2.2.3
> we see 'a lot' more slony connections both on master, forwarders
> as subscribers.
>
> E.g. on the master we see approx 120 more 'idle' slony connections.
> This connections start around 50, groing to 120 over time
> (restarting slons, will bring it down and connections will increase again).

The number of connections should not go up like that.

How does your cluster configuration look like? 50 connection to start 
with sounds already high.

Are there any error messages in the slon logs? What I can imagine is a 
connection leak in some unusual error situation. Slon reconnects but 
never closes the old connection.



>
> This is probably related to the changed failover logic in slony 2.2.x
> over 2.1.x where each forwarding note should know about each
> other node ?

Not sure about that yet.


Regards,
Jan

-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From Ger.Timmens at adyen.com  Thu Jul 17 15:31:39 2014
From: Ger.Timmens at adyen.com (Ger Timmens)
Date: Fri, 18 Jul 2014 00:31:39 +0200
Subject: [Slony1-general] slony 2.2.3 experiences
In-Reply-To: <53C84721.1040704@wi3ck.info>
References: <mailman.1.1405623601.5222.slony1-general@lists.slony.info>
	<53C843F5.8020405@adyen.com> <53C84721.1040704@wi3ck.info>
Message-ID: <53C84ECB.4080807@adyen.com>

Hi Jan,

> On 07/17/14 17:45, Ger Timmens wrote:
>> Hi all,
>>
>> After upgrading our environment from slony 2.1.4 to slony 2.2.3
>> we see 'a lot' more slony connections both on master, forwarders
>> as subscribers.
>>
>> E.g. on the master we see approx 120 more 'idle' slony connections.
>> This connections start around 50, groing to 120 over time
>> (restarting slons, will bring it down and connections will increase
>> again).
>
> The number of connections should not go up like that.
>
> How does your cluster configuration look like? 50 connection to start
> with sounds already high.

Master ==> Forwarder A, B, C, D

Forwarder A ==> clients (13) in location x (forward =no)
Forwarder B idle (backup for A)
Forwarder C ==> clients (10) in location y (forward =no)
Forwarder D idle (backup for D)


>
> Are there any error messages in the slon logs? What I can imagine is a
> connection leak in some unusual error situation. Slon reconnects but
> never closes the old connection.

Don't see anything strange in the logs, apart from running out of client
connections which we fixed by increasing max_connections.

On a client feed by A:

# select client_addr, usename, count(*) from pg_stat_activity group by 1,2;
 client_addr  |   usename    | count
--------------+--------------+-------
 127.0.0.1    | userA |     3
              | postgres     |     1
 A_ForwarderIP | slony        |    31
 Master IP | slony        |    34
 127.0.0.1    | userB       |     3
(5 rows)

I wouldn't have expect here the slony connections from the master.

>
>
>
>>
>> This is probably related to the changed failover logic in slony 2.2.x
>> over 2.1.x where each forwarding note should know about each
>> other node ?
>
> Not sure about that yet.
>
>
> Regards,
> Jan
>

Kind regards,

Ger


-- 
Ger Timmens
Adyen - Payments Made Easy
http://www.adyen.com

Visiting Address: Kantoorgebouw Nijenburg  Mail Address:
Simon Carmiggeltstraat 6-50, 5th floor     P.O. Box 10095
1011 DJ Amsterdam                          1001 EB Amsterdam
The Netherlands                            The Netherlands

Direct +31.20.240.1248
Office +31.20.240.1240
Mobile +31.62.483.8468
Email ger.timmens at adyen.com



From davecramer at gmail.com  Tue Jul 22 11:53:08 2014
From: davecramer at gmail.com (Dave Cramer)
Date: Tue, 22 Jul 2014 14:53:08 -0400
Subject: [Slony1-general] replicating from 9.3 to 8.4
Message-ID: <CADK3HHKha06vmHnhN5O0aM9j3NWiV97TiYwkCbeP4GSrrv_Z3Q@mail.gmail.com>

Are there any known issues ?

Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140722/a04afecd/attachment.html 

From tailofthesun at gmail.com  Wed Jul 23 18:24:01 2014
From: tailofthesun at gmail.com (Jason Yan)
Date: Wed, 23 Jul 2014 18:24:01 -0700
Subject: [Slony1-general] replicating from 9.3 to 8.4
In-Reply-To: <CADK3HHKha06vmHnhN5O0aM9j3NWiV97TiYwkCbeP4GSrrv_Z3Q@mail.gmail.com>
References: <CADK3HHKha06vmHnhN5O0aM9j3NWiV97TiYwkCbeP4GSrrv_Z3Q@mail.gmail.com>
Message-ID: <CAEMAZu77p4p4ov1qL_FzsdZ2BQLH0pvqTKnakrAHozyotSYadw@mail.gmail.com>

We replicated 8.4 to 9.3 using Slony 2.1, and we had both versions of
PostgreSQL in the cluster at the same time with no problems.  We
hadn't replicated 9.3 databases back to 8.4 though.

On Tue, Jul 22, 2014 at 11:53 AM, Dave Cramer <davecramer at gmail.com> wrote:
> Are there any known issues ?
>
> Dave Cramer
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

