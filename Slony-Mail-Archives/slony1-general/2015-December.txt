From greg at endpoint.com  Fri Dec  4 19:11:42 2015
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Fri, 4 Dec 2015 22:11:42 -0500
Subject: [Slony1-general] remote listener serializability
In-Reply-To: <D2737725.437DF%ttignor@akamai.com>
References: <D26F49AE.41208%ttignor@akamai.com>
	<564A203B.9030806@ca.afilias.info>
	<D271E352.42CC8%ttignor@akamai.com>
	<20151118153548.GG31515@localhost.localdomain>
	<D2737725.437DF%ttignor@akamai.com>
Message-ID: <20151205031142.GH3132@localhost.localdomain>

On Thu, Nov 19, 2015 at 06:09:54PM +0000, Tignor, Tom wrote:
> 	Thanks for the feedback. Greg, can you describe the transaction handling
> changes you?re referring to?

Sorry, my bad - I misremembered the serializable overhaul as happening 
in 9.2, not 9.1.

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20151204/7049196b/attachment.pgp 

From josh at agliodbs.com  Mon Dec  7 10:56:36 2015
From: josh at agliodbs.com (Josh Berkus)
Date: Mon, 7 Dec 2015 10:56:36 -0800
Subject: [Slony1-general] prepare clone failure
Message-ID: <5665D664.8030700@agliodbs.com>

Slony folks:

I'm being blocked by an interesting failure of "prepare/finish clone".
There's a bit of a setup on this one, but the complexity of the cluster
may be related to the failure, so I want to give you everything.

Versions:
PostgreSQL 9.2.14
Slony 2.1.4

4 replication sets

5 nodes:
	4: origin of sets 1, 2 and 3
	5: failover for 4, subscribes to 1,2,3
	6: origin of set 4, subscribes to 2
	7: failover for 6, subsribes to 2,4
	10: on AWS, mirror of 6, subscribes to 2,4 from origin

The owner is creating new nodes on AWS which are copies of node 6, for
expanding capacity and testing purposes.  The fastest way for us to spin
up new nodes on AWS works like this:

1. create a new EC2 instance
2. prepare clone of 10
3. make AWS snapshot copy of 10
4. bring up PostgreSQL on the new node
5. finish clone for new node
6. start slony on the new node

We followed this procedure to bring up nodes in this order:

- original clone on AWS was node 8.
- created node 9 via prepare clone method
- dropped node 8 (and shut down instance)
- created node 10 via prepare clone method
- dropped node 9 (but did not shut down the instance)

So, the prepare clone method above worked perfectly twice.  But then we
tried to bring up a new node as a prepared clone from node 11 and things
went to hell.

At step 6, when we brought up slony, we started to see this in the logs:

2015-12-04 14:40:21 PST ERROR slon_connectdb: PQconnectdb("dbname=prod
host=192.168.80.32 port=5432 user=slony") failed - FATAL: no pg_hba.conf
entry for host "172.16.81.31", user "slony", database "prod", SSL off
2015-12-04 14:40:21 PST WARN remoteListenThread_6: DB connection failed
- sleep 10 seconds
2015-12-04 14:40:21 PST CONFIG version for "dbname=prod" is 90214
2015-12-04 14:40:21 PST CONFIG version for "dbname=prod
host=dw3.prod.com port=5432 user=slony" is 90214
2015-12-04 14:40:21 PST ERROR slon_connectdb: PQconnectdb("dbname=prod
host=192.168.80.33 port=5432 user=slony") failed - FATAL: no pg_hba.conf
entry for host "172.16.81.31", user "slony", database "prod", SSL off
2015-12-04 14:40:21 PST WARN remoteListenThread_7: DB connection failed
- sleep 10 seconds
2015-12-04 14:40:21 PST CONFIG remoteWorkerThread_10: update provider
configuration
2015-12-04 14:40:21 PST CONFIG remoteWorkerThread_7: update provider
configuration
2015-12-04 14:40:21 PST ERROR remoteListenThread_10: db_getLocalNodeId()
returned 12 - wrong database?
2015-12-04 14:40:21 PST CONFIG version for "dbname=prod
host=192.168.80.43 port=5432 user=slony" is 90214
2015-12-04 14:40:21 PST CONFIG version for "dbname=prod
host=192.168.80.43 port=5432 user=slony" is 90214
2015-12-04 14:40:21 PST INFO remoteWorkerThread_4: syncing set 2 with
118 table(s) from provider 5
2015-12-04 14:40:21 PST INFO remoteWorkerThread_4: SYNC 5009308944 done
in 0.098 seconds
2015-12-04 14:40:22 PST CONFIG version for "dbname=prod
host=192.168.80.42 port=5432 user=slony" is 90214
2015-12-04 14:40:22 PST ERROR remoteWorkerThread_5: "lock table
"_replication".sl_config_lock;select "_replication".storePath_int(13, 5,
'dbname=prod host=172.16.81.31 port=5432 user=slony', 10); insert into
"_oltp_replication".sl_event (ev_origin, ev_seqno, ev_timestamp,
ev_snapshot, ev_type , ev_data1, ev_data2, ev_data3, ev_data4 ) values
('5', '5001941723', '2015-12-04 14:37:48.196237-08',
'70559671:70559671:', 'STORE_PATH', '13', '5', 'dbname=prod
host=172.16.81.31 port=5432 user=slony', '10'); insert into
"_replication".sl_confirm (con_origin, con_received, con_seqno,
con_timestamp) values (5, 13, '5001941723', now()); commit transaction;"
PGRES_FATAL_ERROR ERROR: duplicate key value violates unique constraint
"sl_event-pkey"
DETAIL: Key (ev_origin, ev_seqno)=(5, 5001941723) already exists.
2015-12-04 14:40:22 PST CONFIG slon: child terminated signal: 9; pid:
4539, current worker pid: 4539
2015-12-04 14:40:22 PST CONFIG slon: restart of worker in 10 seconds
2015-12-04 14:40:25 PST CONFIG slon: child terminated status: 9; pid:
-1, current worker pid: 4511 errno: 10
2015-12-04 14:40:25 PST CONFIG slon: child terminated status: 9; pid:
-1, current worker pid: 4539 errno: 10
2015-12-04 14:40:25 PST FATAL slon: wait returned an error pid:-1 errno:10
2015-12-04 14:40:25 PST FATAL slon: wait returned an error pid:-1 errno:1

This now happens *every time* we try the prepare clone sequence (3 out
of 3 tries).  Any idea what's going on here?

-- 
Josh Berkus
PostgreSQL Experts Inc.
http://pgexperts.com

From josh at agliodbs.com  Mon Dec  7 11:32:53 2015
From: josh at agliodbs.com (Josh Berkus)
Date: Mon, 7 Dec 2015 11:32:53 -0800
Subject: [Slony1-general] prepare clone failure
In-Reply-To: <WM!81d85ebcba1de70270b3d059027101ab81a99991a0eb54bdd471f6d5c36af4a4418d0fc70d43b28eff46ad97eab61e31!@asav-3.01.com>
References: <5665D664.8030700@agliodbs.com>
	<WM!81d85ebcba1de70270b3d059027101ab81a99991a0eb54bdd471f6d5c36af4a4418d0fc70d43b28eff46ad97eab61e31!@asav-3.01.com>
Message-ID: <5665DEE5.2080704@agliodbs.com>

On 12/07/2015 10:56 AM, Josh Berkus wrote:
> So, the prepare clone method above worked perfectly twice.  But then we
> tried to bring up a new node as a prepared clone from node 11 and things
> went to hell.

One thing I just realized was different between the first two,
successful, runs and the failed runs:  the first two times, we didn't
have pg_hba.conf configured, so when we brought up slony on the new node
it couldn't connect until we fixed that.

So I'm wondering if there's a timing issue here somewhere.

-- 
Josh Berkus
PostgreSQL Experts Inc.
http://pgexperts.com

From josh at agliodbs.com  Mon Dec  7 18:25:17 2015
From: josh at agliodbs.com (Josh Berkus)
Date: Mon, 7 Dec 2015 18:25:17 -0800
Subject: [Slony1-general] prepare clone failure
In-Reply-To: <WM!51fab314cebfda919e3cd02362ce67093bedd9db46bf2b04f04dcaef34dd21c5c3fd459d99577e2f7845e3261f999105!@asav-3.01.com>
References: <5665D664.8030700@agliodbs.com>
	<WM!81d85ebcba1de70270b3d059027101ab81a99991a0eb54bdd471f6d5c36af4a4418d0fc70d43b28eff46ad97eab61e31!@asav-3.01.com>
	<5665DEE5.2080704@agliodbs.com>
	<WM!51fab314cebfda919e3cd02362ce67093bedd9db46bf2b04f04dcaef34dd21c5c3fd459d99577e2f7845e3261f999105!@asav-3.01.com>
Message-ID: <56663F8D.5070800@agliodbs.com>

On 12/07/2015 11:32 AM, Josh Berkus wrote:
> On 12/07/2015 10:56 AM, Josh Berkus wrote:
>> So, the prepare clone method above worked perfectly twice.  But then we
>> tried to bring up a new node as a prepared clone from node 11 and things
>> went to hell.
> 
> One thing I just realized was different between the first two,
> successful, runs and the failed runs:  the first two times, we didn't
> have pg_hba.conf configured, so when we brought up slony on the new node
> it couldn't connect until we fixed that.
> 
> So I'm wondering if there's a timing issue here somewhere.

So, this problem was less interesting than I thought.  As it turns out,
the sysadmin was handling "make sure slony doesn't start on the server"
by letting it autostart, then shutting it down.  In the couple minutes
it was running, though, it did enough to prevent finish clone from working.

-- 
Josh Berkus
PostgreSQL Experts Inc.
http://pgexperts.com

From ssinger at ca.afilias.info  Mon Dec  7 18:48:05 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 07 Dec 2015 21:48:05 -0500
Subject: [Slony1-general] prepare clone failure
In-Reply-To: <56663F8D.5070800@agliodbs.com>
References: <5665D664.8030700@agliodbs.com>	<WM!81d85ebcba1de70270b3d059027101ab81a99991a0eb54bdd471f6d5c36af4a4418d0fc70d43b28eff46ad97eab61e31!@asav-3.01.com>	<5665DEE5.2080704@agliodbs.com>	<WM!51fab314cebfda919e3cd02362ce67093bedd9db46bf2b04f04dcaef34dd21c5c3fd459d99577e2f7845e3261f999105!@asav-3.01.com>
	<56663F8D.5070800@agliodbs.com>
Message-ID: <566644E5.1070806@ca.afilias.info>

On 12/07/2015 09:25 PM, Josh Berkus wrote:
> On 12/07/2015 11:32 AM, Josh Berkus wrote:
>> On 12/07/2015 10:56 AM, Josh Berkus wrote:
>>> So, the prepare clone method above worked perfectly twice.  But then we
>>> tried to bring up a new node as a prepared clone from node 11 and things
>>> went to hell.
>>
>> One thing I just realized was different between the first two,
>> successful, runs and the failed runs:  the first two times, we didn't
>> have pg_hba.conf configured, so when we brought up slony on the new node
>> it couldn't connect until we fixed that.
>>
>> So I'm wondering if there's a timing issue here somewhere.
>
> So, this problem was less interesting than I thought.  As it turns out,
> the sysadmin was handling "make sure slony doesn't start on the server"
> by letting it autostart, then shutting it down.  In the couple minutes
> it was running, though, it did enough to prevent finish clone from working.
>


I wonder if there is more going on here


In remoteWorker_event

We have

	if (node->last_event >= ev_seqno)
	{
		rtcfg_unlock();
		slon_log(SLON_DEBUG2,
				 "remoteWorker_event: event %d," INT64_FORMAT
				 " ignored - duplicate\n",
				 ev_origin, ev_seqno);
		return;
	}

	/*
	 * We lock the worker threads message queue before bumping the nodes last
	 * known event sequence to avoid that another listener queues a later
	 * message before we can insert this one.
	 */
	pthread_mutex_lock(&(node->message_lock));
	node->last_event = ev_seqno;
	rtcfg_unlock();


It seems strange to me that we are obtaining the mutex lock after 
checking node->last_event.
Does the rtcfg_lock prevent the race condition making the direct 
message_lock redundent? If not do we need to obtain the 
node->message_lock before we do the comparision?


The CLONE_NODE handler in remote_worker sets last_event by calling 
rtcfg_getNodeLastEvent which obtains the rtcfg_lock but not the message 
lock.

The clone node handler in remote_worker seems to do this
1. call rtcfg_storeNode (which obtains then releases the config lock)
2. calls cloneNodePrepare_int()
3. queries the last event id
4. calls rtcfg_getNodeLastEvent() which would re-obtain then release the 
config lock

I wonder if sometime after step 1 but before step 4 a remote listener 
queries events from the new node and adds them into the queue because 
the last_event hasn't yet been set.

Maybe cloneNodePrepare needs to obtain the message queue lock at step 1 
and hold it until step 4 and then remoteWorker_event needs to obtain 
that lock a bit earlier




From glynastill at yahoo.co.uk  Tue Dec  8 06:04:34 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue, 8 Dec 2015 14:04:34 +0000 (UTC)
Subject: [Slony1-general] prepare clone failure
In-Reply-To: <56663F8D.5070800@agliodbs.com>
References: <56663F8D.5070800@agliodbs.com>
Message-ID: <546498346.28227372.1449583474812.JavaMail.yahoo@mail.yahoo.com>


> From: Josh Berkus <josh at agliodbs.com>
>To: slony1-general at lists.slony.info 
>Sent: Tuesday, 8 December 2015, 2:25
>Subject: Re: [Slony1-general] prepare clone failure
> 
>
>On 12/07/2015 11:32 AM, Josh Berkus wrote:
>> On 12/07/2015 10:56 AM, Josh Berkus wrote:
>>> So, the prepare clone method above worked perfectly twice.  But then we
>>> tried to bring up a new node as a prepared clone from node 11 and things
>>> went to hell.
>> 
>> One thing I just realized was different between the first two,
>> successful, runs and the failed runs:  the first two times, we didn't
>> have pg_hba.conf configured, so when we brought up slony on the new node
>> it couldn't connect until we fixed that.
>> 
>> So I'm wondering if there's a timing issue here somewhere.
>
>So, this problem was less interesting than I thought.  As it turns out,
>the sysadmin was handling "make sure slony doesn't start on the server"
>by letting it autostart, then shutting it down.  In the couple minutes
>it was running, though, it did enough to prevent finish clone from working.
>


I've seen similar before.  I recall being very blas? once and managing to start a slon daemon whilst clone finish was still running, with similar but slightly more subtle behaviour.

From ttignor at akamai.com  Thu Dec 10 11:34:21 2015
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 10 Dec 2015 19:34:21 +0000
Subject: [Slony1-general] remote listener serializability
In-Reply-To: <20151205031142.GH3132@localhost.localdomain>
References: <D26F49AE.41208%ttignor@akamai.com>
	<564A203B.9030806@ca.afilias.info> <D271E352.42CC8%ttignor@akamai.com>
	<20151118153548.GG31515@localhost.localdomain>
	<D2737725.437DF%ttignor@akamai.com>
	<20151205031142.GH3132@localhost.localdomain>
Message-ID: <D28F3CF0.46127%ttignor@akamai.com>


	Hello slony1 folks,
	FYI - I?ve just circled back on this. As advised by Steve S, I?ve updated
bug 336 with our discussion and ideas for providing a change in an
upcoming release. Certainly let me know if there are new thoughts or other
info I can provide. I put myself on the bug cc list, so maybe interested
folks can comment through Bugzilla.
	Thanks,

	Tom    :-)



On 12/4/15, 10:11 PM, "Greg Sabino Mullane" <greg at endpoint.com> wrote:

>On Thu, Nov 19, 2015 at 06:09:54PM +0000, Tignor, Tom wrote:
>> 	Thanks for the feedback. Greg, can you describe the transaction
>>handling
>> changes you?re referring to?
>
>Sorry, my bad - I misremembered the serializable overhaul as happening
>in 9.2, not 9.1.
>
>-- 
>Greg Sabino Mullane greg at endpoint.com
>End Point Corporation
>PGP Key: 0x14964AC8


From mike.james at clutch.com  Thu Dec 10 13:17:21 2015
From: mike.james at clutch.com (Mike James)
Date: Thu, 10 Dec 2015 16:17:21 -0500
Subject: [Slony1-general] renaming / archiving a table
Message-ID: <CADeyJsUipCdLnKiYzEZbX43DQe_aGEgRWFbm3wuh=E4DKT4KwA@mail.gmail.com>

One of our devs wants to "archive" a table by renaming it and then
re-creating a new blank one. Something like this:

rename "loglist" to "loglist-2015"
create loglist

I don't need to replicate loglist-2015. What effect will this have on the
replication of the loglist table? Is there a better or best practice to do
this?

TIA, Mike
  [image: Clutch Holdings, LLC] <http://www.clutch.com> Mike James |
Manager of Infrastructure
267.419.6400, ext 204 | mike.james at clutch.com201 S Maple St. | Suite 250 |
Ambler, PA 19002
Clutch.com <http://www.clutch.com> | Twitter
<https://twitter.com/clutchsuccess> | LinkedIn
<https://www.linkedin.com/company/2837209> | YouTube
<https://www.youtube.com/user/clutchsuccess> | Clutch Support Center
<http://clientsupport.clutch.com/> The only end to end consumer management
platform that empowers consumer-focused businesses to identify, target,
message, and engage their best customers.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20151210/2cab3693/attachment.htm 

From ajs at crankycanuck.ca  Thu Dec 10 13:36:09 2015
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu, 10 Dec 2015 16:36:09 -0500
Subject: [Slony1-general] renaming / archiving a table
In-Reply-To: <CADeyJsUipCdLnKiYzEZbX43DQe_aGEgRWFbm3wuh=E4DKT4KwA@mail.gmail.com>
References: <CADeyJsUipCdLnKiYzEZbX43DQe_aGEgRWFbm3wuh=E4DKT4KwA@mail.gmail.com>
Message-ID: <20151210213609.GF748@anvilwalrusden.com>

On Thu, Dec 10, 2015 at 04:17:21PM -0500, Mike James wrote:
> One of our devs wants to "archive" a table by renaming it and then
> re-creating a new blank one. Something like this:
> 
> rename "loglist" to "loglist-2015"
> create loglist
> 
> I don't need to replicate loglist-2015. What effect will this have on the
> replication of the loglist table? Is there a better or best practice to do
> this?

Your life will probably be better if instead the person does

    CREATE TABLE loglist-2015 AS SELECT * FROM loglist;
    TRUNCATE loglist;

That assumes you're on post-8.4, but I hope you are!

A

-- 
Andrew Sullivan
ajs at anvilwalrusden.com


From gvasquez at waypoint.cl  Thu Dec 17 05:36:44 2015
From: gvasquez at waypoint.cl (=?utf-8?Q?=22Gonzalo_V=C3=A1squez_=40=C2=A0Waypoint=22?=)
Date: Thu, 17 Dec 2015 10:36:44 -0300
Subject: [Slony1-general] Slow "first" synch
Message-ID: <74903940-B27C-404E-9996-E12866D21303@waypoint.cl>

Hi everybody, it seems initially Slony is sending everything to the slave node (700 GB), and not just the ?delta? data on the first sync. Once finished initial synchronization it is actually only sends new/modified rows to the slave node. Is this ?first behavior? expected? If not, what could be misconfigured?

Tried this with different OS (RHEL and Debian) and PostgreSQL versions, so I guess it?s no point stating which ones we?ve used/tested so far.


Atentamente / Regards,

Gonzalo V?squez S?ez
gvasquez at waypoint.cl <mailto:gvasquez at waypoint.cl>
+56 (2) 2963 4180
Director I+D / R&D Director
Waypoint Telecomunicaciones S.A.

Alfredo Barros Err?zuriz 1953 Of. 1004
7500550
Providencia, Santiago, Chile
Mapcode: R3.BR










-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20151217/1f12fa05/attachment.htm 

From vivek at khera.org  Thu Dec 17 05:55:02 2015
From: vivek at khera.org (Vick Khera)
Date: Thu, 17 Dec 2015 08:55:02 -0500
Subject: [Slony1-general] Slow "first" synch
In-Reply-To: <74903940-B27C-404E-9996-E12866D21303@waypoint.cl>
References: <74903940-B27C-404E-9996-E12866D21303@waypoint.cl>
Message-ID: <CALd+dcdKK05ysA5o2MK6nnZkpcE4u_akwvhu7ArR-57xxd=ntQ@mail.gmail.com>

On Thu, Dec 17, 2015 at 8:36 AM, "Gonzalo V?squez @ Waypoint"
<gvasquez at waypoint.cl> wrote:
> Hi everybody, it seems initially Slony is sending everything to the slave
> node (700 GB), and not just the ?delta? data on the first sync. Once
> finished initial synchronization it is actually only sends new/modified rows
> to the slave node. Is this ?first behavior? expected? If not, what could be
> misconfigured?

Yes it is. Slony has to know exactly what is on the replica in order
to keep it consistent. The best way to do that is to start with an
empty database and fill it in.

There is a way to tell slony you already have a copy there, but it has
to be done with a quiet database (ie, no other connections modifying
the DB at all).

From gvasquez at waypoint.cl  Thu Dec 17 05:56:55 2015
From: gvasquez at waypoint.cl (=?utf-8?Q?=22Gonzalo_V=C3=A1squez_=40=C2=A0Waypoint=22?=)
Date: Thu, 17 Dec 2015 10:56:55 -0300
Subject: [Slony1-general] Slow "first" synch
In-Reply-To: <CALd+dcdKK05ysA5o2MK6nnZkpcE4u_akwvhu7ArR-57xxd=ntQ@mail.gmail.com>
References: <74903940-B27C-404E-9996-E12866D21303@waypoint.cl>
	<CALd+dcdKK05ysA5o2MK6nnZkpcE4u_akwvhu7ArR-57xxd=ntQ@mail.gmail.com>
Message-ID: <2C3FDFE4-34D1-42EF-8C48-98DC6FB4B2D7@waypoint.cl>

Thanks Vick such a quick answer. I can tolerate a ?quiet? database for a while, so I?d appreciate if you could please elaborate more on such.

Atentamente / Regards,

Gonzalo V?squez S?ez
gvasquez at waypoint.cl
+56 (2) 2963 4180
Director I+D / R&D Director
Waypoint Telecomunicaciones S.A.

Alfredo Barros Err?zuriz 1953 Of. 1004
7500550
Providencia, Santiago, Chile
Mapcode: R3.BR










> El 17-12-2015, a las 10:55 a.m., Vick Khera <vivek at khera.org> escribi?:
> 
> On Thu, Dec 17, 2015 at 8:36 AM, "Gonzalo V?squez @ Waypoint"
> <gvasquez at waypoint.cl> wrote:
>> Hi everybody, it seems initially Slony is sending everything to the slave
>> node (700 GB), and not just the ?delta? data on the first sync. Once
>> finished initial synchronization it is actually only sends new/modified rows
>> to the slave node. Is this ?first behavior? expected? If not, what could be
>> misconfigured?
> 
> Yes it is. Slony has to know exactly what is on the replica in order
> to keep it consistent. The best way to do that is to start with an
> empty database and fill it in.
> 
> There is a way to tell slony you already have a copy there, but it has
> to be done with a quiet database (ie, no other connections modifying
> the DB at all).


From glynastill at yahoo.co.uk  Thu Dec 17 08:27:20 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 17 Dec 2015 16:27:20 +0000 (UTC)
Subject: [Slony1-general] Slow "first" synch
In-Reply-To: <2C3FDFE4-34D1-42EF-8C48-98DC6FB4B2D7@waypoint.cl>
References: <2C3FDFE4-34D1-42EF-8C48-98DC6FB4B2D7@waypoint.cl>
Message-ID: <2053631902.524343.1450369640652.JavaMail.yahoo@mail.yahoo.com>



> From: "Gonzalo V?squez @ Waypoint" <gvasquez at waypoint.cl>
> To: Vick Khera <vivek at khera.org>
> Cc: slony <slony1-general at lists.slony.info>
> Sent: Thursday, 17 December 2015, 13:56
> Subject: Re: [Slony1-general] Slow "first" synch
> 
> Thanks Vick such a quick answer. I can tolerate a ?quiet? database for a while, 
> so I?d appreciate if you could please elaborate more on such.
> 


Do your initial subscribe with OMIT COPY = ture:

    http://slony.info/documentation/stmtsubscribeset.html

From gvasquez at waypoint.cl  Thu Dec 17 10:45:33 2015
From: gvasquez at waypoint.cl (=?utf-8?Q?=22Gonzalo_V=C3=A1squez_=40=C2=A0Waypoint=22?=)
Date: Thu, 17 Dec 2015 15:45:33 -0300
Subject: [Slony1-general] Slow "first" synch
In-Reply-To: <2053631902.524343.1450369640652.JavaMail.yahoo@mail.yahoo.com>
References: <2C3FDFE4-34D1-42EF-8C48-98DC6FB4B2D7@waypoint.cl>
	<2053631902.524343.1450369640652.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <EBF5365B-C8C9-459F-9FBD-1279D7ED2C8D@waypoint.cl>

Glyn thanks for the tip! We?ll try that.-


Atentamente / Regards,

Gonzalo V?squez S?ez
gvasquez at waypoint.cl
+56 (2) 2963 4180
Director I+D / R&D Director
Waypoint Telecomunicaciones S.A.

Alfredo Barros Err?zuriz 1953 Of. 1004
7500550
Providencia, Santiago, Chile
Mapcode: R3.BR










> El 17-12-2015, a las 1:27 p.m., Glyn Astill <glynastill at yahoo.co.uk> escribi?:
> 
> 
> 
>> From: "Gonzalo V?squez @ Waypoint" <gvasquez at waypoint.cl>
>> To: Vick Khera <vivek at khera.org>
>> Cc: slony <slony1-general at lists.slony.info>
>> Sent: Thursday, 17 December 2015, 13:56
>> Subject: Re: [Slony1-general] Slow "first" synch
>> 
>> Thanks Vick such a quick answer. I can tolerate a ?quiet? database for a while, 
>> so I?d appreciate if you could please elaborate more on such.
>> 
> 
> 
> Do your initial subscribe with OMIT COPY = ture:
> 
>    http://slony.info/documentation/stmtsubscribeset.html


From gvasquez at waypoint.cl  Thu Dec 17 10:50:34 2015
From: gvasquez at waypoint.cl (=?utf-8?Q?=22Gonzalo_V=C3=A1squez_=40=C2=A0Waypoint=22?=)
Date: Thu, 17 Dec 2015 15:50:34 -0300
Subject: [Slony1-general] Wildcards in table specification?
Message-ID: <D5A8F61A-C91A-43DA-A4FF-1647931ECF10@waypoint.cl>

Can one specify a wildcard in the list of tables, or perhaps a whole schema for replication instead of enumerating the whole table list?

We have a certain group of tables that are created on a regular basis. Table creation on the slave node is not an issue as they are created by triggers, but adding them to slony config seems to be an issue.


Atentamente / Regards,

Gonzalo V?squez S?ez
gvasquez at waypoint.cl <mailto:gvasquez at waypoint.cl>
+56 (2) 2963 4180
Director I+D / R&D Director
Waypoint Telecomunicaciones S.A.

Alfredo Barros Err?zuriz 1953 Of. 1004
7500550
Providencia, Santiago, Chile
Mapcode: R3.BR










-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20151217/d42f799a/attachment-0001.htm 

From tmblue at gmail.com  Thu Dec 17 12:21:58 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 17 Dec 2015 12:21:58 -0800
Subject: [Slony1-general] Data in sl_log table that was not replicated,
Message-ID: <CAEaSS0YGS-PgtKp=ymit4xE3jU-S1+rvFsr9fZ=M=bKr+-Pg-A@mail.gmail.com>

I've done a switchover and as part of a migration to another center, but
there was data that didn't replicate, we are unclear why but will be
investigating.

The question at hand, is there any way to dump the sl_log file and somehow
get it into the new cluster (that new cluster does not have this data).
It's been a couple of days so I can't just dump and load the data (more
data would be lost), so it's really about retrieving what is in sl_log and
getting that into the new cluster.

If not, no big deal, but figured I would ask if there is a way.

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20151217/f17111f6/attachment.htm 

From cbbrowne at afilias.info  Thu Dec 17 13:21:15 2015
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 17 Dec 2015 16:21:15 -0500
Subject: [Slony1-general] Wildcards in table specification?
In-Reply-To: <D5A8F61A-C91A-43DA-A4FF-1647931ECF10@waypoint.cl>
References: <D5A8F61A-C91A-43DA-A4FF-1647931ECF10@waypoint.cl>
Message-ID: <CANfbgbZFUNzPDhthiK5s_ziGev_jp=tHPih4yiXVrr7nP1Kysg@mail.gmail.com>

On Thu, Dec 17, 2015 at 1:50 PM, "Gonzalo V?squez @ Waypoint" <
gvasquez at waypoint.cl> wrote:
>
> Can one specify a wildcard in the list of tables, or perhaps a whole
schema for replication instead of enumerating the whole table list?
>
> We have a certain group of tables that are created on a regular basis.
Table creation on the slave node is not an issue as they are created by
triggers, but adding them to slony config seems to be an issue.
>

http://slony.info/documentation/2.2/stmtsetaddtable.html

A set of tables may be specified via a regular expression, thus:

SET ADD TABLE (
    SET ID=1,
    TABLES='public\\.tracker*'
);

The "sharp edge" to that is that if you request that repeatedly, it would
try to re-add tables already being replicated, which presumably fails.

This represents mechanism for what you asked for, all the same.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20151217/ea390600/attachment.htm 

