From ssinger at ca.afilias.info  Mon Jan  5 06:23:47 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 05 Jan 2015 09:23:47 -0500
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <20141231005027.GB25019@fetter.org>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
	<54933BCF.5040003@ca.afilias.info>
	<20141231005027.GB25019@fetter.org>
Message-ID: <54AA9E73.5070109@ca.afilias.info>

On 12/30/2014 07:50 PM, David Fetter wrote:
> On Thu, Dec 18, 2014 at 03:40:47PM -0500, Steve Singer wrote:
>> On 12/18/2014 12:42 PM, Scott Marlowe wrote:
>>
>> I have added a patch for this against
>>
>> http://bugs.slony.info/bugzilla/show_bug.cgi?id=356
>
> ETA on 2.2.4?  This seems like a pretty serious data loss bug and a
> pretty small patch to fix it.
>
> Cheers,
> David.
>

I'd like to see us aim for a 2.2 release maybe next week that includes 
fixes for bug 356,354  and also removes the 'unsupported warning' for 
9.4. Bugs 338 and 345 are already committed on the 2.2 branch and would 
be included as well.

A few weeks ago I said I wanted to do a release close to the end of the 
year but that didn't happen. I make no promises with the above, other 
people might have other ideas.

If the patch (suitable for 2.2) for bug 350 is ready in the next few 
days I'd say we should include it.  Otherwise maybe we want to clarify 
the upgrade documentation to better describe the existing behaviour.



From david at fetter.org  Mon Jan  5 07:04:44 2015
From: david at fetter.org (David Fetter)
Date: Mon, 5 Jan 2015 07:04:44 -0800
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <54AA9E73.5070109@ca.afilias.info>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
	<54933BCF.5040003@ca.afilias.info>
	<20141231005027.GB25019@fetter.org>
	<54AA9E73.5070109@ca.afilias.info>
Message-ID: <20150105150444.GA25657@fetter.org>

On Mon, Jan 05, 2015 at 09:23:47AM -0500, Steve Singer wrote:
> On 12/30/2014 07:50 PM, David Fetter wrote:
> >On Thu, Dec 18, 2014 at 03:40:47PM -0500, Steve Singer wrote:
> >>On 12/18/2014 12:42 PM, Scott Marlowe wrote:
> >>
> >>I have added a patch for this against
> >>
> >>http://bugs.slony.info/bugzilla/show_bug.cgi?id=356
> >
> >ETA on 2.2.4?  This seems like a pretty serious data loss bug and a
> >pretty small patch to fix it.
> >
> 
> I'd like to see us aim for a 2.2 release maybe next week that includes fixes
> for bug 356,354  and also removes the 'unsupported warning' for 9.4. Bugs
> 338 and 345 are already committed on the 2.2 branch and would be included as
> well.

Thanks for the information.

> A few weeks ago I said I wanted to do a release close to the end of
> the year but that didn't happen.  I make no promises with the above,
> other people might have other ideas.

What other stakeholders are involved, and do you have any ideas about
how they might weigh in?

> If the patch (suitable for 2.2) for bug 350 is ready in the next few
> days I'd say we should include it.  Otherwise maybe we want to
> clarify the upgrade documentation to better describe the existing
> behaviour.

350 is well above my experience level to fix, but I'd be happy to
document the issue if that's what's needed.

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From ssinger at ca.afilias.info  Mon Jan  5 08:07:13 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 05 Jan 2015 11:07:13 -0500
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <20150105150444.GA25657@fetter.org>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
	<54933BCF.5040003@ca.afilias.info>
	<20141231005027.GB25019@fetter.org>
	<54AA9E73.5070109@ca.afilias.info>
	<20150105150444.GA25657@fetter.org>
Message-ID: <54AAB6B1.4020802@ca.afilias.info>

On 01/05/2015 10:04 AM, David Fetter wrote:
> On Mon, Jan 05, 2015 at 09:23:47AM -0500, Steve Singer wrote:
>> On 12/30/2014 07:50 PM, David Fetter wrote:
>>> On Thu, Dec 18, 2014 at 03:40:47PM -0500, Steve Singer wrote:
>>>> On 12/18/2014 12:42 PM, Scott Marlowe wrote:
>>>>
>>>> I have added a patch for this against
>>>>
>>>> http://bugs.slony.info/bugzilla/show_bug.cgi?id=356
>>>
>>> ETA on 2.2.4?  This seems like a pretty serious data loss bug and a
>>> pretty small patch to fix it.
>>>
>>
>> I'd like to see us aim for a 2.2 release maybe next week that includes fixes
>> for bug 356,354  and also removes the 'unsupported warning' for 9.4. Bugs
>> 338 and 345 are already committed on the 2.2 branch and would be included as
>> well.
>
> Thanks for the information.
>
>> A few weeks ago I said I wanted to do a release close to the end of
>> the year but that didn't happen.  I make no promises with the above,
>> other people might have other ideas.
>
> What other stakeholders are involved, and do you have any ideas about
> how they might weigh in?

I still haven't gotten a reply to my review request from Jan or Chris on 
those patches.  Also a decision on what todo about bug 350 (see below)

Other than that it involves me actually having the time to package up a 
candidate tar and run tests against various PG versions (mostly 
automated) and to manually test that I can still build against windows 
(not automated).

Any new problems that come up during the above could delay things.



>
>> If the patch (suitable for 2.2) for bug 350 is ready in the next few
>> days I'd say we should include it.  Otherwise maybe we want to
>> clarify the upgrade documentation to better describe the existing
>> behaviour.
>
> 350 is well above my experience level to fix, but I'd be happy to
> document the issue if that's what's needed.

I thought Rose had sent a patch in for this that needed to be tested, 
but I don't see the patch attached to the bug.

The issue here is that the instructions for upgrading to 2.2 say change 
cleanup interval to a few seconds  (
http://www.slony.info/documentation/2.2.2/slonyupgrade.html#AEN2512 )
This doesn't actually work as advertised.

If we aren't going to make cleanup_interval do what something useful 
then we need to provide upgrade instructions work as described. Manually 
forcing a log switch by calling the stored function might be one option.




>
> Cheers,
> David.
>


From smarchand at sgo.fr  Wed Jan  7 02:05:58 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Wed, 7 Jan 2015 11:05:58 +0100
Subject: [Slony1-general] too much work !
Message-ID: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>

Hi,

 

I?m using Slony 2.2.2 with postgresql 9.3.

Slony work well but i found slony work too on pg, too many access, too many
process,


For information slony work in wan with 17 nodes, and 18 replications.

Replication A

Server X -> server 1 ( schema base )

Server X -> server 2 ( schema base )

Server X -> server 3 ( schema base )

Server X -> server 4 ( schema base )




Server X -> server 17 ( schema base )

 

Replication B

 

Server 1 -> server X ( schema backbase1 )

Server 1 -> server X ( schema backbase1 )

 

 

Best regards, 

S?bastien Marchand

Soci?t? SGO

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150107/7dc69096/attachment.htm 

From smarchand at sgo.fr  Wed Jan  7 02:15:49 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Wed, 7 Jan 2015 11:15:49 +0100
Subject: [Slony1-general] too much work for pg with 17 nodes...
Message-ID: <006b01d02a62$e9be7880$bd3b6980$@sgo.fr>

Hi,

 

I?m using Slony 2.2.2 with postgresql 9.3.

Slony work well but i found slony work too on pg, too many access, too many
process,


For information slony work in wan with 17 nodes, and 18 replications.

Replication A

Server X -> server 1 ( schema base )

Server X -> server 2 ( schema base )

Server X -> server 3 ( schema base )

Server X -> server 4 ( schema base )




Server X -> server 17 ( schema base )

 

Replication B

 

Server 1 -> server X ( schema backbase1 )

Server 2 -> server X ( schema backbase2 )

Server 3 -> server X ( schema backbase3 )

Server 4 -> server X ( schema backbase4 )




Server 17 -> server X ( schema backbase17 )

 

Local.monitor was desactivated to decrease impact on pg.

I?m using this conf :

 

check interval 200

interval timeout 60000

group size  50 

desired sync time 60000 

cleanup cycles 0 

log level 0

 

i don?t know if someone can help me but i would like to know if my
organisation with several slon is the good way.

Better use one replication with 17 set ? perhaps


 

Ps : sorry for mistake mail

 

Best regards, 

S?bastien Marchand

Soci?t? SGO

 

 

 

 

Cordialement, 

S?bastien Marchand

Soci?t? SGO

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150107/002fd85c/attachment.htm 

From stephane.schildknecht at postgres.fr  Wed Jan  7 03:15:24 2015
From: stephane.schildknecht at postgres.fr (=?UTF-8?B?U3TDqXBoYW5lIFNjaGlsZGtuZWNodA==?=)
Date: Wed, 07 Jan 2015 12:15:24 +0100
Subject: [Slony1-general] too much work for pg with 17 nodes...
In-Reply-To: <006b01d02a62$e9be7880$bd3b6980$@sgo.fr>
References: <006b01d02a62$e9be7880$bd3b6980$@sgo.fr>
Message-ID: <54AD154C.2020003@postgres.fr>

On 07/01/2015 11:15, Sebastien Marchand wrote:
> Hi,
> 
>  
> 
> I?m using Slony 2.2.2 with postgresql 9.3.
> 
> Slony work well but i found slony work too on pg, too many access, too many
> process,?
> 
> For information slony work in wan with 17 nodes, and 18 replications.
> 
> Replication A
> 
> Server X -> server 1 ( schema base )
> 
> Server X -> server 2 ( schema base )
> 
> Server X -> server 3 ( schema base )
> 
> Server X -> server 4 ( schema base )
> 
> ?
> 
> Server X -> server 17 ( schema base )
> 
>  
> 
> Replication B
> 
>  
> 
> Server 1 -> server X ( schema backbase1 )
> 
> Server 2 -> server X ( schema backbase2 )
> 
> Server 3 -> server X ( schema backbase3 )
> 
> Server 4 -> server X ( schema backbase4 )
> 
> ?
> 
> Server 17 -> server X ( schema backbase17 )
> 
>  
> 
> Local.monitor was desactivated to decrease impact on pg.
> 
> I?m using this conf :
> 
>  
> 
> /check interval 200/
> 
> /interval timeout/ 60000
> 
> group size  50
> 
> /desired sync time/ 60000
> 
> cleanup cycles 0
> 
> log level 0
> 
>  
> 
> i don?t know if someone can help me but i would like to know if my organisation
> with several slon is the good way.
> 
> Better use one replication with 17 set ? perhaps?
> 
>  

Hello,

If I understand what you wrote, you have a provider for one set of objects in a
database that is replicated on 17 different servers.

Another set of of objects is then replicated back from every 17 servers to the
first one.

Could you give more clues on the objects and their organisation in sets ?
Are all objects different in all sets ?

-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150107/c524f6c0/attachment.pgp 

From vivek at khera.org  Wed Jan  7 09:40:00 2015
From: vivek at khera.org (Vick Khera)
Date: Wed, 7 Jan 2015 12:40:00 -0500
Subject: [Slony1-general] too much work !
In-Reply-To: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>
References: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>
Message-ID: <CALd+dcfE-xrHqWGaza2t8jgCnGoQ-vmE7dWOn827P3_rOZCZRw@mail.gmail.com>

On Wed, Jan 7, 2015 at 5:05 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:

> Slony work well but i found slony work too on pg, too many access, too
> many process,?
>
> For information slony work in wan with 17 nodes, and 18 replications.
>
>
If you have 17 replicas for "A", do not have them all directly replicate
from the master. set up a tree structure.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150107/45f33779/attachment.htm 

From smarchand at sgo.fr  Thu Jan  8 00:31:46 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Thu, 8 Jan 2015 09:31:46 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <CALd+dcfE-xrHqWGaza2t8jgCnGoQ-vmE7dWOn827P3_rOZCZRw@mail.gmail.com>
References: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>
	<CALd+dcfE-xrHqWGaza2t8jgCnGoQ-vmE7dWOn827P3_rOZCZRw@mail.gmail.com>
Message-ID: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>

I can?t do this, only my master have a good bandwith (up and down), others server are in warehouse. 

 

De : Vick Khera [mailto:vivek at khera.org] 
Envoy? : mercredi 7 janvier 2015 18:40
? : Sebastien Marchand
Cc : slony
Objet : Re: [Slony1-general] too much work !

 

 

On Wed, Jan 7, 2015 at 5:05 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:

Slony work well but i found slony work too on pg, too many access, too many process,?

For information slony work in wan with 17 nodes, and 18 replications.


If you have 17 replicas for "A", do not have them all directly replicate from the master. set up a tree structure.

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150108/4dcbf95d/attachment.htm 

From glynastill at yahoo.co.uk  Thu Jan  8 04:48:43 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 8 Jan 2015 12:48:43 +0000 (UTC)
Subject: [Slony1-general] too much work !
In-Reply-To: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>
Message-ID: <387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>

Could you add an extra server in the same location as your master to feed from?? Or perhaps you could make use of the log shipping feature: http://slony.info/documentation/logshipping.html

 
      From: Sebastien Marchand <smarchand at sgo.fr>
 To: 'Vick Khera' <vivek at khera.org> 
Cc: 'slony' <slony1-general at lists.slony.info> 
 Sent: Thursday, 8 January 2015, 8:31
 Subject: Re: [Slony1-general] too much work !
   
#yiv1516855320 #yiv1516855320 -- _filtered #yiv1516855320 {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;} _filtered #yiv1516855320 {font-family:Tahoma;panose-1:2 11 6 4 3 5 4 4 2 4;}#yiv1516855320 #yiv1516855320 p.yiv1516855320MsoNormal, #yiv1516855320 li.yiv1516855320MsoNormal, #yiv1516855320 div.yiv1516855320MsoNormal {margin:0cm;margin-bottom:.0001pt;font-size:12.0pt;}#yiv1516855320 a:link, #yiv1516855320 span.yiv1516855320MsoHyperlink {color:blue;text-decoration:underline;}#yiv1516855320 a:visited, #yiv1516855320 span.yiv1516855320MsoHyperlinkFollowed {color:purple;text-decoration:underline;}#yiv1516855320 span.yiv1516855320EmailStyle17 {color:#1F497D;}#yiv1516855320 .yiv1516855320MsoChpDefault {} _filtered #yiv1516855320 {margin:70.85pt 70.85pt 70.85pt 70.85pt;}#yiv1516855320 div.yiv1516855320WordSection1 {}#yiv1516855320 I can?t do this, only my master have a good bandwith (up and down), others server are in warehouse.  ?De?: Vick Khera [mailto:vivek at khera.org] 
Envoy??: mercredi 7 janvier 2015 18:40
??: Sebastien Marchand
Cc?: slony
Objet?: Re: [Slony1-general] too much work ! ?

 ?On Wed, Jan 7, 2015 at 5:05 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:Slony work well but i found slony work too on pg, too many access, too many process,?For information slony work in wan with 17 nodes, and 18 replications.
If you have 17 replicas for "A", do not have them all directly replicate from the master. set up a tree structure. ?
_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150108/0300401b/attachment.htm 

From smarchand at sgo.fr  Thu Jan  8 07:18:20 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Thu, 8 Jan 2015 16:18:20 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>
	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
Message-ID: <00aa01d02b56$575d0480$06170d80$@sgo.fr>

Thx for your answer, i will look this, never saw in action.

 

 

De : Glyn Astill [mailto:glynastill at yahoo.co.uk] 
Envoy? : jeudi 8 janvier 2015 13:49
? : Sebastien Marchand; 'Vick Khera'
Cc : 'slony'
Objet : Re: [Slony1-general] too much work !

 

Could you add an extra server in the same location as your master to feed from?  Or perhaps you could make use of the log shipping feature: http://slony.info/documentation/logshipping.html





  _____  

From: Sebastien Marchand <smarchand at sgo.fr>
To: 'Vick Khera' <vivek at khera.org> 
Cc: 'slony' <slony1-general at lists.slony.info> 
Sent: Thursday, 8 January 2015, 8:31
Subject: Re: [Slony1-general] too much work !

 

I can?t do this, only my master have a good bandwith (up and down), others server are in warehouse. 

 

De : Vick Khera [mailto:vivek at khera.org] 
Envoy? : mercredi 7 janvier 2015 18:40
? : Sebastien Marchand
Cc : slony
Objet : Re: [Slony1-general] too much work !

 

 

 

On Wed, Jan 7, 2015 at 5:05 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:

Slony work well but i found slony work too on pg, too many access, too many process,?

For information slony work in wan with 17 nodes, and 18 replications.


If you have 17 replicas for "A", do not have them all directly replicate from the master. set up a tree structure.

 

 

_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150108/dfc6a016/attachment-0001.htm 

From stephane.schildknecht at postgres.fr  Thu Jan  8 07:30:36 2015
From: stephane.schildknecht at postgres.fr (=?UTF-8?B?U3TDqXBoYW5lIFNjaGlsZGtuZWNodA==?=)
Date: Thu, 08 Jan 2015 16:30:36 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <00aa01d02b56$575d0480$06170d80$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
Message-ID: <54AEA29C.5010301@postgres.fr>

On 08/01/2015 16:18, Sebastien Marchand wrote:
> Thx for your answer, i will look this, never saw in action.
> 

Could you give us a more information of the problem you're trying to solve ?

S.
-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150108/0aff3e30/attachment.pgp 

From smarchand at sgo.fr  Fri Jan  9 03:10:48 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Fri, 9 Jan 2015 12:10:48 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <54AEA29C.5010301@postgres.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
Message-ID: <00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>

Hi,

I try to help my server to work better ^^

I have all the time this in my log 

2015-01-09 12:05:25 CET [11969]: [8-1] user=slony,db=RPP2,remote=127.0.0.1(33237) LOG:  duration: 2202.807 ms  statement: insert into "_repli_local116".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot, ev_type     ) values ('116', '5000282096', '2015-01-09 12:04:35.116816+01', '70712675:70712675:', 'SYNC'); insert into "_repli_local116".sl_confirm   (con_origin, con_received, con_seqno, con_timestamp)    values (116, 1, '5000282096', now()); commit transaction;
2015-01-09 12:06:41 CET [12154]: [12-1] user=slony,db=RPP2,remote=127.0.0.1(33311) LOG:  duration: 2344.793 ms  statement: commit transaction;

I don't know why my server take too much time to insert lines ! 

-----Message d'origine-----
De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr] 
Envoy? : jeudi 8 janvier 2015 16:31
? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
Cc : 'slony'
Objet : Re: [Slony1-general] too much work !

On 08/01/2015 16:18, Sebastien Marchand wrote:
> Thx for your answer, i will look this, never saw in action.
> 

Could you give us a more information of the problem you're trying to solve ?

S.
--
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - Conseil, expertise et formations



From stephane.schildknecht at postgres.fr  Fri Jan  9 06:01:02 2015
From: stephane.schildknecht at postgres.fr (=?UTF-8?B?U3TDqXBoYW5lIFNjaGlsZGtuZWNodA==?=)
Date: Fri, 09 Jan 2015 15:01:02 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
Message-ID: <54AFDF1E.3030407@postgres.fr>

On 09/01/2015 12:10, Sebastien Marchand wrote:
> Hi,
> 
> I try to help my server to work better ^^
> 
> I have all the time this in my log 
> 
> 2015-01-09 12:05:25 CET [11969]: [8-1] user=slony,db=RPP2,remote=127.0.0.1(33237) LOG:  duration: 2202.807 ms  statement: insert into "_repli_local116".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot, ev_type     ) values ('116', '5000282096', '2015-01-09 12:04:35.116816+01', '70712675:70712675:', 'SYNC'); insert into "_repli_local116".sl_confirm   (con_origin, con_received, con_seqno, con_timestamp)    values (116, 1, '5000282096', now()); commit transaction;
> 2015-01-09 12:06:41 CET [12154]: [12-1] user=slony,db=RPP2,remote=127.0.0.1(33311) LOG:  duration: 2344.793 ms  statement: commit transaction;
> 
> I don't know why my server take too much time to insert lines ! 
> 


Could you explain your architecture.
Nodes, sets, tables, connections between nodes, directions of the flows...
Size of the tables/database...

-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150109/b9305df3/attachment.pgp 

From smarchand at sgo.fr  Mon Jan 12 02:38:37 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Mon, 12 Jan 2015 11:38:37 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <54AFDF1E.3030407@postgres.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
	<54AFDF1E.3030407@postgres.fr>
Message-ID: <00e701d02e53$fac74130$f055c390$@sgo.fr>

Hi,

18 nodes, wan network.
2 replications ( one with 1 set and other with 2 sets ( 2 directions ) )
First replication from server 1 to n servers ( same schema, 37 tables )
1 slon on server 1 and 1 slon by remote server
Second replication : 
set 1 : server A -> server 1 ( 90 replicated tables )
set 2 : server A <- server 1 ( 1 replicated tables )
do the same with 17 other servers ( one schema by server )
17 slons on server 1 and 1 slon by  remote server 
database 5 gB.

I don't know if that will help...


-----Message d'origine-----
De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr] 
Envoy? : vendredi 9 janvier 2015 15:01
? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
Cc : 'slony'
Objet : Re: [Slony1-general] too much work !

On 09/01/2015 12:10, Sebastien Marchand wrote:
> Hi,
> 
> I try to help my server to work better ^^
> 
> I have all the time this in my log
> 
> 2015-01-09 12:05:25 CET [11969]: [8-1] user=slony,db=RPP2,remote=127.0.0.1(33237) LOG:  duration: 2202.807 ms  statement: insert into "_repli_local116".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot, ev_type     ) values ('116', '5000282096', '2015-01-09 12:04:35.116816+01', '70712675:70712675:', 'SYNC'); insert into "_repli_local116".sl_confirm   (con_origin, con_received, con_seqno, con_timestamp)    values (116, 1, '5000282096', now()); commit transaction;
> 2015-01-09 12:06:41 CET [12154]: [12-1] 
> user=slony,db=RPP2,remote=127.0.0.1(33311) LOG:  duration: 2344.793 ms  
> statement: commit transaction;
> 
> I don't know why my server take too much time to insert lines ! 
> 


Could you explain your architecture.
Nodes, sets, tables, connections between nodes, directions of the flows...
Size of the tables/database...

--
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - Conseil, expertise et formations



From vivek at khera.org  Tue Jan 13 05:50:30 2015
From: vivek at khera.org (Vick Khera)
Date: Tue, 13 Jan 2015 08:50:30 -0500
Subject: [Slony1-general] too much work !
In-Reply-To: <00e701d02e53$fac74130$f055c390$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>
	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
	<54AFDF1E.3030407@postgres.fr>
	<00e701d02e53$fac74130$f055c390$@sgo.fr>
Message-ID: <CALd+dcdpbKrrmbOr8kmcwubHN20G8uX5bOAQQO+TRpW=BARX7g@mail.gmail.com>

On Mon, Jan 12, 2015 at 5:38 AM, Sebastien Marchand <smarchand at sgo.fr>
wrote:

> 17 slons on server 1 and 1 slon by  remote server
>

why do you have 17 slons on server 1? You only need on slon process per
node.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150113/3feccb7a/attachment.htm 

From stephane.schildknecht at postgres.fr  Tue Jan 13 06:22:09 2015
From: stephane.schildknecht at postgres.fr (=?UTF-8?B?U3TDqXBoYW5lIFNjaGlsZGtuZWNodA==?=)
Date: Tue, 13 Jan 2015 15:22:09 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <00e701d02e53$fac74130$f055c390$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
	<54AFDF1E.3030407@postgres.fr>
	<00e701d02e53$fac74130$f055c390$@sgo.fr>
Message-ID: <54B52A11.2060706@postgres.fr>

On 12/01/2015 11:38, Sebastien Marchand wrote:
> Hi,
> 
> 18 nodes, wan network.
> 2 replications ( one with 1 set and other with 2 sets ( 2 directions ) )
> First replication from server 1 to n servers ( same schema, 37 tables )
> 1 slon on server 1 and 1 slon by remote server
> Second replication : 
> set 1 : server A -> server 1 ( 90 replicated tables )
> set 2 : server A <- server 1 ( 1 replicated tables )
> do the same with 17 other servers ( one schema by server )
> 17 slons on server 1 and 1 slon by  remote server 
> database 5 gB.
> 
> I don't know if that will help...

Seems to me some information is missing. Distinction on databases, for instance.

I guess, the 2 replications are not correlated.They just share same servers.

In fact, the second replication is not 1 replication, it is 17 different
replications. But, do they all end in the same database on server A ?

-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150113/1d77b56f/attachment.pgp 

From smarchand at sgo.fr  Tue Jan 13 06:42:35 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Tue, 13 Jan 2015 15:42:35 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <CALd+dcdpbKrrmbOr8kmcwubHN20G8uX5bOAQQO+TRpW=BARX7g@mail.gmail.com>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>	<00aa01d02b56$575d0480$06170d80$@sgo.fr>	<54AEA29C.5010301@postgres.fr>	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>	<54AFDF1E.3030407@postgres.fr>	<00e701d02e53$fac74130$f055c390$@sgo.fr>
	<CALd+dcdpbKrrmbOr8kmcwubHN20G8uX5bOAQQO+TRpW=BARX7g@mail.gmail.com>
Message-ID: <010501d02f3f$2c650f10$852f2d30$@sgo.fr>

I have 17 cluster name if you prefer.

I don?t know if i  can start one slon for 17 clusters.

 

De : Vick Khera [mailto:vivek at khera.org] 
Envoy? : mardi 13 janvier 2015 14:51
? : Sebastien Marchand
Cc : St?phane Schildknecht; Glyn Astill; slony
Objet : Re: [Slony1-general] too much work !

 

 

On Mon, Jan 12, 2015 at 5:38 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:

17 slons on server 1 and 1 slon by  remote server


why do you have 17 slons on server 1? You only need on slon process per node.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150113/039b0ece/attachment.htm 

From smarchand at sgo.fr  Tue Jan 13 06:45:43 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Tue, 13 Jan 2015 15:45:43 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <54B52A11.2060706@postgres.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
	<54AFDF1E.3030407@postgres.fr>
	<00e701d02e53$fac74130$f055c390$@sgo.fr>
	<54B52A11.2060706@postgres.fr>
Message-ID: <010d01d02f3f$9c590830$d50b1890$@sgo.fr>

Yeah, 17 cluster name if you prefer.

I start one slon by one cluster. 
Samples :

slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_nat dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_lc3 dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_lc2 dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123

-----Message d'origine-----
De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr] 
Envoy? : mardi 13 janvier 2015 15:22
? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
Cc : 'slony'
Objet : Re: [Slony1-general] too much work !

On 12/01/2015 11:38, Sebastien Marchand wrote:
> Hi,
> 
> 18 nodes, wan network.
> 2 replications ( one with 1 set and other with 2 sets ( 2 directions ) 
> ) First replication from server 1 to n servers ( same schema, 37 
> tables )
> 1 slon on server 1 and 1 slon by remote server Second replication :
> set 1 : server A -> server 1 ( 90 replicated tables ) set 2 : server A 
> <- server 1 ( 1 replicated tables ) do the same with 17 other servers 
> ( one schema by server )
> 17 slons on server 1 and 1 slon by  remote server database 5 gB.
> 
> I don't know if that will help...

Seems to me some information is missing. Distinction on databases, for instance.

I guess, the 2 replications are not correlated.They just share same servers.

In fact, the second replication is not 1 replication, it is 17 different replications. But, do they all end in the same database on server A ?

--
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - Conseil, expertise et formations



From glynastill at yahoo.co.uk  Tue Jan 13 07:29:31 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue, 13 Jan 2015 15:29:31 +0000 (UTC)
Subject: [Slony1-general] too much work !
In-Reply-To: <010d01d02f3f$9c590830$d50b1890$@sgo.fr>
References: <010d01d02f3f$9c590830$d50b1890$@sgo.fr>
Message-ID: <2047386319.448483.1421162971802.JavaMail.yahoo@jws11147.mail.ir2.yahoo.com>

> From: Sebastien Marchand <smarchand at sgo.fr>
>To: 'St?phane Schildknecht' <stephane.schildknecht at postgres.fr>; 'Glyn Astill' <glynastill at yahoo.co.uk>; 'Vick Khera' <vivek at khera.org> 
>Cc: 'slony' <slony1-general at lists.slony.info> 
>Sent: Tuesday, 13 January 2015, 14:45
>Subject: RE: [Slony1-general] too much work !
> 
>
>Yeah, 17 cluster name if you prefer.
>
>I start one slon by one cluster. 
>Samples :
>
>slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_nat dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
>slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_lc3 dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
>slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_lc2 dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
>
>


Is there a particular reason for all the different clusters?  Surely you could achieve something similar with just separate replication sets in one cluster.

Can you elaborate on the actual table layout of database "DB2" and how each table or set of tables are replicated between nodes.

Glyn

From jan at wi3ck.info  Thu Jan 15 08:50:56 2015
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 15 Jan 2015 11:50:56 -0500
Subject: [Slony1-general] too much work !
In-Reply-To: <010d01d02f3f$9c590830$d50b1890$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>	<00aa01d02b56$575d0480$06170d80$@sgo.fr>	<54AEA29C.5010301@postgres.fr>	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>	<54AFDF1E.3030407@postgres.fr>	<00e701d02e53$fac74130$f055c390$@sgo.fr>	<54B52A11.2060706@postgres.fr>
	<010d01d02f3f$9c590830$d50b1890$@sgo.fr>
Message-ID: <54B7EFF0.2040302@wi3ck.info>

On 01/13/2015 09:45 AM, Sebastien Marchand wrote:
> Yeah, 17 cluster name if you prefer.
>
> I start one slon by one cluster.
> Samples :
>
> slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_nat dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
> slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_lc3 dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123
> slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f /home/scripts/slon.conf repli_lc2 dbname=DB2 host=127.0.0.1 port=5432 user=slony password=123

This looks as if you created 17 Slony clusters instead of one cluster 
with 18 nodes.


Jan



>
> -----Message d'origine-----
> De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr]
> Envoy? : mardi 13 janvier 2015 15:22
> ? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
> Cc : 'slony'
> Objet : Re: [Slony1-general] too much work !
>
> On 12/01/2015 11:38, Sebastien Marchand wrote:
>> Hi,
>>
>> 18 nodes, wan network.
>> 2 replications ( one with 1 set and other with 2 sets ( 2 directions )
>> ) First replication from server 1 to n servers ( same schema, 37
>> tables )
>> 1 slon on server 1 and 1 slon by remote server Second replication :
>> set 1 : server A -> server 1 ( 90 replicated tables ) set 2 : server A
>> <- server 1 ( 1 replicated tables ) do the same with 17 other servers
>> ( one schema by server )
>> 17 slons on server 1 and 1 slon by  remote server database 5 gB.
>>
>> I don't know if that will help...
>
> Seems to me some information is missing. Distinction on databases, for instance.
>
> I guess, the 2 replications are not correlated.They just share same servers.
>
> In fact, the second replication is not 1 replication, it is 17 different replications. But, do they all end in the same database on server A ?
>
> --
> St?phane Schildknecht
> Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - Conseil, expertise et formations
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From ssinger at ca.afilias.info  Sun Jan 18 20:22:19 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Sun, 18 Jan 2015 23:22:19 -0500
Subject: [Slony1-general] Slony 2.2.4 released
Message-ID: <54BC867B.6020205@ca.afilias.info>

The Slony team is pleased to announce Slony 2.2.4 the next minor release 
of the Slony 2.2.x series.

Slony 2.2.4 includes the following changes

  - Bug 352 :: Handle changes from PG HEAD ("9.5")
  - Bug 349 :: Issue with quoting of cluster name - only hit when 
processing DDL
  - Bug 350 :: Make cleanup_interval config parameter work as expected
  - Include alloca.h in slonik(fix for solaris)
  - Bug     :: 345 Fix bug when dropping multiple nodes at once
  - Bug 354 :: Fix race condition in FAILOVER
  - Bug 356 :: Perform TRUNCATE ONLY on replicas (when replicating a 
truncate)

Slony 2.2.4 can be downloaded from from the following URL


http://main.slony.info/downloads/2.2/source/slony1-2.2.4.tar.bz2

From smarchand at sgo.fr  Mon Jan 19 02:12:51 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Mon, 19 Jan 2015 11:12:51 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <54B7EFF0.2040302@wi3ck.info>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>	<00aa01d02b56$575d0480$06170d80$@sgo.fr>	<54AEA29C.5010301@postgres.fr>	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>	<54AFDF1E.3030407@postgres.fr>	<00e701d02e53$fac74130$f055c390$@sgo.fr>	<54B52A11.2060706@postgres.fr>
	<010d01d02f3f$9c590830$d50b1890$@sgo.fr>
	<54B7EFF0.2040302@wi3ck.info>
Message-ID: <012001d033d0$7d15a8d0$7740fa70$@sgo.fr>

Hi,

I think it's a problem to have just one slon in wan architecture.
Sometimes one node is not up for long time so with one slon i can't stop slon master server side.
I don't know if i can stop just one node with a command for example, so maintenance in wan architecture is not easy


-----Message d'origine-----
De : Jan Wieck [mailto:jan at wi3ck.info] 
Envoy? : jeudi 15 janvier 2015 17:51
? : Sebastien Marchand; 'St?phane Schildknecht'; 'Glyn Astill'; 'Vick Khera'
Cc : 'slony'
Objet : Re: [Slony1-general] too much work !

On 01/13/2015 09:45 AM, Sebastien Marchand wrote:
> Yeah, 17 cluster name if you prefer.
>
> I start one slon by one cluster.
> Samples :
>
> slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f 
> /home/scripts/slon.conf repli_nat dbname=DB2 host=127.0.0.1 port=5432 
> user=slony password=123 slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 
> -f /home/scripts/slon.conf repli_lc3 dbname=DB2 host=127.0.0.1 
> port=5432 user=slony password=123 slon -s 200 -t 60000 -g 50 -o 60000 
> -c 0 -d 1 -f /home/scripts/slon.conf repli_lc2 dbname=DB2 
> host=127.0.0.1 port=5432 user=slony password=123

This looks as if you created 17 Slony clusters instead of one cluster with 18 nodes.


Jan



>
> -----Message d'origine-----
> De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr]
> Envoy? : mardi 13 janvier 2015 15:22
> ? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
> Cc : 'slony'
> Objet : Re: [Slony1-general] too much work !
>
> On 12/01/2015 11:38, Sebastien Marchand wrote:
>> Hi,
>>
>> 18 nodes, wan network.
>> 2 replications ( one with 1 set and other with 2 sets ( 2 directions )
>> ) First replication from server 1 to n servers ( same schema, 37
>> tables )
>> 1 slon on server 1 and 1 slon by remote server Second replication :
>> set 1 : server A -> server 1 ( 90 replicated tables ) set 2 : server A
>> <- server 1 ( 1 replicated tables ) do the same with 17 other servers
>> ( one schema by server )
>> 17 slons on server 1 and 1 slon by  remote server database 5 gB.
>>
>> I don't know if that will help...
>
> Seems to me some information is missing. Distinction on databases, for instance.
>
> I guess, the 2 replications are not correlated.They just share same servers.
>
> In fact, the second replication is not 1 replication, it is 17 different replications. But, do they all end in the same database on server A ?
>
> --
> St?phane Schildknecht
> Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - Conseil, expertise et formations
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Jan Wieck
Senior Software Engineer
http://slony.info


From ajs at crankycanuck.ca  Mon Jan 19 05:21:36 2015
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon, 19 Jan 2015 08:21:36 -0500
Subject: [Slony1-general] too much work !
In-Reply-To: <012001d033d0$7d15a8d0$7740fa70$@sgo.fr>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>
	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
	<54AFDF1E.3030407@postgres.fr>
	<00e701d02e53$fac74130$f055c390$@sgo.fr>
	<54B52A11.2060706@postgres.fr>
	<010d01d02f3f$9c590830$d50b1890$@sgo.fr>
	<54B7EFF0.2040302@wi3ck.info>
	<012001d033d0$7d15a8d0$7740fa70$@sgo.fr>
Message-ID: <19D7B7C0-5BAA-4AC7-B368-5648939298D5@crankycanuck.ca>

Who said anything about "one slon"?  Jan's point was that you've set this up differently than it is designed to work, and now you're complaining that it doesn't work too well.  Yes, quite right. 

A

-- 
Andrew Sullivan 
Please excuse my clumbsy thums. 

> On Jan 19, 2015, at 5:12, "Sebastien Marchand" <smarchand at sgo.fr> wrote:
> 
> Hi,
> 
> I think it's a problem to have just one slon in wan architecture.
> Sometimes one node is not up for long time so with one slon i can't stop slon master server side.
> I don't know if i can stop just one node with a command for example, so maintenance in wan architecture is not easy
> 
> 
> -----Message d'origine-----
> De : Jan Wieck [mailto:jan at wi3ck.info] 
> Envoy? : jeudi 15 janvier 2015 17:51
> ? : Sebastien Marchand; 'St?phane Schildknecht'; 'Glyn Astill'; 'Vick Khera'
> Cc : 'slony'
> Objet : Re: [Slony1-general] too much work !
> 
>> On 01/13/2015 09:45 AM, Sebastien Marchand wrote:
>> Yeah, 17 cluster name if you prefer.
>> 
>> I start one slon by one cluster.
>> Samples :
>> 
>> slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f 
>> /home/scripts/slon.conf repli_nat dbname=DB2 host=127.0.0.1 port=5432 
>> user=slony password=123 slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 
>> -f /home/scripts/slon.conf repli_lc3 dbname=DB2 host=127.0.0.1 
>> port=5432 user=slony password=123 slon -s 200 -t 60000 -g 50 -o 60000 
>> -c 0 -d 1 -f /home/scripts/slon.conf repli_lc2 dbname=DB2 
>> host=127.0.0.1 port=5432 user=slony password=123
> 
> This looks as if you created 17 Slony clusters instead of one cluster with 18 nodes.
> 
> 
> Jan
> 
> 
> 
>> 
>> -----Message d'origine-----
>> De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr]
>> Envoy? : mardi 13 janvier 2015 15:22
>> ? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
>> Cc : 'slony'
>> Objet : Re: [Slony1-general] too much work !
>> 
>>> On 12/01/2015 11:38, Sebastien Marchand wrote:
>>> Hi,
>>> 
>>> 18 nodes, wan network.
>>> 2 replications ( one with 1 set and other with 2 sets ( 2 directions )
>>> ) First replication from server 1 to n servers ( same schema, 37
>>> tables )
>>> 1 slon on server 1 and 1 slon by remote server Second replication :
>>> set 1 : server A -> server 1 ( 90 replicated tables ) set 2 : server A
>>> <- server 1 ( 1 replicated tables ) do the same with 17 other servers
>>> ( one schema by server )
>>> 17 slons on server 1 and 1 slon by  remote server database 5 gB.
>>> 
>>> I don't know if that will help...
>> 
>> Seems to me some information is missing. Distinction on databases, for instance.
>> 
>> I guess, the 2 replications are not correlated.They just share same servers.
>> 
>> In fact, the second replication is not 1 replication, it is 17 different replications. But, do they all end in the same database on server A ?
>> 
>> --
>> St?phane Schildknecht
>> Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - Conseil, expertise et formations
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> -- 
> Jan Wieck
> Senior Software Engineer
> http://slony.info
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From smarchand at sgo.fr  Mon Jan 19 06:56:20 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Mon, 19 Jan 2015 15:56:20 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <19D7B7C0-5BAA-4AC7-B368-5648939298D5@crankycanuck.ca>
References: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>
	<387756275.6426580.1420721323777.JavaMail.yahoo@jws11104.mail.ir2.yahoo.com>
	<00aa01d02b56$575d0480$06170d80$@sgo.fr>
	<54AEA29C.5010301@postgres.fr>
	<00ca01d02bfc$ec90cd10$c5b26730$@sgo.fr>
	<54AFDF1E.3030407@postgres.fr>
	<00e701d02e53$fac74130$f055c390$@sgo.fr>
	<54B52A11.2060706@postgres.fr>
	<010d01d02f3f$9c590830$d50b1890$@sgo.fr>
	<54B7EFF0.2040302@wi3ck.info>
	<012001d033d0$7d15a8d0$7740fa70$@sgo.fr>
	<19D7B7C0-5BAA-4AC7-B368-5648939298D5@crankycanuck.ca>
Message-ID: <012101d033f8$169369d0$43ba3d70$@sgo.fr>

"that you've set this up differently than it is designed to work"   :  what are you talking about ?

-----Message d'origine-----
De : Andrew Sullivan [mailto:ajs at crankycanuck.ca] 
Envoy? : lundi 19 janvier 2015 14:22
? : Sebastien Marchand
Cc : Jan Wieck; St?phane Schildknecht; Glyn Astill; Vick Khera; slony
Objet : Re: [Slony1-general] too much work !

Who said anything about "one slon"?  Jan's point was that you've set this up differently than it is designed to work, and now you're complaining that it doesn't work too well.  Yes, quite right. 

A

--
Andrew Sullivan
Please excuse my clumbsy thums. 

> On Jan 19, 2015, at 5:12, "Sebastien Marchand" <smarchand at sgo.fr> wrote:
> 
> Hi,
> 
> I think it's a problem to have just one slon in wan architecture.
> Sometimes one node is not up for long time so with one slon i can't stop slon master server side.
> I don't know if i can stop just one node with a command for example, 
> so maintenance in wan architecture is not easy
> 
> 
> -----Message d'origine-----
> De : Jan Wieck [mailto:jan at wi3ck.info] Envoy? : jeudi 15 janvier 2015 
> 17:51 ? : Sebastien Marchand; 'St?phane Schildknecht'; 'Glyn Astill'; 
> 'Vick Khera'
> Cc : 'slony'
> Objet : Re: [Slony1-general] too much work !
> 
>> On 01/13/2015 09:45 AM, Sebastien Marchand wrote:
>> Yeah, 17 cluster name if you prefer.
>> 
>> I start one slon by one cluster.
>> Samples :
>> 
>> slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 -f 
>> /home/scripts/slon.conf repli_nat dbname=DB2 host=127.0.0.1 port=5432 
>> user=slony password=123 slon -s 200 -t 60000 -g 50 -o 60000 -c 0 -d 1 
>> -f /home/scripts/slon.conf repli_lc3 dbname=DB2 host=127.0.0.1
>> port=5432 user=slony password=123 slon -s 200 -t 60000 -g 50 -o 60000 
>> -c 0 -d 1 -f /home/scripts/slon.conf repli_lc2 dbname=DB2
>> host=127.0.0.1 port=5432 user=slony password=123
> 
> This looks as if you created 17 Slony clusters instead of one cluster with 18 nodes.
> 
> 
> Jan
> 
> 
> 
>> 
>> -----Message d'origine-----
>> De : St?phane Schildknecht [mailto:stephane.schildknecht at postgres.fr]
>> Envoy? : mardi 13 janvier 2015 15:22
>> ? : Sebastien Marchand; 'Glyn Astill'; 'Vick Khera'
>> Cc : 'slony'
>> Objet : Re: [Slony1-general] too much work !
>> 
>>> On 12/01/2015 11:38, Sebastien Marchand wrote:
>>> Hi,
>>> 
>>> 18 nodes, wan network.
>>> 2 replications ( one with 1 set and other with 2 sets ( 2 directions 
>>> )
>>> ) First replication from server 1 to n servers ( same schema, 37 
>>> tables )
>>> 1 slon on server 1 and 1 slon by remote server Second replication :
>>> set 1 : server A -> server 1 ( 90 replicated tables ) set 2 : server 
>>> A
>>> <- server 1 ( 1 replicated tables ) do the same with 17 other 
>>> servers ( one schema by server )
>>> 17 slons on server 1 and 1 slon by  remote server database 5 gB.
>>> 
>>> I don't know if that will help...
>> 
>> Seems to me some information is missing. Distinction on databases, for instance.
>> 
>> I guess, the 2 replications are not correlated.They just share same servers.
>> 
>> In fact, the second replication is not 1 replication, it is 17 different replications. But, do they all end in the same database on server A ?
>> 
>> --
>> St?phane Schildknecht
>> Contact r?gional PostgreSQL pour l'Europe francophone Loxodata - 
>> Conseil, expertise et formations
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From davecramer at gmail.com  Wed Jan 21 06:24:13 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Wed, 21 Jan 2015 09:24:13 -0500
Subject: [Slony1-general] Lots of data in sl_log_? but sl_status shows lag
	of 0
Message-ID: <CADK3HHJovJMdxQ6uxLmKtUaiQzz-krd5BqQRTKVUxqK1r3b++g@mail.gmail.com>

How is this possible ?

Dave Cramer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150121/0a99e24e/attachment.htm 

From jeff at pgexperts.com  Wed Jan 21 08:02:41 2015
From: jeff at pgexperts.com (Jeff Frost)
Date: Wed, 21 Jan 2015 08:02:41 -0800
Subject: [Slony1-general] Lots of data in sl_log_? but sl_status shows
	lag of 0
In-Reply-To: <CADK3HHJovJMdxQ6uxLmKtUaiQzz-krd5BqQRTKVUxqK1r3b++g@mail.gmail.com>
References: <CADK3HHJovJMdxQ6uxLmKtUaiQzz-krd5BqQRTKVUxqK1r3b++g@mail.gmail.com>
Message-ID: <E23AB152-98B9-452B-8E83-EF83BE2D68D3@pgexperts.com>



> On Jan 21, 2015, at 06:24, Dave Cramer <davecramer at gmail.com> wrote:
> 
> How is this possible ?
> 


Cleanup hasn't run?

Is the slon for the provider still running? Anything interesting in the log?



From davecramer at gmail.com  Wed Jan 21 08:24:43 2015
From: davecramer at gmail.com (Dave Cramer)
Date: Wed, 21 Jan 2015 11:24:43 -0500
Subject: [Slony1-general] Lots of data in sl_log_? but sl_status shows
 lag of 0
In-Reply-To: <E23AB152-98B9-452B-8E83-EF83BE2D68D3@pgexperts.com>
References: <CADK3HHJovJMdxQ6uxLmKtUaiQzz-krd5BqQRTKVUxqK1r3b++g@mail.gmail.com>
	<E23AB152-98B9-452B-8E83-EF83BE2D68D3@pgexperts.com>
Message-ID: <CADK3HHKtRXipv0WvFO56E7XB9kmH0vGJaJcOvKgoEnasTGB_aw@mail.gmail.com>

Hi Jeff,

Pretty sure this is a pg_dump blocking the cleanup.

Thanks,



Dave Cramer

On 21 January 2015 at 11:02, Jeff Frost <jeff at pgexperts.com> wrote:

>
>
> > On Jan 21, 2015, at 06:24, Dave Cramer <davecramer at gmail.com> wrote:
> >
> > How is this possible ?
> >
>
>
> Cleanup hasn't run?
>
> Is the slon for the provider still running? Anything interesting in the
> log?
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150121/b62c4327/attachment.htm 

From ssinger at ca.afilias.info  Wed Jan 21 08:27:42 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 21 Jan 2015 11:27:42 -0500
Subject: [Slony1-general] Lots of data in sl_log_? but sl_status shows
 lag of 0
In-Reply-To: <CADK3HHKtRXipv0WvFO56E7XB9kmH0vGJaJcOvKgoEnasTGB_aw@mail.gmail.com>
References: <CADK3HHJovJMdxQ6uxLmKtUaiQzz-krd5BqQRTKVUxqK1r3b++g@mail.gmail.com>	<E23AB152-98B9-452B-8E83-EF83BE2D68D3@pgexperts.com>
	<CADK3HHKtRXipv0WvFO56E7XB9kmH0vGJaJcOvKgoEnasTGB_aw@mail.gmail.com>
Message-ID: <54BFD37E.6050106@ca.afilias.info>

On 01/21/2015 11:24 AM, Dave Cramer wrote:
> Hi Jeff,
>
> Pretty sure this is a pg_dump blocking the cleanup.
>
> Thanks,
>
>

If your on slony 2.2 you might want to exclude sl_event_lock from your 
pg_dump

If your on an earlier version of slony you might need to exclude 
sl_event or the entire slony schema.


>
> Dave Cramer
>
> On 21 January 2015 at 11:02, Jeff Frost <jeff at pgexperts.com
> <mailto:jeff at pgexperts.com>> wrote:
>
>
>
>     > On Jan 21, 2015, at 06:24, Dave Cramer <davecramer at gmail.com <mailto:davecramer at gmail.com>> wrote:
>     >
>     > How is this possible ?
>     >
>
>
>     Cleanup hasn't run?
>
>     Is the slon for the provider still running? Anything interesting in
>     the log?
>
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From stephane.schildknecht at postgres.fr  Tue Jan 27 10:31:56 2015
From: stephane.schildknecht at postgres.fr (=?UTF-8?B?U3TDqXBoYW5lIFNjaGlsZGtuZWNodA==?=)
Date: Tue, 27 Jan 2015 19:31:56 +0100
Subject: [Slony1-general] slony1-ctl 1.3.0 released
Message-ID: <54C7D99C.90602@postgres.fr>

Hello,

The slony1-ctl development team is proud to announce version 1.3.0 of
slony1-ctl, a collection of shell scripts aiming at simplifying everyday
admnistration of a Slony replication.

This version adds no new feature but compatibility with slony 2.2.
The major change is a better use of variables and a deep comments cleaning.

The project homepage :
  http://pgfoundry.org/projects/slony1-ctl/

The package may be downloaded at :
  http://pgfoundry.org/frs/download.php/3838/slony1-ctl-REL1_3_0.tar.gz

Best regards,
-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150127/086c5944/attachment.pgp 

