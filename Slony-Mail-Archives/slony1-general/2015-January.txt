From ssinger at ca.afilias.info  Mon Jan  5 06:23:47 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 05 Jan 2015 09:23:47 -0500
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <20141231005027.GB25019@fetter.org>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
	<54933BCF.5040003@ca.afilias.info>
	<20141231005027.GB25019@fetter.org>
Message-ID: <54AA9E73.5070109@ca.afilias.info>

On 12/30/2014 07:50 PM, David Fetter wrote:
> On Thu, Dec 18, 2014 at 03:40:47PM -0500, Steve Singer wrote:
>> On 12/18/2014 12:42 PM, Scott Marlowe wrote:
>>
>> I have added a patch for this against
>>
>> http://bugs.slony.info/bugzilla/show_bug.cgi?id=356
>
> ETA on 2.2.4?  This seems like a pretty serious data loss bug and a
> pretty small patch to fix it.
>
> Cheers,
> David.
>

I'd like to see us aim for a 2.2 release maybe next week that includes 
fixes for bug 356,354  and also removes the 'unsupported warning' for 
9.4. Bugs 338 and 345 are already committed on the 2.2 branch and would 
be included as well.

A few weeks ago I said I wanted to do a release close to the end of the 
year but that didn't happen. I make no promises with the above, other 
people might have other ideas.

If the patch (suitable for 2.2) for bug 350 is ready in the next few 
days I'd say we should include it.  Otherwise maybe we want to clarify 
the upgrade documentation to better describe the existing behaviour.



From david at fetter.org  Mon Jan  5 07:04:44 2015
From: david at fetter.org (David Fetter)
Date: Mon, 5 Jan 2015 07:04:44 -0800
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <54AA9E73.5070109@ca.afilias.info>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
	<54933BCF.5040003@ca.afilias.info>
	<20141231005027.GB25019@fetter.org>
	<54AA9E73.5070109@ca.afilias.info>
Message-ID: <20150105150444.GA25657@fetter.org>

On Mon, Jan 05, 2015 at 09:23:47AM -0500, Steve Singer wrote:
> On 12/30/2014 07:50 PM, David Fetter wrote:
> >On Thu, Dec 18, 2014 at 03:40:47PM -0500, Steve Singer wrote:
> >>On 12/18/2014 12:42 PM, Scott Marlowe wrote:
> >>
> >>I have added a patch for this against
> >>
> >>http://bugs.slony.info/bugzilla/show_bug.cgi?id=356
> >
> >ETA on 2.2.4?  This seems like a pretty serious data loss bug and a
> >pretty small patch to fix it.
> >
> 
> I'd like to see us aim for a 2.2 release maybe next week that includes fixes
> for bug 356,354  and also removes the 'unsupported warning' for 9.4. Bugs
> 338 and 345 are already committed on the 2.2 branch and would be included as
> well.

Thanks for the information.

> A few weeks ago I said I wanted to do a release close to the end of
> the year but that didn't happen.  I make no promises with the above,
> other people might have other ideas.

What other stakeholders are involved, and do you have any ideas about
how they might weigh in?

> If the patch (suitable for 2.2) for bug 350 is ready in the next few
> days I'd say we should include it.  Otherwise maybe we want to
> clarify the upgrade documentation to better describe the existing
> behaviour.

350 is well above my experience level to fix, but I'd be happy to
document the issue if that's what's needed.

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From ssinger at ca.afilias.info  Mon Jan  5 08:07:13 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 05 Jan 2015 11:07:13 -0500
Subject: [Slony1-general] Truncate ONLY propagates as only Truncate. PG
 9.2.9/Slony 2.2.1
In-Reply-To: <20150105150444.GA25657@fetter.org>
References: <CAOR=d=1xOgwmvsF5mhiAivthUskyxqu4hfftfH2ceksWPyr_Fg@mail.gmail.com>
	<5492C9A4.5010802@ca.afilias.info>
	<CAOR=d=3zqWGTozncs9ATB+7BrnH-D7oV290UzDWFXoS5gNh5vg@mail.gmail.com>
	<54933BCF.5040003@ca.afilias.info>
	<20141231005027.GB25019@fetter.org>
	<54AA9E73.5070109@ca.afilias.info>
	<20150105150444.GA25657@fetter.org>
Message-ID: <54AAB6B1.4020802@ca.afilias.info>

On 01/05/2015 10:04 AM, David Fetter wrote:
> On Mon, Jan 05, 2015 at 09:23:47AM -0500, Steve Singer wrote:
>> On 12/30/2014 07:50 PM, David Fetter wrote:
>>> On Thu, Dec 18, 2014 at 03:40:47PM -0500, Steve Singer wrote:
>>>> On 12/18/2014 12:42 PM, Scott Marlowe wrote:
>>>>
>>>> I have added a patch for this against
>>>>
>>>> http://bugs.slony.info/bugzilla/show_bug.cgi?id=356
>>>
>>> ETA on 2.2.4?  This seems like a pretty serious data loss bug and a
>>> pretty small patch to fix it.
>>>
>>
>> I'd like to see us aim for a 2.2 release maybe next week that includes fixes
>> for bug 356,354  and also removes the 'unsupported warning' for 9.4. Bugs
>> 338 and 345 are already committed on the 2.2 branch and would be included as
>> well.
>
> Thanks for the information.
>
>> A few weeks ago I said I wanted to do a release close to the end of
>> the year but that didn't happen.  I make no promises with the above,
>> other people might have other ideas.
>
> What other stakeholders are involved, and do you have any ideas about
> how they might weigh in?

I still haven't gotten a reply to my review request from Jan or Chris on 
those patches.  Also a decision on what todo about bug 350 (see below)

Other than that it involves me actually having the time to package up a 
candidate tar and run tests against various PG versions (mostly 
automated) and to manually test that I can still build against windows 
(not automated).

Any new problems that come up during the above could delay things.



>
>> If the patch (suitable for 2.2) for bug 350 is ready in the next few
>> days I'd say we should include it.  Otherwise maybe we want to
>> clarify the upgrade documentation to better describe the existing
>> behaviour.
>
> 350 is well above my experience level to fix, but I'd be happy to
> document the issue if that's what's needed.

I thought Rose had sent a patch in for this that needed to be tested, 
but I don't see the patch attached to the bug.

The issue here is that the instructions for upgrading to 2.2 say change 
cleanup interval to a few seconds  (
http://www.slony.info/documentation/2.2.2/slonyupgrade.html#AEN2512 )
This doesn't actually work as advertised.

If we aren't going to make cleanup_interval do what something useful 
then we need to provide upgrade instructions work as described. Manually 
forcing a log switch by calling the stored function might be one option.




>
> Cheers,
> David.
>


From smarchand at sgo.fr  Wed Jan  7 02:05:58 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Wed, 7 Jan 2015 11:05:58 +0100
Subject: [Slony1-general] too much work !
Message-ID: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>

Hi,

 

I?m using Slony 2.2.2 with postgresql 9.3.

Slony work well but i found slony work too on pg, too many access, too many
process,


For information slony work in wan with 17 nodes, and 18 replications.

Replication A

Server X -> server 1 ( schema base )

Server X -> server 2 ( schema base )

Server X -> server 3 ( schema base )

Server X -> server 4 ( schema base )




Server X -> server 17 ( schema base )

 

Replication B

 

Server 1 -> server X ( schema backbase1 )

Server 1 -> server X ( schema backbase1 )

 

 

Best regards, 

S?bastien Marchand

Soci?t? SGO

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150107/7dc69096/attachment.htm 

From smarchand at sgo.fr  Wed Jan  7 02:15:49 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Wed, 7 Jan 2015 11:15:49 +0100
Subject: [Slony1-general] too much work for pg with 17 nodes...
Message-ID: <006b01d02a62$e9be7880$bd3b6980$@sgo.fr>

Hi,

 

I?m using Slony 2.2.2 with postgresql 9.3.

Slony work well but i found slony work too on pg, too many access, too many
process,


For information slony work in wan with 17 nodes, and 18 replications.

Replication A

Server X -> server 1 ( schema base )

Server X -> server 2 ( schema base )

Server X -> server 3 ( schema base )

Server X -> server 4 ( schema base )




Server X -> server 17 ( schema base )

 

Replication B

 

Server 1 -> server X ( schema backbase1 )

Server 2 -> server X ( schema backbase2 )

Server 3 -> server X ( schema backbase3 )

Server 4 -> server X ( schema backbase4 )




Server 17 -> server X ( schema backbase17 )

 

Local.monitor was desactivated to decrease impact on pg.

I?m using this conf :

 

check interval 200

interval timeout 60000

group size  50 

desired sync time 60000 

cleanup cycles 0 

log level 0

 

i don?t know if someone can help me but i would like to know if my
organisation with several slon is the good way.

Better use one replication with 17 set ? perhaps


 

Ps : sorry for mistake mail

 

Best regards, 

S?bastien Marchand

Soci?t? SGO

 

 

 

 

Cordialement, 

S?bastien Marchand

Soci?t? SGO

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150107/002fd85c/attachment.htm 

From stephane.schildknecht at postgres.fr  Wed Jan  7 03:15:24 2015
From: stephane.schildknecht at postgres.fr (=?UTF-8?B?U3TDqXBoYW5lIFNjaGlsZGtuZWNodA==?=)
Date: Wed, 07 Jan 2015 12:15:24 +0100
Subject: [Slony1-general] too much work for pg with 17 nodes...
In-Reply-To: <006b01d02a62$e9be7880$bd3b6980$@sgo.fr>
References: <006b01d02a62$e9be7880$bd3b6980$@sgo.fr>
Message-ID: <54AD154C.2020003@postgres.fr>

On 07/01/2015 11:15, Sebastien Marchand wrote:
> Hi,
> 
>  
> 
> I?m using Slony 2.2.2 with postgresql 9.3.
> 
> Slony work well but i found slony work too on pg, too many access, too many
> process,?
> 
> For information slony work in wan with 17 nodes, and 18 replications.
> 
> Replication A
> 
> Server X -> server 1 ( schema base )
> 
> Server X -> server 2 ( schema base )
> 
> Server X -> server 3 ( schema base )
> 
> Server X -> server 4 ( schema base )
> 
> ?
> 
> Server X -> server 17 ( schema base )
> 
>  
> 
> Replication B
> 
>  
> 
> Server 1 -> server X ( schema backbase1 )
> 
> Server 2 -> server X ( schema backbase2 )
> 
> Server 3 -> server X ( schema backbase3 )
> 
> Server 4 -> server X ( schema backbase4 )
> 
> ?
> 
> Server 17 -> server X ( schema backbase17 )
> 
>  
> 
> Local.monitor was desactivated to decrease impact on pg.
> 
> I?m using this conf :
> 
>  
> 
> /check interval 200/
> 
> /interval timeout/ 60000
> 
> group size  50
> 
> /desired sync time/ 60000
> 
> cleanup cycles 0
> 
> log level 0
> 
>  
> 
> i don?t know if someone can help me but i would like to know if my organisation
> with several slon is the good way.
> 
> Better use one replication with 17 set ? perhaps?
> 
>  

Hello,

If I understand what you wrote, you have a provider for one set of objects in a
database that is replicated on 17 different servers.

Another set of of objects is then replicated back from every 17 servers to the
first one.

Could you give more clues on the objects and their organisation in sets ?
Are all objects different in all sets ?

-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150107/c524f6c0/attachment.pgp 

From vivek at khera.org  Wed Jan  7 09:40:00 2015
From: vivek at khera.org (Vick Khera)
Date: Wed, 7 Jan 2015 12:40:00 -0500
Subject: [Slony1-general] too much work !
In-Reply-To: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>
References: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>
Message-ID: <CALd+dcfE-xrHqWGaza2t8jgCnGoQ-vmE7dWOn827P3_rOZCZRw@mail.gmail.com>

On Wed, Jan 7, 2015 at 5:05 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:

> Slony work well but i found slony work too on pg, too many access, too
> many process,?
>
> For information slony work in wan with 17 nodes, and 18 replications.
>
>
If you have 17 replicas for "A", do not have them all directly replicate
from the master. set up a tree structure.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150107/45f33779/attachment.htm 

From smarchand at sgo.fr  Thu Jan  8 00:31:46 2015
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Thu, 8 Jan 2015 09:31:46 +0100
Subject: [Slony1-general] too much work !
In-Reply-To: <CALd+dcfE-xrHqWGaza2t8jgCnGoQ-vmE7dWOn827P3_rOZCZRw@mail.gmail.com>
References: <006601d02a61$88e24970$9aa6dc50$@sgo.fr>
	<CALd+dcfE-xrHqWGaza2t8jgCnGoQ-vmE7dWOn827P3_rOZCZRw@mail.gmail.com>
Message-ID: <008701d02b1d$8ab50730$a01f1590$@sgo.fr>

I can?t do this, only my master have a good bandwith (up and down), others server are in warehouse. 

 

De : Vick Khera [mailto:vivek at khera.org] 
Envoy? : mercredi 7 janvier 2015 18:40
? : Sebastien Marchand
Cc : slony
Objet : Re: [Slony1-general] too much work !

 

 

On Wed, Jan 7, 2015 at 5:05 AM, Sebastien Marchand <smarchand at sgo.fr> wrote:

Slony work well but i found slony work too on pg, too many access, too many process,?

For information slony work in wan with 17 nodes, and 18 replications.


If you have 17 replicas for "A", do not have them all directly replicate from the master. set up a tree structure.

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150108/4dcbf95d/attachment.htm 

