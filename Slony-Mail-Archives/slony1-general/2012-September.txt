From knut.ingvald.dietzel at redpill-linpro.com  Wed Sep  5 02:46:40 2012
From: knut.ingvald.dietzel at redpill-linpro.com (Knut Ingvald Dietzel)
Date: Wed, 5 Sep 2012 11:46:40 +0200
Subject: [Slony1-general] Issue with slonik failover
In-Reply-To: <5040B54C.8080602@ca.afilias.info>
References: <20120831081630.GA4164@localhost.localdomain>
	<5040B54C.8080602@ca.afilias.info>
Message-ID: <20120905094640.GA377@localhost.localdomain>

On Fri, Aug 31, 2012 at 08:59:56AM -0400, Steve Singer wrote:
> On 12-08-31 04:16 AM, Knut Ingvald Dietzel wrote:
[cut]
> >  From what I have been able to find out so far, slonik should wait for
> > the slon engine to restart, and then call failedNode2() on the node with
> > the highest SYNC.  Though, from the log above failedNode2() appears to
> > be called twice, the second instance fails in getting lock, and the
> > process of failing over node 1 to 4 fails.
> >
> > Firstly, is my interpretation in the vicinity of being correct?
> 
> When Slonik (<=2.1.x) does a fail over it generates a 'fake'
> FAILOVER event using a ev_origin=$failed_node with the highest
> sequence number it can see of that failed node.  It pushes this
> event into sl_event on one of the remaining nodes.  In the test case
> you describe it sounds like that slon is still running on the failed
> node.  Slony <=2.1.x have numerous race conditions with failover one
> of the ones I've seen is where a 'real' SYNC event ie 1,1234 that
> escaped from the failed node can conflict with the faked FAILOVER
> event 1,1234.

Hi, Steve.

Thanks for the insight, and your explanation sounds very reasonable.

> I rewrote a lot of the failover logic in 2.2 to try to address many
> of these issues.  It should do a much better job at waiting for
> slons to restart etc...  2.2 is still beta and I wouldn't recommend
> it for production use yet but I encourage you to look at it to see
> if it addresses your issues.

That's very good to hear. We'll look into possibilities of testing the
2.2b version.

Again, thanks!


-- 
Best regards,
Knut Ingvald Dietzel
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: Digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120905/0eeaa5d6/attachment.pgp 

From stephane.schildknecht at postgresql.fr  Thu Sep  6 00:58:55 2012
From: stephane.schildknecht at postgresql.fr (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Thu, 06 Sep 2012 09:58:55 +0200
Subject: [Slony1-general] Using Slony to migrate from PostgreSQL 8.2 to 9.2
Message-ID: <504857BF.8070906@postgresql.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello,

I currently have a 1.2 slony cluster replicating PostgreSQL 8.2 databases.

On our way to migrate to PostgreSQL 9.2, we would like to use Slony to ease
the migration process.

But, I wonder which version of Slony I could use, as the version currently
used won't compile vs PostgreSQL 9.2rc1. Neither would slony 2.1.2 compile
with PG 8.2.

Thanks in advance for any advice.

Regards,
- -- 
St?phane Schildknecht
http://www.Loxodata.com
Contact r?gional PostgreSQL


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://www.enigmail.net/

iEYEARECAAYFAlBIV78ACgkQA+REPKWGI0E/uACglepw4b1o+Wq79gUxVA75yqbO
il8An2fynXRvQaBMoVH9JvFZo8nK/OJH
=S8Sm
-----END PGP SIGNATURE-----

From cbbrowne at afilias.info  Thu Sep  6 09:56:03 2012
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 6 Sep 2012 12:56:03 -0400
Subject: [Slony1-general] Using Slony to migrate from PostgreSQL 8.2 to
	9.2
In-Reply-To: <504857BF.8070906@postgresql.fr>
References: <504857BF.8070906@postgresql.fr>
Message-ID: <CANfbgbbhm3Dc3UEeHQPHL=Y5iYu9k7Sg1Z_HKBJmyD=cNm4_Hg@mail.gmail.com>

On Thu, Sep 6, 2012 at 3:58 AM, "St?phane A. Schildknecht"
<stephane.schildknecht at postgresql.fr> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hello,
>
> I currently have a 1.2 slony cluster replicating PostgreSQL 8.2 databases.
>
> On our way to migrate to PostgreSQL 9.2, we would like to use Slony to ease
> the migration process.
>
> But, I wonder which version of Slony I could use, as the version currently
> used won't compile vs PostgreSQL 9.2rc1. Neither would slony 2.1.2 compile
> with PG 8.2.

We did something of a "sea change" with version 2 of Slony; there were
substantial changes provided in PostgreSQL 8.3 that 2.0 and up depend
on, and there is no intent to attempt to get 2.0 and later versions to
work with earlier versions of Postgres.

I'm not certain offhand what is the latest version of Postgres which
version 1.2 supports; what is likely disappointing to you is that I
don't think it gets as far as 9.2.  I see notes in git history
indicating that it has had changes that might bring it as far as 9.0,
but I'd be inclined to be a bit tentative about 9.0.  No doubt there
have been changes in 9.1 and 9.2 that have not been backported into
the 1.2 branch.

This does not indicate that Slony is useless to the purpose, but you
might need to have two phases.

1.  Upgrade from 8.2 to 8.3 or so using Slony 1.2

Theoretically, you could get an upgrade to 9.0, but I'd be inclined to
go from 8.2 to 8.3; that's got much less risk of running into any
issues of parts of 9.0 that weren't totally supported in the 1.2
branch.

2.  Then upgrade from (say) 8.3 to 9.2 using Slony 2.1

That's not as wonderful an answer as you might have wanted, certainly
more work than you'd have preferred.  But it ought to work all the
same.

From brianf at consistentstate.com  Thu Sep  6 14:09:05 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 06 Sep 2012 15:09:05 -0600
Subject: [Slony1-general] Slony lagging, no errors, due to bulk data change?
Message-ID: <504910F1.7080706@consistentstate.com>

Hi all,

postgresql 8.4
slony 1.2
centos system

I have single master to single slave slony cluster where slony is very 
far behind. There are no errors in the logs or anything, but what looks 
to be happeneing is that queries slony is executing on the slony log 
table on the master are taking a long time to complete.

The query is "fetch 100 from LOG;" which can take a long time to 
complete, over 15 minutes at times. Each time this happens we process 1 
event. This usually takes milliseconds to complete.

At this point, the query on the master 'fetch 100 from log' takes about 
20 minutes to complete, and after it completes the slave processes 1 
more event, and then 'fetch 100 from log' kicks off again and takes yet 
another 20 or so minutes. So the slave is processing an event about once 
per 20 minutes.

As for a cause I believe it's due to the follow up work after adding a 
column to a table in replication. After adding the column, I believe 
that the table is being updated to set new values in the newly added 
column. This could result in millions of new items for slony to process, 
which may have caused the tables to become so large that they are 
resulting in sequential scans or something.

I'm trying to dig in and see what exactly 'fetch 100 from log' is doing 
on the master, and if I can speed it up. Is this querying sl_log_<1/2> 
tables?

the pg table pg_notify does not have outstanding dead rows, it's at 0. 
also out of all the slony tables in the slony schema, the one with the 
most dead rows is at about 2K dead rows.

sl_log_1 has 0 rows, sl_log_2 has about 9,326,260 rows (and zero dead rows).

I'm going to see if we can reduce group size, see if for whatever reason 
that reduces the query result set so it does a index scan vs sequential 
(if that's even the issue).

Any help is greatly appreciated.

- Brian F

From brianf at consistentstate.com  Thu Sep  6 14:49:34 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 06 Sep 2012 15:49:34 -0600
Subject: [Slony1-general] Slony lagging, no errors,
	due to bulk data change?
In-Reply-To: <504910F1.7080706@consistentstate.com>
References: <504910F1.7080706@consistentstate.com>
Message-ID: <50491A6E.5040501@consistentstate.com>

More information:

I can confirm that over the course of 3 or so hours, our sl_log_2 table 
grew from 1,000 rows to over 9 million rows. This is due to us doing 
huge table wide updates I believe.

So now sl_log_2 is huge, and the 'fetch 100 from LOG' is a query hitting 
the cursor LOG, which is defined as:

                                                  "declare LOG cursor 
for select "
                                                  "    log_origin, 
log_xid, log_tableid, "
                                                  "    log_actionseq, 
log_cmdtype, "
                                                  "    
octet_length(log_cmddata), "
                                                  "    case when 
octet_length(log_cmddata) <= %d "
                                                  "        then 
log_cmddata "
                                                  "        else null end "
                                                  "from %s.sl_log_2 %s 
order by log_actionseq; ",


Looking at the pg_stat_activity, it looks like 'fetch 1200 from LOG' has 
the same transaction start time as the query start time, which suggests 
that each time it fetches 100 from the log it is starting a new cursor, 
is this normal / expected?


select now() - xact_start as transaction_age, now() - query_start as 
query_age, current_query::varchar(80), waiting from pg_stat_activity 
where usename = 'slony' and current_query like '%LOG%';
  transaction_age |    query_age    |    current_query     | waiting
-----------------+-----------------+----------------------+---------
  00:27:01.652087 | 00:27:01.650933 | fetch 100 from LOG;  | f
(1 row)


So this has to be my issue. The query that hits sl_log_2 is a sequential 
scan, does this mean every time we retrieve 100 from the cursor, we do 
another sequential scan on the table to get the next 100 rows?

- Brian F


On 09/06/2012 03:09 PM, Brian Fehrle wrote:
> Hi all,
>
> postgresql 8.4
> slony 1.2
> centos system
>
> I have single master to single slave slony cluster where slony is very
> far behind. There are no errors in the logs or anything, but what looks
> to be happeneing is that queries slony is executing on the slony log
> table on the master are taking a long time to complete.
>
> The query is "fetch 100 from LOG;" which can take a long time to
> complete, over 15 minutes at times. Each time this happens we process 1
> event. This usually takes milliseconds to complete.
>
> At this point, the query on the master 'fetch 100 from log' takes about
> 20 minutes to complete, and after it completes the slave processes 1
> more event, and then 'fetch 100 from log' kicks off again and takes yet
> another 20 or so minutes. So the slave is processing an event about once
> per 20 minutes.
>
> As for a cause I believe it's due to the follow up work after adding a
> column to a table in replication. After adding the column, I believe
> that the table is being updated to set new values in the newly added
> column. This could result in millions of new items for slony to process,
> which may have caused the tables to become so large that they are
> resulting in sequential scans or something.
>
> I'm trying to dig in and see what exactly 'fetch 100 from log' is doing
> on the master, and if I can speed it up. Is this querying sl_log_<1/2>
> tables?
>
> the pg table pg_notify does not have outstanding dead rows, it's at 0.
> also out of all the slony tables in the slony schema, the one with the
> most dead rows is at about 2K dead rows.
>
> sl_log_1 has 0 rows, sl_log_2 has about 9,326,260 rows (and zero dead rows).
>
> I'm going to see if we can reduce group size, see if for whatever reason
> that reduces the query result set so it does a index scan vs sequential
> (if that's even the issue).
>
> Any help is greatly appreciated.
>
> - Brian F
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120906/ec9367a4/attachment.htm 

From steve at ssinger.info  Thu Sep  6 18:46:42 2012
From: steve at ssinger.info (Steve Singer)
Date: Thu, 6 Sep 2012 21:46:42 -0400
Subject: [Slony1-general] Using Slony to migrate from PostgreSQL 8.2 to
 9.2
In-Reply-To: <504857BF.8070906@postgresql.fr>
References: <504857BF.8070906@postgresql.fr>
Message-ID: <BLU0-SMTP176BAD969A624FA481915AADCAF0@phx.gbl>

On 09/06/2012 03:58 AM, "St?phane A. Schildknecht" wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hello,
>
> I currently have a 1.2 slony cluster replicating PostgreSQL 8.2 databases.
>
> On our way to migrate to PostgreSQL 9.2, we would like to use Slony to ease
> the migration process.
>
> But, I wonder which version of Slony I could use, as the version currently
> used won't compile vs PostgreSQL 9.2rc1. Neither would slony 2.1.2 compile
> with PG 8.2.
>
> Thanks in advance for any advice.

You won't be able to get slony 2.0.x or 2.1.x to work with 8.2.    I 
suspect it won't be that hard to get 1.2 to compile against 9.2, the 
patches to get 2.1.x compiling against 9.2 were not that involved.  It 
won't work well because of the serialization conflicts that slony prior 
to 2.1.1 gets with PG 9.1+ but it might work well enough for an 
upgrade.  (I also can't say if there are any other issues with running 
1.2 against 9.2 since I've never tried it).

Your other option is, as Chris mentioned, to upgrade in two steps.

Steve

> Regards,
> - -- 
> St?phane Schildknecht
> http://www.Loxodata.com
> Contact r?gional PostgreSQL
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.11 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://www.enigmail.net/
>
> iEYEARECAAYFAlBIV78ACgkQA+REPKWGI0E/uACglepw4b1o+Wq79gUxVA75yqbO
> il8An2fynXRvQaBMoVH9JvFZo8nK/OJH
> =S8Sm
> -----END PGP SIGNATURE-----
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>


From smarchand at sgo.fr  Fri Sep  7 06:46:33 2012
From: smarchand at sgo.fr (Sebastien Marchand)
Date: Fri, 7 Sep 2012 15:46:33 +0200
Subject: [Slony1-general] Lagtime by set ?
Message-ID: <001d01cd8cff$338efb50$9aacf1f0$@sgo.fr>

Hi everybody,

 

I would like to know if i can find the lagtime by Set.

 

Just for information i got 1 cluster and 3 sets in. Example :

 

Instance              set         master                 slave

F_repli                  1             1             2

F_repli                  2             2             1

F_repli                  3             2             1

 

(Set 2 and 3 same direction but different schema)                        

Thanks a lot.

 

Cordialement, 

S?bastien Marchand

Soci?t? SGO

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120907/a943eda6/attachment.htm 

From steve at ssinger.info  Fri Sep  7 08:46:54 2012
From: steve at ssinger.info (Steve Singer)
Date: Fri, 7 Sep 2012 11:46:54 -0400
Subject: [Slony1-general] Slony lagging, no errors,
	due to bulk data change?
In-Reply-To: <50491A6E.5040501@consistentstate.com>
References: <504910F1.7080706@consistentstate.com>
	<50491A6E.5040501@consistentstate.com>
Message-ID: <BLU0-SMTP4436669A0E30A1BFF5C0D9DCAF0@phx.gbl>

On 09/06/2012 05:49 PM, Brian Fehrle wrote:
> More information:

> Looking at the pg_stat_activity, it looks like 'fetch 1200 from LOG' 
> has the same transaction start time as the query start time, which 
> suggests that each time it fetches 100 from the log it is starting a 
> new cursor, is this normal / expected?
>

Yes this is normal for your version of slony, but a bug.  It was fixed 
in slony 2.1.  I think the bug number was bug 167.

>
> select now() - xact_start as transaction_age, now() - query_start as 
> query_age, current_query::varchar(80), waiting from pg_stat_activity 
> where usename = 'slony' and current_query like '%LOG%';
>  transaction_age |    query_age    |    current_query     | waiting
> -----------------+-----------------+----------------------+---------
>  00:27:01.652087 | 00:27:01.650933 | fetch 100 from LOG;  | f
> (1 row)
>
>
> So this has to be my issue. The query that hits sl_log_2 is a 
> sequential scan, does this mean every time we retrieve 100 from the 
> cursor, we do another sequential scan on the table to get the next 100 
> rows?
>
> - Brian F
>
>
> On 09/06/2012 03:09 PM, Brian Fehrle wrote:
>> Hi all,
>>
>> postgresql 8.4
>> slony 1.2
>> centos system
>>
>> I have single master to single slave slony cluster where slony is very
>> far behind. There are no errors in the logs or anything, but what looks
>> to be happeneing is that queries slony is executing on the slony log
>> table on the master are taking a long time to complete.
>>
>> The query is "fetch 100 from LOG;" which can take a long time to
>> complete, over 15 minutes at times. Each time this happens we process 1
>> event. This usually takes milliseconds to complete.
>>
>> At this point, the query on the master 'fetch 100 from log' takes about
>> 20 minutes to complete, and after it completes the slave processes 1
>> more event, and then 'fetch 100 from log' kicks off again and takes yet
>> another 20 or so minutes. So the slave is processing an event about once
>> per 20 minutes.
>>
>> As for a cause I believe it's due to the follow up work after adding a
>> column to a table in replication. After adding the column, I believe
>> that the table is being updated to set new values in the newly added
>> column. This could result in millions of new items for slony to process,
>> which may have caused the tables to become so large that they are
>> resulting in sequential scans or something.
>>
>> I'm trying to dig in and see what exactly 'fetch 100 from log' is doing
>> on the master, and if I can speed it up. Is this querying sl_log_<1/2>
>> tables?
>>
>> the pg table pg_notify does not have outstanding dead rows, it's at 0.
>> also out of all the slony tables in the slony schema, the one with the
>> most dead rows is at about 2K dead rows.
>>
>> sl_log_1 has 0 rows, sl_log_2 has about 9,326,260 rows (and zero dead rows).
>>
>> I'm going to see if we can reduce group size, see if for whatever reason
>> that reduces the query result set so it does a index scan vs sequential
>> (if that's even the issue).
>>
>> Any help is greatly appreciated.
>>
>> - Brian F
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120907/24e0bb6b/attachment-0001.htm 

From brianf at consistentstate.com  Fri Sep  7 08:50:59 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Fri, 07 Sep 2012 09:50:59 -0600
Subject: [Slony1-general] Slony lagging, no errors,
	due to bulk data change?
In-Reply-To: <BLU0-SMTP4436669A0E30A1BFF5C0D9DCAF0@phx.gbl>
References: <504910F1.7080706@consistentstate.com>
	<50491A6E.5040501@consistentstate.com>
	<BLU0-SMTP4436669A0E30A1BFF5C0D9DCAF0@phx.gbl>
Message-ID: <504A17E3.6030604@consistentstate.com>

On 09/07/2012 09:46 AM, Steve Singer wrote:
> On 09/06/2012 05:49 PM, Brian Fehrle wrote:
>> More information:
>
>> Looking at the pg_stat_activity, it looks like 'fetch 1200 from LOG' 
>> has the same transaction start time as the query start time, which 
>> suggests that each time it fetches 100 from the log it is starting a 
>> new cursor, is this normal / expected?
>>
>
> Yes this is normal for your version of slony, but a bug.  It was fixed 
> in slony 2.1.  I think the bug number was bug 167.
>
Ok thanks. I did look at the source for both 1.2 and 2.1 and noticed 
that the worker thread changed significantly, and suspected it changed.

Unfortunately it's been running for a full day in this state, and in 1 
day it processed about 1 hour worth of events. So we're going to be 
uninstalling both nodes and re-setting it up. Long process because we 
have dozens of remote subscribers via log shipping too.

In the future, if we need to perform mass updates to tables like this, 
it may be a good idea to remove the table from replication, do all the 
updates, then add it back. It would still  be annoying for remote 
subscribers but we have processes in place to handle it.

- Brian F
>>
>> select now() - xact_start as transaction_age, now() - query_start as 
>> query_age, current_query::varchar(80), waiting from pg_stat_activity 
>> where usename = 'slony' and current_query like '%LOG%';
>>  transaction_age |    query_age    |    current_query     | waiting
>> -----------------+-----------------+----------------------+---------
>>  00:27:01.652087 | 00:27:01.650933 | fetch 100 from LOG;  | f
>> (1 row)
>>
>>
>> So this has to be my issue. The query that hits sl_log_2 is a 
>> sequential scan, does this mean every time we retrieve 100 from the 
>> cursor, we do another sequential scan on the table to get the next 
>> 100 rows?
>>
>> - Brian F
>>
>>
>> On 09/06/2012 03:09 PM, Brian Fehrle wrote:
>>> Hi all,
>>>
>>> postgresql 8.4
>>> slony 1.2
>>> centos system
>>>
>>> I have single master to single slave slony cluster where slony is very
>>> far behind. There are no errors in the logs or anything, but what looks
>>> to be happeneing is that queries slony is executing on the slony log
>>> table on the master are taking a long time to complete.
>>>
>>> The query is "fetch 100 from LOG;" which can take a long time to
>>> complete, over 15 minutes at times. Each time this happens we process 1
>>> event. This usually takes milliseconds to complete.
>>>
>>> At this point, the query on the master 'fetch 100 from log' takes about
>>> 20 minutes to complete, and after it completes the slave processes 1
>>> more event, and then 'fetch 100 from log' kicks off again and takes yet
>>> another 20 or so minutes. So the slave is processing an event about once
>>> per 20 minutes.
>>>
>>> As for a cause I believe it's due to the follow up work after adding a
>>> column to a table in replication. After adding the column, I believe
>>> that the table is being updated to set new values in the newly added
>>> column. This could result in millions of new items for slony to process,
>>> which may have caused the tables to become so large that they are
>>> resulting in sequential scans or something.
>>>
>>> I'm trying to dig in and see what exactly 'fetch 100 from log' is doing
>>> on the master, and if I can speed it up. Is this querying sl_log_<1/2>
>>> tables?
>>>
>>> the pg table pg_notify does not have outstanding dead rows, it's at 0.
>>> also out of all the slony tables in the slony schema, the one with the
>>> most dead rows is at about 2K dead rows.
>>>
>>> sl_log_1 has 0 rows, sl_log_2 has about 9,326,260 rows (and zero dead rows).
>>>
>>> I'm going to see if we can reduce group size, see if for whatever reason
>>> that reduces the query result set so it does a index scan vs sequential
>>> (if that's even the issue).
>>>
>>> Any help is greatly appreciated.
>>>
>>> - Brian F
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120907/ff466aef/attachment.htm 

From stephane.schildknecht at postgresql.fr  Mon Sep 10 10:26:50 2012
From: stephane.schildknecht at postgresql.fr (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Mon, 10 Sep 2012 19:26:50 +0200
Subject: [Slony1-general] Using Slony to migrate from PostgreSQL 8.2 to
 9.2
In-Reply-To: <BLU0-SMTP176BAD969A624FA481915AADCAF0@phx.gbl>
References: <504857BF.8070906@postgresql.fr>
	<BLU0-SMTP176BAD969A624FA481915AADCAF0@phx.gbl>
Message-ID: <504E22DA.4070809@postgresql.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Le 07/09/2012 03:46, Steve Singer a ?crit :



Steve, Christopher,

Thanks for your answers.
In fact, I couldn't get Slony 1.2 to compile with PG9.2.

I wondered if there was a magic plan I could rely on to ease my multiple node
replication cluster.

As there isn't any magical plan, I will have to consider in more deep details
the idea of a  two phase migration, as proposed by Christopher.

Regards,
- -- 
St?phane Schildknecht
http://www.Loxodata.com
Contact r?gional PostgreSQL
http://bistri.me/sas

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://www.enigmail.net/

iEYEARECAAYFAlBOItoACgkQA+REPKWGI0FqEQCgtmI9WBEI+Qp/xMqm+zZyF9qh
GnwAoMf1qqZAKuaVpiAijXBs2CEQ0vkB
=fvMg
-----END PGP SIGNATURE-----

From vivek at khera.org  Mon Sep 10 10:54:30 2012
From: vivek at khera.org (Vick Khera)
Date: Mon, 10 Sep 2012 13:54:30 -0400
Subject: [Slony1-general] Using Slony to migrate from PostgreSQL 8.2 to
	9.2
In-Reply-To: <504E22DA.4070809@postgresql.fr>
References: <504857BF.8070906@postgresql.fr>
	<BLU0-SMTP176BAD969A624FA481915AADCAF0@phx.gbl>
	<504E22DA.4070809@postgresql.fr>
Message-ID: <CALd+dcewX8VFHac6h0N2KOzCnCQ-yyW4X6mt3Ug3pLM+O1Eptw@mail.gmail.com>

On Mon, Sep 10, 2012 at 1:26 PM, "St?phane A. Schildknecht" <
stephane.schildknecht at postgresql.fr> wrote:

> I wondered if there was a magic plan I could rely on to ease my multiple
> node
> replication cluster.
>
> As there isn't any magical plan, I will have to consider in more deep
> details
> the idea of a  two phase migration, as proposed by Christopher.
>

I think the magic would be "go back in time and do not wait so long between
major version upgrades."

It certainly will be safest to do the multi-step upgrade.  I'd guess you
don't need to do multiple steps for every node in your cluster depending on
how they are used...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120910/4962c8f8/attachment.htm 

From stephane.schildknecht at postgresql.fr  Mon Sep 10 13:11:21 2012
From: stephane.schildknecht at postgresql.fr (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Mon, 10 Sep 2012 22:11:21 +0200
Subject: [Slony1-general] Using Slony to migrate from PostgreSQL 8.2 to
 9.2
In-Reply-To: <CALd+dcewX8VFHac6h0N2KOzCnCQ-yyW4X6mt3Ug3pLM+O1Eptw@mail.gmail.com>
References: <504857BF.8070906@postgresql.fr>
	<BLU0-SMTP176BAD969A624FA481915AADCAF0@phx.gbl>
	<504E22DA.4070809@postgresql.fr>
	<CALd+dcewX8VFHac6h0N2KOzCnCQ-yyW4X6mt3Ug3pLM+O1Eptw@mail.gmail.com>
Message-ID: <504E4969.4050905@postgresql.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Le 10/09/2012 19:54, Vick Khera a ?crit :
> On Mon, Sep 10, 2012 at 1:26 PM, "St?phane A. Schildknecht" 
> <stephane.schildknecht at postgresql.fr 
> <mailto:stephane.schildknecht at postgresql.fr>> wrote:
> 
> I wondered if there was a magic plan I could rely on to ease my multiple 
> node replication cluster.
> 
> As there isn't any magical plan, I will have to consider in more deep 
> details the idea of a  two phase migration, as proposed by Christopher.
> 
> 
> I think the magic would be "go back in time and do not wait so long 
> between major version upgrades."

That is the key point.
I won't try to explain that I was no decision-maker in that story.

> 
> It certainly will be safest to do the multi-step upgrade.  I'd guess you 
> don't need to do multiple steps for every node in your cluster depending 
> on how they are used...

Not every one of them, but surely more than one.

S.

PS : To be complete, I join the error I got when trying to compile Slony 1.2
with PG 9.2 :
slony1_funcs.c: In function ?_Slony_I_logTrigger?:
slony1_funcs.c:515:38: error: dereferencing pointer to incomplete type
...
- -- 
St?phane Schildknecht
http://www.Loxodata.com
Contact r?gional PostgreSQL

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://www.enigmail.net/

iEYEARECAAYFAlBOSWkACgkQA+REPKWGI0Hd5wCeOosDZK7dhR3FNedasyeMaGBv
/ksAoJSLsGFra9SUXT0J4iFUbW8bYfpf
=cOEp
-----END PGP SIGNATURE-----

From cbbrowne at afilias.info  Fri Sep 14 07:35:54 2012
From: cbbrowne at afilias.info (Christopher Browne)
Date: Fri, 14 Sep 2012 10:35:54 -0400
Subject: [Slony1-general] Removing some elderly documentation
Message-ID: <CANfbgbZQnRuA6-zJ4F05e8SAkMueQYf9e5qPXN9wa59DP73gag@mail.gmail.com>

Joe Conway pointed out to me recently that people have been taking
heed of some pretty ancient documentation in the Slony doc tree that
indicates "initial node ID MUST be 1".

This was true, in a very precise context, back in the 1.2 version and
earlier.   It misses the extra caveat that "it MUST be 1, because we
are simplifying configuration by omitting EVENT NODE everywhere that
we can, and *that* would require having a node with ID 1 that, by
default, gets events first."

In version 2, we changed slonik to require event nodes in pretty well
all cases, so that "MUST" is now pretty well untrue.

I took a peek back at the documentation that said that, and am finding
that it would require a pretty broad rewrite in order to validate that
it is anywhere near accurate.  I think we have more recent and better
material, so I'll drop this old doc out.

From mark.steben at drivedominion.com  Fri Sep 14 08:27:30 2012
From: mark.steben at drivedominion.com (Mark Steben)
Date: Fri, 14 Sep 2012 11:27:30 -0400
Subject: [Slony1-general] question on setup scripts for slony.2.1.1
Message-ID: <CADyzmyzrV0=mDr_Smr4-gX2bMAy0DUzgb9f0Fk5xfcs1NnKWNA@mail.gmail.com>

*Hi, my system administrator installed slony 2.1.1 on our master and future
slave machines.
I've noticed that the slonik setup scripts I've used in prior versions of
slony are not there
  I'm referring to:
    slonik_init_cluster
    slonik_create_set
    slonik_subscribe_set
   (and others)

These are the scripts I've used in the past.  Are they supposed to be there
or
 are there updated alternatives?

  Thank you
*
-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase<http://www.autobase.net/>

  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120914/d746bd28/attachment.htm 

From sebpaa at gmail.com  Tue Sep 18 06:56:14 2012
From: sebpaa at gmail.com (=?iso-8859-2?Q?Sebastian_Paw=B3owski?=)
Date: Tue, 18 Sep 2012 15:56:14 +0200
Subject: [Slony1-general] cant set up replication - problem with subscribe
In-Reply-To: <CADyzmyzrV0=mDr_Smr4-gX2bMAy0DUzgb9f0Fk5xfcs1NnKWNA@mail.gmail.com>
References: <CADyzmyzrV0=mDr_Smr4-gX2bMAy0DUzgb9f0Fk5xfcs1NnKWNA@mail.gmail.com>
Message-ID: <373F779F-E3BB-4524-B351-EF7CD4645DAE@gmail.com>

HI, 

os: ubuntu
slony 2.1.2
postgres 9.0

i've problem with setting up replicaction, when i try to subscribe set from master to slave i get:

2012-09-18 14:49:13 CEST CONFIG remoteWorkerThread_2: Begin COPY of table "public"."auth_user"
NOTICE:  truncate of "public"."auth_user" failed - doing delete
2012-09-18 14:54:42 CEST CONFIG remoteWorkerThread_2: 2745697400 bytes copied for table "public"."auth_user"
2012-09-18 15:26:52 CEST CONFIG remoteWorkerThread_2: 2259.200 seconds to copy table "public"."auth_user"
2012-09-18 15:26:52 CEST CONFIG remoteWorkerThread_2: copy table "public"."payments_grouppacket"
2012-09-18 15:26:52 CEST CONFIG remoteWorkerThread_2: Begin COPY of table "public"."payments_grouppacket"
2012-09-18 15:26:52 CEST ERROR  remoteWorkerThread_2: "select "_getmedia".copyFields(2);" FATAL:  terminating connection due to administrator command
server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

2012-09-18 15:26:52 CEST WARN   remoteWorkerThread_2: data copy for set 1 failed 1 times - sleep 15 seconds

table auth_user syncs, but next table payments_grouppacket fails to sync, every time the same: FATAL, and the process go back to copy auth_user
i've checked in logs, and there is not nothing else

postgres logs shows only these:

Sep 18 15:26:52 db01a postgres[11628]: [68-1] 2012-09-18 15:26:52.188 CEST slony at getmedia 11628 192.168.10.241(58888) LOG:  duration: 1929804.203 ms  statement: select "_getmedia".finishTableAfterCopy(1); analyze "public"."auth_user"; 
Sep 18 15:26:52 db01a postgres[11629]: [4667-1] 2012-09-18 15:26:52.192 CEST slony at getmedia 11629 192.168.10.241(58891) LOG:  duration: 0.082 ms  statement: start transaction;set transaction isolation level serializable;lock table "_getmedia".sl_event_lock;select last_value from "_getmedia".sl_action_seq;
Sep 18 15:26:52 db01a postgres[11629]: [4668-1] 2012-09-18 15:26:52.192 CEST slony at getmedia 11629 192.168.10.241(58891) LOG:  duration: 0.019 ms  statement: rollback transaction;
Sep 18 15:26:52 db01a postgres[11619]: [3361-1] 2012-09-18 15:26:52.229 CEST slony at getmedia 11619 192.168.10.241(58887) LOG:  duration: 0.034 ms  statement: start transaction; set transaction isolation level serializable;
Sep 18 15:26:52 db01a postgres[11619]: [3362-1] 2012-09-18 15:26:52.229 CEST slony at getmedia 11619 192.168.10.241(58887) LOG:  duration: 0.228 ms  statement: select ev_seqno, ev_timestamp,        'dummy', 'dummy', 'dummy',        ev_type,        ev_data1, ev_data2, ev_data3, ev_data4,        ev_data5, ev_data6, ev_data7, ev_data8 from "_getmedia".sl_event where  ev_origin = '1'        and ev_seqno > '5000000576' order by ev_seqno
Sep 18 15:26:52 db01a postgres[11619]: [3363-1] 2012-09-18 15:26:52.229 CEST slony at getmedia 11619 192.168.10.241(58887) LOG:  duration: 0.022 ms  statement: rollback transaction;
Sep 18 15:26:52 db01a postgres[11628]: [69-1] 2012-09-18 15:26:52.235 CEST slony at getmedia 11628 192.168.10.241(58888) LOG:  duration: 46.190 ms  statement: lock table "_getmedia".sl_config_lock;select "_getmedia".setAddTable_int(1, 2, '"public"."payments_grouppacket"', 'payments_grouppacket_pkey', 'Table public.payments_grouppacket with primary key'); 


any ideas, could You help me?

best regards

Sebastian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120918/1802882e/attachment.htm 

From sebpaa at gmail.com  Tue Sep 18 08:59:52 2012
From: sebpaa at gmail.com (=?iso-8859-2?Q?Sebastian_Paw=B3owski?=)
Date: Tue, 18 Sep 2012 17:59:52 +0200
Subject: [Slony1-general] cant set up replication - problem with
	subscribe
In-Reply-To: <373F779F-E3BB-4524-B351-EF7CD4645DAE@gmail.com>
References: <CADyzmyzrV0=mDr_Smr4-gX2bMAy0DUzgb9f0Fk5xfcs1NnKWNA@mail.gmail.com>
	<373F779F-E3BB-4524-B351-EF7CD4645DAE@gmail.com>
Message-ID: <9CBF61E2-024A-46D1-9E88-DA99FAF68F83@gmail.com>

problem solved

Wiadomo?? napisana przez Sebastian Paw?owski <sebpaa at gmail.com> w dniu 18 wrz 2012, o godz. 15:56:

> HI, 
> 
> os: ubuntu
> slony 2.1.2
> postgres 9.0
> 
> i've problem with setting up replicaction, when i try to subscribe set from master to slave i get:
> 
> 2012-09-18 14:49:13 CEST CONFIG remoteWorkerThread_2: Begin COPY of table "public"."auth_user"
> NOTICE:  truncate of "public"."auth_user" failed - doing delete
> 2012-09-18 14:54:42 CEST CONFIG remoteWorkerThread_2: 2745697400 bytes copied for table "public"."auth_user"
> 2012-09-18 15:26:52 CEST CONFIG remoteWorkerThread_2: 2259.200 seconds to copy table "public"."auth_user"
> 2012-09-18 15:26:52 CEST CONFIG remoteWorkerThread_2: copy table "public"."payments_grouppacket"
> 2012-09-18 15:26:52 CEST CONFIG remoteWorkerThread_2: Begin COPY of table "public"."payments_grouppacket"
> 2012-09-18 15:26:52 CEST ERROR  remoteWorkerThread_2: "select "_getmedia".copyFields(2);" FATAL:  terminating connection due to administrator command
> server closed the connection unexpectedly
> 	This probably means the server terminated abnormally
> 	before or while processing the request.
> 
> 2012-09-18 15:26:52 CEST WARN   remoteWorkerThread_2: data copy for set 1 failed 1 times - sleep 15 seconds
> 
> table auth_user syncs, but next table payments_grouppacket fails to sync, every time the same: FATAL, and the process go back to copy auth_user
> i've checked in logs, and there is not nothing else
> 
> postgres logs shows only these:
> 
> Sep 18 15:26:52 db01a postgres[11628]: [68-1] 2012-09-18 15:26:52.188 CEST slony at getmedia 11628 192.168.10.241(58888) LOG:  duration: 1929804.203 ms  statement: select "_getmedia".finishTableAfterCopy(1); analyze "public"."auth_user"; 
> Sep 18 15:26:52 db01a postgres[11629]: [4667-1] 2012-09-18 15:26:52.192 CEST slony at getmedia 11629 192.168.10.241(58891) LOG:  duration: 0.082 ms  statement: start transaction;set transaction isolation level serializable;lock table "_getmedia".sl_event_lock;select last_value from "_getmedia".sl_action_seq;
> Sep 18 15:26:52 db01a postgres[11629]: [4668-1] 2012-09-18 15:26:52.192 CEST slony at getmedia 11629 192.168.10.241(58891) LOG:  duration: 0.019 ms  statement: rollback transaction;
> Sep 18 15:26:52 db01a postgres[11619]: [3361-1] 2012-09-18 15:26:52.229 CEST slony at getmedia 11619 192.168.10.241(58887) LOG:  duration: 0.034 ms  statement: start transaction; set transaction isolation level serializable;
> Sep 18 15:26:52 db01a postgres[11619]: [3362-1] 2012-09-18 15:26:52.229 CEST slony at getmedia 11619 192.168.10.241(58887) LOG:  duration: 0.228 ms  statement: select ev_seqno, ev_timestamp,        'dummy', 'dummy', 'dummy',        ev_type,        ev_data1, ev_data2, ev_data3, ev_data4,        ev_data5, ev_data6, ev_data7, ev_data8 from "_getmedia".sl_event where  ev_origin = '1'        and ev_seqno > '5000000576' order by ev_seqno
> Sep 18 15:26:52 db01a postgres[11619]: [3363-1] 2012-09-18 15:26:52.229 CEST slony at getmedia 11619 192.168.10.241(58887) LOG:  duration: 0.022 ms  statement: rollback transaction;
> Sep 18 15:26:52 db01a postgres[11628]: [69-1] 2012-09-18 15:26:52.235 CEST slony at getmedia 11628 192.168.10.241(58888) LOG:  duration: 46.190 ms  statement: lock table "_getmedia".sl_config_lock;select "_getmedia".setAddTable_int(1, 2, '"public"."payments_grouppacket"', 'payments_grouppacket_pkey', 'Table public.payments_grouppacket with primary key'); 
> 
> 
> any ideas, could You help me?
> 
> best regards
> 
> Sebastian
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120918/8a985935/attachment.htm 

From stephane.schildknecht at postgresql.fr  Tue Sep 18 23:49:05 2012
From: stephane.schildknecht at postgresql.fr (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed, 19 Sep 2012 08:49:05 +0200
Subject: [Slony1-general] cant set up replication - problem with
	subscribe
In-Reply-To: <9CBF61E2-024A-46D1-9E88-DA99FAF68F83@gmail.com>
References: <CADyzmyzrV0=mDr_Smr4-gX2bMAy0DUzgb9f0Fk5xfcs1NnKWNA@mail.gmail.com>
	<373F779F-E3BB-4524-B351-EF7CD4645DAE@gmail.com>
	<9CBF61E2-024A-46D1-9E88-DA99FAF68F83@gmail.com>
Message-ID: <50596AE1.5070200@postgresql.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Le 18/09/2012 17:59, Sebastian Paw?owski a ?crit :
> problem solved
> 

Which was?


- -- 
St?phane Schildknecht
http://www.Loxodata.com
Contact r?gional PostgreSQL
http://bistri.me/sas

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://www.enigmail.net/

iEYEARECAAYFAlBZauEACgkQA+REPKWGI0FmrACfflkU1yY0Gln5kZQPWQPF5g0H
FNsAmwUeSc2NSm/UCGBUiQ+YynHqpXo/
=Psjz
-----END PGP SIGNATURE-----

From sebpaa at gmail.com  Wed Sep 19 03:27:55 2012
From: sebpaa at gmail.com (=?iso-8859-2?Q?Sebastian_Paw=B3owski?=)
Date: Wed, 19 Sep 2012 12:27:55 +0200
Subject: [Slony1-general] cant set up replication - problem with
	subscribe
In-Reply-To: <50596AE1.5070200@postgresql.fr>
References: <CADyzmyzrV0=mDr_Smr4-gX2bMAy0DUzgb9f0Fk5xfcs1NnKWNA@mail.gmail.com>
	<373F779F-E3BB-4524-B351-EF7CD4645DAE@gmail.com>
	<9CBF61E2-024A-46D1-9E88-DA99FAF68F83@gmail.com>
	<50596AE1.5070200@postgresql.fr>
Message-ID: <043FDEFA-592D-41EC-9DD1-F136D3E7AE29@gmail.com>

it was very stupid :) on master there was cron which was killing very long idle in transaction postgres processes, and every time when on slave huge table was analyzing (by finishTableAfterCopy) on master some process became idle in transaction ...
i haven't  thougt that could be problem generated by another machine, and on the slave wasn't any info in logs.

Sebastian Pawlowski

Wiadomo?? napisana przez St?phane A. Schildknecht <stephane.schildknecht at postgresql.fr> w dniu 19 wrz 2012, o godz. 08:49:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Le 18/09/2012 17:59, Sebastian Paw?owski a ?crit :
>> problem solved
>> 
> 
> Which was?
> 
> 
> - -- 
> St?phane Schildknecht
> http://www.Loxodata.com
> Contact r?gional PostgreSQL
> http://bistri.me/sas
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.11 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://www.enigmail.net/
> 
> iEYEARECAAYFAlBZauEACgkQA+REPKWGI0FmrACfflkU1yY0Gln5kZQPWQPF5g0H
> FNsAmwUeSc2NSm/UCGBUiQ+YynHqpXo/
> =Psjz
> -----END PGP SIGNATURE-----
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From brianf at consistentstate.com  Wed Sep 19 15:46:08 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Wed, 19 Sep 2012 16:46:08 -0600
Subject: [Slony1-general] Manually kicking off a logswitch
Message-ID: <505A4B30.9010509@consistentstate.com>

Hi all,

Postgres 8.4, slony 1.2.21

Previously I had reached out on an issue where the sl_log_1 or sl_log_2 
table would get so full that replication would come to a crawl, only 
processing one event at a time. It seems as though HUGE data 
insert,updates,or deletes to replicated tables are to cause, and being 
on slony 1.2 there isn't much we can do to get around it.

The size of the sl_log table was above 9 million rows where we saw this 
as an issue. We are now going along the path of doing much smaller 
groups of updates so we don't get into the same condition as before. We 
just did 1.2 million rows worth of updates and it only took a few 
minutes to replicate it all to the slave. Good news.

But now our sl_log_1 table is sitting at 1.2 million rows, and we'd like 
to let it be switched and truncated by slony before kicking off a few 
more million rows worth of updates. From what I can tell via 
documentation, this is not all that often.

So what is the thoughts on manually kicking off the logswitch via 
"select _slony.logswitch_start()" on the master? I reviewed the code and 
it won't let a switch occure if it's already in progress, so it seems 
it's being pretty safe in its execution. However it looks like all it 
really does is update a sequence to say "we're currently switching" and 
then slony does it in the background.

So my questions are. 1. is this a safe practice to do? We may be doing 
it multiple times a day (guestimate, ten or more times?). and 2. what is 
slony doing in the background for this to occur? It looks like it 
actually switches to the new log right away, but takes some time before 
the old log is truncated, does it need to wait until a cleanevent can 
run on the data within, aka about 10 minutes?  (#2 is more out of 
curiosity).

Thanks in advance,
- Brian F

From JanWieck at Yahoo.com  Wed Sep 19 16:11:42 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 19 Sep 2012 19:11:42 -0400
Subject: [Slony1-general] Manually kicking off a logswitch
In-Reply-To: <505A4B30.9010509@consistentstate.com>
References: <505A4B30.9010509@consistentstate.com>
Message-ID: <505A512E.10602@Yahoo.com>

On 9/19/2012 6:46 PM, Brian Fehrle wrote:
> Hi all,
>
> Postgres 8.4, slony 1.2.21
>
> Previously I had reached out on an issue where the sl_log_1 or sl_log_2
> table would get so full that replication would come to a crawl, only
> processing one event at a time. It seems as though HUGE data
> insert,updates,or deletes to replicated tables are to cause, and being
> on slony 1.2 there isn't much we can do to get around it.
>
> The size of the sl_log table was above 9 million rows where we saw this
> as an issue. We are now going along the path of doing much smaller
> groups of updates so we don't get into the same condition as before. We
> just did 1.2 million rows worth of updates and it only took a few
> minutes to replicate it all to the slave. Good news.
>
> But now our sl_log_1 table is sitting at 1.2 million rows, and we'd like
> to let it be switched and truncated by slony before kicking off a few
> more million rows worth of updates. From what I can tell via
> documentation, this is not all that often.
>
> So what is the thoughts on manually kicking off the logswitch via
> "select _slony.logswitch_start()" on the master? I reviewed the code and
> it won't let a switch occure if it's already in progress, so it seems
> it's being pretty safe in its execution. However it looks like all it
> really does is update a sequence to say "we're currently switching" and
> then slony does it in the background.

The issue itself is caused by a problem with the log select query that 
was fixed in 2.1 (commit d4118d... from Jan 27, 2011).

>
> So my questions are. 1. is this a safe practice to do? We may be doing
> it multiple times a day (guestimate, ten or more times?). and 2. what is
> slony doing in the background for this to occur? It looks like it
> actually switches to the new log right away, but takes some time before
> the old log is truncated, does it need to wait until a cleanevent can
> run on the data within, aka about 10 minutes?  (#2 is more out of
> curiosity).

slon is calling the stored procedure cleanupEvent(interval). You can 
safely call that with an interval of a few minutes. The interval is how 
old events must be at least to be purged from sl_event. Even a zero 
interval should be safe.

However, Slony is normally trying to do this every 10 minutes (if memory 
serves). With that backlog it is very likely that the previous logswitch 
attempt hasn't finished yet because the backlog is in the "old" log table.

As said, the problem is fixed in 2.1. If upgrading to Slony 2.1 is not 
an option for you, you may consider backpatching the above commit into 
your 1.2.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From nicolas.gineau at productys.com  Wed Sep 26 06:53:18 2012
From: nicolas.gineau at productys.com (nicogineau)
Date: Wed, 26 Sep 2012 06:53:18 -0700 (PDT)
Subject: [Slony1-general] Slony replication between two database (server and
 workshop) sometimes not fired
Message-ID: <34482368.post@talk.nabble.com>


Hello everyone,

I have a worry concerning the replication between a Workshop database and a
Server database with for example a A table, replicated in the sense Server
--> Workshop and a B table replicated in the sense Workshop --> Server.
The replication works fine except in the following case:
- Modification of a field of the A table on the Server,
- Replication of the modification of the field of the A table on the
Workshop database --> OK,
- Fire of a trigger on modification, which updates a field of the B table on
the Workshop --> OK,
- Here is my problem: the modification of the field of the B table on the
Workshop is not replicated on the Server.
However, if I directly modify the field of the A table on the Workshop (not
on the Server), the trigger updates well the field of the B table, and this
modification is well replicated on the B table of the Server.

There should be something somewhere to configure to authorize the
modifications from the Workshop to be replicated on the Server???

For information, I use Windows 7 with Postgres V9.0 and Slony-I V2.0.4-1.

Cordially

(Apologies for my possible English mistakes)
-- 
View this message in context: http://old.nabble.com/Slony-replication-between-two-database-%28server-and-workshop%29-sometimes-not-fired-tp34482368p34482368.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.


From brianf at consistentstate.com  Thu Sep 27 11:34:16 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 27 Sep 2012 12:34:16 -0600
Subject: [Slony1-general] Issue when adding node to replication
Message-ID: <50649C28.2010002@consistentstate.com>

Hi all,

PostgreSQL v 9.1.5 - 9.1.6
Slony version 2.1.0

I'm having an issue that's occurred twice now. I have 4 node slony 
cluster, and one of the operations is to drop a node from replication, 
do maintenance on it, then add it back to replication.

Node 1 = master
Node 2 = slave
Node 3 = slave  -> dropped then readded
Node 4 = slave

I dropped node 2 from replication, then a while later added it back. I 
have my own set of scripts that add it to replication all automatically. 
They have been used dozens of times over the past few months without 
issue. But recently, I've had the issue where after adding back the node 
(in this example, node 3), node 4 suddenly stops syncing. I've attached 
a snip of the log for node 4 in this email. The main thing I'm seeing is 
this:

Node 4 (the one that stopped syncing) was proccessing the following SYNC 
events
5000030355
5000030357
5000030358
5000030359

Then at this point it receives the event recognizing the new node 
addition to the slony cluster.

then we have the error where it's attempting to insert the event 
5000000032 (see log attached).

Are these sequential event id's supposed to be related?

I looked at the sl_event on the node 4 box that's erroring, and did the 
following query:

select now() -  ev_timestamp, ev_seqno from _slony.sl_event where 
ev_origin = 4 order by 1;
(how long ago was this event
     ?column?     |  ev_seqno
-----------------+------------
  01:48:30.7277   | 5000000031
  01:48:40.731095 | 5000000030
  01:48:50.734454 | 5000000029
  01:49:00.735438 | 5000000028
  01:49:10.728117 | 5000000027
  01:49:20.731387 | 5000000026
  01:49:30.734199 | 5000000025
  01:49:40.726768 | 5000000024
  01:49:50.730152 | 5000000023
  01:50:00.732747 | 5000000022
  01:50:10.736126 | 5000000021
  01:50:20.737394 | 5000000020
  01:50:30.733839 | 5000000019
  01:50:40.736928 | 5000000018
  01:50:50.739931 | 5000000017
  01:51:00.742365 | 5000000016
  01:51:10.745724 | 5000000015
  01:51:20.748913 | 5000000014
  01:51:30.751388 | 5000000013
  01:51:40.754439 | 5000000012
  01:51:50.757796 | 5000000011
  01:52:00.760079 | 5000000010
  01:52:10.763595 | 5000000009
  01:52:20.762106 | 5000000008
  01:52:30.764838 | 5000000007
  01:52:40.768154 | 5000000006
  01:52:50.771847 | 5000000005
  01:53:00.775925 | 5000000004
  01:53:38.2839   | 5000000003
  01:53:38.422047 | 5000000002
  01:53:38.435689 | 5000000001
  01:55:29.173397 | 5000000032

NOTE: At the moment I took this snapshot, the st_lag_time for this node 
was '01:55:28.411043', which falls between the last two timestamps above.

What strikes me as 'interesting' is that 5000000032 (the one it's 
erroring on) happened way before the rest of them timestamp wise. So I'm 
thinking that perhaps it was leftover from the previous 'node' before I 
dropped it, and it was never cleaned up?

The last time this happened, I deleted that offending row myself 
(desperate, i know to usually never touch the slony schema data), and it 
actually did completely break slony replication. It's a smaller set so 
wasn't too hard to set up again from scratch.

So I'm going to do some testing to see if it's due to lack of cleanup, 
but thought I'd throw this out there. Is there anything else that could 
possibly cause this?

thanks in advance,
- Brian F

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120927/e4ac6750/attachment.htm 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: temp.db8.log
Url: http://lists.slony.info/pipermail/slony1-general/attachments/20120927/e4ac6750/attachment.txt 

From JanWieck at Yahoo.com  Thu Sep 27 12:26:17 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 27 Sep 2012 15:26:17 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <50649C28.2010002@consistentstate.com>
References: <50649C28.2010002@consistentstate.com>
Message-ID: <5064A859.9030606@Yahoo.com>

On 9/27/2012 2:34 PM, Brian Fehrle wrote:
> Hi all,
>
> PostgreSQL v 9.1.5 - 9.1.6
> Slony version 2.1.0
>
> I'm having an issue that's occurred twice now. I have 4 node slony
> cluster, and one of the operations is to drop a node from replication,
> do maintenance on it, then add it back to replication.
>
> Node 1 = master
> Node 2 = slave
> Node 3 = slave  -> dropped then readded
> Node 4 = slave

First, why is the node actually dropped and readded so fast, instead of 
just doing the maintenance while it falls behind, then let it catch up?

You apparently have a full blown path network from everyone to everyone. 
This is not good under normal circumstances since the automatic listen 
generation will cause every node to listen on every other node for 
events, from non-origins. Way too many useless database connections.

What seems to happen here are some race conditions. The node is dropped 
and when it is added back again, some third node still didn't process 
the DROP NODE and when node 4 looks for events from node 3, it finds old 
ones somewhere else (like on 1 or 2). When node 3 then comes around to 
use those event IDs again, you get the dupkey error.

What you could do if you really need to drop/readd it, use an explicit 
WAIT FOR EVENT for the DROP NODE to make sure all traces of that node 
are gone from the whole cluster.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From brianf at consistentstate.com  Thu Sep 27 12:28:48 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 27 Sep 2012 13:28:48 -0600
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064A859.9030606@Yahoo.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
Message-ID: <5064A8F0.9080801@consistentstate.com>

On 09/27/2012 01:26 PM, Jan Wieck wrote:
> On 9/27/2012 2:34 PM, Brian Fehrle wrote:
>> Hi all,
>>
>> PostgreSQL v 9.1.5 - 9.1.6
>> Slony version 2.1.0
>>
>> I'm having an issue that's occurred twice now. I have 4 node slony
>> cluster, and one of the operations is to drop a node from replication,
>> do maintenance on it, then add it back to replication.
>>
>> Node 1 = master
>> Node 2 = slave
>> Node 3 = slave  -> dropped then readded
>> Node 4 = slave
>
> First, why is the node actually dropped and readded so fast, instead 
> of just doing the maintenance while it falls behind, then let it catch 
> up?
>
We have several cases where it makes sense, such as re-installing the OS 
or in todays case, we replaced the physical machine with a new one.

> You apparently have a full blown path network from everyone to 
> everyone. This is not good under normal circumstances since the 
> automatic listen generation will cause every node to listen on every 
> other node for events, from non-origins. Way too many useless database 
> connections.
 From my understanding, without this set-up, all events must then be 
passed through the master node to relay it. So master node = 1, slave = 
2 and 3, 3 must communicate with 2, and without direct access it will 
relay through the master. Is this understanding wrong?

>
> What seems to happen here are some race conditions. The node is 
> dropped and when it is added back again, some third node still didn't 
> process the DROP NODE and when node 4 looks for events from node 3, it 
> finds old ones somewhere else (like on 1 or 2). When node 3 then comes 
> around to use those event IDs again, you get the dupkey error.
>
> What you could do if you really need to drop/readd it, use an explicit 
> WAIT FOR EVENT for the DROP NODE to make sure all traces of that node 
> are gone from the whole cluster.
>
Ok, I'll look into implementing that. Another thought was to issue a 
cleanupEvent() on each of the nodes still attached to replication after 
I do the dump.

Thanks
- Brian F
>
> Jan
>


From brianf at consistentstate.com  Thu Sep 27 12:58:00 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 27 Sep 2012 13:58:00 -0600
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064A8F0.9080801@consistentstate.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
Message-ID: <5064AFC8.6080105@consistentstate.com>

Follow up:

I executed this on the master:
mydatabase=# select * from _slony.sl_event where ev_origin not in 
(select no_id from _slony.sl_node);
  ev_origin |  ev_seqno  |         ev_timestamp          |    
ev_snapshot     | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | 
ev_data5 | ev_data6 | ev_data7 | ev_data8
-----------+------------+-------------------------------+--------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
          3 | 5000290161 | 2012-09-27 09:48:03.749424-04 | 
40580084:40580084: | SYNC    |          |          |          |          
|          |          |          |
(1 row)

There is a row in sl_event that shouldn't be there, because it's 
referencing a node that nolonger exists. I need to add this node back to 
replication, but I don't want to run into the same issue as before. I 
ran a cleanupEvent('10 minute') and it did nothing (even did it with 0 
minutes).

Will this row eventually go away? will it cause issue if we attempt to 
add a new node to replication with node = 3? How can I safely clean this up?

thanks,
- Brian F

On 09/27/2012 01:28 PM, Brian Fehrle wrote:
> On 09/27/2012 01:26 PM, Jan Wieck wrote:
>> On 9/27/2012 2:34 PM, Brian Fehrle wrote:
>>> Hi all,
>>>
>>> PostgreSQL v 9.1.5 - 9.1.6
>>> Slony version 2.1.0
>>>
>>> I'm having an issue that's occurred twice now. I have 4 node slony
>>> cluster, and one of the operations is to drop a node from replication,
>>> do maintenance on it, then add it back to replication.
>>>
>>> Node 1 = master
>>> Node 2 = slave
>>> Node 3 = slave  ->  dropped then readded
>>> Node 4 = slave
>> First, why is the node actually dropped and readded so fast, instead
>> of just doing the maintenance while it falls behind, then let it catch
>> up?
>>
> We have several cases where it makes sense, such as re-installing the OS
> or in todays case, we replaced the physical machine with a new one.
>
>> You apparently have a full blown path network from everyone to
>> everyone. This is not good under normal circumstances since the
>> automatic listen generation will cause every node to listen on every
>> other node for events, from non-origins. Way too many useless database
>> connections.
>    From my understanding, without this set-up, all events must then be
> passed through the master node to relay it. So master node = 1, slave =
> 2 and 3, 3 must communicate with 2, and without direct access it will
> relay through the master. Is this understanding wrong?
>
>> What seems to happen here are some race conditions. The node is
>> dropped and when it is added back again, some third node still didn't
>> process the DROP NODE and when node 4 looks for events from node 3, it
>> finds old ones somewhere else (like on 1 or 2). When node 3 then comes
>> around to use those event IDs again, you get the dupkey error.
>>
>> What you could do if you really need to drop/readd it, use an explicit
>> WAIT FOR EVENT for the DROP NODE to make sure all traces of that node
>> are gone from the whole cluster.
>>
> Ok, I'll look into implementing that. Another thought was to issue a
> cleanupEvent() on each of the nodes still attached to replication after
> I do the dump.
>
> Thanks
> - Brian F
>> Jan
>>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120927/100d8e0c/attachment.htm 

From JanWieck at Yahoo.com  Thu Sep 27 14:26:31 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 27 Sep 2012 17:26:31 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064AFC8.6080105@consistentstate.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
Message-ID: <5064C487.6040709@Yahoo.com>

On 9/27/2012 3:58 PM, Brian Fehrle wrote:
> Follow up:
>
> I executed this on the master:
> mydatabase=# select * from _slony.sl_event where ev_origin not in
> (select no_id from _slony.sl_node);
>   ev_origin |  ev_seqno  | ev_timestamp          |    ev_snapshot     |
> ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 |
> ev_data6 | ev_data7 | ev_data8
> -----------+------------+-------------------------------+--------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
>           3 | 5000290161 | 2012-09-27 09:48:03.749424-04 |
> 40580084:40580084: | SYNC    |          |          | |
> |          |          |          |
> (1 row)
>
> There is a row in sl_event that shouldn't be there, because it's
> referencing a node that nolonger exists. I need to add this node back to
> replication, but I don't want to run into the same issue as before. I
> ran a cleanupEvent('10 minute') and it did nothing (even did it with 0
> minutes).
>
> Will this row eventually go away? will it cause issue if we attempt to
> add a new node to replication with node = 3? How can I safely clean this up?

Hmmm,

this actually looks like a more severe race condition or even a bug.

The thing is that processing the DROP NODE and replicating the SYNC are 
different worker threads, since the events originate on different nodes. 
Cleaning out the sl_event is part of dropNode_int(). But the 
remoteWorker for 3 may just have inserted that SYNC concurrently and 
therefore it was left behind.

My guess is that the right solution to this is to clean out everything 
again when a STORE NODE comes along. We had been thinking of making the 
node ID non-reusable to prevent this sort of race conditions.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cbbrowne at afilias.info  Thu Sep 27 14:30:47 2012
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 27 Sep 2012 17:30:47 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064C487.6040709@Yahoo.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com> <5064C487.6040709@Yahoo.com>
Message-ID: <CANfbgba9=ijsAtDbhkNC4xSck3HF2MR02KBfFT5QmEnXCoUU9w@mail.gmail.com>

On Thu, Sep 27, 2012 at 5:26 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
> My guess is that the right solution to this is to clean out everything
> again when a STORE NODE comes along. We had been thinking of making the
> node ID non-reusable to prevent this sort of race conditions.

I'm not sure I'm totally comfortable with cleaning it all out
instantly; as a step towards that, I'd think it a good idea for slonik
to check all the nodes for existence of a node ID, and refuse if it's
found anywhere.

Under that circumstance, you might need to wait, to run the STORE
NODE, until the cleanup thread has run on all the nodes to expunge the
last bits of the node on all nodes' databases.

Smells a bit safer to me...

From JanWieck at Yahoo.com  Thu Sep 27 14:40:46 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 27 Sep 2012 17:40:46 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <CANfbgba9=ijsAtDbhkNC4xSck3HF2MR02KBfFT5QmEnXCoUU9w@mail.gmail.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com> <5064C487.6040709@Yahoo.com>
	<CANfbgba9=ijsAtDbhkNC4xSck3HF2MR02KBfFT5QmEnXCoUU9w@mail.gmail.com>
Message-ID: <5064C7DE.8050402@Yahoo.com>

On 9/27/2012 5:30 PM, Christopher Browne wrote:
> On Thu, Sep 27, 2012 at 5:26 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
>> My guess is that the right solution to this is to clean out everything
>> again when a STORE NODE comes along. We had been thinking of making the
>> node ID non-reusable to prevent this sort of race conditions.
>
> I'm not sure I'm totally comfortable with cleaning it all out
> instantly; as a step towards that, I'd think it a good idea for slonik
> to check all the nodes for existence of a node ID, and refuse if it's
> found anywhere.
>
> Under that circumstance, you might need to wait, to run the STORE
> NODE, until the cleanup thread has run on all the nodes to expunge the
> last bits of the node on all nodes' databases.
>
> Smells a bit safer to me...
>

Check cleanupEvent(). I think it will never remove that stale event.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From brianf at consistentstate.com  Thu Sep 27 15:48:30 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 27 Sep 2012 16:48:30 -0600
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064C7DE.8050402@Yahoo.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com> <5064C487.6040709@Yahoo.com>
	<CANfbgba9=ijsAtDbhkNC4xSck3HF2MR02KBfFT5QmEnXCoUU9w@mail.gmail.com>
	<5064C7DE.8050402@Yahoo.com>
Message-ID: <5064D7BE.4080201@consistentstate.com>

On 09/27/2012 03:40 PM, Jan Wieck wrote:
> On 9/27/2012 5:30 PM, Christopher Browne wrote:
>> On Thu, Sep 27, 2012 at 5:26 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
>>> My guess is that the right solution to this is to clean out everything
>>> again when a STORE NODE comes along. We had been thinking of making the
>>> node ID non-reusable to prevent this sort of race conditions.
>>
>> I'm not sure I'm totally comfortable with cleaning it all out
>> instantly; as a step towards that, I'd think it a good idea for slonik
>> to check all the nodes for existence of a node ID, and refuse if it's
>> found anywhere.
>>
>> Under that circumstance, you might need to wait, to run the STORE
>> NODE, until the cleanup thread has run on all the nodes to expunge the
>> last bits of the node on all nodes' databases.
>>
>> Smells a bit safer to me...
>>
>
> Check cleanupEvent(). I think it will never remove that stale event.
>
Yeah, it looks like it will only remove confirmed ones.

--------------code from cleanupEvent()-----------------
     -- ----
     -- Then remove all events that are confirmed by all nodes in the
     -- whole cluster up to the last SYNC
     -- ----
     for v_min_row in select con_origin, min(con_seqno) as con_seqno
                 from sl_confirm
                 group by con_origin
     loop
         select coalesce(max(ev_seqno), 0) into v_max_sync
                 from sl_event
                 where ev_origin = v_min_row.con_origin
                 and ev_seqno <= v_min_row.con_seqno
                 and ev_type = 'SYNC';
         if v_max_sync > 0 then
             delete from sl_event
                     where ev_origin = v_min_row.con_origin
                     and ev_seqno < v_max_sync;
         end if;
     end loop;

the query that hits sl_confirm for the loop returns the following:
  con_origin | con_seqn0
------------+------------
           1 | 5000242178
           2 | 5000661718
           4 | 5000060743

So it never hits node 3 to do any delets from sl_event on node three. 
This is the only place in cleanupEvent i believe will do any deletes 
from sl_event.

So should I try to delete this row myself, or would that cause major 
issues also? I'm still wrapping my head around how sl_confirm and 
sl_event work together when adding/removing nodes.

- Brian F

>
> Jan
>


From JanWieck at Yahoo.com  Thu Sep 27 17:32:10 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 27 Sep 2012 20:32:10 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064D7BE.4080201@consistentstate.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com> <5064C487.6040709@Yahoo.com>
	<CANfbgba9=ijsAtDbhkNC4xSck3HF2MR02KBfFT5QmEnXCoUU9w@mail.gmail.com>
	<5064C7DE.8050402@Yahoo.com> <5064D7BE.4080201@consistentstate.com>
Message-ID: <5064F00A.6080905@Yahoo.com>

On 9/27/2012 6:48 PM, Brian Fehrle wrote:
> On 09/27/2012 03:40 PM, Jan Wieck wrote:
>> On 9/27/2012 5:30 PM, Christopher Browne wrote:
>>> On Thu, Sep 27, 2012 at 5:26 PM, Jan Wieck <JanWieck at yahoo.com> wrote:
>>>> My guess is that the right solution to this is to clean out everything
>>>> again when a STORE NODE comes along. We had been thinking of making the
>>>> node ID non-reusable to prevent this sort of race conditions.
>>>
>>> I'm not sure I'm totally comfortable with cleaning it all out
>>> instantly; as a step towards that, I'd think it a good idea for slonik
>>> to check all the nodes for existence of a node ID, and refuse if it's
>>> found anywhere.
>>>
>>> Under that circumstance, you might need to wait, to run the STORE
>>> NODE, until the cleanup thread has run on all the nodes to expunge the
>>> last bits of the node on all nodes' databases.
>>>
>>> Smells a bit safer to me...
>>>
>>
>> Check cleanupEvent(). I think it will never remove that stale event.
>>
> Yeah, it looks like it will only remove confirmed ones.
>
> --------------code from cleanupEvent()-----------------
>       -- ----
>       -- Then remove all events that are confirmed by all nodes in the
>       -- whole cluster up to the last SYNC
>       -- ----
>       for v_min_row in select con_origin, min(con_seqno) as con_seqno
>                   from sl_confirm
>                   group by con_origin
>       loop
>           select coalesce(max(ev_seqno), 0) into v_max_sync
>                   from sl_event
>                   where ev_origin = v_min_row.con_origin
>                   and ev_seqno <= v_min_row.con_seqno
>                   and ev_type = 'SYNC';
>           if v_max_sync > 0 then
>               delete from sl_event
>                       where ev_origin = v_min_row.con_origin
>                       and ev_seqno < v_max_sync;
>           end if;
>       end loop;
>
> the query that hits sl_confirm for the loop returns the following:
>    con_origin | con_seqn0
> ------------+------------
>             1 | 5000242178
>             2 | 5000661718
>             4 | 5000060743
>
> So it never hits node 3 to do any delets from sl_event on node three.
> This is the only place in cleanupEvent i believe will do any deletes
> from sl_event.
>
> So should I try to delete this row myself, or would that cause major
> issues also? I'm still wrapping my head around how sl_confirm and
> sl_event work together when adding/removing nodes.

Since there isn't even a node 3 in sl_node, it is safe to delete that row.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From ssinger at ca.afilias.info  Fri Sep 28 06:15:22 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 28 Sep 2012 09:15:22 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5064C487.6040709@Yahoo.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com> <5064C487.6040709@Yahoo.com>
Message-ID: <5065A2EA.3030606@ca.afilias.info>

On 12-09-27 05:26 PM, Jan Wieck wrote:
> On 9/27/2012 3:58 PM, Brian Fehrle wrote:
>> Follow up:
>>
>> I executed this on the master:
>> mydatabase=# select * from _slony.sl_event where ev_origin not in
>> (select no_id from _slony.sl_node);
>>    ev_origin |  ev_seqno  | ev_timestamp          |    ev_snapshot     |
>> ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 |
>> ev_data6 | ev_data7 | ev_data8
>> -----------+------------+-------------------------------+--------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
>>            3 | 5000290161 | 2012-09-27 09:48:03.749424-04 |
>> 40580084:40580084: | SYNC    |          |          | |
>> |          |          |          |
>> (1 row)
>>
>> There is a row in sl_event that shouldn't be there, because it's
>> referencing a node that nolonger exists. I need to add this node back to
>> replication, but I don't want to run into the same issue as before. I
>> ran a cleanupEvent('10 minute') and it did nothing (even did it with 0
>> minutes).
>>
>> Will this row eventually go away? will it cause issue if we attempt to
>> add a new node to replication with node = 3? How can I safely clean this up?
>
> Hmmm,
>
> this actually looks like a more severe race condition or even a bug.
>

The slonik_drop_node function has changed a bit in 2.2(master).  I 
apparently added the following comment to it while doing some of the 
FAILOVER work.  It is possible that it addresses this situation as well?


* find the last event (including SYNC events)
* from the node being dropped that is visible on
* any of the remaining nodes.
* we must wait for ALL remaining nodes to get caught up.
*
* we can't ignore SYNC events even though the dropped
* node is not an origin it might have been an old
* origin before a FAILOVER. Some behind node still
* might need to get caught up from its provider.
*/
ev_id = get_last_escaped_event_id((SlonikStmt *) stmt,
stmt->no_id_list[no_id_idx],stmt->no_id_list);

Also, Brian,

Did your slonik script (for the DROP NODE command) include admin 
conninfo lines to all your nodes?



> The thing is that processing the DROP NODE and replicating the SYNC are
> different worker threads, since the events originate on different nodes.
> Cleaning out the sl_event is part of dropNode_int(). But the
> remoteWorker for 3 may just have inserted that SYNC concurrently and
> therefore it was left behind.
>
> My guess is that the right solution to this is to clean out everything
> again when a STORE NODE comes along. We had been thinking of making the
> node ID non-reusable to prevent this sort of race conditions.
>
>
> Jan
>


From brianf at consistentstate.com  Fri Sep 28 06:47:32 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Fri, 28 Sep 2012 07:47:32 -0600
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5065A2EA.3030606@ca.afilias.info>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
	<5064C487.6040709@Yahoo.com> <5065A2EA.3030606@ca.afilias.info>
Message-ID: <5065AA74.7020302@consistentstate.com>

On 09/28/2012 07:15 AM, Steve Singer wrote:
> On 12-09-27 05:26 PM, Jan Wieck wrote:
>> On 9/27/2012 3:58 PM, Brian Fehrle wrote:
>>> Follow up:
>>>
>>> I executed this on the master:
>>> mydatabase=# select * from _slony.sl_event where ev_origin not in
>>> (select no_id from _slony.sl_node);
>>>    ev_origin |  ev_seqno  | ev_timestamp          |    
>>> ev_snapshot     |
>>> ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 |
>>> ev_data6 | ev_data7 | ev_data8
>>> -----------+------------+-------------------------------+--------------------+---------+----------+----------+----------+----------+----------+----------+----------+---------- 
>>>
>>>            3 | 5000290161 | 2012-09-27 09:48:03.749424-04 |
>>> 40580084:40580084: | SYNC    |          |          | |
>>> |          |          |          |
>>> (1 row)
>>>
>>> There is a row in sl_event that shouldn't be there, because it's
>>> referencing a node that nolonger exists. I need to add this node 
>>> back to
>>> replication, but I don't want to run into the same issue as before. I
>>> ran a cleanupEvent('10 minute') and it did nothing (even did it with 0
>>> minutes).
>>>
>>> Will this row eventually go away? will it cause issue if we attempt to
>>> add a new node to replication with node = 3? How can I safely clean 
>>> this up?
>>
>> Hmmm,
>>
>> this actually looks like a more severe race condition or even a bug.
>>
>
> The slonik_drop_node function has changed a bit in 2.2(master).  I 
> apparently added the following comment to it while doing some of the 
> FAILOVER work.  It is possible that it addresses this situation as well?
>
>
> * find the last event (including SYNC events)
> * from the node being dropped that is visible on
> * any of the remaining nodes.
> * we must wait for ALL remaining nodes to get caught up.
> *
> * we can't ignore SYNC events even though the dropped
> * node is not an origin it might have been an old
> * origin before a FAILOVER. Some behind node still
> * might need to get caught up from its provider.
> */
> ev_id = get_last_escaped_event_id((SlonikStmt *) stmt,
> stmt->no_id_list[no_id_idx],stmt->no_id_list);
>
> Also, Brian,
>
> Did your slonik script (for the DROP NODE command) include admin 
> conninfo lines to all your nodes?
>
Yes, I include all nodes that are in replication, including the one to 
be dropped.

- Brian F
>
>
>> The thing is that processing the DROP NODE and replicating the SYNC are
>> different worker threads, since the events originate on different nodes.
>> Cleaning out the sl_event is part of dropNode_int(). But the
>> remoteWorker for 3 may just have inserted that SYNC concurrently and
>> therefore it was left behind.
>>
>> My guess is that the right solution to this is to clean out everything
>> again when a STORE NODE comes along. We had been thinking of making the
>> node ID non-reusable to prevent this sort of race conditions.
>>
>>
>> Jan
>>
>


From JanWieck at Yahoo.com  Fri Sep 28 08:23:11 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 28 Sep 2012 11:23:11 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5065A2EA.3030606@ca.afilias.info>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
	<5064C487.6040709@Yahoo.com> <5065A2EA.3030606@ca.afilias.info>
Message-ID: <5065C0DF.60609@Yahoo.com>

On 9/28/2012 9:15 AM, Steve Singer wrote:
> * we can't ignore SYNC events even though the dropped
> * node is not an origin it might have been an old
> * origin before a FAILOVER. Some behind node still
> * might need to get caught up from its provider.

That assessment is actually wrong. Any data provider as well as the 
origin will keep the data until all subscribers have confirmed it. 
Simply changing the data provider of the behind node will be enough to 
make it catch up from another provider.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From ssinger at ca.afilias.info  Fri Sep 28 08:29:52 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 28 Sep 2012 11:29:52 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5065C0DF.60609@Yahoo.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
	<5064C487.6040709@Yahoo.com> <5065A2EA.3030606@ca.afilias.info>
	<5065C0DF.60609@Yahoo.com>
Message-ID: <5065C270.9090109@ca.afilias.info>

On 12-09-28 11:23 AM, Jan Wieck wrote:
> On 9/28/2012 9:15 AM, Steve Singer wrote:
>> * we can't ignore SYNC events even though the dropped
>> * node is not an origin it might have been an old
>> * origin before a FAILOVER. Some behind node still
>> * might need to get caught up from its provider.
>
> That assessment is actually wrong. Any data provider as well as the
> origin will keep the data until all subscribers have confirmed it.
> Simply changing the data provider of the behind node will be enough to
> make it catch up from another provider.
>

But it can't get caught up if we execute/process a DROP NODE on the 
other provider first which is why the slonik_drop_node can't ignore 
non-sync events when computing the last event id to use for waiting.



>
> Jan
>


From brianf at consistentstate.com  Fri Sep 28 11:40:02 2012
From: brianf at consistentstate.com (Brian Fehrle)
Date: Fri, 28 Sep 2012 12:40:02 -0600
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5065C270.9090109@ca.afilias.info>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
	<5064C487.6040709@Yahoo.com> <5065A2EA.3030606@ca.afilias.info>
	<5065C0DF.60609@Yahoo.com> <5065C270.9090109@ca.afilias.info>
Message-ID: <5065EF02.2010007@consistentstate.com>

On 09/28/2012 09:29 AM, Steve Singer wrote:
> On 12-09-28 11:23 AM, Jan Wieck wrote:
>> On 9/28/2012 9:15 AM, Steve Singer wrote:
>>> * we can't ignore SYNC events even though the dropped
>>> * node is not an origin it might have been an old
>>> * origin before a FAILOVER. Some behind node still
>>> * might need to get caught up from its provider.
>>
>> That assessment is actually wrong. Any data provider as well as the
>> origin will keep the data until all subscribers have confirmed it.
>> Simply changing the data provider of the behind node will be enough to
>> make it catch up from another provider.
>>
>
> But it can't get caught up if we execute/process a DROP NODE on the 
> other provider first which is why the slonik_drop_node can't ignore 
> non-sync events when computing the last event id to use for waiting.
>
>
Hi Guys,

I'm going to go ahead and delete the offending row in sl_event for the 
node that doesn't exist. This is a production environment so I need to 
get up and running again.

Is there anything I can provide that could help us dig into this more? 
Are we counting this as a bug that may have been fixed in 2.2(haven't 
looked at what you mentioned yet Steve)?

Thanks,
- Brian F

>
>>
>> Jan
>>
>


