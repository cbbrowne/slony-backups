From smccloud at geo-comm.com  Tue Apr  3 10:51:58 2012
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Tue, 3 Apr 2012 17:51:58 +0000
Subject: [Slony1-general] Query to see when Slony is done replicating
	changes?
Message-ID: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>

Hello,

I'm wondering if its possible to query any of the databases in my cluster to see when Slony is done replicating changes to a single node.  I was going to use the Acks Outstanding statistic but then realized that the slave nodes that my controller service is waiting to update (and do not have Slony running on) will keep adding to that count.  If there is no statistic to query, is my assumption that tables are replicated in the order they are added to the replication set correct?

As for why I can't just let the changes replicate to all slaves at once, it is because of a no single point of failure for the web app that is reading the replicated data.

Shaun McCloud - Associate Software Developer
Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit www.geo-comm.com<http://www.geo-comm.com/>
[cid:image001.jpg at 01CD1198.8E1EB250]<http://www.linkedin.com/companies/geocomm> [cid:image002.jpg at 01CD1198.8E1EB250] <http://twitter.com/_geocomm>
P Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120403/00163103/attachment.htm 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 1129 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120403/00163103/attachment.jpg 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 1068 bytes
Desc: image002.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120403/00163103/attachment-0001.jpg 

From ssinger at ca.afilias.info  Wed Apr  4 08:33:29 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 04 Apr 2012 11:33:29 -0400
Subject: [Slony1-general] Query to see when Slony is done replicating
 changes?
In-Reply-To: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>
References: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>
Message-ID: <4F7C69C9.2030006@ca.afilias.info>

On 12-04-03 01:51 PM, Shaun McCloud wrote:
> Hello,
>
> I?m wondering if its possible to query any of the databases in my
> cluster to see when Slony is done replicating changes to a single node.
> I was going to use the Acks Outstanding statistic but then realized that
> the slave nodes that my controller service is waiting to update (and do
> not have Slony running on) will keep adding to that count. If there is
> no statistic to query, is my assumption that tables are replicated in
> the order they are added to the replication set correct?
>

Slony replicates SYNC events.  Those SYNC events might have associated 
rows in sl_log_1 or sl_log_2 (that need to be replicated) or there might 
be no changes associated with a SYNC event.

The slon process on the master generates SYNC events x seconds.  When 
that SYNC event is replicated to a slave any data changes will also be 
replicated to the slave so that the slave is 'caught up' with the master 
as of that SYNC event.

So if node 100 is our master and node 200 is a slave.

sl_event
ev_origin ,ev_seqno,ev_type,.....
100       , 1, SYNC

is generated

then you know that this event has been replicated to the slave when the 
slaves sl_confirm tables has a row like

sl_confirm
-----------
con_origin, con_received, con_seqno
100,       200,         1


When the confirmation makes it back to the master then the sl_confirm on 
the master would have that row also








> As for why I can?t just let the changes replicate to all slaves at once,
> it is because of a no single point of failure for the web app that is
> reading the replicated data.
>
> *Shaun McCloud ? Associate Software Developer
> Geo-Comm, Inc*
> 601 W. Saint Germain St., Saint Cloud, MN 56301
> *Office:* 320.240.0040 *Fax:* 320.240.2389 *Toll Free:* 888.436.2666
> */click here to visit /**/www.geo-comm.com/* <http://www.geo-comm.com/>
>
> <http://www.linkedin.com/companies/geocomm> <http://twitter.com/_geocomm>
>
> PThink before you print!
>
> *Microsoft Certified Desktop Support Technician (MCDST)*
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From greg at endpoint.com  Wed Apr  4 14:44:54 2012
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 4 Apr 2012 17:44:54 -0400
Subject: [Slony1-general] Query to see when Slony is done replicating
 changes?
In-Reply-To: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>
References: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>
Message-ID: <20120404214454.GJ2947@tinybird.home>

On Tue, Apr 03, 2012 at 05:51:58PM +0000, Shaun McCloud wrote:
> I'm wondering if its possible to query any of the databases 
> in my cluster to see when Slony is done replicating changes 
> to a single node.

It might help to expand on what you mean exactly by "done 
replicating changes", as replication is an ongoing process, 
but you could use the replicate_row action of check_postgres.pl:

http://bucardo.org/check_postgres/check_postgres.pl.html

Since Slony does things in order, you know that if this check 
works, the node is "caught up" and has finished anything that might 
have been going on before the check started. It simply flips a value on 
a replicated table and reports back how long until that value 
propagates to another slave (or slaves).

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120404/c8acbfe4/attachment.pgp 

From johan at snowflake.nu  Thu Apr  5 02:47:58 2012
From: johan at snowflake.nu (=?ISO-8859-1?Q?Johan_W=E4rlander?=)
Date: Thu, 5 Apr 2012 11:47:58 +0200
Subject: [Slony1-general] Second replication set as a subset of the first?
In-Reply-To: <CAMVaVjYV9XnC6MVcRssEH+AAnbTE1WXEf6qEx6pD0YhyW3_+ag@mail.gmail.com>
References: <CAMVaVjYV9XnC6MVcRssEH+AAnbTE1WXEf6qEx6pD0YhyW3_+ag@mail.gmail.com>
Message-ID: <CAMVaVjZ8jvVnTNStqLdf0vQqJ5ROTmS4aUZ8YD_Sgu=s4VjVgA@mail.gmail.com>

Hello!

I've recently rebuilt a Slony setup that was structured as master-slave
pairs, each in a separate Slony cluster. Now it's a single cluster instead,
running Slony 2.1.1 with multiple levels of slaves ; node1 -> node2 ->
{node3, node4}. These all subscribe to set 1,with *all* tables from node1.

I'd like to add node5, which should appear below node2 and subscribe to
just a small subset of the tables from node1.

Do I just add a new set with these particular tables, even though they
already exist in set1? Will this add a lot of traffic, or simply reuse the
existing logs?

Grateful for any suggestions,
Johan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120405/60281433/attachment.htm 

From smccloud at geo-comm.com  Thu Apr  5 07:01:14 2012
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Thu, 5 Apr 2012 14:01:14 +0000
Subject: [Slony1-general] Query to see when Slony is done replicating
 changes?
In-Reply-To: <20120404214454.GJ2947@tinybird.home>
References: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>
	<20120404214454.GJ2947@tinybird.home>
Message-ID: <7742DD496427B743BC8B7BBF6D380BA037769E9F@EXCHANGE10.geo-comm.local>

Sorry,

What I mean by "done replicating changes" is that Slony has finished replicating the changes we just made to the master database to the slave database we are currently working with.

Our process is to stop Slony on all servers, remove the replication engine for our live database, restart Slony so the other database we are replicating can finish, make the changes to the master, then add the engine back to the master & one slave, restart Slony on them and then we need a way to tell that the changes have made it to that slave before we move on to the next slave.

Shaun McCloud ? Associate Software Developer
Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit www.geo-comm.com
 
? Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)

-----Original Message-----
From: Greg Sabino Mullane [mailto:greg at endpoint.com] 
Sent: Wednesday, April 04, 2012 16:45
To: Shaun McCloud
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Query to see when Slony is done replicating changes?

On Tue, Apr 03, 2012 at 05:51:58PM +0000, Shaun McCloud wrote:
> I'm wondering if its possible to query any of the databases in my 
> cluster to see when Slony is done replicating changes to a single 
> node.

It might help to expand on what you mean exactly by "done replicating changes", as replication is an ongoing process, but you could use the replicate_row action of check_postgres.pl:

http://bucardo.org/check_postgres/check_postgres.pl.html

Since Slony does things in order, you know that if this check works, the node is "caught up" and has finished anything that might have been going on before the check started. It simply flips a value on a replicated table and reports back how long until that value propagates to another slave (or slaves).

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8

From smccloud at geo-comm.com  Thu Apr  5 09:24:38 2012
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Thu, 5 Apr 2012 16:24:38 +0000
Subject: [Slony1-general] Query to see when Slony is done replicating
 changes?
In-Reply-To: <20120404214454.GJ2947@tinybird.home>
References: <7742DD496427B743BC8B7BBF6D380BA037766C7D@EXCHANGE10.geo-comm.local>
	<20120404214454.GJ2947@tinybird.home>
Message-ID: <7742DD496427B743BC8B7BBF6D380BA03776A0A1@EXCHANGE10.geo-comm.local>

Going along with my last email, how do you view the current configuration values for Slony (specifically the cleanup_interval) and change the cleanup _interval after the cluster has been created in a Windows environment?

Shaun McCloud ? Associate Software Developer
Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit www.geo-comm.com
 
? Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)


-----Original Message-----
From: Greg Sabino Mullane [mailto:greg at endpoint.com] 
Sent: Wednesday, April 04, 2012 16:45
To: Shaun McCloud
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Query to see when Slony is done replicating changes?

On Tue, Apr 03, 2012 at 05:51:58PM +0000, Shaun McCloud wrote:
> I'm wondering if its possible to query any of the databases in my 
> cluster to see when Slony is done replicating changes to a single 
> node.

It might help to expand on what you mean exactly by "done replicating changes", as replication is an ongoing process, but you could use the replicate_row action of check_postgres.pl:

http://bucardo.org/check_postgres/check_postgres.pl.html

Since Slony does things in order, you know that if this check works, the node is "caught up" and has finished anything that might have been going on before the check started. It simply flips a value on a replicated table and reports back how long until that value propagates to another slave (or slaves).

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8

From vivek at khera.org  Sat Apr  7 17:35:17 2012
From: vivek at khera.org (Vick Khera)
Date: Sat, 7 Apr 2012 20:35:17 -0400
Subject: [Slony1-general] Second replication set as a subset of the
	first?
In-Reply-To: <CAMVaVjZ8jvVnTNStqLdf0vQqJ5ROTmS4aUZ8YD_Sgu=s4VjVgA@mail.gmail.com>
References: <CAMVaVjYV9XnC6MVcRssEH+AAnbTE1WXEf6qEx6pD0YhyW3_+ag@mail.gmail.com>
	<CAMVaVjZ8jvVnTNStqLdf0vQqJ5ROTmS4aUZ8YD_Sgu=s4VjVgA@mail.gmail.com>
Message-ID: <CALd+dcdPoqMKZzpPWcDEJoiC3U5PvjMgZ_12oZafWQX3X3r7BQ@mail.gmail.com>

On Thu, Apr 5, 2012 at 5:47 AM, Johan W?rlander <johan at snowflake.nu> wrote:
> Do I just add a new set with these particular tables, even though they
> already exist in set1? Will this add a lot of traffic, or simply reuse the
> existing logs?

The sets need to be non-intersecting.

From johan at snowflake.nu  Sat Apr  7 23:38:07 2012
From: johan at snowflake.nu (=?ISO-8859-1?Q?Johan_W=E4rlander?=)
Date: Sun, 8 Apr 2012 08:38:07 +0200
Subject: [Slony1-general] Second replication set as a subset of the
	first?
In-Reply-To: <CALd+dcdPoqMKZzpPWcDEJoiC3U5PvjMgZ_12oZafWQX3X3r7BQ@mail.gmail.com>
References: <CAMVaVjYV9XnC6MVcRssEH+AAnbTE1WXEf6qEx6pD0YhyW3_+ag@mail.gmail.com>
	<CAMVaVjZ8jvVnTNStqLdf0vQqJ5ROTmS4aUZ8YD_Sgu=s4VjVgA@mail.gmail.com>
	<CALd+dcdPoqMKZzpPWcDEJoiC3U5PvjMgZ_12oZafWQX3X3r7BQ@mail.gmail.com>
Message-ID: <CAMVaVjYHdKeY7c3cHQOx3NgFt4KmFqW=L4YuyZMubuHjT5iHDw@mail.gmail.com>

Right, Steve mentioned as much on #slony earlier. My understanding from his
and your comment, and the documentation, is that I'll need to follow the
sequence below:

1. Create a new, empty set
2. Subscribe all old nodes to the new set
3. Do 'set move table' and 'set move sequence' for the tables and sequences
to split out
4. Finally, subscribe the new node to only this new set

Will that do what I want, without disturbing the already-loaded tables on
the old nodes?
 Den 8 apr 2012 02:35 skrev "Vick Khera" <vivek at khera.org>:

> On Thu, Apr 5, 2012 at 5:47 AM, Johan W?rlander <johan at snowflake.nu>
> wrote:
> > Do I just add a new set with these particular tables, even though they
> > already exist in set1? Will this add a lot of traffic, or simply reuse
> the
> > existing logs?
>
> The sets need to be non-intersecting.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120408/3a052f1a/attachment.htm 

From vivek at khera.org  Sun Apr  8 06:51:02 2012
From: vivek at khera.org (Vick Khera)
Date: Sun, 8 Apr 2012 09:51:02 -0400
Subject: [Slony1-general] Second replication set as a subset of the
	first?
In-Reply-To: <CAMVaVjYHdKeY7c3cHQOx3NgFt4KmFqW=L4YuyZMubuHjT5iHDw@mail.gmail.com>
References: <CAMVaVjYV9XnC6MVcRssEH+AAnbTE1WXEf6qEx6pD0YhyW3_+ag@mail.gmail.com>
	<CAMVaVjZ8jvVnTNStqLdf0vQqJ5ROTmS4aUZ8YD_Sgu=s4VjVgA@mail.gmail.com>
	<CALd+dcdPoqMKZzpPWcDEJoiC3U5PvjMgZ_12oZafWQX3X3r7BQ@mail.gmail.com>
	<CAMVaVjYHdKeY7c3cHQOx3NgFt4KmFqW=L4YuyZMubuHjT5iHDw@mail.gmail.com>
Message-ID: <CALd+dccqit57Z3475f20N+urSpszt5fWqgyiFdKH_VZQ--wjbQ@mail.gmail.com>

On Sun, Apr 8, 2012 at 2:38 AM, Johan W?rlander <johan at snowflake.nu> wrote:
> Will that do what I want, without disturbing the already-loaded tables on
> the old nodes?

I've never done a "set move table" so I can't say specifically, but in
general slony does do what the docs say.  I have done set merges
before when adding new tables, and those operate flawlessly.

Your strategy seems correct.

From vivek at khera.org  Tue Apr 10 07:24:15 2012
From: vivek at khera.org (Vick Khera)
Date: Tue, 10 Apr 2012 10:24:15 -0400
Subject: [Slony1-general] confusing sl_status output and replication stalled
Message-ID: <CALd+dceoj9-gNheqVkLMoBisyXkagF=9hrHVFx=c9g0nm9fuwg@mail.gmail.com>

Yesterday I set up a DB to replicate.  It was previously set up with
slony 1.2 but the backup server died, so I took the opportunity to
move to 2.1.  I did the dropnode and uninstallnode to clean up the
master.

I followed my normal, init, create set, subscribe sequence that has
always worked.  I had done the same thing about a week ago with the
other DB on this same server pair.

When I left work yesterday, it was dutifully copying the DB.  This
morning I went to check and the master has not generated an event
since 2012-04-10 05:35:49.782917-04.

The sl_status on the master shows this:


-[ RECORD 1 ]-------------+------------------------------
st_origin                 | 20
st_received               | 31
st_last_event             | 5000008023
st_last_event_ts          | 2012-04-10 05:35:49.782917-04
st_last_received          | 5000008023
st_last_received_ts       | 2012-04-10 05:35:50.00566-04
st_last_received_event_ts | 2012-04-10 05:35:49.782917-04
st_lag_num_events         | 0
st_lag_time               | 04:27:58.674963

I verified that no replication is flowing by updating one row in a
table.  The lag_time and lag_num_events seem to be contradictory to
me.

The logs on the master show nothing exciting at this timestamp.  In
fact, the whole of the logs I have (which only go back to about 10pm
last night), says basically this:


2012-04-09 21:51:17.872892500 DEBUG1 calc sync size - last time: 1
last length: 18010 ideal: 3 proposed size: 3
2012-04-09 21:51:17.873042500 DEBUG1 remoteWorkerThread_31: no sets
need syncing for this event
2012-04-09 21:51:19.876883500 DEBUG1 calc sync size - last time: 1
last length: 2003 ideal: 29 proposed size: 3
2012-04-09 21:51:19.877031500 DEBUG1 remoteWorkerThread_31: no sets
need syncing for this event

This is logged over and over and over.  The "length" is slightly
different on each log, but only by a few.

The logs on the replica at the time of the last event shown in the
status line are shown below. There is nothing remarkable in the
postgres.log files around this time, either.

Restarting slons on both ends did not change anything.

Versions:

Origin: FreeBSD 7.2, slony1 2.1.1, postgres 8.3.18
Replica: FreeBSD 9.0, slony1 2.1.1, postgres 9.1.3


I'm pretty sure I did not do anything incorrectly.  The DB I set up to
replicate on this pair of machines last week is properly replicating
still.

What should I look to fix?  The DB is small enough that re-starting
from scratch is not out of the question.



Logs from replica up to the point I restarted it:


2012-04-10 05:34:49.969486500 DEBUG1 calc sync size - last time: 1
last length: 2003 ideal: 29 proposed size: 3
2012-04-10 05:34:49.970585500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:34:49.973240500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:34:49.976524500 DEBUG1 remoteHelperThread_20_20: 0.002
seconds delay for first row
2012-04-10 05:34:49.977010500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:34:49.977028500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:34:49.977031500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.003/3 - subscriber
0.000/3
2012-04-10 05:34:49.977035500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:34:49.978175500 INFO   remoteWorkerThread_20: SYNC
5000008017 done in 0.008 seconds
2012-04-10 05:34:49.978202500 DEBUG1 remoteWorkerThread_20: SYNC
5000008017 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:35:07.978560500 DEBUG1 calc sync size - last time: 1
last length: 18009 ideal: 3 proposed size: 3
2012-04-10 05:35:07.979742500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:35:07.982419500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:35:07.985717500 DEBUG1 remoteHelperThread_20_20: 0.002
seconds delay for first row
2012-04-10 05:35:07.986202500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:35:07.986226500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:35:07.986228500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.003/3 - subscriber
0.000/3
2012-04-10 05:35:07.986232500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:35:07.987233500 INFO   remoteWorkerThread_20: SYNC
5000008018 done in 0.008 seconds
2012-04-10 05:35:07.987250500 DEBUG1 remoteWorkerThread_20: SYNC
5000008018 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:35:09.981610500 DEBUG1 calc sync size - last time: 1
last length: 2003 ideal: 29 proposed size: 3
2012-04-10 05:35:09.982726500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:35:09.985421500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:35:09.988536500 DEBUG1 remoteHelperThread_20_20: 0.002
seconds delay for first row
2012-04-10 05:35:09.989020500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:35:09.989044500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:35:09.989047500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.002/3 - subscriber
0.000/3
2012-04-10 05:35:09.989050500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:35:09.990168500 INFO   remoteWorkerThread_20: SYNC
5000008019 done in 0.008 seconds
2012-04-10 05:35:09.990190500 DEBUG1 remoteWorkerThread_20: SYNC
5000008019 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:35:27.990339500 DEBUG1 calc sync size - last time: 1
last length: 18008 ideal: 3 proposed size: 3
2012-04-10 05:35:27.991575500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:35:27.994443500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:35:27.997586500 DEBUG1 remoteHelperThread_20_20: 0.002
seconds delay for first row
2012-04-10 05:35:27.998073500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:35:27.998098500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:35:27.998100500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.002/3 - subscriber
0.000/3
2012-04-10 05:35:27.998124500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:35:27.999188500 INFO   remoteWorkerThread_20: SYNC
5000008020 done in 0.009 seconds
2012-04-10 05:35:27.999206500 DEBUG1 remoteWorkerThread_20: SYNC
5000008020 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:35:29.994615500 DEBUG1 calc sync size - last time: 1
last length: 2004 ideal: 29 proposed size: 3
2012-04-10 05:35:29.995579500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:35:29.998284500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:35:30.001519500 DEBUG1 remoteHelperThread_20_20: 0.002
seconds delay for first row
2012-04-10 05:35:30.002015500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:35:30.002041500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:35:30.002043500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.003/3 - subscriber
0.000/3
2012-04-10 05:35:30.002071500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:35:30.003135500 INFO   remoteWorkerThread_20: SYNC
5000008021 done in 0.008 seconds
2012-04-10 05:35:30.003153500 DEBUG1 remoteWorkerThread_20: SYNC
5000008021 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:35:48.002553500 DEBUG1 calc sync size - last time: 1
last length: 18008 ideal: 3 proposed size: 3
2012-04-10 05:35:48.003694500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:35:48.006392500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:35:48.010208500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds delay for first row
2012-04-10 05:35:48.010704500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:35:48.010727500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:35:48.010730500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.003/3 - subscriber
0.000/3
2012-04-10 05:35:48.010760500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:35:48.011887500 INFO   remoteWorkerThread_20: SYNC
5000008022 done in 0.009 seconds
2012-04-10 05:35:48.011905500 DEBUG1 remoteWorkerThread_20: SYNC
5000008022 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:35:50.005622500 DEBUG1 calc sync size - last time: 1
last length: 2003 ideal: 29 proposed size: 3
2012-04-10 05:35:50.006807500 DEBUG1 about to monitor_subscriber_query
- pulling big actionid list for 20
2012-04-10 05:35:50.009542500 INFO   remoteWorkerThread_20: syncing
set 1 with 52 table(s) from provider 20
2012-04-10 05:35:50.012900500 DEBUG1 remoteHelperThread_20_20: 0.002
seconds delay for first row
2012-04-10 05:35:50.013507500 DEBUG1 remoteHelperThread_20_20: 0.003
seconds until close cursor
2012-04-10 05:35:50.013532500 DEBUG1 remoteHelperThread_20_20:
inserts=0 updates=0 deletes=0 truncates=0
2012-04-10 05:35:50.013535500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  pqexec (s/count)- provider 0.002/3 - subscriber
0.000/3
2012-04-10 05:35:50.013538500 DEBUG1 remoteWorkerThread_20:
sync_helper timing:  large tuples 0.000/0
2012-04-10 05:35:50.014538500 INFO   remoteWorkerThread_20: SYNC
5000008023 done in 0.009 seconds
2012-04-10 05:35:50.014566500 DEBUG1 remoteWorkerThread_20: SYNC
5000008023 sync_event timing:  pqexec (s/count)- provider 0.001/2 -
subscriber 0.004/2 - IUD 0.000/0
2012-04-10 05:42:49.135088500 NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
2012-04-10 05:42:49.135091500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 05:42:49.135093500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 05:42:49.136708500 INFO   cleanupThread:    0.007 seconds
for cleanupEvent()
2012-04-10 05:42:49.137023500 DEBUG1 cleanupThread: minxid: 914784
2012-04-10 05:42:49.148330500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 05:42:49.148355500 INFO   cleanupThread:    0.011 seconds
for vacuuming
2012-04-10 05:53:12.962525500 NOTICE:  Slony-I: log switch to sl_log_1
complete - truncate sl_log_2
2012-04-10 05:53:12.962529500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 05:53:12.998818500 INFO   cleanupThread:    0.042 seconds
for cleanupEvent()
2012-04-10 06:04:36.665595500 NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
2012-04-10 06:04:36.665599500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 06:04:36.665601500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 06:04:36.666893500 INFO   cleanupThread:    0.006 seconds
for cleanupEvent()
2012-04-10 06:15:29.657504500 NOTICE:  Slony-I: log switch to sl_log_2
complete - truncate sl_log_1
2012-04-10 06:15:29.657508500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 06:15:29.762306500 INFO   cleanupThread:    0.111 seconds
for cleanupEvent()
2012-04-10 06:15:29.762954500 DEBUG1 cleanupThread: minxid: 917868
2012-04-10 06:15:29.776761500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 06:15:29.776800500 INFO   cleanupThread:    0.014 seconds
for vacuuming
2012-04-10 06:26:16.397469500 NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
2012-04-10 06:26:16.397472500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 06:26:16.397475500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 06:26:16.413889500 INFO   cleanupThread:    0.021 seconds
for cleanupEvent()
2012-04-10 06:37:55.058224500 NOTICE:  Slony-I: log switch to sl_log_1
complete - truncate sl_log_2
2012-04-10 06:37:55.058227500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 06:37:55.095899500 INFO   cleanupThread:    0.045 seconds
for cleanupEvent()
2012-04-10 06:48:22.843065500 NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
2012-04-10 06:48:22.843069500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 06:48:22.843071500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 06:48:22.844274500 INFO   cleanupThread:    0.007 seconds
for cleanupEvent()
2012-04-10 06:48:22.844680500 DEBUG1 cleanupThread: minxid: 921509
2012-04-10 06:48:22.858273500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 06:48:22.858310500 INFO   cleanupThread:    0.014 seconds
for vacuuming
2012-04-10 06:59:57.970356500 NOTICE:  Slony-I: log switch to sl_log_2
complete - truncate sl_log_1
2012-04-10 06:59:57.970359500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 06:59:58.030446500 INFO   cleanupThread:    0.066 seconds
for cleanupEvent()
2012-04-10 07:11:06.624593500 NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
2012-04-10 07:11:06.624596500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 07:11:06.624598500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 07:11:06.626036500 INFO   cleanupThread:    0.006 seconds
for cleanupEvent()
2012-04-10 07:21:41.656625500 NOTICE:  Slony-I: log switch to sl_log_1
complete - truncate sl_log_2
2012-04-10 07:21:41.656628500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 07:21:41.743337500 INFO   cleanupThread:    0.092 seconds
for cleanupEvent()
2012-04-10 07:21:41.744003500 DEBUG1 cleanupThread: minxid: 924619
2012-04-10 07:21:41.756041500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 07:21:41.756077500 INFO   cleanupThread:    0.012 seconds
for vacuuming
2012-04-10 07:33:27.381353500 NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
2012-04-10 07:33:27.381356500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 07:33:27.381358500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 07:33:27.389331500 INFO   cleanupThread:    0.013 seconds
for cleanupEvent()
2012-04-10 07:45:16.467253500 NOTICE:  Slony-I: log switch to sl_log_2
complete - truncate sl_log_1
2012-04-10 07:45:16.467256500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 07:45:16.527380500 INFO   cleanupThread:    0.066 seconds
for cleanupEvent()
2012-04-10 07:57:08.352485500 NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
2012-04-10 07:57:08.352488500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 07:57:08.352493500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 07:57:08.353528500 INFO   cleanupThread:    0.005 seconds
for cleanupEvent()
2012-04-10 07:57:08.353792500 DEBUG1 cleanupThread: minxid: 928158
2012-04-10 07:57:08.364604500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 07:57:08.364624500 INFO   cleanupThread:    0.011 seconds
for vacuuming
2012-04-10 08:08:19.444508500 NOTICE:  Slony-I: log switch to sl_log_1
complete - truncate sl_log_2
2012-04-10 08:08:19.444512500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 08:08:19.497679500 INFO   cleanupThread:    0.062 seconds
for cleanupEvent()
2012-04-10 08:20:02.446460500 NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
2012-04-10 08:20:02.446463500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 08:20:02.446464500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 08:20:02.447564500 INFO   cleanupThread:    0.006 seconds
for cleanupEvent()
2012-04-10 08:30:26.285213500 NOTICE:  Slony-I: log switch to sl_log_2
complete - truncate sl_log_1
2012-04-10 08:30:26.285216500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 08:30:26.322580500 INFO   cleanupThread:    0.043 seconds
for cleanupEvent()
2012-04-10 08:30:26.323128500 DEBUG1 cleanupThread: minxid: 931518
2012-04-10 08:30:26.335580500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 08:30:26.335616500 INFO   cleanupThread:    0.012 seconds
for vacuuming
2012-04-10 08:41:27.217247500 NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
2012-04-10 08:41:27.217250500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 08:41:27.217252500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 08:41:27.218533500 INFO   cleanupThread:    0.006 seconds
for cleanupEvent()
2012-04-10 08:52:47.346424500 NOTICE:  Slony-I: log switch to sl_log_1
complete - truncate sl_log_2
2012-04-10 08:52:47.346427500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 08:52:47.383147500 INFO   cleanupThread:    0.041 seconds
for cleanupEvent()
2012-04-10 09:03:05.473715500 NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
2012-04-10 09:03:05.473719500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 09:03:05.473721500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 09:03:05.474930500 INFO   cleanupThread:    0.007 seconds
for cleanupEvent()
2012-04-10 09:03:05.475164500 DEBUG1 cleanupThread: minxid: 934871
2012-04-10 09:03:05.485908500 DEBUG1 cleanupThread: number of tables to clean: 0
2012-04-10 09:03:05.485928500 INFO   cleanupThread:    0.011 seconds
for vacuuming
2012-04-10 09:13:34.729921500 NOTICE:  Slony-I: log switch to sl_log_2
complete - truncate sl_log_1
2012-04-10 09:13:34.729924500 CONTEXT:  PL/pgSQL function
"cleanupevent" line 94 at assignment
2012-04-10 09:13:34.766498500 INFO   cleanupThread:    0.043 seconds
for cleanupEvent()
2012-04-10 09:25:15.279623500 NOTICE:  Slony-I: cleanup stale
sl_nodelock entry for pid=36103
2012-04-10 09:25:15.279626500 CONTEXT:  SQL statement "SELECT
"_listserver".cleanupNodelock()"
2012-04-10 09:25:15.279628500 PL/pgSQL function "cleanupevent" line 82
at PERFORM
2012-04-10 09:25:15.281075500 NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
2012-04-10 09:25:15.281078500 CONTEXT:  SQL statement "SELECT
"_listserver".logswitch_start()"
2012-04-10 09:25:15.281079500 PL/pgSQL function "cleanupevent" line 96
at PERFORM
2012-04-10 09:25:15.282365500 INFO   cleanupThread:    0.007 seconds
for cleanupEvent()
2012-04-10 09:25:30.415269500 CONFIG slon: child terminated signal: 9;
pid: 58636, current worker pid: 58636
2012-04-10 09:25:30.415273500 INFO   slon: done
2012-04-10 09:25:30.415275500 INFO   slon: exit(0)

From vivek at khera.org  Tue Apr 10 08:05:22 2012
From: vivek at khera.org (Vick Khera)
Date: Tue, 10 Apr 2012 11:05:22 -0400
Subject: [Slony1-general] confusing sl_status output and replication
	stalled
In-Reply-To: <CALd+dceoj9-gNheqVkLMoBisyXkagF=9hrHVFx=c9g0nm9fuwg@mail.gmail.com>
References: <CALd+dceoj9-gNheqVkLMoBisyXkagF=9hrHVFx=c9g0nm9fuwg@mail.gmail.com>
Message-ID: <CALd+dcd_S9P1NbRu3z8=egYVX-ytztg+YpDCkjqiwBvZHhiRoQ@mail.gmail.com>

On Tue, Apr 10, 2012 at 10:24 AM, Vick Khera <vivek at khera.org> wrote:
> The sl_status on the master shows this:
>
>
> -[ RECORD 1 ]-------------+------------------------------
> st_origin ? ? ? ? ? ? ? ? | 20
> st_received ? ? ? ? ? ? ? | 31
> st_last_event ? ? ? ? ? ? | 5000008023
> st_last_event_ts ? ? ? ? ?| 2012-04-10 05:35:49.782917-04
> st_last_received ? ? ? ? ?| 5000008023
> st_last_received_ts ? ? ? | 2012-04-10 05:35:50.00566-04
> st_last_received_event_ts | 2012-04-10 05:35:49.782917-04
> st_lag_num_events ? ? ? ? | 0
> st_lag_time ? ? ? ? ? ? ? | 04:27:58.674963

Wow. no idea how (nor can I see anything obvious in the logs) but
suddenly now I see this:


-[ RECORD 1 ]-------------+------------------------------
st_origin                 | 20
st_received               | 31
st_last_event             | 5000008284
st_last_event_ts          | 2012-04-10 11:03:00.694014-04
st_last_received          | 5000008282
st_last_received_ts       | 2012-04-10 11:02:41.186753-04
st_last_received_event_ts | 2012-04-10 11:02:40.667261-04
st_lag_num_events         | 2
st_lag_time               | 00:00:21.678165

and my test row is updated on the replica...

From smccloud at geo-comm.com  Tue Apr 10 13:49:30 2012
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Tue, 10 Apr 2012 20:49:30 +0000
Subject: [Slony1-general] cleanup_interval
Message-ID: <7742DD496427B743BC8B7BBF6D380BA037840FD9@EXCHANGE10.geo-comm.local>

How do you change the cleanup_interval option on a Windows cluster?  I have tried adding it to the engine file and that doesn't change anything and everything I've found online shows how to do it in a *NIX cluster, not a Windows one.

Shaun McCloud - Associate Software Developer
Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit www.geo-comm.com<http://www.geo-comm.com/>
[cid:image001.jpg at 01CD1731.83E40E90]<http://www.linkedin.com/companies/geocomm> [cid:image002.jpg at 01CD1731.83E40E90] <http://twitter.com/_geocomm>
P Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120410/947ebec0/attachment.htm 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 1129 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120410/947ebec0/attachment.jpg 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 1068 bytes
Desc: image002.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120410/947ebec0/attachment-0001.jpg 

From ssinger at ca.afilias.info  Tue Apr 10 14:51:40 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 10 Apr 2012 17:51:40 -0400
Subject: [Slony1-general] cleanup_interval
In-Reply-To: <7742DD496427B743BC8B7BBF6D380BA037840FD9@EXCHANGE10.geo-comm.local>
References: <7742DD496427B743BC8B7BBF6D380BA037840FD9@EXCHANGE10.geo-comm.local>
Message-ID: <4F84AB6C.2030704@ca.afilias.info>

On 12-04-10 04:49 PM, Shaun McCloud wrote:

The "cleanup_interval" setting in your slon.conf file? I think that is 
the same as on Unix

The path to your conf file should be available on the command line 
string in the Windows Service window/application.




> How do you change the cleanup_interval option on a Windows cluster? I
> have tried adding it to the engine file and that doesn?t change anything
> and everything I?ve found online shows how to do it in a *NIX cluster,
> not a Windows one.
>
> *Shaun McCloud ? Associate Software Developer
> Geo-Comm, Inc*
> 601 W. Saint Germain St., Saint Cloud, MN 56301
> *Office:* 320.240.0040 *Fax:* 320.240.2389 *Toll Free:* 888.436.2666
> */click here to visit /**/www.geo-comm.com/* <http://www.geo-comm.com/>
>
> <http://www.linkedin.com/companies/geocomm> <http://twitter.com/_geocomm>
>
> PThink before you print!
>
> *Microsoft Certified Desktop Support Technician (MCDST)*
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From ngramsky at cs.umd.edu  Tue Apr 10 20:59:20 2012
From: ngramsky at cs.umd.edu (NewToSlony)
Date: Tue, 10 Apr 2012 20:59:20 -0700 (PDT)
Subject: [Slony1-general]  Can you delete rows from the slave database?
Message-ID: <33665986.post@talk.nabble.com>


If I am replicating from one database to another, can I delete rows from the
slave database and not have Slony re-populate those rows (provided the rows
in the master database have not been updated)?
-- 
View this message in context: http://old.nabble.com/Can-you-delete-rows-from-the-slave-database--tp33665986p33665986.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.


From scott.marlowe at gmail.com  Tue Apr 10 22:03:32 2012
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Tue, 10 Apr 2012 23:03:32 -0600
Subject: [Slony1-general] Can you delete rows from the slave database?
In-Reply-To: <33665986.post@talk.nabble.com>
References: <33665986.post@talk.nabble.com>
Message-ID: <CAOR=d=35gRi47HdvVcrLz7jMm9LNQzSBeQ_+q-7gBn6qWHj3kw@mail.gmail.com>

You CAN do it, but whether or not you SHOULD do it is questionable.
What are you trying to accomplish here?  If you try to update master
rows that are no longer in the slave db you'll have problems.

On Tue, Apr 10, 2012 at 9:59 PM, NewToSlony <ngramsky at cs.umd.edu> wrote:
>
> If I am replicating from one database to another, can I delete rows from the
> slave database and not have Slony re-populate those rows (provided the rows
> in the master database have not been updated)?
> --
> View this message in context: http://old.nabble.com/Can-you-delete-rows-from-the-slave-database--tp33665986p33665986.html
> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



-- 
To understand recursion, one must first understand recursion.

From gangulynj at gmail.com  Wed Apr 11 00:23:00 2012
From: gangulynj at gmail.com (Lakshmi Priya)
Date: Wed, 11 Apr 2012 12:53:00 +0530
Subject: [Slony1-general] Can you delete rows from the slave database?
In-Reply-To: <CAOR=d=35gRi47HdvVcrLz7jMm9LNQzSBeQ_+q-7gBn6qWHj3kw@mail.gmail.com>
References: <33665986.post@talk.nabble.com>
	<CAOR=d=35gRi47HdvVcrLz7jMm9LNQzSBeQ_+q-7gBn6qWHj3kw@mail.gmail.com>
Message-ID: <CAOy5HfOWGh=EhZ9xcDcMiowhUFy6S9vKQ0ODCeyUx7rmhom9HQ@mail.gmail.com>

Can any one say,what this error shows that..??

<stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql:
PGRES_FATAL_ERROR ERROR:  Slonik version: 2.0.6 != Slony-I version in PG
build 2.1.1.

On Wed, Apr 11, 2012 at 10:33 AM, Scott Marlowe <scott.marlowe at gmail.com>wrote:

> You CAN do it, but whether or not you SHOULD do it is questionable.
> What are you trying to accomplish here?  If you try to update master
> rows that are no longer in the slave db you'll have problems.
>
> On Tue, Apr 10, 2012 at 9:59 PM, NewToSlony <ngramsky at cs.umd.edu> wrote:
> >
> > If I am replicating from one database to another, can I delete rows from
> the
> > slave database and not have Slony re-populate those rows (provided the
> rows
> > in the master database have not been updated)?
> > --
> > View this message in context:
> http://old.nabble.com/Can-you-delete-rows-from-the-slave-database--tp33665986p33665986.html
> > Sent from the Slony-I -- General mailing list archive at Nabble.com.
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>
> --
> To understand recursion, one must first understand recursion.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120411/175b5c8b/attachment.htm 

From gangulynj at gmail.com  Wed Apr 11 00:24:46 2012
From: gangulynj at gmail.com (Lakshmi Priya)
Date: Wed, 11 Apr 2012 12:54:46 +0530
Subject: [Slony1-general] Error in slony
Message-ID: <CAOy5HfMfq7YoGu8CyysrFNWedn8yaQpE+_pOq6jK2oVgU=A3iw@mail.gmail.com>

Can any one say what this error meant..??

<stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql:
PGRES_FATAL_ERROR ERROR:  Slonik version: 2.0.6 != Slony-I version in PG
build 2.1.1.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120411/a1e9bc6c/attachment.htm 

From glynastill at yahoo.co.uk  Wed Apr 11 01:26:00 2012
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed, 11 Apr 2012 09:26:00 +0100 (BST)
Subject: [Slony1-general] Error in slony
In-Reply-To: <CAOy5HfMfq7YoGu8CyysrFNWedn8yaQpE+_pOq6jK2oVgU=A3iw@mail.gmail.com>
References: <CAOy5HfMfq7YoGu8CyysrFNWedn8yaQpE+_pOq6jK2oVgU=A3iw@mail.gmail.com>
Message-ID: <1334132760.48765.YahooMailNeo@web171403.mail.ir2.yahoo.com>

It means that the version of the functions in your schema does not match the compiled version of slony.? I'm guessing you need to run "update functions"




>________________________________
> From: Lakshmi Priya <gangulynj at gmail.com>
>To: slony1-general at lists.slony.info 
>Sent: Wednesday, 11 April 2012, 8:24
>Subject: [Slony1-general] Error in slony
> 
>
>Can any one say what this error meant..??
>
>
><stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql: PGRES_FATAL_ERROR ERROR: ?Slonik version: 2.0.6 != Slony-I version in PG build 2.1.1. 
>_______________________________________________
>Slony1-general mailing list
>Slony1-general at lists.slony.info
>http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120411/532f8036/attachment-0001.htm 

From glynastill at yahoo.co.uk  Wed Apr 11 03:05:50 2012
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed, 11 Apr 2012 11:05:50 +0100 (BST)
Subject: [Slony1-general] Error in slony
In-Reply-To: <CAOy5HfPUZfBTbHOv3FtqFfK38wvGUwqE11WZfsTTwU2qKhT=uA@mail.gmail.com>
References: <CAOy5HfMfq7YoGu8CyysrFNWedn8yaQpE+_pOq6jK2oVgU=A3iw@mail.gmail.com>
	<1334132760.48765.YahooMailNeo@web171403.mail.ir2.yahoo.com>
	<CAOy5HfPUZfBTbHOv3FtqFfK38wvGUwqE11WZfsTTwU2qKhT=uA@mail.gmail.com>
Message-ID: <1334138750.37552.YahooMailNeo@web171402.mail.ir2.yahoo.com>

Please don't reply off list.

First check the version of the slony binaries installed across your servers, and the slonik tool you are using are the same and the version you want to be using.? Then run update functions as follows.


http://slony.info/documentation/2.1/stmtupdatefunctions.html




>________________________________
> From: Lakshmi Priya <gangulynj at gmail.com>
>To: Glyn Astill <glynastill at yahoo.co.uk> 
>Sent: Wednesday, 11 April 2012, 10:44
>Subject: Re: [Slony1-general] Error in slony
> 
>
>How to run the Update functions..
>
>
>On Wed, Apr 11, 2012 at 1:56 PM, Glyn Astill <glynastill at yahoo.co.uk> wrote:
>
>It means that the version of the functions in your schema does not match the compiled version of slony.? I'm guessing you need to run "update functions"
>>
>>
>>
>>
>>>________________________________
>>> From: Lakshmi Priya <gangulynj at gmail.com>
>>>To: slony1-general at lists.slony.info 
>>>Sent: Wednesday, 11 April 2012, 8:24
>>>Subject: [Slony1-general] Error in slony
>>> 
>>>
>>>
>>>Can any one say what this error meant..??
>>>
>>>
>>><stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql: PGRES_FATAL_ERROR ERROR: ?Slonik version: 2.0.6 != Slony-I version in PG build 2.1.1. 
>>>_______________________________________________
>>>Slony1-general mailing list
>>>Slony1-general at lists.slony.info
>>>http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>>
>>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120411/06425bab/attachment.htm 

From smccloud at geo-comm.com  Wed Apr 11 05:47:10 2012
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Wed, 11 Apr 2012 12:47:10 +0000
Subject: [Slony1-general] cleanup_interval
In-Reply-To: <4F84AB6C.2030704@ca.afilias.info>
References: <7742DD496427B743BC8B7BBF6D380BA037840FD9@EXCHANGE10.geo-comm.local>
	<4F84AB6C.2030704@ca.afilias.info>
Message-ID: <7742DD496427B743BC8B7BBF6D380BA037842466@EXCHANGE10.geo-comm.local>

Steve,

I have tried putting cleanup_interval="20 minutes" in the conf file and it doesn't change anything.  Still shows 10 minutes for the cleanup_interval.

Shaun McCloud ? Associate Software Developer
Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit www.geo-comm.com
 
? Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)


-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info] 
Sent: Tuesday, April 10, 2012 16:52
To: Shaun McCloud
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] cleanup_interval

On 12-04-10 04:49 PM, Shaun McCloud wrote:

The "cleanup_interval" setting in your slon.conf file? I think that is the same as on Unix

The path to your conf file should be available on the command line string in the Windows Service window/application.




> How do you change the cleanup_interval option on a Windows cluster? I
> have tried adding it to the engine file and that doesn?t change anything
> and everything I?ve found online shows how to do it in a *NIX cluster,
> not a Windows one.
>
> *Shaun McCloud ? Associate Software Developer
> Geo-Comm, Inc*
> 601 W. Saint Germain St., Saint Cloud, MN 56301
> *Office:* 320.240.0040 *Fax:* 320.240.2389 *Toll Free:* 888.436.2666
> */click here to visit /**/www.geo-comm.com/* <http://www.geo-comm.com/>
>
> <http://www.linkedin.com/companies/geocomm> <http://twitter.com/_geocomm>
>
> PThink before you print!
>
> *Microsoft Certified Desktop Support Technician (MCDST)*
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From scott.marlowe at gmail.com  Wed Apr 11 06:16:18 2012
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 11 Apr 2012 07:16:18 -0600
Subject: [Slony1-general] Can you delete rows from the slave database?
In-Reply-To: <CAOy5HfOWGh=EhZ9xcDcMiowhUFy6S9vKQ0ODCeyUx7rmhom9HQ@mail.gmail.com>
References: <33665986.post@talk.nabble.com>
	<CAOR=d=35gRi47HdvVcrLz7jMm9LNQzSBeQ_+q-7gBn6qWHj3kw@mail.gmail.com>
	<CAOy5HfOWGh=EhZ9xcDcMiowhUFy6S9vKQ0ODCeyUx7rmhom9HQ@mail.gmail.com>
Message-ID: <CAOR=d=3bh42VjWX5W7Tk4E5pK5UH2+p9wvcf2Oni6x4P1OLnPA@mail.gmail.com>

In the future please start a new thread instead of replying to a previous one.

On Wed, Apr 11, 2012 at 1:23 AM, Lakshmi Priya <gangulynj at gmail.com> wrote:
> Can any one say,what this error shows that..??
>
> <stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql:
> PGRES_FATAL_ERROR ERROR: ?Slonik version: 2.0.6 != Slony-I version in PG
> build 2.1.1.

It means you're trying to run two different versions f slony. Slony
versions must be exact matches, down the last digit in the version
string.

From smccloud at geo-comm.com  Wed Apr 11 06:38:54 2012
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Wed, 11 Apr 2012 13:38:54 +0000
Subject: [Slony1-general] cleanup_interval
In-Reply-To: <7742DD496427B743BC8B7BBF6D380BA037842466@EXCHANGE10.geo-comm.local>
References: <7742DD496427B743BC8B7BBF6D380BA037840FD9@EXCHANGE10.geo-comm.local>
	<4F84AB6C.2030704@ca.afilias.info>
	<7742DD496427B743BC8B7BBF6D380BA037842466@EXCHANGE10.geo-comm.local>
Message-ID: <7742DD496427B743BC8B7BBF6D380BA0378425C7@EXCHANGE10.geo-comm.local>

I just figured out why I was having an issue.  In Windows you have to use a single quote around the value instead of a double quote around the value.  So cleanup_interval='30 minutes' works while cleanup_interval="30 minutes" doesn't.

Shaun McCloud ? Associate Software Developer
Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit www.geo-comm.com
 
? Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)


-----Original Message-----
From: slony1-general-bounces at lists.slony.info [mailto:slony1-general-bounces at lists.slony.info] On Behalf Of Shaun McCloud
Sent: Wednesday, April 11, 2012 07:47
To: Steve Singer
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] cleanup_interval

Steve,

I have tried putting cleanup_interval="20 minutes" in the conf file and it doesn't change anything.  Still shows 10 minutes for the cleanup_interval.

Shaun McCloud ? Associate Software Developer Geo-Comm, Inc
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666 click here to visit www.geo-comm.com
 
P Think before you print!
Microsoft Certified Desktop Support Technician (MCDST)


-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info]
Sent: Tuesday, April 10, 2012 16:52
To: Shaun McCloud
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] cleanup_interval

On 12-04-10 04:49 PM, Shaun McCloud wrote:

The "cleanup_interval" setting in your slon.conf file? I think that is the same as on Unix

The path to your conf file should be available on the command line string in the Windows Service window/application.




> How do you change the cleanup_interval option on a Windows cluster? I 
> have tried adding it to the engine file and that doesn?t change 
> anything and everything I?ve found online shows how to do it in a *NIX 
> cluster, not a Windows one.
>
> *Shaun McCloud ? Associate Software Developer Geo-Comm, Inc*
> 601 W. Saint Germain St., Saint Cloud, MN 56301
> *Office:* 320.240.0040 *Fax:* 320.240.2389 *Toll Free:* 888.436.2666 
> */click here to visit /**/www.geo-comm.com/* 
> <http://www.geo-comm.com/>
>
> <http://www.linkedin.com/companies/geocomm> 
> <http://twitter.com/_geocomm>
>
> PThink before you print!
>
> *Microsoft Certified Desktop Support Technician (MCDST)*
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general

From richyen at iparadigms.com  Wed Apr 11 06:24:37 2012
From: richyen at iparadigms.com (Richard Yen)
Date: Wed, 11 Apr 2012 09:24:37 -0400
Subject: [Slony1-general] Logship files printing incorrectly
Message-ID: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>

Hi,

I've recently come across a situation where slony logshipping files were
being written incorrectly.  Normally, the files are written in this order:
- Begin transaction
- Set archive tracking index
- INSERTs/UPDATEs/DELETEs
- Set sequences
- Vacuum
- Commit

I noticed that in my particular instance, the files were being written as
such:
- Begin transaction
- Set archive tracking index
- Vacuum
- Commit
- INSERTs/UPDATEs/DELETEs
- Set sequences

Obviously, this sounds pretty dangerous, and I actually encountered the
situation where my logship destination got corrupted, and needed to rebuild
that node.

Would anyone have any insight into why this happens?

Using 2.0.6 on CentOS, with Postgres 8.4.5

Thanks!
--Richard
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120411/4fdbd10a/attachment.htm 

From ssinger at ca.afilias.info  Wed Apr 11 06:50:00 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 11 Apr 2012 09:50:00 -0400
Subject: [Slony1-general] Logship files printing incorrectly
In-Reply-To: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
References: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
Message-ID: <4F858C08.6080608@ca.afilias.info>

On 12-04-11 09:24 AM, Richard Yen wrote:
> Hi,
>
> I've recently come across a situation where slony logshipping files were
> being written incorrectly.  Normally, the files are written in this order:
> - Begin transaction
> - Set archive tracking index
> - INSERTs/UPDATEs/DELETEs
> - Set sequences
> - Vacuum
> - Commit
>
> I noticed that in my particular instance, the files were being written
> as such:
> - Begin transaction
> - Set archive tracking index
> - Vacuum
> - Commit
> - INSERTs/UPDATEs/DELETEs
> - Set sequences
>

Was slony_logshipper applying the actions in the wrong order, or were 
the .sql files generated by slon showing things in the wrong order?

If the .sql files generated by slon had things in the wrong order(which 
is what I think you are saying) can you send me a sample .sql file?




> Obviously, this sounds pretty dangerous, and I actually encountered the
> situation where my logship destination got corrupted, and needed to
> rebuild that node.
>
> Would anyone have any insight into why this happens?
>
> Using 2.0.6 on CentOS, with Postgres 8.4.5
>
> Thanks!
> --Richard
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From gangulynj at gmail.com  Wed Apr 11 23:11:10 2012
From: gangulynj at gmail.com (Lakshmi Priya)
Date: Thu, 12 Apr 2012 11:41:10 +0530
Subject: [Slony1-general] Update function
Message-ID: <CAOy5HfOae0GmtMwOogaXW_FA=VzDfTJAh=ihQe9LcoJmJm8xbw@mail.gmail.com>

When i specify

    UPDATE FUNCTIONS (
        ID = 3        # Update functions on node 3
    );

I am getting event node error,even i specified event node as 2 or 3.
Event node function will not work in update node.

Can anyone help me..
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120412/cb0e4f93/attachment.htm 

From gangulynj at gmail.com  Thu Apr 12 03:39:38 2012
From: gangulynj at gmail.com (Lakshmi Priya)
Date: Thu, 12 Apr 2012 16:09:38 +0530
Subject: [Slony1-general] New to slony
Message-ID: <CAOy5HfNcCmbWE_+xAJ7=WvRxfG1owDqaMVzjnZ+yR4Svxgii9w@mail.gmail.com>

I am getting this error..i dont know where to change..can anyone say..


<stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql:
PGRES_FATAL_ERROR ERROR:  Slonik version: 2.0.6 != Slony-I version in PG
build 2.1.1
ERROR:  Slonik version: 2.0.6 != Slony-I version in PG build 2.1.1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120412/136b7149/attachment.htm 

From devrim at gunduz.org  Thu Apr 12 03:42:15 2012
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu, 12 Apr 2012 13:42:15 +0300
Subject: [Slony1-general] New to slony
In-Reply-To: <CAOy5HfNcCmbWE_+xAJ7=WvRxfG1owDqaMVzjnZ+yR4Svxgii9w@mail.gmail.com>
References: <CAOy5HfNcCmbWE_+xAJ7=WvRxfG1owDqaMVzjnZ+yR4Svxgii9w@mail.gmail.com>
Message-ID: <1334227335.28448.44.camel@lenovo01-laptop03.gunduz.org>

On Thu, 2012-04-12 at 16:09 +0530, Lakshmi Priya wrote:
> <stdin>:8: loading of file /usr/local/pgsql/share//slony1_funcs.sql:
> PGRES_FATAL_ERROR ERROR:  Slonik version: 2.0.6 != Slony-I version in
> PG
> build 2.1.1
> ERROR:  Slonik version: 2.0.6 != Slony-I version in PG build 2.1.1 

The version of slony1_funcs.sql does not match the PG version -- use
Slony built against your PostgreSQL version.
-- 
Devrim G?ND?Z
Principal Systems Engineer @ EnterpriseDB: http://www.enterprisedb.com
PostgreSQL Dan??man?/Consultant, Red Hat Certified Engineer
Community: devrim~PostgreSQL.org, devrim.gunduz~linux.org.tr
http://www.gunduz.org  Twitter: http://twitter.com/devrimgunduz
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120412/4827f1fd/attachment.pgp 

From lists at nanl.de  Tue Apr 24 07:40:08 2012
From: lists at nanl.de (Mirko Vogt)
Date: Tue, 24 Apr 2012 16:40:08 +0200
Subject: [Slony1-general] some questions which popped up while setting up...
Message-ID: <4F96BB48.3070003@nanl.de>

Hey all!

I successfully setup my first slony replication - however quite a few
conceptual questions raised on the way:

I have the following setup (taken from slon_tools.conf):

add_node(node     => 1,
         host     => 'master.foo',
         dbname   => 'foo',
         port     => 5432,
         user     => 'slony',
         password => 'XXX');

add_node(node     => 11,
         host     => 'slave1.foo',
         dbname   => 'foo',
         port     => 6254,
         user     => 'slony',
         password => 'XXX');

add_node(node     => 12,
         host     => 'slave2.foo',
         dbname   => 'foo',
         port     => 2254,
         user     => 'slony',
         password => 'XXX');

This config got deployed on every node, every pg_hba.conf-file contained
a line which allowed all other servers to connect as user slony.

Everything was working - until I tried to optimize.

I thought, well, there needs to be a connection between the master and
the slaves but no direct connection between the slaves - so I dropped
the access lines in pg_hba.conf on the slaves for the other slave
respectively.

The setup seemed to still work, however on the slaves I noticed error
messages like:
  FATAL:  no pg_hba.conf entry for host "slave1.foo", user "slony",
database "foo"

Okay, fine, obviously they try to connect to each other: so I purged out
the respective node-definitions out of the slon_tools.conf file on the
slaves (on node 11 I deleted the definition of node 12 and vice versa).

After restarting slony the slaves still tried to connect to each other.
Where do they have the connect information from? And why are they trying
to connect to each other at all?

Anyway, next thought: if one node gets hacked the attacker shouldn't be
able to access the database on the other nodes. Idea was: The slaves do
not need to access the master with a user who has write access to that
database (slony). That's why I created a read-only user on the master
(slony_ro) and tried to tell the slaves - by changing the user 'slony'
to 'slony_ro' within the slon_tools.conf-files - to connect as 'slony_ro'.
However also that change didn't show any effect after restarting slony.

It seems to me - by initializing the cluster, creating and subscribing
to the going-to-be-replicated sets - the information got pushed to the
slaves from the master.

That raises 2 (sub-)questions:
a) Where is this information stored?
b) why there is the need of a slon_tools.conf file if its data is not
used anyway (at least on the slaves)?

Maybe somebody could lighten me up here? I didn't find any information
able to clear my confusion about that yet :/

Cheers, thanks a lot in advance and have a nice week!

  mirko

