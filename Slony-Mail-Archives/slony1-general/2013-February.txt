From mail at joeconway.com  Tue Feb  5 14:07:13 2013
From: mail at joeconway.com (Joe Conway)
Date: Tue, 05 Feb 2013 16:07:13 -0600
Subject: [Slony1-general] slony cluster snapshot
Message-ID: <51118291.2030005@joeconway.com>

What is the recommended way to capture a "snapshot" of an entire slony
cluster at a point in time, such that it is possible to restore the
cluster to that point in time (without having to drop/resubscribe nodes)?

We can stop activity on the cluster while taking the snapshot, but
ideally would like to avoid having to dump every node independently. We
will have both slony-managed and non-slony-managed tables (although the
non-slony-managed tables should also be in sync across the nodes).

The ultimate goal it to provide a way to reset the cluster to a known
state in order to facilitate testing.

Pointers, ideas, URLs appreciated.

Thanks,

Joe




-- 
Joe Conway
credativ LLC: http://www.credativ.us
Linux, PostgreSQL, and general Open Source
Training, Service, Consulting, & 24x7 Support

From cbbrowne at afilias.info  Tue Feb  5 14:48:07 2013
From: cbbrowne at afilias.info (Christopher Browne)
Date: Tue, 5 Feb 2013 17:48:07 -0500
Subject: [Slony1-general] slony cluster snapshot
In-Reply-To: <51118291.2030005@joeconway.com>
References: <51118291.2030005@joeconway.com>
Message-ID: <CANfbgbbzf452V7AWnAiSiCnNF7jUtCf0jwRvGBRujg27Phz5yA@mail.gmail.com>

On Tue, Feb 5, 2013 at 5:07 PM, Joe Conway <mail at joeconway.com> wrote:
> What is the recommended way to capture a "snapshot" of an entire slony
> cluster at a point in time, such that it is possible to restore the
> cluster to that point in time (without having to drop/resubscribe nodes)?
>
> We can stop activity on the cluster while taking the snapshot, but
> ideally would like to avoid having to dump every node independently. We
> will have both slony-managed and non-slony-managed tables (although the
> non-slony-managed tables should also be in sync across the nodes).
>
> The ultimate goal it to provide a way to reset the cluster to a known
> state in order to facilitate testing.
>
> Pointers, ideas, URLs appreciated.

There's a built-in tool for dumping the configuration of a cluster:
http://git.postgresql.org/gitweb/?p=slony1-engine.git;a=blob;f=tools/slonikconfdump.sh

I'm pretty sure that's not what you have in mind; I think you're
thinking not just about the slony configuration, but about the
combination of all the nodes and their cross-database state.

If all of the databases are located on reasonably sophisticated disk
array hardware, then you could probably use the disk array's
capabilities to generate snapshots of the states of the respective
databases, perhaps complete with deduplication of data via
copy-on-write cloning.

I'd think that's the only particularly reasonable way to do this
without doing a bunch of pg_dumps.

Similar could be done on Linux using LVM, copying via dd, or doing
something filesystem-specific using btrfs (btrfs subvolume snapshot)
or XFS (xfs_copy).

On FreeBSD (perhaps other BSDs), ZFS has a "zfs clone" command that is
analogous.

It's probably also conceivable to use PITR to this end, checking the
end XID on each database node, and setting things up to recover to
that point.  But I rather expect that filesystem/disk array snapshots
will be the easiest way to do this.

From mail at joeconway.com  Tue Feb  5 15:12:43 2013
From: mail at joeconway.com (Joe Conway)
Date: Tue, 05 Feb 2013 17:12:43 -0600
Subject: [Slony1-general] slony cluster snapshot
In-Reply-To: <CANfbgbbzf452V7AWnAiSiCnNF7jUtCf0jwRvGBRujg27Phz5yA@mail.gmail.com>
References: <51118291.2030005@joeconway.com>
	<CANfbgbbzf452V7AWnAiSiCnNF7jUtCf0jwRvGBRujg27Phz5yA@mail.gmail.com>
Message-ID: <511191EB.1070400@joeconway.com>

On 02/05/2013 04:48 PM, Christopher Browne wrote:
> It's probably also conceivable to use PITR to this end, checking the
> end XID on each database node, and setting things up to recover to
> that point.  But I rather expect that filesystem/disk array snapshots
> will be the easiest way to do this.

Is there a function or query we could use to verify that the nodes are
all in sync as far as the slony replicated data?

If so, I'm thinking we could then do something like:

On backup:
1) stop all activity on the cluster
2) check the nodes are all in sync
   -> if not wait, check again
3) once all in sync
   -> back up data-only from slon schema on each node
   -> back up user tables data-only on master node
4) start up slony, recommence activity

On restore:
1) stop all activity on the cluster
2) stop all the slons
3) disable the slony triggers
4) truncate all slony and user tables
5) reload all nodes from previous backups
   -> slony data from local node backup
   -> user data from master node backup
6) re-enable triggers
7) start all the slons, recommence activity

Does this sound feasible?

Thanks,

Joe


-- 
Joe Conway
credativ LLC: http://www.credativ.us
Linux, PostgreSQL, and general Open Source
Training, Service, Consulting, & 24x7 Support

From steve at ssinger.info  Wed Feb  6 13:43:31 2013
From: steve at ssinger.info (Steve Singer)
Date: Wed, 6 Feb 2013 21:43:31 +0000
Subject: [Slony1-general] slony cluster snapshot
Message-ID: <SNT002-W75C1AD1B7D18862507CEB6DC070@phx.gbl>

Joe, 

I see a few issues with what you are proposing.
1. When you restore the data the next xid on the target system might be lower than the xid's used on the source system.  This will break the logic used to connect SYNC events in sl_event with the proper rows in sl_log.  You could get around this by setting the xid wraparound counter on the target system to be higher than the wraparound counter on the source.

2. "All nodes in sync" is a bit of a loaded statement.  Do you mean "all data has been replicated" or all sync events have been confirmed on the remote node.  In order to get to a state where a sync event has been confirmed on a remote node but the next sync event has not yet been generated you will need to do something to prevent the next sync event from being generated.  You can do that by changing the sync interval too something much higher(a slon.conf) option

I think Chris's ideas will be easier to get working in practice, but otherwise I think you could do a backup as you describe if you address the above two issues.

Steve
 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130206/fbebc8da/attachment.htm 

From andrew at asi-web.com  Thu Feb  7 09:28:07 2013
From: andrew at asi-web.com (andrew)
Date: Thu, 7 Feb 2013 17:28:07 +0000
Subject: [Slony1-general] Is there a way to cap the file size of slony log
	shipping files?
Message-ID: <88CAA3D677DBF347A10B6CA1B28D6DFE02BC2BF2@MSSERVER.asi-web.local>

Hello.



I am working with a SuSE machine (cat /etc/issue: SUSE Linux Enterprise Server 11 SP1  (i586)) running Postgresql 8.1.3 and the Slony-I replication system (slon version 1.1.5).  We have a working replication setup going between the databases on this server, which is generating log shipping files to be sent to the remote machines we are tasked to maintain.  The cluster itself is set up around three nodes (Node1, Node2, Node3) - Node 1 is the master, and Node 3 the slave.  Node 2 appears to be a legacy of the original build done by my predecessor; that database is blank.



As of this morning, we ran into a problem with this.



For a while now, we've had strange memory problems on this machine - the oom-killer seems to be striking even when there is plenty of free memory left.  That has set the stage for our current issue to occur - we ran a massive update on our system last night, while replication was turned off.  Now, as things currently stand, we cannot replicate the changes out - slony is attempting to compile all the changes into a single massive log file, and after about half an hour or so of running, it trips over the oom-killer issue, which appears to restart the replication package.  Since it is constantly trying to rebuild that same package, it never gets anywhere.



My first question is this: Is there a way to cap the size of Slony log shipping files, so that it writes out no more than 'X' bytes (or K, or Meg, etc.) and after going over that size, closes the current log shipping file and starts a new one?  We've been able to hit about four megs in size before the oom-killer hits with fair regularity, so if I could cap it there, I could at least start generating the smaller files and hopefully eventually get through this.



My second question, I guess, is this: Does anyone have a better solution for this issue than the one I'm asking about?  It's quite possible I'm getting tunnel vision looking at the problem, and all I really need is -a- solution, not necessarily -my- solution.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130207/74bd3d5d/attachment.htm 

From cbbrowne at afilias.info  Thu Feb  7 13:21:20 2013
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 7 Feb 2013 16:21:20 -0500
Subject: [Slony1-general] Is there a way to cap the file size of slony
 log shipping files?
In-Reply-To: <88CAA3D677DBF347A10B6CA1B28D6DFE02BC2BF2@MSSERVER.asi-web.local>
References: <88CAA3D677DBF347A10B6CA1B28D6DFE02BC2BF2@MSSERVER.asi-web.local>
Message-ID: <CANfbgbaNSrb43rBS+7Ma5aDGEf-xO5ttWKqhKRMY6HqtZ0SZVg@mail.gmail.com>

On Thu, Feb 7, 2013 at 12:28 PM, andrew <andrew at asi-web.com> wrote:
> I am working with a SuSE machine (cat /etc/issue: SUSE Linux Enterprise
> Server 11 SP1  (i586)) running Postgresql 8.1.3 and the Slony-I replication
> system (slon version 1.1.5).  We have a working replication setup going
> between the databases on this server, which is generating log shipping files
> to be sent to the remote machines we are tasked to maintain.  The cluster
> itself is set up around three nodes (Node1, Node2, Node3) - Node 1 is the
> master, and Node 3 the slave.  Node 2 appears to be a legacy of the original
> build done by my predecessor; that database is blank.
>
>
>
> As of this morning, we ran into a problem with this.
>
>
>
> For a while now, we've had strange memory problems on this machine - the
> oom-killer seems to be striking even when there is plenty of free memory
> left.  That has set the stage for our current issue to occur - we ran a
> massive update on our system last night, while replication was turned off.
> Now, as things currently stand, we cannot replicate the changes out - slony
> is attempting to compile all the changes into a single massive log file, and
> after about half an hour or so of running, it trips over the oom-killer
> issue, which appears to restart the replication package.  Since it is
> constantly trying to rebuild that same package, it never gets anywhere.
>
>
>
> My first question is this: Is there a way to cap the size of Slony log
> shipping files, so that it writes out no more than 'X' bytes (or K, or Meg,
> etc.) and after going over that size, closes the current log shipping file
> and starts a new one?  We've been able to hit about four megs in size before
> the oom-killer hits with fair regularity, so if I could cap it there, I
> could at least start generating the smaller files and hopefully eventually
> get through this.
>
>
>
> My second question, I guess, is this: Does anyone have a better solution for
> this issue than the one I'm asking about?  It's quite possible I'm getting
> tunnel vision looking at the problem, and all I really need is -a- solution,
> not necessarily -my- solution.

Here are some observations, not necessarily particularly ordered...

1.  A 4MB limit seems rather small.

2.  I wouldn't expect the file size to have much, if anything, to do with
how much memory is consumed.  Data is thrown into the file and
then thrown away on a more or less query-by-query basis.  So really,
I'd expect limiting file size to not actually help with OOM.

The "predecessor" system to Slony, eRServer, had quite different
habits; it would draw query data into memory, and not throw anything
out of memory until it finished its notion of a "Sync."  Which meant
that if a replica fell far enough behind, you'd run out of memory,
perhaps in a tough place, like the maximum that a JVM (eRServer
was written in Java) could allocate, e.g. - 2GB, and replication was
at that point "toast," as it could never catch up.

But Slony shouldn't be pulling *too* much data into memory at any
given time, so I'm thinking you're looking at the wrong problem.

3.  Well, there *is* a place where Slony can chew memory, and that's
if you have enormous tuples.  We used to replicate an RT/3
(Request Tracker 3) instance, and ticket attachments could get very
large, and would get drawn into memory.  A few tuples 50MB in size
within the same SYNC would lead to memory running out pretty
quickly.

That was changed in version 1.2, so it's pretty possible that this
would be the root of your troubles.

4. Note that we released 1.2.0 back in 2006, some seven years ago,
and basically stopped maintaining the 1.1 branch in 2007.

5.  If the issues of #4 are the problem, then a temporary fix that
might help would be to to reduce the #define for
SLON_DATA_FETCH_DATA_SIZE from 100 to maybe just 1,
and similarly, SLON_COMMANDS_PER_LINE from 10 to 1.
(see src/slon/slon.h)

That'll lower efficiency, but it means you won't be blowing memory
as much on those big tuples.

But you're on an exceeding old version, and should try to get to
something rather less than 7 years old.

From ssinger at ca.afilias.info  Thu Feb 14 11:41:36 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 14 Feb 2013 14:41:36 -0500
Subject: [Slony1-general] Slony 2.1.3 released
Message-ID: <511D3DF0.30104@ca.afilias.info>

The Slony team has released a bug fix release for Slony 2.1.  Slony 
2.1.3 is the next release in the 2.1 series. This release includes the 
following changes from 2.1.2



- Bug #285 :: Fix race condition with MOVE SET where a third node
   could get updated to the wrong SYNC number
- Bug #276 :: Fixed duplicate key detection on sl_nodelock
- Bug #278 :: assorted spelling fixes
- Add in explicit support to recognise PostgreSQL 9.2, rather than
   warning that it may not be supported
- Add --with-pgport option to configure and when set to yes let slonik
   use these functions to determine the PGSHARE directory at runtime.
   This defaults to no

Users of 2.1.1 or 2.1.2 that make use of the MOVE SET command and have 
three or more nodes in the cluster are encouraged to update to 2.1.3 as 
soon as possible.



From ssinger at ca.afilias.info  Wed Feb 20 08:27:49 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 20 Feb 2013 11:27:49 -0500
Subject: [Slony1-general] Slony 2.2.0 beta 3 released
Message-ID: <5124F985.20507@ca.afilias.info>

The Slony team is pleased to announce the release of the third beta for 
Slony 2.2.0.

Slony 2.2.0 will be the next major release of the Slony-I replication 
system for PostgreSQL.

Key features of this release include:

* The storage and transport and application of the slony log 
(sl_log_1/sl_log_2) has changed providing performance improvements. Data 
is now stored in a different format and the postgresql COPY protocol and 
triggers are used to replicate and apply changes to replicas.

* DDL handling with the EXECUTE SCRIPT command has changed.  The DDL is 
no longer stored as a special event in sl_event but is instead stored in 
sl_log_script and is processed as part of a SYNC event inline with data 
changes.

* FAILOVER has been reworked to be more reliable but not all nodes can 
be used as failover targets.

* A RESUBSCRIBE NODE command was added because the provider of a 
subscribed set can no longer be changed with the SUBSCRIBE SET command 
in some cases.  All sets from a particular origin must send data to 
receivers through the same path/forwarder nodes. This must remain  true 
during cluster reshaping.


Numerous bug fixes.  See the release notes for details and changes from 
the last beta

The release notes are available at
http://git.postgresql.org/gitweb/?p=slony1-engine.git;a=blob_plain;f=RELEASE;h=9405e0e1b9059344477861f6213ff461dab6b754;hb=50958a3fb9ffd3f293236861ab8404ae88252b0f


Slony-I 2.2.0 beta 3 can be downloaded from:

http://www.slony.info/downloads/2.2/source/slony1-2.2.0.b3.tar.bz2
http://www.slony.info/downloads/2.2/source/slony1-2.2.0.b3-docs.tar.bz2

This release is a beta release. Users are encouraged to try the release 
out and report bugs.

I would like people who test this beta to report both successes and bugs 
to me, either on the list or through private email.   When we release a 
beta and hear no feedback it is hard to know if that is because 
everything is working great and we are ready for a release or if it is 
because no one has tested it.  Test this beta, report your results to me 
and earn Slony karma points.

Thanks


Steve



From smccloud at geo-comm.com  Thu Feb 21 12:49:08 2013
From: smccloud at geo-comm.com (Shaun McCloud)
Date: Thu, 21 Feb 2013 20:49:08 +0000
Subject: [Slony1-general] Visual C++ runtimes
Message-ID: <7742DD496427B743BC8B7BBF6D380BA047887208@EXCHANGE10.geo-comm.local>

When compiling Slony on Windows, which Visual C++ runtimes are needed to run it?  It appears that it needs 2008 & 2010, but does the SP level matter?

Shaun McCloud, MCDST | Associate Software Developer
Geo-Comm Inc. | www.geo-comm.com<http://www.geo-comm.com/>
Toll Free 888.436.2666
P Please consider the environment before printing this email

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130221/acfb2fd9/attachment.html 

From mjames at profitpoint.com  Fri Feb 22 13:20:20 2013
From: mjames at profitpoint.com (Mike James)
Date: Fri, 22 Feb 2013 21:20:20 +0000
Subject: [Slony1-general] Schema upgrade problem
Message-ID: <F024DCE3402750409CD407C414C07E1A16C37EFF@BY2PRD0811MB404.namprd08.prod.outlook.com>

Hi, I'm used the pgdg92 (PostgreSQL 9.2.6 - x86_64) repo to install postgesql 9.2 and slony1-92 on a Centos 6 machine. After running yum update recently, my slony1 was updated to slony1-92-2.1.3-1.rhel6.x86_64.

Now when I run slony1, I get the following entries created in slond-system.log, which terminate with an error:

2013-02-22 16:02:20 EST CONFIG main: slon version 2.1.3 starting up
2013-02-22 16:02:20 EST INFO   slon: watchdog process started
2013-02-22 16:02:20 EST CONFIG slon: watchdog ready - pid = 8097
2013-02-22 16:02:20 EST CONFIG slon: worker process created - pid = 8098
2013-02-22 16:02:20 EST CONFIG main: Integer option vac_frequency = 3
2013-02-22 16:02:20 EST CONFIG main: Integer option log_level = 0
2013-02-22 16:02:20 EST CONFIG main: Integer option sync_interval = 2000
2013-02-22 16:02:20 EST CONFIG main: Integer option sync_interval_timeout = 10000
2013-02-22 16:02:20 EST CONFIG main: Integer option sync_group_maxsize = 20
2013-02-22 16:02:20 EST CONFIG main: Integer option desired_sync_time = 60000
2013-02-22 16:02:20 EST CONFIG main: Integer option syslog = 0
2013-02-22 16:02:20 EST CONFIG main: Integer option quit_sync_provider = 0
2013-02-22 16:02:20 EST CONFIG main: Integer option sync_max_rowsize = 8192
2013-02-22 16:02:20 EST CONFIG main: Integer option sync_max_largemem = 5242880
2013-02-22 16:02:20 EST CONFIG main: Integer option remote_listen_timeout = 300
2013-02-22 16:02:20 EST CONFIG main: Integer option monitor_interval = 500
2013-02-22 16:02:20 EST CONFIG main: Integer option explain_interval = 0
2013-02-22 16:02:20 EST CONFIG main: Integer option tcp_keepalive_idle = 0
2013-02-22 16:02:20 EST CONFIG main: Integer option tcp_keepalive_interval = 0
2013-02-22 16:02:20 EST CONFIG main: Integer option tcp_keepalive_count = 0
2013-02-22 16:02:20 EST CONFIG main: Boolean option log_pid = 0
2013-02-22 16:02:20 EST CONFIG main: Boolean option log_timestamp = 1
2013-02-22 16:02:20 EST CONFIG main: Boolean option tcp_keepalive = 1
2013-02-22 16:02:20 EST CONFIG main: Boolean option monitor_threads = 1
2013-02-22 16:02:20 EST CONFIG main: Real option real_placeholder = 0.000000
2013-02-22 16:02:20 EST CONFIG main: String option cluster_name = sys_cluster
2013-02-22 16:02:20 EST CONFIG main: String option conn_info = dbname=system
user=slonyusr host=192.168.1.1 password=amessofit
2013-02-22 16:02:20 EST CONFIG main: String option pid_file = [NULL]
2013-02-22 16:02:20 EST CONFIG main: String option log_timestamp_format = %Y-%m-%d %H:%M:%S %Z
2013-02-22 16:02:20 EST CONFIG main: String option archive_dir = [NULL]
2013-02-22 16:02:20 EST CONFIG main: String option sql_on_connection = [NULL]
2013-02-22 16:02:20 EST CONFIG main: String option lag_interval = [NULL]
2013-02-22 16:02:20 EST CONFIG main: String option command_on_logarchive = [NULL]
2013-02-22 16:02:20 EST CONFIG main: String option syslog_facility = LOCAL0
2013-02-22 16:02:20 EST CONFIG main: String option syslog_ident = slon
2013-02-22 16:02:20 EST CONFIG main: String option cleanup_interval = 10 minutes
2013-02-22 16:02:20 EST ERROR  Slony-I schema version is 2.1.2
2013-02-22 16:02:20 EST ERROR  please upgrade Slony-I schema to version 2.1.3
2013-02-22 16:02:20 EST FATAL  main: Node has wrong Slony-I schema or module version loaded
2013-02-22 16:02:20 EST CONFIG slon: child terminated signal: 9; pid: 8098, current worker pid: 8098
2013-02-22 16:02:20 EST INFO   slon: done
2013-02-22 16:02:20 EST INFO   slon: exit(0)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20130222/f7cfb1e4/attachment.htm 

From ssinger at ca.afilias.info  Fri Feb 22 14:25:09 2013
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 22 Feb 2013 17:25:09 -0500
Subject: [Slony1-general] Schema upgrade problem
In-Reply-To: <F024DCE3402750409CD407C414C07E1A16C37EFF@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A16C37EFF@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <5127F045.2030207@ca.afilias.info>

On 13-02-22 04:20 PM, Mike James wrote:

See http://www.slony.info/documentation/2.1/stmtupdatefunctions.html

You need to use the UPDATE FUNCTIONS  command for each node in your cluster


> Hi, I?m used the pgdg92 (PostgreSQL 9.2.6 ? x86_64) repo to install
> postgesql 9.2 and slony1-92 on a Centos 6 machine. After running yum
> update recently, my slony1 was updated to slony1-92-2.1.3-1.rhel6.x86_64.
>
> Now when I run slony1, I get the following entries created in
> slond-system.log, which terminate with an error:
>
> 2013-02-22 16:02:20 EST CONFIG main: slon version 2.1.3 starting up
>
> 2013-02-22 16:02:20 EST INFO   slon: watchdog process started
>
> 2013-02-22 16:02:20 EST CONFIG slon: watchdog ready - pid = 8097
>
> 2013-02-22 16:02:20 EST CONFIG slon: worker process created - pid = 8098
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option vac_frequency = 3
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option log_level = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option sync_interval = 2000
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option
> sync_interval_timeout = 10000
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option sync_group_maxsize = 20
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option desired_sync_time =
> 60000
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option syslog = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option quit_sync_provider = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option sync_max_rowsize = 8192
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option sync_max_largemem =
> 5242880
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option
> remote_listen_timeout = 300
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option monitor_interval = 500
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option explain_interval = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option tcp_keepalive_idle = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option
> tcp_keepalive_interval = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Integer option tcp_keepalive_count = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Boolean option log_pid = 0
>
> 2013-02-22 16:02:20 EST CONFIG main: Boolean option log_timestamp = 1
>
> 2013-02-22 16:02:20 EST CONFIG main: Boolean option tcp_keepalive = 1
>
> 2013-02-22 16:02:20 EST CONFIG main: Boolean option monitor_threads = 1
>
> 2013-02-22 16:02:20 EST CONFIG main: Real option real_placeholder = 0.000000
>
> 2013-02-22 16:02:20 EST CONFIG main: String option cluster_name =
> sys_cluster
>
> 2013-02-22 16:02:20 EST CONFIG main: String option conn_info = dbname=system
>
> user=slonyusr host=192.168.1.1 password=amessofit
>
> 2013-02-22 16:02:20 EST CONFIG main: String option pid_file = [NULL]
>
> 2013-02-22 16:02:20 EST CONFIG main: String option log_timestamp_format
> = %Y-%m-%d %H:%M:%S %Z
>
> 2013-02-22 16:02:20 EST CONFIG main: String option archive_dir = [NULL]
>
> 2013-02-22 16:02:20 EST CONFIG main: String option sql_on_connection =
> [NULL]
>
> 2013-02-22 16:02:20 EST CONFIG main: String option lag_interval = [NULL]
>
> 2013-02-22 16:02:20 EST CONFIG main: String option command_on_logarchive
> = [NULL]
>
> 2013-02-22 16:02:20 EST CONFIG main: String option syslog_facility = LOCAL0
>
> 2013-02-22 16:02:20 EST CONFIG main: String option syslog_ident = slon
>
> 2013-02-22 16:02:20 EST CONFIG main: String option cleanup_interval = 10
> minutes
>
> 2013-02-22 16:02:20 EST ERROR  Slony-I schema version is 2.1.2
>
> 2013-02-22 16:02:20 EST ERROR  please upgrade Slony-I schema to version
> 2.1.3
>
> 2013-02-22 16:02:20 EST FATAL  main: Node has wrong Slony-I schema or
> module version loaded
>
> 2013-02-22 16:02:20 EST CONFIG slon: child terminated signal: 9; pid:
> 8098, current worker pid: 8098
>
> 2013-02-22 16:02:20 EST INFO   slon: done
>
> 2013-02-22 16:02:20 EST INFO   slon: exit(0)
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


