From mark.steben at drivedominion.com  Wed Feb 14 09:25:43 2018
From: mark.steben at drivedominion.com (Mark Steben)
Date: Wed, 14 Feb 2018 12:25:43 -0500
Subject: [Slony1-general] running slony1 2.2.5
Message-ID: <CADyzmyxQ2vZPb5cNFwZOY3r1OPuUQT-LCYNUq10omvC33dOfeA@mail.gmail.com>

Good morning, hope you can help.

I use the perlscripts provided in the installation.  I wanted to create and
subscribe a new set with one table

Here is the set info from the slon_tools file:

#       "set20" => {
#        "set_id" => 20,
#        "table_id"    => 416,
#        "sequence_id" => 340,
#        "pkeyedtables" => [
#"public.dba_food_preferences",
# ],
# "sequences" =>
#             [
#"public.dba_food_preferences_dbafood_pkey_seq",
#]
#},

I issued a ./slonik_create_set for set 20 - no problem
I issued a ./slonik_subscribe set for set 20 node2 and got this error:

 remoteWorkerThread_1: provider node -1 for set 20not found in runtime
configuration

I issued unsubscribe_set , drop_set, stopped and restarted the slon daemon
and still get this error.
In fact slony is not working at all.  It looks like it continues to add the
previous active sets then errors out:


2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: update provider
configuration
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 1 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 2 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 3 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 4 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 5 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 6 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 7 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 8 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 9 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 10 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 11 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 12 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 13 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 14 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 15 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 16 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 17 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 18 to
provider 1
2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 19 to
provider 1
2018-02-14 12:17:20 EST CONFIG cleanupThread: bias = 60
2018-02-14 12:17:20 EST CONFIG version for "host=10.93.156.52
dbname=mavmail user=slony port=5432" is 90408
2018-02-14 12:17:20 EST INFO   copy_set 20 - omit=f - bool=0
2018-02-14 12:17:20 EST INFO   omit is FALSE
2018-02-14 12:17:20 EST ERROR  remoteWorkerThread_1: provider node -1 for
set 20not found in runtime configuration
2018-02-14 12:17:20 EST CONFIG slon: child terminated signal: 9; pid:
24519, current worker pid: 24519
2018-02-14 12:17:20 EST CONFIG slon: restart of worker in 10 seconds


Any ideas short of uninstalling the entire package, that might get me
around this?

Thank you


-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase
<http://www.autobase.net/>
  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>

-- 
 <https://www.drivedominion.com/transform-your-vue/>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20180214/b6cf2248/attachment.htm 

From mark.steben at drivedominion.com  Wed Feb 14 10:20:53 2018
From: mark.steben at drivedominion.com (Mark Steben)
Date: Wed, 14 Feb 2018 13:20:53 -0500
Subject: [Slony1-general] running slony1 2.2.5
In-Reply-To: <CADyzmyxQ2vZPb5cNFwZOY3r1OPuUQT-LCYNUq10omvC33dOfeA@mail.gmail.com>
References: <CADyzmyxQ2vZPb5cNFwZOY3r1OPuUQT-LCYNUq10omvC33dOfeA@mail.gmail.com>
Message-ID: <CADyzmyxV3HGxZwGzzBDKJ6pU9YuO9MrUdBRqBMxOVf9JziXs8Q@mail.gmail.com>

Good morning I thought I would pass additional info from the slony tables
to help add a bit of context to the picture :

mavmail=# select ev_origin, ev_seqno, ev_timestamp, ev_snapshot, ev_type,
ev_data1, ev_data2, ev_data3, ev_data4  from _replicationtwo.sl_event where
ev_type <> 'SYNC' order by ev_timestamp;
 ev_origin |  ev_seqno  |         ev_timestamp          |
 ev_snapshot                 |       ev_type       | ev_data1 | ev_data2 |
                 ev_data3                   |
 ev_data4
-----------+------------+-------------------------------+---
------------------------------------------+-----------------
----+----------+----------+---------------------------------
-------------+-------------------------------------------------------
         1 | 5000880660 | 2018-02-14 11:02:03.762749-05 |
3704200728:3704209595:3704200728                       | SUBSCRIBE_SET
 | 20       | 1        | 2                                            | t
         1 | 5000880661 | 2018-02-14 11:02:03.762749-05 |
3704200728:3704209595:3704200728                       |
ENABLE_SUBSCRIPTION | 20       | 1        | 2
              | t
         1 | 5000881066 | 2018-02-14 11:11:00.982413-05 |
3704200728:3704217695:3704200728                       | DROP_SET
  | 20       |          |                                              |
         1 | 5000881076 | 2018-02-14 11:11:28.673615-05 |
3704200728:3704217775:3704200728                       | STORE_SET
 | 20       | 1        | Set 20 (set20) for replicationtwo            |
         1 | 5000881078 | 2018-02-14 11:11:28.795985-05 |
3704200728:3704217778:3704200728                       | SET_ADD_TABLE
 | 20       | 416      | public.dba_food_preferences                  |
dbafood_pkey
         1 | 5000881081 | 2018-02-14 11:11:29.076247-05 |
3704200728:3704217783:3704200728                       | SET_ADD_SEQUENCE
  | 20       | 340      | public.dba_food_preferences_dbafood_pkey_seq |
Sequence public.dba_food_preferences_dbafood_pkey_seq
         1 | 5000881236 | 2018-02-14 11:14:58.638468-05 |
3704200728:3704219464:3704200728                       | SUBSCRIBE_SET
 | 20       | 1        | 2                                            | t
         1 | 5000881237 | 2018-02-14 11:14:58.638468-05 |
3704200728:3704219464:3704200728                       |
ENABLE_SUBSCRIPTION | 20       | 1        | 2
              | t
         2 | 5000160982 | 2018-02-14 11:15:37.34594-05  |
154224842:154224842:                                                |
UNSUBSCRIBE_SET     | 20       | 2        |
              |
         1 | 5000881411 | 2018-02-14 11:20:11.76759-05  |
3704200728:3704220797:3704200728                        | DROP_SET
  | 20       |          |                                              |
         1 | 5000881419 | 2018-02-14 11:20:27.813263-05 |
3704200728:3704220881:3704200728,3704220866 | STORE_SET           | 20
 | 1        | Set 20 (set20) for replicationtwo            |
         1 | 5000881421 | 2018-02-14 11:20:27.922722-05 |
3704200728:3704220886:3704200728,3704220866 | SET_ADD_TABLE       | 20
 | 416      | public.dba_food_preferences                  | dbafood_pkey
         1 | 5000881423 | 2018-02-14 11:20:28.120457-05 |
3704200728:3704220888:3704200728,3704220866  | SET_ADD_SEQUENCE    | 20
   | 340      | public.dba_food_preferences_dbafood_pkey_seq | Sequence
public.dba_food_preferences_dbafood_pkey_seq
         2 | 5000160983 | 2018-02-14 11:21:33.455822-05 |
154225614:154225614:                                                |
UNSUBSCRIBE_SET     | 20       | 2        |
              |
         1 | 5000881694 | 2018-02-14 11:28:43.306001-05 |
3704200728:3704226617:3704200728                        | DROP_SET
  | 20       |          |                                              |
         2 | 5000160984 | 2018-02-14 11:31:39.175163-05 |
154226919:154226919:                                                |
UNSUBSCRIBE_SET     | 20       | 2        |
              |
         2 | 5000160985 | 2018-02-14 11:37:23.732968-05 |
154227660:154227660:                                                |
UNSUBSCRIBE_SET     | 20       | 2        |
              |
         2 | 5000160986 | 2018-02-14 11:37:31.31214-05  |
154227677:154227677:                                                 |
UNSUBSCRIBE_SET     | 20       | 2        |
              |
         1 | 5000883198 | 2018-02-14 12:09:56.431871-05 |
3704247815:3704247815:                                            |
DROP_SET            | 19       |          |
              |
         2 | 5000160987 | 2018-02-14 12:11:25.380175-05 |
154232130:154232130:                                                 |
UNSUBSCRIBE_SET     | 19       | 2        |
              |
(20 rows)

 mavmail=# select * from _replicationtwo.sl_path;
 pa_server | pa_client |                      pa_conninfo
    | pa_connretry
-----------+-----------+------------------------------------
-------------------+--------------
         2 |         1 | host=10.93.156.52 dbname=mavmail user=slony
port=5432 |           10
         1 |         2 | host=10.93.156.47 dbname=mavmail user=slony
port=5432 |           10
(2 rows)

 select * from _replicationtwo.sl_listen;
 li_origin | li_provider | li_receiver
-----------+-------------+-------------
         2 |           2 |           1
         1 |           1 |           2

 select * from _replicationtwo.sl_apply_stats
;
-[ RECORD 1 ]--------+------------------------------
as_origin            | 2
as_num_insert        | 0
as_num_update        | 0
as_num_delete        | 0
as_num_truncate      | 0
as_num_script        | 0
as_num_total         | 0
as_duration          | 00:07:32.004
as_apply_first       | 2018-01-26 19:13:43.434628-05
as_apply_last        | 2018-02-14 11:13:51.191299-05
as_cache_prepare     | 0
as_cache_hit         | 0
as_cache_evict       | 0
as_cache_prepare_max | 0

Thanks for your time.



On Wed, Feb 14, 2018 at 12:25 PM, Mark Steben <mark.steben at drivedominion.com
> wrote:

> Good morning, hope you can help.
>
> I use the perlscripts provided in the installation.  I wanted to create
> and subscribe a new set with one table
>
> Here is the set info from the slon_tools file:
>
> #       "set20" => {
> #        "set_id" => 20,
> #        "table_id"    => 416,
> #        "sequence_id" => 340,
> #        "pkeyedtables" => [
> #"public.dba_food_preferences",
> # ],
> # "sequences" =>
> #             [
> #"public.dba_food_preferences_dbafood_pkey_seq",
> #]
> #},
>
> I issued a ./slonik_create_set for set 20 - no problem
> I issued a ./slonik_subscribe set for set 20 node2 and got this error:
>
>  remoteWorkerThread_1: provider node -1 for set 20not found in runtime
> configuration
>
> I issued unsubscribe_set , drop_set, stopped and restarted the slon daemon
> and still get this error.
> In fact slony is not working at all.  It looks like it continues to add
> the previous active sets then errors out:
>
>
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: update provider
> configuration
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 1 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 2 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 3 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 4 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 5 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 6 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 7 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 8 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 9 to
> provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 10
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 11
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 12
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 13
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 14
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 15
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 16
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 17
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 18
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG remoteWorkerThread_1: added active set 19
> to provider 1
> 2018-02-14 12:17:20 EST CONFIG cleanupThread: bias = 60
> 2018-02-14 12:17:20 EST CONFIG version for "host=10.93.156.52
> dbname=mavmail user=slony port=5432" is 90408
> 2018-02-14 12:17:20 EST INFO   copy_set 20 - omit=f - bool=0
> 2018-02-14 12:17:20 EST INFO   omit is FALSE
> 2018-02-14 12:17:20 EST ERROR  remoteWorkerThread_1: provider node -1 for
> set 20not found in runtime configuration
> 2018-02-14 12:17:20 EST CONFIG slon: child terminated signal: 9; pid:
> 24519, current worker pid: 24519
> 2018-02-14 12:17:20 EST CONFIG slon: restart of worker in 10 seconds
>
>
> Any ideas short of uninstalling the entire package, that might get me
> around this?
>
> Thank you
>
>
> --
> *Mark Steben*
>  Database Administrator
> @utoRevenue <http://www.autorevenue.com/> | Autobase
> <http://www.autobase.net/>
>   CRM division of Dominion Dealer Solutions
> 95D Ashley Ave.
> West Springfield, MA 01089
> t: 413.327-3045
> f: 413.383-9567
>
> www.fb.com/DominionDealerSolutions
> www.twitter.com/DominionDealer
>  www.drivedominion.com <http://www.autorevenue.com/>
>
> <http://autobasedigital.net/marketing/DD12_sig.jpg>
>
>
>
>


-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase
<http://www.autobase.net/>
  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>

-- 
 <https://www.drivedominion.com/transform-your-vue/>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20180214/5ad1dd31/attachment-0001.htm 

From ttignor at akamai.com  Thu Feb 22 13:25:42 2018
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 22 Feb 2018 21:25:42 +0000
Subject: [Slony1-general] slony1 drop node failure
Message-ID: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>


                Hello slony1 community,
                We have a head scratcher here. It appears a DROP NODE command was not fully processed. The command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is there a potential race condition somewhere which could leave behind state?
                Thanks in advance,

---- master log replication freeze error ----
2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot, ev\
_type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert into "_ams_cluster".sl_confirm       (con_origi\
n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint "sl_event-pkey"
DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
---- master log replication freeze error ----


---- master DB leftover event ----
ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
psql (9.1.24)
Type "help" for help.

ams=# select * from sl_event_bak;
 ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
data7 | ev_data8
-----------+------------+-------------------------------+--------------------+---------+----------+----------+----------+----------+----------+----------+----
------+----------
         8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          |          |          |          |          |
      |
(1 row)

ams=#
---- master DB leftover event ----

---- master log drop node record ----
2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
---- master log drop node record ----

---- replica log drop node record ----
2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
NOTICE:  Slony-I: Please drop schema "_ams_cluster"
2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
NOTICE:  drop cascades to 243 other objects
DETAIL:  drop cascades to table _ams_cluster.sl_node
drop cascades to table _ams_cluster.sl_nodelock
drop cascades to table _ams_cluster.sl_set
drop cascades to table _ams_cluster.sl_setsync
drop cascades to table _ams_cluster.sl_table
drop cascades to table _ams_cluster.sl_sequence
---- replica log drop node record ----


                Tom    ?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20180222/b8f14f30/attachment.htm 

From steve at ssinger.info  Thu Feb 22 15:02:30 2018
From: steve at ssinger.info (Steve Singer)
Date: Thu, 22 Feb 2018 18:02:30 -0500 (EST)
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
Message-ID: <alpine.DEB.2.11.1802221720080.14809@opti.atlantida>

On Thu, 22 Feb 2018, Tignor, Tom wrote:

Looks like?
http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html

I can't remember if that was what prompted 
http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html


https://github.com/ssinger/slony1-engine/tree/bug375

I can't seem to find a reason why this wasn't committed.


> 
> ?
> 
> ??????????????? Hello slony1 community,
> 
> ??????????????? We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
> command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
> restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
> showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
> is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
> there a potential race condition somewhere which could leave behind state?
> 
> ??????????????? Thanks in advance,
> 
> ?
> 
> ---- master log replication freeze error ----
> 
> 2018-02-21 21:38:52 UTC [5775] ERROR? remoteWorkerThread_8: "insert into "_ams_cluster".sl_event ? ? (ev_origin,
> ev_seqno, ev_timestamp,? ? ? ev_snapshot, ev\
> 
> _type ? ? ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
> into "_ams_cluster".sl_confirm ? ? ? (con_origi\
> 
> n, con_received, con_seqno, con_timestamp)? ? values (8, 1, '5002075962', now()); select
> "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
> 
> val); commit transaction;" PGRES_FATAL_ERROR ERROR:? duplicate key value violates unique constraint
> "sl_event-pkey"
> 
> DETAIL:? Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
> 
> 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
> 
> 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
> 
> ---- master log replication freeze error ----
> 
> ?
> 
> ?
> 
> ---- master DB leftover event ----
> 
> ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
> 
> psql (9.1.24)
> 
> Type "help" for help.
> 
> ?
> 
> ams=# select * from sl_event_bak;
> 
> ?ev_origin |? ev_seqno? | ? ? ? ? ev_timestamp? ? ? ? ? |? ? ev_snapshot ? ? | ev_type | ev_data1 | ev_data2 |
> ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
> 
> data7 | ev_data8?
> 
> -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
> ---------+----------+----------+----------+----
> 
> ------+----------
> 
> ?? ? ? ? 8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC? ? |? ? ? ? ? |? ? ? ? ? |?
> ? ? ? ? |? ? ? ? ? |? ? ? ? ? |? ? ? ? ? | ? ?
> 
> ? ? ? |?
> 
> (1 row)
> 
> ?
> 
> ams=#?
> 
> ---- master DB leftover event ----
> 
> ?
> 
> ---- master log drop node record ----
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
> 
> 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
> 
> ---- master log drop node record ----
> 
> ?
> 
> ---- replica log drop node record ----
> 
> 2018-02-21 19:19:51 UTC [22650] WARN ? remoteWorkerThread_1: got DROP NODE for local node ID
> 
> NOTICE:? Slony-I: Please drop schema "_ams_cluster"
> 
> 2018-02-21 19:19:53 UTC [22650] INFO ? remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
> 
> NOTICE:? drop cascades to 243 other objects
> 
> DETAIL:? drop cascades to table _ams_cluster.sl_node
> 
> drop cascades to table _ams_cluster.sl_nodelock
> 
> drop cascades to table _ams_cluster.sl_set
> 
> drop cascades to table _ams_cluster.sl_setsync
> 
> drop cascades to table _ams_cluster.sl_table
> 
> drop cascades to table _ams_cluster.sl_sequence
> 
> ---- replica log drop node record ----
> 
> ?
> 
> ?
> 
> ??????????????? Tom??? ?
> 
> ?
> 
> ?
> 
> 
>

From ttignor at akamai.com  Thu Feb 22 15:11:36 2018
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 22 Feb 2018 23:11:36 +0000
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
Message-ID: <E80D0B4D-B251-470E-B52D-9863FED2ACE5@akamai.com>


	Wow. Yes, this looks a lot like the tmblue 9/10/16 event. I?ll be very interested to take a look at that patch.
	Haven?t forgotten the 2.2.6 patches I have in flight. Need to finish some other work and pull those off the back burner?
	Thanks,

	Tom    (


On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:

    On Thu, 22 Feb 2018, Tignor, Tom wrote:
    
    Looks like?
    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
    
    I can't remember if that was what prompted 
    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
    
    
    https://github.com/ssinger/slony1-engine/tree/bug375
    
    I can't seem to find a reason why this wasn't committed.
    
    
    > 
    >  
    > 
    >                 Hello slony1 community,
    > 
    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
    > there a potential race condition somewhere which could leave behind state?
    > 
    >                 Thanks in advance,
    > 
    >  
    > 
    > ---- master log replication freeze error ----
    > 
    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
    > 
    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
    > into "_ams_cluster".sl_confirm       (con_origi\
    > 
    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
    > 
    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
    > "sl_event-pkey"
    > 
    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
    > 
    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
    > 
    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
    > 
    > ---- master log replication freeze error ----
    > 
    >  
    > 
    >  
    > 
    > ---- master DB leftover event ----
    > 
    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
    > 
    > psql (9.1.24)
    > 
    > Type "help" for help.
    > 
    >  
    > 
    > ams=# select * from sl_event_bak;
    > 
    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
    > 
    > data7 | ev_data8 
    > 
    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
    > ---------+----------+----------+----------+----
    > 
    > ------+----------
    > 
    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          | 
    >         |          |          |          |    
    > 
    >       | 
    > 
    > (1 row)
    > 
    >  
    > 
    > ams=# 
    > 
    > ---- master DB leftover event ----
    > 
    >  
    > 
    > ---- master log drop node record ----
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
    > 
    > ---- master log drop node record ----
    > 
    >  
    > 
    > ---- replica log drop node record ----
    > 
    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
    > 
    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
    > 
    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
    > 
    > NOTICE:  drop cascades to 243 other objects
    > 
    > DETAIL:  drop cascades to table _ams_cluster.sl_node
    > 
    > drop cascades to table _ams_cluster.sl_nodelock
    > 
    > drop cascades to table _ams_cluster.sl_set
    > 
    > drop cascades to table _ams_cluster.sl_setsync
    > 
    > drop cascades to table _ams_cluster.sl_table
    > 
    > drop cascades to table _ams_cluster.sl_sequence
    > 
    > ---- replica log drop node record ----
    > 
    >  
    > 
    >  
    > 
    >                 Tom    ?
    > 
    >  
    > 
    >  
    > 
    > 
    >
    


From ttignor at akamai.com  Mon Feb 26 07:32:42 2018
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 26 Feb 2018 15:32:42 +0000
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
Message-ID: <499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>


	Steve,
	The patch link actually goes to a page which says ?Bugzilla is down for maintenance.? Is there a way to see the patch currently? Does it exist or is it scheduled in some Slony-I release?

	Tom    (


On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:

    On Thu, 22 Feb 2018, Tignor, Tom wrote:
    
    Looks like?
    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
    
    I can't remember if that was what prompted 
    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
    
    
    https://github.com/ssinger/slony1-engine/tree/bug375
    
    I can't seem to find a reason why this wasn't committed.
    
    
    > 
    >  
    > 
    >                 Hello slony1 community,
    > 
    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
    > there a potential race condition somewhere which could leave behind state?
    > 
    >                 Thanks in advance,
    > 
    >  
    > 
    > ---- master log replication freeze error ----
    > 
    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
    > 
    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
    > into "_ams_cluster".sl_confirm       (con_origi\
    > 
    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
    > 
    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
    > "sl_event-pkey"
    > 
    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
    > 
    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
    > 
    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
    > 
    > ---- master log replication freeze error ----
    > 
    >  
    > 
    >  
    > 
    > ---- master DB leftover event ----
    > 
    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
    > 
    > psql (9.1.24)
    > 
    > Type "help" for help.
    > 
    >  
    > 
    > ams=# select * from sl_event_bak;
    > 
    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
    > 
    > data7 | ev_data8 
    > 
    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
    > ---------+----------+----------+----------+----
    > 
    > ------+----------
    > 
    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          | 
    >         |          |          |          |    
    > 
    >       | 
    > 
    > (1 row)
    > 
    >  
    > 
    > ams=# 
    > 
    > ---- master DB leftover event ----
    > 
    >  
    > 
    > ---- master log drop node record ----
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
    > 
    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
    > 
    > ---- master log drop node record ----
    > 
    >  
    > 
    > ---- replica log drop node record ----
    > 
    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
    > 
    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
    > 
    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
    > 
    > NOTICE:  drop cascades to 243 other objects
    > 
    > DETAIL:  drop cascades to table _ams_cluster.sl_node
    > 
    > drop cascades to table _ams_cluster.sl_nodelock
    > 
    > drop cascades to table _ams_cluster.sl_set
    > 
    > drop cascades to table _ams_cluster.sl_setsync
    > 
    > drop cascades to table _ams_cluster.sl_table
    > 
    > drop cascades to table _ams_cluster.sl_sequence
    > 
    > ---- replica log drop node record ----
    > 
    >  
    > 
    >  
    > 
    >                 Tom    ?
    > 
    >  
    > 
    >  
    > 
    > 
    >
    


From steve at ssinger.info  Mon Feb 26 07:59:46 2018
From: steve at ssinger.info (Steve Singer)
Date: Mon, 26 Feb 2018 10:59:46 -0500 (EST)
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
	<499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>
Message-ID: <alpine.DEB.2.11.1802261059200.14809@opti.atlantida>

On Mon, 26 Feb 2018, Tignor, Tom wrote:

You can get it from the github branch (latest commit) at 
https://github.com/ssinger/slony1-engine/tree/bug375




>
> 	Steve,
> 	The patch link actually goes to a page which says ?Bugzilla is down for maintenance.? Is there a way to see the patch currently? Does it exist or is it scheduled in some Slony-I release?
>
> 	Tom    (
>
>
> On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:
>
>    On Thu, 22 Feb 2018, Tignor, Tom wrote:
>
>    Looks like?
>    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
>
>    I can't remember if that was what prompted
>    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
>
>
>    https://github.com/ssinger/slony1-engine/tree/bug375
>
>    I can't seem to find a reason why this wasn't committed.
>
>
>    >
>    >
>    >
>    >                 Hello slony1 community,
>    >
>    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
>    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
>    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
>    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
>    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
>    > there a potential race condition somewhere which could leave behind state?
>    >
>    >                 Thanks in advance,
>    >
>    >
>    >
>    > ---- master log replication freeze error ----
>    >
>    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
>    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
>    >
>    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
>    > into "_ams_cluster".sl_confirm       (con_origi\
>    >
>    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
>    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
>    >
>    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
>    > "sl_event-pkey"
>    >
>    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
>    >
>    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
>    >
>    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
>    >
>    > ---- master log replication freeze error ----
>    >
>    >
>    >
>    >
>    >
>    > ---- master DB leftover event ----
>    >
>    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
>    >
>    > psql (9.1.24)
>    >
>    > Type "help" for help.
>    >
>    >
>    >
>    > ams=# select * from sl_event_bak;
>    >
>    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
>    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
>    >
>    > data7 | ev_data8
>    >
>    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
>    > ---------+----------+----------+----------+----
>    >
>    > ------+----------
>    >
>    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          |
>    >         |          |          |          |
>    >
>    >       |
>    >
>    > (1 row)
>    >
>    >
>    >
>    > ams=#
>    >
>    > ---- master DB leftover event ----
>    >
>    >
>    >
>    > ---- master log drop node record ----
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
>    >
>    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
>    >
>    > ---- master log drop node record ----
>    >
>    >
>    >
>    > ---- replica log drop node record ----
>    >
>    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
>    >
>    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>    >
>    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
>    >
>    > NOTICE:  drop cascades to 243 other objects
>    >
>    > DETAIL:  drop cascades to table _ams_cluster.sl_node
>    >
>    > drop cascades to table _ams_cluster.sl_nodelock
>    >
>    > drop cascades to table _ams_cluster.sl_set
>    >
>    > drop cascades to table _ams_cluster.sl_setsync
>    >
>    > drop cascades to table _ams_cluster.sl_table
>    >
>    > drop cascades to table _ams_cluster.sl_sequence
>    >
>    > ---- replica log drop node record ----
>    >
>    >
>    >
>    >
>    >
>    >                 Tom    ?
>    >
>    >
>    >
>    >
>    >
>    >
>    >
>
>
>

From ttignor at akamai.com  Mon Feb 26 08:25:44 2018
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 26 Feb 2018 16:25:44 +0000
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <alpine.DEB.2.11.1802261059200.14809@opti.atlantida>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
	<499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>
	<alpine.DEB.2.11.1802261059200.14809@opti.atlantida>
Message-ID: <A3993CAB-639D-4028-91FF-40907187E52C@akamai.com>


	Thanks. I see the deletes added for sl_seqlog and sl_log_script. The constraint violation appearing in the errors was for sl_event. Do we expect these changes fully remove all state, including sl_event? The checkNodeDeleted function doesn?t look at sl_event.

	Tom    (


On 2/26/18, 11:03 AM, "Steve Singer" <steve at ssinger.info> wrote:

    On Mon, 26 Feb 2018, Tignor, Tom wrote:
    
    You can get it from the github branch (latest commit) at 
    https://github.com/ssinger/slony1-engine/tree/bug375
    
    
    
    
    >
    > 	Steve,
    > 	The patch link actually goes to a page which says ?Bugzilla is down for maintenance.? Is there a way to see the patch currently? Does it exist or is it scheduled in some Slony-I release?
    >
    > 	Tom    (
    >
    >
    > On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:
    >
    >    On Thu, 22 Feb 2018, Tignor, Tom wrote:
    >
    >    Looks like?
    >    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
    >
    >    I can't remember if that was what prompted
    >    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
    >
    >
    >    https://github.com/ssinger/slony1-engine/tree/bug375
    >
    >    I can't seem to find a reason why this wasn't committed.
    >
    >
    >    >
    >    >
    >    >
    >    >                 Hello slony1 community,
    >    >
    >    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
    >    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
    >    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
    >    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
    >    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
    >    > there a potential race condition somewhere which could leave behind state?
    >    >
    >    >                 Thanks in advance,
    >    >
    >    >
    >    >
    >    > ---- master log replication freeze error ----
    >    >
    >    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
    >    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
    >    >
    >    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
    >    > into "_ams_cluster".sl_confirm       (con_origi\
    >    >
    >    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
    >    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
    >    >
    >    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
    >    > "sl_event-pkey"
    >    >
    >    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
    >    >
    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
    >    >
    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
    >    >
    >    > ---- master log replication freeze error ----
    >    >
    >    >
    >    >
    >    >
    >    >
    >    > ---- master DB leftover event ----
    >    >
    >    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
    >    >
    >    > psql (9.1.24)
    >    >
    >    > Type "help" for help.
    >    >
    >    >
    >    >
    >    > ams=# select * from sl_event_bak;
    >    >
    >    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
    >    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
    >    >
    >    > data7 | ev_data8
    >    >
    >    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
    >    > ---------+----------+----------+----------+----
    >    >
    >    > ------+----------
    >    >
    >    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          |
    >    >         |          |          |          |
    >    >
    >    >       |
    >    >
    >    > (1 row)
    >    >
    >    >
    >    >
    >    > ams=#
    >    >
    >    > ---- master DB leftover event ----
    >    >
    >    >
    >    >
    >    > ---- master log drop node record ----
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
    >    >
    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
    >    >
    >    > ---- master log drop node record ----
    >    >
    >    >
    >    >
    >    > ---- replica log drop node record ----
    >    >
    >    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
    >    >
    >    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
    >    >
    >    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
    >    >
    >    > NOTICE:  drop cascades to 243 other objects
    >    >
    >    > DETAIL:  drop cascades to table _ams_cluster.sl_node
    >    >
    >    > drop cascades to table _ams_cluster.sl_nodelock
    >    >
    >    > drop cascades to table _ams_cluster.sl_set
    >    >
    >    > drop cascades to table _ams_cluster.sl_setsync
    >    >
    >    > drop cascades to table _ams_cluster.sl_table
    >    >
    >    > drop cascades to table _ams_cluster.sl_sequence
    >    >
    >    > ---- replica log drop node record ----
    >    >
    >    >
    >    >
    >    >
    >    >
    >    >                 Tom    ?
    >    >
    >    >
    >    >
    >    >
    >    >
    >    >
    >    >
    >
    >
    >
    


From steve at ssinger.info  Mon Feb 26 08:57:36 2018
From: steve at ssinger.info (Steve Singer)
Date: Mon, 26 Feb 2018 11:57:36 -0500 (EST)
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <A3993CAB-639D-4028-91FF-40907187E52C@akamai.com>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
	<499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>
	<alpine.DEB.2.11.1802261059200.14809@opti.atlantida>
	<A3993CAB-639D-4028-91FF-40907187E52C@akamai.com>
Message-ID: <alpine.DEB.2.11.1802261133090.14809@opti.atlantida>

On Mon, 26 Feb 2018, Tignor, Tom wrote:

>
> 	Thanks. I see the deletes added for sl_seqlog and sl_log_script. The 
> constraint violation appearing in the errors was for sl_event. Do we 
> expect these changes fully remove all state, including sl_event? The 
> checkNodeDeleted function doesn?t look at sl_event.

It could be that this is a different issue then.
That function (the dropNode_int) should have removed the rows from sl_event. 
The question the becomes did it not remove them for some reason, or did they 
get added back later, and if so how?


> 	Tom    (
>
>
> On 2/26/18, 11:03 AM, "Steve Singer" <steve at ssinger.info> wrote:
>
>    On Mon, 26 Feb 2018, Tignor, Tom wrote:
>
>    You can get it from the github branch (latest commit) at
>    https://github.com/ssinger/slony1-engine/tree/bug375
>
>
>
>
>    >
>    > 	Steve,
>    > 	The patch link actually goes to a page which says ?Bugzilla is down for maintenance.? Is there a way to see the patch currently? Does it exist or is it scheduled in some Slony-I release?
>    >
>    > 	Tom    (
>    >
>    >
>    > On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:
>    >
>    >    On Thu, 22 Feb 2018, Tignor, Tom wrote:
>    >
>    >    Looks like?
>    >    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
>    >
>    >    I can't remember if that was what prompted
>    >    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
>    >
>    >
>    >    https://github.com/ssinger/slony1-engine/tree/bug375
>    >
>    >    I can't seem to find a reason why this wasn't committed.
>    >
>    >
>    >    >
>    >    >
>    >    >
>    >    >                 Hello slony1 community,
>    >    >
>    >    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
>    >    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
>    >    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
>    >    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
>    >    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
>    >    > there a potential race condition somewhere which could leave behind state?
>    >    >
>    >    >                 Thanks in advance,
>    >    >
>    >    >
>    >    >
>    >    > ---- master log replication freeze error ----
>    >    >
>    >    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
>    >    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
>    >    >
>    >    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
>    >    > into "_ams_cluster".sl_confirm       (con_origi\
>    >    >
>    >    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
>    >    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
>    >    >
>    >    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
>    >    > "sl_event-pkey"
>    >    >
>    >    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
>    >    >
>    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
>    >    >
>    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
>    >    >
>    >    > ---- master log replication freeze error ----
>    >    >
>    >    >
>    >    >
>    >    >
>    >    >
>    >    > ---- master DB leftover event ----
>    >    >
>    >    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
>    >    >
>    >    > psql (9.1.24)
>    >    >
>    >    > Type "help" for help.
>    >    >
>    >    >
>    >    >
>    >    > ams=# select * from sl_event_bak;
>    >    >
>    >    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
>    >    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
>    >    >
>    >    > data7 | ev_data8
>    >    >
>    >    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
>    >    > ---------+----------+----------+----------+----
>    >    >
>    >    > ------+----------
>    >    >
>    >    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          |
>    >    >         |          |          |          |
>    >    >
>    >    >       |
>    >    >
>    >    > (1 row)
>    >    >
>    >    >
>    >    >
>    >    > ams=#
>    >    >
>    >    > ---- master DB leftover event ----
>    >    >
>    >    >
>    >    >
>    >    > ---- master log drop node record ----
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
>    >    >
>    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
>    >    >
>    >    > ---- master log drop node record ----
>    >    >
>    >    >
>    >    >
>    >    > ---- replica log drop node record ----
>    >    >
>    >    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
>    >    >
>    >    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>    >    >
>    >    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
>    >    >
>    >    > NOTICE:  drop cascades to 243 other objects
>    >    >
>    >    > DETAIL:  drop cascades to table _ams_cluster.sl_node
>    >    >
>    >    > drop cascades to table _ams_cluster.sl_nodelock
>    >    >
>    >    > drop cascades to table _ams_cluster.sl_set
>    >    >
>    >    > drop cascades to table _ams_cluster.sl_setsync
>    >    >
>    >    > drop cascades to table _ams_cluster.sl_table
>    >    >
>    >    > drop cascades to table _ams_cluster.sl_sequence
>    >    >
>    >    > ---- replica log drop node record ----
>    >    >
>    >    >
>    >    >
>    >    >
>    >    >
>    >    >                 Tom    ?
>    >    >
>    >    >
>    >    >
>    >    >
>    >    >
>    >    >
>    >    >
>    >
>    >
>    >
>
>
>

From ttignor at akamai.com  Mon Feb 26 09:46:31 2018
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 26 Feb 2018 17:46:31 +0000
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <alpine.DEB.2.11.1802261133090.14809@opti.atlantida>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
	<499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>
	<alpine.DEB.2.11.1802261059200.14809@opti.atlantida>
	<A3993CAB-639D-4028-91FF-40907187E52C@akamai.com>
	<alpine.DEB.2.11.1802261133090.14809@opti.atlantida>
Message-ID: <89D3AEFD-20A5-4C96-B584-425C71F2315B@akamai.com>


	In the slony1 log of our primary host (the same one which later showed the bug) we had:

2018-02-21 19:19:49 UTC [22582] INFO   remoteWorkerThread_8: SYNC 5002075962 done in 1.725 seconds
2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8

	The timestamp of the problem event was somewhat earlier: 2018-02-21 19:19:41.
	To me it looks like there is a race condition and despite the order of the log events, the DROP NODE was processed first and the SYNC was accepted later, and thus created an sl_event record. Of course, that?s simply a guess.

	Tom    (


On 2/26/18, 12:01 PM, "Steve Singer" <steve at ssinger.info> wrote:

    On Mon, 26 Feb 2018, Tignor, Tom wrote:
    
    >
    > 	Thanks. I see the deletes added for sl_seqlog and sl_log_script. The 
    > constraint violation appearing in the errors was for sl_event. Do we 
    > expect these changes fully remove all state, including sl_event? The 
    > checkNodeDeleted function doesn?t look at sl_event.
    
    It could be that this is a different issue then.
    That function (the dropNode_int) should have removed the rows from sl_event. 
    The question the becomes did it not remove them for some reason, or did they 
    get added back later, and if so how?
    
    
    > 	Tom    (
    >
    >
    > On 2/26/18, 11:03 AM, "Steve Singer" <steve at ssinger.info> wrote:
    >
    >    On Mon, 26 Feb 2018, Tignor, Tom wrote:
    >
    >    You can get it from the github branch (latest commit) at
    >    https://github.com/ssinger/slony1-engine/tree/bug375
    >
    >
    >
    >
    >    >
    >    > 	Steve,
    >    > 	The patch link actually goes to a page which says ?Bugzilla is down for maintenance.? Is there a way to see the patch currently? Does it exist or is it scheduled in some Slony-I release?
    >    >
    >    > 	Tom    (
    >    >
    >    >
    >    > On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:
    >    >
    >    >    On Thu, 22 Feb 2018, Tignor, Tom wrote:
    >    >
    >    >    Looks like?
    >    >    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
    >    >
    >    >    I can't remember if that was what prompted
    >    >    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
    >    >
    >    >
    >    >    https://github.com/ssinger/slony1-engine/tree/bug375
    >    >
    >    >    I can't seem to find a reason why this wasn't committed.
    >    >
    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >                 Hello slony1 community,
    >    >    >
    >    >    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
    >    >    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
    >    >    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
    >    >    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
    >    >    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
    >    >    > there a potential race condition somewhere which could leave behind state?
    >    >    >
    >    >    >                 Thanks in advance,
    >    >    >
    >    >    >
    >    >    >
    >    >    > ---- master log replication freeze error ----
    >    >    >
    >    >    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
    >    >    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
    >    >    >
    >    >    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
    >    >    > into "_ams_cluster".sl_confirm       (con_origi\
    >    >    >
    >    >    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
    >    >    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
    >    >    >
    >    >    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
    >    >    > "sl_event-pkey"
    >    >    >
    >    >    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
    >    >    >
    >    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
    >    >    >
    >    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
    >    >    >
    >    >    > ---- master log replication freeze error ----
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    > ---- master DB leftover event ----
    >    >    >
    >    >    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
    >    >    >
    >    >    > psql (9.1.24)
    >    >    >
    >    >    > Type "help" for help.
    >    >    >
    >    >    >
    >    >    >
    >    >    > ams=# select * from sl_event_bak;
    >    >    >
    >    >    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
    >    >    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
    >    >    >
    >    >    > data7 | ev_data8
    >    >    >
    >    >    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
    >    >    > ---------+----------+----------+----------+----
    >    >    >
    >    >    > ------+----------
    >    >    >
    >    >    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          |
    >    >    >         |          |          |          |
    >    >    >
    >    >    >       |
    >    >    >
    >    >    > (1 row)
    >    >    >
    >    >    >
    >    >    >
    >    >    > ams=#
    >    >    >
    >    >    > ---- master DB leftover event ----
    >    >    >
    >    >    >
    >    >    >
    >    >    > ---- master log drop node record ----
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
    >    >    >
    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
    >    >    >
    >    >    > ---- master log drop node record ----
    >    >    >
    >    >    >
    >    >    >
    >    >    > ---- replica log drop node record ----
    >    >    >
    >    >    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
    >    >    >
    >    >    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
    >    >    >
    >    >    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
    >    >    >
    >    >    > NOTICE:  drop cascades to 243 other objects
    >    >    >
    >    >    > DETAIL:  drop cascades to table _ams_cluster.sl_node
    >    >    >
    >    >    > drop cascades to table _ams_cluster.sl_nodelock
    >    >    >
    >    >    > drop cascades to table _ams_cluster.sl_set
    >    >    >
    >    >    > drop cascades to table _ams_cluster.sl_setsync
    >    >    >
    >    >    > drop cascades to table _ams_cluster.sl_table
    >    >    >
    >    >    > drop cascades to table _ams_cluster.sl_sequence
    >    >    >
    >    >    > ---- replica log drop node record ----
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >                 Tom    ?
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >    >
    >    >
    >    >
    >    >
    >
    >
    >
    


From steve at ssinger.info  Wed Feb 28 19:24:28 2018
From: steve at ssinger.info (Steve Singer)
Date: Wed, 28 Feb 2018 22:24:28 -0500 (EST)
Subject: [Slony1-general] slony1 drop node failure
In-Reply-To: <89D3AEFD-20A5-4C96-B584-425C71F2315B@akamai.com>
References: <45A09441-A5F0-48DF-9EFB-9BBF7F4C5F43@akamai.com>
	<alpine.DEB.2.11.1802221720080.14809@opti.atlantida>
	<499EBAE0-FEAA-4188-83C2-A0DE7CF2E8F6@akamai.com>
	<alpine.DEB.2.11.1802261059200.14809@opti.atlantida>
	<A3993CAB-639D-4028-91FF-40907187E52C@akamai.com>
	<alpine.DEB.2.11.1802261133090.14809@opti.atlantida>
	<89D3AEFD-20A5-4C96-B584-425C71F2315B@akamai.com>
Message-ID: <alpine.DEB.2.11.1802282216430.14809@opti.atlantida>

On Mon, 26 Feb 2018, Tignor, Tom wrote:

>
> 	In the slony1 log of our primary host (the same one which later showed the bug) we had:
>
> 2018-02-21 19:19:49 UTC [22582] INFO   remoteWorkerThread_8: SYNC 5002075962 done in 1.725 seconds
> 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
>
> 	The timestamp of the problem event was somewhat earlier: 2018-02-21 
> 19:19:41.
> 	To me it looks like there is a race condition and despite the order 
> of the log events, the DROP NODE was processed first and the SYNC was 
> accepted later, and thus created an sl_event record. Of course, that?s 
> simply a guess.

This might be possible.

If the drop being droped is 8, and the event node from the drop is 4.
Some third node say 3 might have a sequence like this

remoteWorkerThread_8: Starts processing SYNC
remoteWorkerThread_4: process drop node. Updates runtime config
remoteWorkerThread_8: finishes processing sync COMMITS

I'd feel more comfortable if we had a way of reproducing this though.

One option might be to have the remoteWorkerThread recheck the runtime 
config before the COMMIT to make sure that the node is still active.




>
> 	Tom    (
>
>
> On 2/26/18, 12:01 PM, "Steve Singer" <steve at ssinger.info> wrote:
>
>    On Mon, 26 Feb 2018, Tignor, Tom wrote:
>
>    >
>    > 	Thanks. I see the deletes added for sl_seqlog and sl_log_script. The
>    > constraint violation appearing in the errors was for sl_event. Do we
>    > expect these changes fully remove all state, including sl_event? The
>    > checkNodeDeleted function doesn?t look at sl_event.
>
>    It could be that this is a different issue then.
>    That function (the dropNode_int) should have removed the rows from sl_event.
>    The question the becomes did it not remove them for some reason, or did they
>    get added back later, and if so how?
>
>
>    > 	Tom    (
>    >
>    >
>    > On 2/26/18, 11:03 AM, "Steve Singer" <steve at ssinger.info> wrote:
>    >
>    >    On Mon, 26 Feb 2018, Tignor, Tom wrote:
>    >
>    >    You can get it from the github branch (latest commit) at
>    >    https://github.com/ssinger/slony1-engine/tree/bug375
>    >
>    >
>    >
>    >
>    >    >
>    >    > 	Steve,
>    >    > 	The patch link actually goes to a page which says ?Bugzilla is down for maintenance.? Is there a way to see the patch currently? Does it exist or is it scheduled in some Slony-I release?
>    >    >
>    >    > 	Tom    (
>    >    >
>    >    >
>    >    > On 2/22/18, 6:06 PM, "Steve Singer" <steve at ssinger.info> wrote:
>    >    >
>    >    >    On Thu, 22 Feb 2018, Tignor, Tom wrote:
>    >    >
>    >    >    Looks like?
>    >    >    http://lists.slony.info/pipermail/slony1-general/2016-September/013331.html
>    >    >
>    >    >    I can't remember if that was what prompted
>    >    >    http://lists.slony.info/pipermail/slony1-hackers/2016-December/000560.html
>    >    >
>    >    >
>    >    >    https://github.com/ssinger/slony1-engine/tree/bug375
>    >    >
>    >    >    I can't seem to find a reason why this wasn't committed.
>    >    >
>    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >                 Hello slony1 community,
>    >    >    >
>    >    >    >                 We have a head scratcher here. It appears a DROP NODE command was not fully processed. The
>    >    >    > command was issued and confirmed on all our nodes at approximately 2018-02-21 19:19:50 UTC. When we went to
>    >    >    > restore it over two hours later, all replication stopped on an sl_event constraint violation. Investigation
>    >    >    > showed a SYNC event for the dropped node with a timestamp of just a few seconds before the drop. I believe this
>    >    >    > is a first for us. The DROP NODE command is supposed to remove all state for the dropped node. Is that right? Is
>    >    >    > there a potential race condition somewhere which could leave behind state?
>    >    >    >
>    >    >    >                 Thanks in advance,
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    > ---- master log replication freeze error ----
>    >    >    >
>    >    >    > 2018-02-21 21:38:52 UTC [5775] ERROR  remoteWorkerThread_8: "insert into "_ams_cluster".sl_event     (ev_origin,
>    >    >    > ev_seqno, ev_timestamp,      ev_snapshot, ev\
>    >    >    >
>    >    >    > _type     ) values ('8', '5002075962', '2018-02-21 19:19:41.958719+00', '87044110:87044110:', 'SYNC'); insert
>    >    >    > into "_ams_cluster".sl_confirm       (con_origi\
>    >    >    >
>    >    >    > n, con_received, con_seqno, con_timestamp)    values (8, 1, '5002075962', now()); select
>    >    >    > "_ams_cluster".logApplySaveStats('_ams_cluster', 8, '0.139 s'::inter\
>    >    >    >
>    >    >    > val); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates unique constraint
>    >    >    > "sl_event-pkey"
>    >    >    >
>    >    >    > DETAIL:  Key (ev_origin, ev_seqno)=(8, 5002075962) already exists.
>    >    >    >
>    >    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: child terminated signal: 9; pid: 5775, current worker pid: 5775
>    >    >    >
>    >    >    > 2018-02-21 21:38:52 UTC [13649] CONFIG slon: restart of worker in 10 seconds
>    >    >    >
>    >    >    > ---- master log replication freeze error ----
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    > ---- master DB leftover event ----
>    >    >    >
>    >    >    > ams at ams6.cmb.netmgmt:~$ psql -U akamai -d ams
>    >    >    >
>    >    >    > psql (9.1.24)
>    >    >    >
>    >    >    > Type "help" for help.
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    > ams=# select * from sl_event_bak;
>    >    >    >
>    >    >    >  ev_origin |  ev_seqno  |         ev_timestamp          |    ev_snapshot     | ev_type | ev_data1 | ev_data2 |
>    >    >    > ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_
>    >    >    >
>    >    >    > data7 | ev_data8
>    >    >    >
>    >    >    > -----------+------------+-------------------------------+--------------------+---------+----------+----------+-
>    >    >    > ---------+----------+----------+----------+----
>    >    >    >
>    >    >    > ------+----------
>    >    >    >
>    >    >    >          8 | 5002075962 | 2018-02-21 19:19:41.958719+00 | 87044110:87044110: | SYNC    |          |          |
>    >    >    >         |          |          |          |
>    >    >    >
>    >    >    >       |
>    >    >    >
>    >    >    > (1 row)
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    > ams=#
>    >    >    >
>    >    >    > ---- master DB leftover event ----
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    > ---- master log drop node record ----
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG disableNode: no_id=8
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=7 li_receiver=1 li_provider=7
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: update provider configuration
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 4 terminated
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: disconnecting from data provider 4
>    >    >    >
>    >    >    > 2018-02-21 19:19:50 UTC [22582] CONFIG remoteWorkerThread_4: connection for provider 7 terminated
>    >    >    >
>    >    >    > ---- master log drop node record ----
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    > ---- replica log drop node record ----
>    >    >    >
>    >    >    > 2018-02-21 19:19:51 UTC [22650] WARN   remoteWorkerThread_1: got DROP NODE for local node ID
>    >    >    >
>    >    >    > NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>    >    >    >
>    >    >    > 2018-02-21 19:19:53 UTC [22650] INFO   remoteWorkerThread_7: SYNC 5001868819 done in 2.153 seconds
>    >    >    >
>    >    >    > NOTICE:  drop cascades to 243 other objects
>    >    >    >
>    >    >    > DETAIL:  drop cascades to table _ams_cluster.sl_node
>    >    >    >
>    >    >    > drop cascades to table _ams_cluster.sl_nodelock
>    >    >    >
>    >    >    > drop cascades to table _ams_cluster.sl_set
>    >    >    >
>    >    >    > drop cascades to table _ams_cluster.sl_setsync
>    >    >    >
>    >    >    > drop cascades to table _ams_cluster.sl_table
>    >    >    >
>    >    >    > drop cascades to table _ams_cluster.sl_sequence
>    >    >    >
>    >    >    > ---- replica log drop node record ----
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >                 Tom    ?
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >    >
>    >    >
>    >    >
>    >    >
>    >
>    >
>    >
>
>
>

