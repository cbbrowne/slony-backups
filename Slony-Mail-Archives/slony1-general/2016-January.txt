From tmblue at gmail.com  Thu Jan  7 14:26:44 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 7 Jan 2016 14:26:44 -0800
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in progress
	- sl_log_1 not truncated
Message-ID: <CAEaSS0aCUuNmtsekJmG+fgt2EqgA5ArnPnn-Z3ZUT2kza6ixmw@mail.gmail.com>

So I'm backing up in a big way. I know what started it, "adding a new
insert slave which took 13 hours to complete (indexes etc)".. But now it
doesn't appear I am able to catch up. I see the slave doing what it's
suppose to, get a bunch of data, truncate the sl_log files move on. But the
master is having a hard time.

Postgres 9.4.5 and Slony 2.2.3

All other nodes don't have any errors or issues.

this is Node 1 (the master)
node 2 is a slave
node 3-5 are query slaves with only 1 of 3 sets being replicated too.

I have interval at 5 minutes and sync_group_maxsize=50

Any suggestions on where to thump it. At some point this will cause issues
on my master and when I see that starting, I'll have to drop node 2 again,
and when i add it, it will take 13+ hours and I'll be back in the same
position :)

Thanks
Tory



Node:  Old Transactions Kept Open
================================================
Old Transaction still running with age 01:48:00 > 01:30:00

Query: autovacuum: VACUUM


Node: 0 threads seem stuck
================================================
Slony-I components have not reported into sl_components in interval 00:05:00

Perhaps slon is not running properly?

Query:
     select co_actor, co_pid, co_node, co_connection_pid, co_activity,
co_starttime, now() - co_starttime, co_event, co_eventtype
     from "_admissioncls".sl_components
     where  (now() - co_starttime) > '00:05:00'::interval
     order by co_starttime;



Node: 1 sl_log_1 tuples = 219700 > 200000
================================================
Number of tuples in Slony-I table sl_log_1 is 219700 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.


Node: 1 sl_log_2 tuples = 1.74558e+07 > 200000
================================================
Number of tuples in Slony-I table sl_log_2 is 1.74558e+07 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.


Node: 2 sl_log_2 tuples = 440152 > 200000
================================================
Number of tuples in Slony-I table sl_log_2 is 440152 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160107/677d8782/attachment.htm 

From steve at ssinger.info  Thu Jan  7 14:47:04 2016
From: steve at ssinger.info (Steve Singer)
Date: Thu, 7 Jan 2016 17:47:04 -0500 (EST)
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <CAEaSS0aCUuNmtsekJmG+fgt2EqgA5ArnPnn-Z3ZUT2kza6ixmw@mail.gmail.com>
References: <CAEaSS0aCUuNmtsekJmG+fgt2EqgA5ArnPnn-Z3ZUT2kza6ixmw@mail.gmail.com>
Message-ID: <alpine.DEB.2.02.1601071743020.11931@mini.atlantida>

On Thu, 7 Jan 2016, Tory M Blue wrote:

> 
> So I'm backing up in a big way. I know what started it, "adding a new insert slave which took 13 hours to complete (indexes
> etc)".. But now it doesn't appear I am able to catch up. I see the slave doing what it's suppose to, get a bunch of data,
> truncate the sl_log files move on. But the master is having a hard time.
> 
> Postgres 9.4.5 and Slony 2.2.3
> 
> All other nodes don't have any errors or issues.
> 
> this is Node 1 (the master)
> node 2 is a slave
> node 3-5 are query slaves with only 1 of 3 sets being replicated too.
> 
> I have interval at 5 minutes and sync_group_maxsize=50
> 
> Any suggestions on where to thump it. At some point this will cause issues on my master and when I see that starting, I'll
> have to drop node 2 again, and when i add it, it will take 13+ hours and I'll be back in the same position :)

Bump sync_group_maxsize to be much bigger, I'm not saying that will solve 
the problem but it might help(max allowed is 10,000). I'm also suspect when 
you say your have a sync_interval of 5 minutes, since I thought 60 seconds was the largest 
allowed.



> 
> Thanks
> Tory
> 
> 
> 
> Node: ?Old Transactions Kept Open
> ================================================
> Old Transaction still running with age 01:48:00 > 01:30:00
> 
> Query: autovacuum: VACUUM
> 
> 
> Node: 0 threads seem stuck
> ================================================
> Slony-I components have not reported into sl_components in interval 00:05:00
> 
> Perhaps slon is not running properly?
> 
> Query:
> ? ? ?select co_actor, co_pid, co_node, co_connection_pid, co_activity, co_starttime, now() - co_starttime, co_event,
> co_eventtype
> ? ? ?from "_admissioncls".sl_components
> ? ? ?where ?(now() - co_starttime) > '00:05:00'::interval
> ? ? ?order by co_starttime;
> ??
> 
> 
> Node: 1 sl_log_1 tuples = 219700 > 200000
> ================================================
> Number of tuples in Slony-I table sl_log_1 is 219700 which
> exceeds 200000.
> 
> You may wish to investigate whether or not a node is down, or perhaps
> if sl_confirm entries have not been propagating properly.
> 
> 
> Node: 1 sl_log_2 tuples = 1.74558e+07 > 200000
> ================================================
> Number of tuples in Slony-I table sl_log_2 is 1.74558e+07 which
> exceeds 200000.
> 
> You may wish to investigate whether or not a node is down, or perhaps
> if sl_confirm entries have not been propagating properly.
> 
> 
> Node: 2 sl_log_2 tuples = 440152 > 200000
> ================================================
> Number of tuples in Slony-I table sl_log_2 is 440152 which
> exceeds 200000.
> 
> You may wish to investigate whether or not a node is down, or perhaps
> if sl_confirm entries have not been propagating properly.
> 
> 
>

From tmblue at gmail.com  Thu Jan  7 14:50:24 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 7 Jan 2016 14:50:24 -0800
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <alpine.DEB.2.02.1601071743020.11931@mini.atlantida>
References: <CAEaSS0aCUuNmtsekJmG+fgt2EqgA5ArnPnn-Z3ZUT2kza6ixmw@mail.gmail.com>
	<alpine.DEB.2.02.1601071743020.11931@mini.atlantida>
Message-ID: <CAEaSS0ZLoz8pofz9EySn7rZ+U1kXhor+0eNs-f7PF2t-SLvQkw@mail.gmail.com>

On Thu, Jan 7, 2016 at 2:47 PM, Steve Singer <steve at ssinger.info> wrote:

> On Thu, 7 Jan 2016, Tory M Blue wrote:
>
>
>> So I'm backing up in a big way. I know what started it, "adding a new
>> insert slave which took 13 hours to complete (indexes
>> etc)".. But now it doesn't appear I am able to catch up. I see the slave
>> doing what it's suppose to, get a bunch of data,
>> truncate the sl_log files move on. But the master is having a hard time.
>>
>> Postgres 9.4.5 and Slony 2.2.3
>>
>> All other nodes don't have any errors or issues.
>>
>> this is Node 1 (the master)
>> node 2 is a slave
>> node 3-5 are query slaves with only 1 of 3 sets being replicated too.
>>
>> I have interval at 5 minutes and sync_group_maxsize=50
>>
>> Any suggestions on where to thump it. At some point this will cause
>> issues on my master and when I see that starting, I'll
>> have to drop node 2 again, and when i add it, it will take 13+ hours and
>> I'll be back in the same position :)
>>
>
> Bump sync_group_maxsize to be much bigger, I'm not saying that will solve
> the problem but it might help(max allowed is 10,000). I'm also suspect when
> you say your have a sync_interval of 5 minutes, since I thought 60 seconds
> was the largest allowed.
>
> My apologies

cleanup_interval="5 minutes"

my interval is 1000ms
And sync, group cites 100 is the max

# Range:  [0,100], default: 6

sync_group_maxsize=50
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160107/81014146/attachment.htm 

From steve at ssinger.info  Thu Jan  7 14:59:52 2016
From: steve at ssinger.info (Steve Singer)
Date: Thu, 7 Jan 2016 17:59:52 -0500 (EST)
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <CAEaSS0ZLoz8pofz9EySn7rZ+U1kXhor+0eNs-f7PF2t-SLvQkw@mail.gmail.com>
References: <CAEaSS0aCUuNmtsekJmG+fgt2EqgA5ArnPnn-Z3ZUT2kza6ixmw@mail.gmail.com>
	<alpine.DEB.2.02.1601071743020.11931@mini.atlantida>
	<CAEaSS0ZLoz8pofz9EySn7rZ+U1kXhor+0eNs-f7PF2t-SLvQkw@mail.gmail.com>
Message-ID: <alpine.DEB.2.02.1601071754040.11931@mini.atlantida>

On Thu, 7 Jan 2016, Tory M Blue wrote:

>       Bump sync_group_maxsize to be much bigger, I'm not saying that will solve the problem but it might help(max
>       allowed is 10,000). I'm also suspect when you say your have a sync_interval of 5 minutes, since I thought 60
>       seconds was the largest allowed.
> 
> My apologies
> 
> cleanup_interval="5 minutes" ?
> 
> my interval is 1000ms
> 
> And sync, group cites 100 is the max
> 
> # Range:? [0,100], default: 6
> 
> sync_group_maxsize=50

Where does that come from?
http://www.slony.info/documentation/2.2/slon-config-interval.html

says the max is 10,000 and the code looks like it agrees.  Try it and see if 
you start to catch up.  Also an analyze on your sl_log_1 and sl_log_2 can't 
hurt.

With a sync_group_size of 20 slon will select the data for at most 20 SYNC's 
at once and apply them (using a select from sl_log_1 ... union select from 
sl_log_2 ...)

With a sync_group_size of 10,000 it can in theory select 10,000 sync's at 
once (but I think it takes a while to work up to that point).

If your bottleneck is the master then it is possible that the selecting from 
sl_log is causing the problem.

Slon can't truncate a log until all the data in that log has been 
replicated and there are only 2 logs.




> 
> 
> 
> 
>

From tmblue at gmail.com  Thu Jan  7 15:13:11 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 7 Jan 2016 15:13:11 -0800
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <alpine.DEB.2.02.1601071754040.11931@mini.atlantida>
References: <CAEaSS0aCUuNmtsekJmG+fgt2EqgA5ArnPnn-Z3ZUT2kza6ixmw@mail.gmail.com>
	<alpine.DEB.2.02.1601071743020.11931@mini.atlantida>
	<CAEaSS0ZLoz8pofz9EySn7rZ+U1kXhor+0eNs-f7PF2t-SLvQkw@mail.gmail.com>
	<alpine.DEB.2.02.1601071754040.11931@mini.atlantida>
Message-ID: <CAEaSS0ZYWr5fqpuw+ihB=Qe8MBvqAi-T_YGB7vUJ2WYrYGhVwA@mail.gmail.com>

On Thu, Jan 7, 2016 at 2:59 PM, Steve Singer <steve at ssinger.info> wrote:

> On Thu, 7 Jan 2016, Tory M Blue wrote:
>
>       Bump sync_group_maxsize to be much bigger, I'm not saying that will
>> solve the problem but it might help(max
>>       allowed is 10,000). I'm also suspect when you say your have a
>> sync_interval of 5 minutes, since I thought 60
>>       seconds was the largest allowed.
>>
>> My apologies
>>
>> cleanup_interval="5 minutes"
>>
>> my interval is 1000ms
>>
>> And sync, group cites 100 is the max
>>
>> # Range:  [0,100], default: 6
>>
>> sync_group_maxsize=50
>>
>
> Where does that come from?
> http://www.slony.info/documentation/2.2/slon-config-interval.html
>
> says the max is 10,000 and the code looks like it agrees.  Try it and see
> if you start to catch up.  Also an analyze on your sl_log_1 and sl_log_2
> can't hurt.
>
> With a sync_group_size of 20 slon will select the data for at most 20
> SYNC's at once and apply them (using a select from sl_log_1 ... union
> select from sl_log_2 ...)
>
> With a sync_group_size of 10,000 it can in theory select 10,000 sync's at
> once (but I think it takes a while to work up to that point).
>
> If your bottleneck is the master then it is possible that the selecting
> from sl_log is causing the problem.
>
> Thanks Steve, I had that in my slon.conf, it may have been left over from
many years 10+ of using slon and a much earlier version :) I'll keep
incrementing it and see what happens.

Analyze was quick, but I think you are on to something when you cited that
it may be taking a bit to grab from sl_log1 and thus the snowball.. Can't
say I've ever had this many records backed up, but the idea of dropping
node 2, when i know I'll be in the same situation bothers me :)

2016-01-07 15:10:29 PST clsdb postgres [local] 16066 2016-01-07
15:10:29.128 PSTLOG:  duration: 2096.165 ms  statement: analyze
_cls.sl_log_2;
2016-01-07 15:10:36 PST aclsdb postgres [local] 16066 2016-01-07
15:10:36.162 PSTLOG:  duration: 2426.762 ms  statement: analyze
_cls.sl_log_1

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160107/41df5853/attachment-0001.htm 

From krzysztof.jakowczyk at unity.pl  Tue Jan 12 08:21:57 2016
From: krzysztof.jakowczyk at unity.pl (Krzysztof Jakowczyk)
Date: Tue, 12 Jan 2016 17:21:57 +0100
Subject: [Slony1-general] log insert/update query executed on subscriber
Message-ID: <56952825.8040801@unity.pl>

Hello,

Is it possible to log query with parameters executed on subscriber node?
I've tried to add __audit trigger before insert or update, but when
denyaccess trigger is enabled, nothing is logged. Below is my try for
table emp:

CREATE SCHEMA
audit;                                                                                                                                                  


CREATE TABLE audit.slon_audit (
    operation         char(1)   NOT NULL,
    stamp             timestamp NOT NULL,
    userid            text      NOT NULL,
    query             text      NOT NULL
);


CREATE OR REPLACE FUNCTION save_query() RETURNS TRIGGER AS $$
    BEGIN
        IF (TG_OP = 'DELETE') THEN
            INSERT INTO audit.slon_audit SELECT 'D', now(), user, OLD.*;
            RETURN OLD;
        ELSIF (TG_OP = 'UPDATE') THEN
            INSERT INTO audit.slon_audit SELECT 'U', now(), user,
current_query();
            RETURN NEW;
        ELSIF (TG_OP = 'INSERT') THEN
            INSERT INTO audit.slon_audit SELECT 'I', now(), user,
current_query();
            RETURN NEW;
        END IF;
        RETURN NULL;
    END;
$$ LANGUAGE plpgsql;


CREATE TRIGGER __audit
BEFORE INSERT OR UPDATE OR DELETE ON emp
    FOR EACH ROW EXECUTE PROCEDURE save_query();

-- 
Pozdrawiam,

Krzysztof Jakowczyk
Administrator System?w Unix
 
Grupa Unity | ul. Przedmiejska 6-10, 54-201 Wroc?aw
ul. Conrada 55B, 31-357 Krak?w | ul. Z?ota 59, 00-120 Warszawa




From steve at ssinger.info  Tue Jan 12 18:43:22 2016
From: steve at ssinger.info (Steve Singer)
Date: Tue, 12 Jan 2016 21:43:22 -0500 (EST)
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <56952825.8040801@unity.pl>
References: <56952825.8040801@unity.pl>
Message-ID: <alpine.DEB.2.02.1601122141490.11931@mini.atlantida>

On Tue, 12 Jan 2016, Krzysztof Jakowczyk wrote:

> Hello,
>
> Is it possible to log query with parameters executed on subscriber node?
> I've tried to add __audit trigger before insert or update, but when
> denyaccess trigger is enabled, nothing is logged. Below is my try for
> table emp:
>

>
> CREATE TRIGGER __audit
> BEFORE INSERT OR UPDATE OR DELETE ON emp
>    FOR EACH ROW EXECUTE PROCEDURE save_query();

Setting the trigger to fire always should work.

ALTER TABLE foo ENABLE ALWAYS TRIGGER __audit;

http://www.postgresql.org/docs/9.1/interactive/sql-altertable.html



>
> -- 
> Pozdrawiam,
>
> Krzysztof Jakowczyk
> Administrator System?w Unix
> 
> Grupa Unity | ul. Przedmiejska 6-10, 54-201 Wroc?aw
> ul. Conrada 55B, 31-357 Krak?w | ul. Z?ota 59, 00-120 Warszawa
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From krzysztof.jakowczyk at unity.pl  Tue Jan 12 22:52:07 2016
From: krzysztof.jakowczyk at unity.pl (Krzysztof Jakowczyk)
Date: Wed, 13 Jan 2016 07:52:07 +0100
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
References: <56952825.8040801@unity.pl>
	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
Message-ID: <5695F417.7020903@unity.pl>

No, it doesn't. Take a look:

# \d emp
      Table "public.emp"
 Column  |  Type   | Modifiers
---------+---------+-----------
 empname | text    | not null
 salary  | integer |
Triggers:
    _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
Triggers firing always:
    __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
EXECUTE PROCEDURE save_query()


-- with disabled trigger denyaccess
# insert into emp values ('TEST dasdasdas',123123);
INSERT 0 1
# select * from audit.slon_audit;
 operation |           stamp            |  userid 
|                       query                      
-----------+----------------------------+----------+---------------------------------------------------
 I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
values ('TEST dasdasdas',123123);
(1 row)

-- enabling trigger denyaccess
# alter table emp enable trigger _b24v2_denyaccess;
ALTER TABLE
# insert into emp values ('test2',31337);
ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
subscriber node - role=0
# select * from audit.slon_audit;
 operation |           stamp            |  userid 
|                       query                      
-----------+----------------------------+----------+---------------------------------------------------
 I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
values ('TEST dasdasdas',123123);
(1 row)


Nothing happend. Any other ideas?

-- 
Pozdrawiam,

Krzysztof Jakowczyk
Administrator System?w Unix
 
Grupa Unity | ul. Przedmiejska 6-10, 54-201 Wroc?aw
ul. Conrada 55B, 31-357 Krak?w | ul. Z?ota 59, 00-120 Warszawa


On 13.01.2016 03:43, Steve Singer wrote:
> On Tue, 12 Jan 2016, Krzysztof Jakowczyk wrote:
>
>> Hello,
>>
>> Is it possible to log query with parameters executed on subscriber node?
>> I've tried to add __audit trigger before insert or update, but when
>> denyaccess trigger is enabled, nothing is logged. Below is my try for
>> table emp:
>>
>
>>
>> CREATE TRIGGER __audit
>> BEFORE INSERT OR UPDATE OR DELETE ON emp
>>    FOR EACH ROW EXECUTE PROCEDURE save_query();
>
> Setting the trigger to fire always should work.
>
> ALTER TABLE foo ENABLE ALWAYS TRIGGER __audit;
>
> http://www.postgresql.org/docs/9.1/interactive/sql-altertable.html
>
>
>
>>
>> -- 
>> Pozdrawiam,
>>
>> Krzysztof Jakowczyk
>> Administrator System?w Unix
>>
>> Grupa Unity | ul. Przedmiejska 6-10, 54-201 Wroc?aw
>> ul. Conrada 55B, 31-357 Krak?w | ul. Z?ota 59, 00-120 Warszawa
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general



From stephane.schildknecht at postgres.fr  Wed Jan 13 00:17:04 2016
From: stephane.schildknecht at postgres.fr (=?UTF-8?Q?St=c3=a9phane_Schildknecht?=)
Date: Wed, 13 Jan 2016 09:17:04 +0100
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <5695F417.7020903@unity.pl>
References: <56952825.8040801@unity.pl>
	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
	<5695F417.7020903@unity.pl>
Message-ID: <56960800.8040904@postgres.fr>

On 13/01/2016 07:52, Krzysztof Jakowczyk wrote:
> No, it doesn't. Take a look:
> 
> # \d emp
>       Table "public.emp"
>  Column  |  Type   | Modifiers
> ---------+---------+-----------
>  empname | text    | not null
>  salary  | integer |
> Triggers:
>     _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
> ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
> Triggers firing always:
>     __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
> EXECUTE PROCEDURE save_query()
> 
> 
> -- with disabled trigger denyaccess
> # insert into emp values ('TEST dasdasdas',123123);
> INSERT 0 1
> # select * from audit.slon_audit;
>  operation |           stamp            |  userid 
> |                       query                      
> -----------+----------------------------+----------+---------------------------------------------------
>  I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
> values ('TEST dasdasdas',123123);
> (1 row)
> 
> -- enabling trigger denyaccess
> # alter table emp enable trigger _b24v2_denyaccess;
> ALTER TABLE
> # insert into emp values ('test2',31337);
> ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
> subscriber node - role=0
> # select * from audit.slon_audit;
>  operation |           stamp            |  userid 
> |                       query                      
> -----------+----------------------------+----------+---------------------------------------------------
>  I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
> values ('TEST dasdasdas',123123);
> (1 row)
> 
> 
> Nothing happend. Any other ideas?
> 

I guess it is related to the alphabetical order of triggers, as they are both
BEFORE triggers.

-- 
St?phane Schildknecht
Contact r?gional PostgreSQL pour l'Europe francophone
Loxodata - Conseil, expertise et formations
06.17.11.37.42

From david at fetter.org  Wed Jan 13 10:43:51 2016
From: david at fetter.org (David Fetter)
Date: Wed, 13 Jan 2016 10:43:51 -0800
Subject: [Slony1-general] Make all slon options actually settable from the
	command line?
Message-ID: <20160113184351.GB28070@fetter.org>

Folks,

Is there a reason other than lack of tuits why $Subject is not already
the case?  If not, I'd like to prepare a patch for the next minor
release to address it.

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From cbbrowne at afilias.info  Wed Jan 13 13:33:57 2016
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 13 Jan 2016 16:33:57 -0500
Subject: [Slony1-general] Make all slon options actually settable from
 the command line?
In-Reply-To: <20160113184351.GB28070@fetter.org>
References: <20160113184351.GB28070@fetter.org>
Message-ID: <CANfbgbZT0w4FjiJQfo9xJvkyxOaQiMsmTTe6pkOGtQW_nti5cA@mail.gmail.com>

On Wed, Jan 13, 2016 at 1:43 PM, David Fetter <david at fetter.org> wrote:

> Folks,
>
> Is there a reason other than lack of tuits why $Subject is not already
> the case?  If not, I'd like to prepare a patch for the next minor
> release to address it.
>

The mild contraindication, to my mind, was that when we set up the -f
option to allow pulling substantially all configuration (save for the -f
value!)
from a file, I thought it would be preferable to pull config from files,
hence meaning there wasn't much reason to put any tuits into the
matter.

With the bit of "contraindication" that it's traditionally tempting to have
short option names, hence leading to potential fights over what "-s" stands
for.

If you're keen on patching in long-ish option names for everything,
I don't see a big reason to struggle against that.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160113/0d25661c/attachment.htm 

From ssinger at ca.afilias.info  Wed Jan 13 14:37:24 2016
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 13 Jan 2016 17:37:24 -0500
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <5695F417.7020903@unity.pl>
References: <56952825.8040801@unity.pl>	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
	<5695F417.7020903@unity.pl>
Message-ID: <5696D1A4.6010904@ca.afilias.info>

On 01/13/2016 01:52 AM, Krzysztof Jakowczyk wrote:
> No, it doesn't. Take a look:
>
> # \d emp
>        Table "public.emp"
>   Column  |  Type   | Modifiers
> ---------+---------+-----------
>   empname | text    | not null
>   salary  | integer |
> Triggers:
>      _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
> ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
> Triggers firing always:
>      __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
> EXECUTE PROCEDURE save_query()
>
>
> -- with disabled trigger denyaccess
> # insert into emp values ('TEST dasdasdas',123123);
> INSERT 0 1
> # select * from audit.slon_audit;
>   operation |           stamp            |  userid
> |                       query
> -----------+----------------------------+----------+---------------------------------------------------
>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
> values ('TEST dasdasdas',123123);
> (1 row)
>
> -- enabling trigger denyaccess
> # alter table emp enable trigger _b24v2_denyaccess;
> ALTER TABLE
> # insert into emp values ('test2',31337);
> ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
> subscriber node - role=0

I guess I don't understand what it is your trying to do.

What I thought you were asking was this

You have the table "emp" which has node1 as the origin.

You replicate the table "emp" to node 2.

You have the audit.slon_audit table on both node1 and node2. You want 
the trigger to fire on both node1 and node2 instead of replicating 
audit.slon_audit

If that's the case why are you trying your insert on node2?
You want to perform the insert on node1,and then the audit trigger 
should run (assuming you configure it as an always trigger) on both 
node1 and on node2 inserting data into your audit table





> # select * from audit.slon_audit;
>   operation |           stamp            |  userid
> |                       query
> -----------+----------------------------+----------+---------------------------------------------------
>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
> values ('TEST dasdasdas',123123);
> (1 row)
>
>
> Nothing happend. Any other ideas?
>


From steve at ssinger.info  Wed Jan 13 14:38:14 2016
From: steve at ssinger.info (Steve Singer)
Date: Wed, 13 Jan 2016 17:38:14 -0500
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <5695F417.7020903@unity.pl>
References: <56952825.8040801@unity.pl>	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
	<5695F417.7020903@unity.pl>
Message-ID: <5696D1D6.6010505@ssinger.info>

On 01/13/2016 01:52 AM, Krzysztof Jakowczyk wrote:
> No, it doesn't. Take a look:
>
> # \d emp
>        Table "public.emp"
>   Column  |  Type   | Modifiers
> ---------+---------+-----------
>   empname | text    | not null
>   salary  | integer |
> Triggers:
>      _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
> ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
> Triggers firing always:
>      __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
> EXECUTE PROCEDURE save_query()
>
>
> -- with disabled trigger denyaccess
> # insert into emp values ('TEST dasdasdas',123123);
> INSERT 0 1
> # select * from audit.slon_audit;
>   operation |           stamp            |  userid
> |                       query
> -----------+----------------------------+----------+---------------------------------------------------
>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
> values ('TEST dasdasdas',123123);
> (1 row)
>
> -- enabling trigger denyaccess
> # alter table emp enable trigger _b24v2_denyaccess;
> ALTER TABLE
> # insert into emp values ('test2',31337);
> ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
> subscriber node - role=0

I guess I don't understand what it is your trying to do.

What I thought you were asking was this

You have the table "emp" which has node1 as the origin.

You replicate the table "emp" to node 2.

You have the audit.slon_audit table on both node1 and node2. You want 
the trigger to fire on both node1 and node2 instead of replicating 
audit.slon_audit

If that's the case why are you trying your insert on node2?
You want to perform the insert on node1,and then the audit trigger 
should run (assuming you configure it as an always trigger) on both 
node1 and on node2 inserting data into your audit table





> # select * from audit.slon_audit;
>   operation |           stamp            |  userid
> |                       query
> -----------+----------------------------+----------+---------------------------------------------------
>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
> values ('TEST dasdasdas',123123);
> (1 row)
>
>
> Nothing happend. Any other ideas?
>


From david at fetter.org  Wed Jan 13 15:59:29 2016
From: david at fetter.org (David Fetter)
Date: Wed, 13 Jan 2016 15:59:29 -0800
Subject: [Slony1-general] Make all slon options actually settable from
 the command line?
In-Reply-To: <CANfbgbZT0w4FjiJQfo9xJvkyxOaQiMsmTTe6pkOGtQW_nti5cA@mail.gmail.com>
References: <20160113184351.GB28070@fetter.org>
	<CANfbgbZT0w4FjiJQfo9xJvkyxOaQiMsmTTe6pkOGtQW_nti5cA@mail.gmail.com>
Message-ID: <20160113235929.GA11754@fetter.org>

On Wed, Jan 13, 2016 at 04:33:57PM -0500, Christopher Browne wrote:
> On Wed, Jan 13, 2016 at 1:43 PM, David Fetter <david at fetter.org> wrote:
> 
> > Folks,
> >
> > Is there a reason other than lack of tuits why $Subject is not already
> > the case?  If not, I'd like to prepare a patch for the next minor
> > release to address it.
> >
> 
> The mild contraindication, to my mind, was that when we set up the -f
> option to allow pulling substantially all configuration (save for the -f
> value!)
> from a file, I thought it would be preferable to pull config from files,
> hence meaning there wasn't much reason to put any tuits into the
> matter.

The altperl tools, and probably others, are not super fun to operate
when only some of the options are settable from the command line.

Then there's the consistency issue, although that's mostly aesthetic.

> With the bit of "contraindication" that it's traditionally tempting to have
> short option names, hence leading to potential fights over what "-s" stands
> for.

I'm happy to add in some form of getopt_long, which I generally prefer
for self-documentation purposes.  Do we need to do something special
the way PostgreSQL does?  I presume simply mandating GNU wouldn't
work.

> If you're keen on patching in long-ish option names for everything,
> I don't see a big reason to struggle against that.

I'd be delighted to improve the clarity here.

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

From tmblue at gmail.com  Wed Jan 13 17:23:15 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 13 Jan 2016 17:23:15 -0800
Subject: [Slony1-general] Slon blocked by an index on a non replicated
	table?? 2.2.3
Message-ID: <CAEaSS0aby6Gk868aoUmDUTvuLEQjMc=tHUO1-3CsGegP2ua2Tg@mail.gmail.com>

Postgres 9.4.5 slon 2.2.3 CentOS 6.6

Afternoon

Based on some searching this is not new, but I have failed to find an
answer for it.. I had an index running on a non replicated table but slon
would not initialize due to that index, something about set 1 failed due to
a previous PID.

Now I'm trying to to reindex 2 tables that are not in the slon
relationship, but slon is backing up, once I stop the reindex, slon
replicates just fine (immediately). Something is wonky (its a technical
term, you can look it up!: ) )

So killing the index, I was able to add my host, but now I'm trying to add
another 2 indexes and it's backing up slon, and or one is holding up the
other.

So now I'm trying to create 2 indexes concurrently...

These are the 2 i am creating.

One is waiting  on the other. (Waiting is true in pg_stat_Activity)

If I kill the 1st the second starts. Nothing is querying this table.


CREATE INDEX CONCURRENTLY idx_impsarchive_psttstamp

  ON torque.impressions_archive

  USING btree

  (timezone('US/Pacific'::text, timezone('UTC'::text, log_tstamp)))

TABLESPACE torquespace;


-- Index: torque.idx_impsarchive_system


-- DROP INDEX torque.idx_impsarchive_system;


CREATE INDEX CONCURRENTLY idx_impsarchive_system

  ON torque.impressions_archive

  USING btree

  (system COLLATE pg_catalog."default")

TABLESPACE torquespace;


Also when I had it running non-concurrently they both ran at the same time.
But slony got backed up. It didn?t explicitly say that it was waiting. But
after about 45 minutes, data had not replicated. I killed the indices and
it replicated immediately. This table is not being replicated.


I also queried sl_table on the master and this table is not there


* tab_id | tab_reloid |     tab_relname     | tab_nspname | tab_set |
tab_idxname        | tab_altered |        tab*

*_comment         *

*--------+------------+---------------------+-------------+---------+--------------------------+-------------+-----------*

*-----------------*

*    225 |      17959 | variables           | torque      |       2 |
pk_variables_name        | f           | replicated**table*

*    226 |      17935 | impsbulkimporteven  | torque      |       2 |
pk_impsbulkimporteven_id | f           | torque.imp**sbulkimporteven*

*    228 |      17947 | impsbulkimportodd   | torque      |       2 |
pk_impsbulkimportodd_id  | f           | torque.imp**sbulkimportodd*

*    233 |      17919 | impressions_daily | torque      |       2 |
pk_impsdaily_id        | f           | torque.i**mpressions_daily*

*    235 |      17907 | impressions       | torque      |       2 |
pk_impressions_id      | f           | torque.i**mpressions*

*(5 rows)*

So I want to create these indexes, they are not being replicated but they
completely freeze replication. Same thing happened when I added a node,
slon would not initialize until we killed the same index creation on this
same non replicated table. I'm not sure what is going on and my apologies i
deleted the set 1 failed, due to a previous pid message, but if this table
is not in replication, why is it causing slon to be blocked?

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160113/d4dba4d4/attachment-0001.htm 

From steve at ssinger.info  Wed Jan 13 18:30:05 2016
From: steve at ssinger.info (Steve Singer)
Date: Wed, 13 Jan 2016 21:30:05 -0500 (EST)
Subject: [Slony1-general] Slon blocked by an index on a non replicated
 table?? 2.2.3
In-Reply-To: <CAEaSS0aby6Gk868aoUmDUTvuLEQjMc=tHUO1-3CsGegP2ua2Tg@mail.gmail.com>
References: <CAEaSS0aby6Gk868aoUmDUTvuLEQjMc=tHUO1-3CsGegP2ua2Tg@mail.gmail.com>
Message-ID: <alpine.DEB.2.02.1601132116520.11931@mini.atlantida>

On Wed, 13 Jan 2016, Tory M Blue wrote:

Tory,

You talk about slon 'initializing'.  When subscriptions start it needs to 
wait until all in progress transactions are committed before starting the 
copy. Once your cluster is subscribed a create index shouldn't block things.




> Postgres 9.4.5 slon 2.2.3 CentOS 6.6
> 
> Afternoon
> 
> Based on some searching this is not new, but I have failed to find an answer for it.. I had an index running on a non
> replicated table but slon would not initialize due to that index, something about set 1 failed due to a previous PID.
> 
> Now I'm trying to to reindex 2 tables that are not in the slon relationship, but slon is backing up, once I stop the
> reindex, slon replicates just fine (immediately). Something is wonky (its a technical term, you can look it up!: ) )
> 
> So killing the index, I was able to add my host, but now I'm trying to add another 2 indexes and it's backing up slon, and
> or one is holding up the other.
> 
> So now I'm trying to create 2 indexes concurrently...
> 
> These are the 2 i am creating.?
> 
> One is waiting ?on the other. (Waiting is true in pg_stat_Activity)
> 
> If I kill the 1st the second starts. Nothing is querying this table.
> 
> 
> CREATE INDEX CONCURRENTLY idx_impsarchive_psttstamp
> 
> ? ON torque.impressions_archive
> 
> ? USING btree
> 
> ? (timezone('US/Pacific'::text, timezone('UTC'::text, log_tstamp)))
> 
> TABLESPACE torquespace;
> 
> 
> -- Index: torque.idx_impsarchive_system
> 
> 
> -- DROP INDEX torque.idx_impsarchive_system;
> 
> 
> CREATE INDEX CONCURRENTLY idx_impsarchive_system
> 
> ? ON torque.impressions_archive
> 
> ? USING btree
> 
> ? (system COLLATE pg_catalog."default")
> 
> TABLESPACE torquespace;
> 
> 
> Also when I had it running non-concurrently they both ran at the same time. But slony got backed up. It didn?t explicitly
> say that it was waiting. But after about 45 minutes, data had not replicated. I killed the indices and it replicated
> immediately. This table is not being replicated.?
> 
> 
> I also queried sl_table on the master and this table is not there
> 
> 
> ?tab_id | tab_reloid | ? ? tab_relname ? ? | tab_nspname | tab_set | ? ? ? tab_idxname? ? ? ? | tab_altered |? ? ? ? tab
> 
> _comment? ? ? ? ?
> 
> --------+------------+---------------------+-------------+---------+--------------------------+-------------+-----------
> 
> -----------------
> 
> ? ? 225 |? ? ? 17959 | variables ? ? ? ? ? | torque? ? ? | ? ? ? 2 | pk_variables_name? ? ? ? | f ? ? ? ? ? |
> replicatedtable
> 
> ? ? 226 |? ? ? 17935 | impsbulkimporteven? | torque? ? ? | ? ? ? 2 | pk_impsbulkimporteven_id | f ? ? ? ? ? |
> torque.impsbulkimporteven
> 
> ? ? 228 |? ? ? 17947 | impsbulkimportodd ? | torque? ? ? | ? ? ? 2 | pk_impsbulkimportodd_id? | f ? ? ? ? ? |
> torque.impsbulkimportodd
> 
> ? ? 233 |? ? ? 17919 | impressions_daily | torque? ? ? | ? ? ? 2 | pk_impsdaily_id? ? ? ? | f ? ? ? ? ? |
> torque.impressions_daily
> 
> ? ? 235 |? ? ? 17907 | impressions ? ? ? | torque? ? ? | ? ? ? 2 | pk_impressions_id? ? ? | f ? ? ? ? ? | torque.impressions
> 
> (5 rows)
> 
> 
> So I want to create these indexes, they are not being replicated but they completely freeze replication. Same thing happened
> when I added a node, slon would not initialize until we killed the same index creation on this same non replicated table.
> I'm not sure what is going on and my apologies i deleted the set 1 failed, due to a previous pid message, but if this table
> is not in replication, why is it causing slon to be blocked?
> 
> Thanks
> Tory
> 
> 
>

From tmblue at gmail.com  Wed Jan 13 18:38:44 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 13 Jan 2016 18:38:44 -0800
Subject: [Slony1-general] Slon blocked by an index on a non replicated
 table?? 2.2.3
In-Reply-To: <alpine.DEB.2.02.1601132116520.11931@mini.atlantida>
References: <CAEaSS0aby6Gk868aoUmDUTvuLEQjMc=tHUO1-3CsGegP2ua2Tg@mail.gmail.com>
	<alpine.DEB.2.02.1601132116520.11931@mini.atlantida>
Message-ID: <CAEaSS0airdGM18FVie0-84ys+Gh+3MFFcxFOhtziAToaJLJA-Q@mail.gmail.com>

On Wed, Jan 13, 2016 at 6:30 PM, Steve Singer <steve at ssinger.info> wrote:

> On Wed, 13 Jan 2016, Tory M Blue wrote:
>
> Tory,
>
> You talk about slon 'initializing'.  When subscriptions start it needs to
> wait until all in progress transactions are committed before starting the
> copy. Once your cluster is subscribed a create index shouldn't block things.
>
>
Ya 2 different issues, sorry. but the Initialization part, even if the
table being indexed is not part of the set? That rings weird and I really
wish i could find the other thread that had a discussion on this, as it has
the correct error etc.

But an index on a table that is not part of any replication set, blocks
slony from starting the copy?  We are talking table based replication here
right, so we are not looking at the db level, which I could sort of
understand. Since slony is replicating tables, if this table is not part of
any set and thus is not being replicated, why does that hold true?

And while I'm fully replicated now, if I try to index these tables, slon
backs up and if I kill the index, the replications set happen immediately.
So something is happening with these tables. These are archive tables,
nothing is accessing them, they are here purely for historical purposes.
So I'm at a loss and I don't expect anyone to have an immediate answer, but
it seems weird and would love to provide any necessary information to help
frame the question/issue better, if someone can help me do that :)

Thanks again Steve!

Tory



>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160113/c84f610e/attachment.htm 

From krzysztof.jakowczyk at unity.pl  Thu Jan 14 01:48:29 2016
From: krzysztof.jakowczyk at unity.pl (Krzysztof Jakowczyk)
Date: Thu, 14 Jan 2016 10:48:29 +0100
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <56960800.8040904@postgres.fr>
References: <56952825.8040801@unity.pl>	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>	<5695F417.7020903@unity.pl>
	<56960800.8040904@postgres.fr>
Message-ID: <56976EED.6000801@unity.pl>

On 13.01.2016 09:17, St?phane Schildknecht wrote:
> On 13/01/2016 07:52, Krzysztof Jakowczyk wrote:
>> No, it doesn't. Take a look:
>>
>> # \d emp
>>       Table "public.emp"
>>  Column  |  Type   | Modifiers
>> ---------+---------+-----------
>>  empname | text    | not null
>>  salary  | integer |
>> Triggers:
>>     _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
>> ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
>> Triggers firing always:
>>     __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
>> EXECUTE PROCEDURE save_query()
>>
>>
>> -- with disabled trigger denyaccess
>> # insert into emp values ('TEST dasdasdas',123123);
>> INSERT 0 1
>> # select * from audit.slon_audit;
>>  operation |           stamp            |  userid 
>> |                       query                      
>> -----------+----------------------------+----------+---------------------------------------------------
>>  I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
>> values ('TEST dasdasdas',123123);
>> (1 row)
>>
>> -- enabling trigger denyaccess
>> # alter table emp enable trigger _b24v2_denyaccess;
>> ALTER TABLE
>> # insert into emp values ('test2',31337);
>> ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
>> subscriber node - role=0
>> # select * from audit.slon_audit;
>>  operation |           stamp            |  userid 
>> |                       query                      
>> -----------+----------------------------+----------+---------------------------------------------------
>>  I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
>> values ('TEST dasdasdas',123123);
>> (1 row)
>>
>>
>> Nothing happend. Any other ideas?
>>
> I guess it is related to the alphabetical order of triggers, as they are both
> BEFORE triggers.
>
Yes, it's true. Quote from documentation:


"If multiple triggers of the same kind are defined for the same event,
they will be fired in alphabetical order by name".

So I named this trigger __audit. That is first in alphabetical order but
it seems not working well.



From krzysztof.jakowczyk at unity.pl  Thu Jan 14 02:04:37 2016
From: krzysztof.jakowczyk at unity.pl (Krzysztof Jakowczyk)
Date: Thu, 14 Jan 2016 11:04:37 +0100
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <5696D1A4.6010904@ca.afilias.info>
References: <56952825.8040801@unity.pl>	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
	<5695F417.7020903@unity.pl> <5696D1A4.6010904@ca.afilias.info>
Message-ID: <569772B5.90409@unity.pl>

Our java aplication have bug and from time to time it's trying to insert
row on subscriber node (which shouldn't happend). My developers wants to
have query with parametrs which is executed on subscriber node. Table
audit.slon_audit is created only on subscriber node to catch "bad"
INSERT/UPDATE queries. I know that this queries shoud executed only on
origin. I want to find the cause of this behavior of our aplication.


On 13.01.2016 23:37, Steve Singer wrote:
> On 01/13/2016 01:52 AM, Krzysztof Jakowczyk wrote:
>> No, it doesn't. Take a look:
>>
>> # \d emp
>>        Table "public.emp"
>>   Column  |  Type   | Modifiers
>> ---------+---------+-----------
>>   empname | text    | not null
>>   salary  | integer |
>> Triggers:
>>      _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
>> ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
>> Triggers firing always:
>>      __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
>> EXECUTE PROCEDURE save_query()
>>
>>
>> -- with disabled trigger denyaccess
>> # insert into emp values ('TEST dasdasdas',123123);
>> INSERT 0 1
>> # select * from audit.slon_audit;
>>   operation |           stamp            |  userid
>> |                       query
>> -----------+----------------------------+----------+---------------------------------------------------
>>
>>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
>> values ('TEST dasdasdas',123123);
>> (1 row)
>>
>> -- enabling trigger denyaccess
>> # alter table emp enable trigger _b24v2_denyaccess;
>> ALTER TABLE
>> # insert into emp values ('test2',31337);
>> ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
>> subscriber node - role=0
>
> I guess I don't understand what it is your trying to do.
>
> What I thought you were asking was this
>
> You have the table "emp" which has node1 as the origin.
>
> You replicate the table "emp" to node 2.
>
> You have the audit.slon_audit table on both node1 and node2. You want
> the trigger to fire on both node1 and node2 instead of replicating
> audit.slon_audit
>
> If that's the case why are you trying your insert on node2?
> You want to perform the insert on node1,and then the audit trigger
> should run (assuming you configure it as an always trigger) on both
> node1 and on node2 inserting data into your audit table
>
>
>
>
>
>> # select * from audit.slon_audit;
>>   operation |           stamp            |  userid
>> |                       query
>> -----------+----------------------------+----------+---------------------------------------------------
>>
>>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
>> values ('TEST dasdasdas',123123);
>> (1 row)
>>
>>
>> Nothing happend. Any other ideas?
>>
>
> .
>



From glynastill at yahoo.co.uk  Thu Jan 14 03:16:40 2016
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 14 Jan 2016 11:16:40 +0000 (UTC)
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <569772B5.90409@unity.pl>
References: <569772B5.90409@unity.pl>
Message-ID: <1951480679.7908114.1452770200257.JavaMail.yahoo@mail.yahoo.com>


> From: Krzysztof Jakowczyk <krzysztof.jakowczyk at unity.pl>
>To: Steve Singer <ssinger at ca.afilias.info>; Steve Singer <steve at ssinger.info> 
>Cc: slony1-general <slony1-general at lists.slony.info>
>Sent: Thursday, 14 January 2016, 10:04
>Subject: Re: [Slony1-general] log insert/update query executed on subscriber
> 
>
>Our java aplication have bug and from time to time it's trying to insert
>row on subscriber node (which shouldn't happend). My developers wants to
>have query with parametrs which is executed on subscriber node. Table
>audit.slon_audit is created only on subscriber node to catch "bad"
>INSERT/UPDATE queries. I know that this queries shoud executed only on
>origin. I want to find the cause of this behavior of our aplication.
>
>


In that case why not just set log_min_duration_statement=0 for the user(s) of the java app?

From krzysztof.jakowczyk at unity.pl  Thu Jan 14 03:32:30 2016
From: krzysztof.jakowczyk at unity.pl (Krzysztof Jakowczyk)
Date: Thu, 14 Jan 2016 12:32:30 +0100
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <1951480679.7908114.1452770200257.JavaMail.yahoo@mail.yahoo.com>
References: <569772B5.90409@unity.pl>
	<1951480679.7908114.1452770200257.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5697874E.2070907@unity.pl>

Cause databases have huge load and this setting will kill disk array. ;)

-- 
Pozdrawiam,

Krzysztof Jakowczyk
Administrator System?w Unix
 
Grupa Unity | ul. Przedmiejska 6-10, 54-201 Wroc?aw
ul. Conrada 55B, 31-357 Krak?w | ul. Z?ota 59, 00-120 Warszawa


On 14.01.2016 12:16, Glyn Astill wrote:
>> From: Krzysztof Jakowczyk <krzysztof.jakowczyk at unity.pl>
>> To: Steve Singer <ssinger at ca.afilias.info>; Steve Singer <steve at ssinger.info> 
>> Cc: slony1-general <slony1-general at lists.slony.info>
>> Sent: Thursday, 14 January 2016, 10:04
>> Subject: Re: [Slony1-general] log insert/update query executed on subscriber
>>
>>
>> Our java aplication have bug and from time to time it's trying to insert
>> row on subscriber node (which shouldn't happend). My developers wants to
>> have query with parametrs which is executed on subscriber node. Table
>> audit.slon_audit is created only on subscriber node to catch "bad"
>> INSERT/UPDATE queries. I know that this queries shoud executed only on
>> origin. I want to find the cause of this behavior of our aplication.
>>
>>
>
> In that case why not just set log_min_duration_statement=0 for the user(s) of the java app?
> .
>


From steve at ssinger.info  Thu Jan 14 04:33:39 2016
From: steve at ssinger.info (Steve Singer)
Date: Thu, 14 Jan 2016 07:33:39 -0500 (EST)
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <569772B5.90409@unity.pl>
References: <56952825.8040801@unity.pl>
	<alpine.DEB.2.02.1601122141490.11931@mini.atlantida>
	<5695F417.7020903@unity.pl> <5696D1A4.6010904@ca.afilias.info>
	<569772B5.90409@unity.pl>
Message-ID: <alpine.DEB.2.02.1601140731570.11931@mini.atlantida>

On Thu, 14 Jan 2016, Krzysztof Jakowczyk wrote:

> Our java aplication have bug and from time to time it's trying to insert
> row on subscriber node (which shouldn't happend). My developers wants to
> have query with parametrs which is executed on subscriber node. Table
> audit.slon_audit is created only on subscriber node to catch "bad"
> INSERT/UPDATE queries. I know that this queries shoud executed only on
> origin. I want to find the cause of this behavior of our aplication.

Even if the audit trigger fires first the denyAccess trigger will abort the 
transaction causing any changes added to the audit table to be rolled back.



>
>
> On 13.01.2016 23:37, Steve Singer wrote:
>> On 01/13/2016 01:52 AM, Krzysztof Jakowczyk wrote:
>>> No, it doesn't. Take a look:
>>>
>>> # \d emp
>>>        Table "public.emp"
>>>   Column  |  Type   | Modifiers
>>> ---------+---------+-----------
>>>   empname | text    | not null
>>>   salary  | integer |
>>> Triggers:
>>>      _b24v2_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH
>>> ROW EXECUTE PROCEDURE _b24v2.denyaccess('_b24v2')
>>> Triggers firing always:
>>>      __audit BEFORE INSERT OR DELETE OR UPDATE ON emp FOR EACH ROW
>>> EXECUTE PROCEDURE save_query()
>>>
>>>
>>> -- with disabled trigger denyaccess
>>> # insert into emp values ('TEST dasdasdas',123123);
>>> INSERT 0 1
>>> # select * from audit.slon_audit;
>>>   operation |           stamp            |  userid
>>> |                       query
>>> -----------+----------------------------+----------+---------------------------------------------------
>>>
>>>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
>>> values ('TEST dasdasdas',123123);
>>> (1 row)
>>>
>>> -- enabling trigger denyaccess
>>> # alter table emp enable trigger _b24v2_denyaccess;
>>> ALTER TABLE
>>> # insert into emp values ('test2',31337);
>>> ERROR:  Slony-I: Table emp is replicated and cannot be modified on a
>>> subscriber node - role=0
>>
>> I guess I don't understand what it is your trying to do.
>>
>> What I thought you were asking was this
>>
>> You have the table "emp" which has node1 as the origin.
>>
>> You replicate the table "emp" to node 2.
>>
>> You have the audit.slon_audit table on both node1 and node2. You want
>> the trigger to fire on both node1 and node2 instead of replicating
>> audit.slon_audit
>>
>> If that's the case why are you trying your insert on node2?
>> You want to perform the insert on node1,and then the audit trigger
>> should run (assuming you configure it as an always trigger) on both
>> node1 and on node2 inserting data into your audit table
>>
>>
>>
>>
>>
>>> # select * from audit.slon_audit;
>>>   operation |           stamp            |  userid
>>> |                       query
>>> -----------+----------------------------+----------+---------------------------------------------------
>>>
>>>   I         | 2016-01-13 06:46:15.417799 | postgres | insert into emp
>>> values ('TEST dasdasdas',123123);
>>> (1 row)
>>>
>>>
>>> Nothing happend. Any other ideas?
>>>
>>
>> .
>>
>
>


From glynastill at yahoo.co.uk  Thu Jan 14 05:00:23 2016
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu, 14 Jan 2016 13:00:23 +0000 (UTC)
Subject: [Slony1-general] log insert/update query executed on subscriber
In-Reply-To: <5697874E.2070907@unity.pl>
References: <5697874E.2070907@unity.pl>
Message-ID: <740308734.8086051.1452776423128.JavaMail.yahoo@mail.yahoo.com>





----- Original Message -----
> From: Krzysztof Jakowczyk <krzysztof.jakowczyk at unity.pl>
> To: Glyn Astill <glynastill at yahoo.co.uk>; Steve Singer <ssinger at ca.afilias.info>; Steve Singer <steve at ssinger.info>
> Cc: slony1-general <slony1-general at lists.slony.info>
> Sent: Thursday, 14 January 2016, 11:32
> Subject: Re: [Slony1-general] log insert/update query executed on subscriber
> 
> Cause databases have huge load and this setting will kill disk array. ;)


We'll regardless, any action performed by your triggers will roll back.

Perhaps you could log the information you need with a RAISE LOG/NOTICE in your trigger instead of trying to store it in a transactional table?

From steve at ssinger.info  Thu Jan 14 06:47:36 2016
From: steve at ssinger.info (Steve Singer)
Date: Thu, 14 Jan 2016 09:47:36 -0500
Subject: [Slony1-general] Slon blocked by an index on a non replicated
 table?? 2.2.3
In-Reply-To: <CAEaSS0airdGM18FVie0-84ys+Gh+3MFFcxFOhtziAToaJLJA-Q@mail.gmail.com>
References: <CAEaSS0aby6Gk868aoUmDUTvuLEQjMc=tHUO1-3CsGegP2ua2Tg@mail.gmail.com>	<alpine.DEB.2.02.1601132116520.11931@mini.atlantida>
	<CAEaSS0airdGM18FVie0-84ys+Gh+3MFFcxFOhtziAToaJLJA-Q@mail.gmail.com>
Message-ID: <5697B508.3090503@ssinger.info>

On 01/13/2016 09:38 PM, Tory M Blue wrote:
>
>
> On Wed, Jan 13, 2016 at 6:30 PM, Steve Singer <steve at ssinger.info
> <mailto:steve at ssinger.info>> wrote:
>
>     On Wed, 13 Jan 2016, Tory M Blue wrote:
>
>     Tory,
>
>     You talk about slon 'initializing'.  When subscriptions start it
>     needs to wait until all in progress transactions are committed
>     before starting the copy. Once your cluster is subscribed a create
>     index shouldn't block things.
>
>
> Ya 2 different issues, sorry. but the Initialization part, even if the
> table being indexed is not part of the set? That rings weird and I
> really wish i could find the other thread that had a discussion on this,
> as it has the correct error etc.
>
> But an index on a table that is not part of any replication set, blocks
> slony from starting the copy?  We are talking table based replication
> here right, so we are not looking at the db level, which I could sort of
> understand. Since slony is replicating tables, if this table is not part
> of any set and thus is not being replicated, why does that hold true?
>

ANY in progress transaction on the master blocks the initial copy even 
if it hasn't yet touched a replicated table.

The comment in the code explains the reason for this and is as follows


	/*
	 * Begin a serialized transaction and verify that the event's snapshot
	 * xxid is less than the present snapshot. This ensures that all
	 * transactions that have been in progress when the subscription got
	 * enabled (which is after the triggers on the tables have been defined),
	 * have finished. Otherwise a long running open transaction would not have
	 * the trigger definitions yet, and an insert would not get logged. But if
	 * it still runs when we start to copy the set, then we don't see the row
	 * either and it would get lost.



> And while I'm fully replicated now, if I try to index these tables, slon
> backs up and if I kill the index, the replications set happen
> immediately. So something is happening with these tables. These are
> archive tables, nothing is accessing them, they are here purely for
> historical purposes.  So I'm at a loss and I don't expect anyone to have
> an immediate answer, but it seems weird and would love to provide any
> necessary information to help frame the question/issue better, if
> someone can help me do that :)
>
> Thanks again Steve!
>
> Tory
>
>
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From steve at ssinger.info  Thu Jan 14 07:02:28 2016
From: steve at ssinger.info (Steve Singer)
Date: Thu, 14 Jan 2016 10:02:28 -0500
Subject: [Slony1-general] Make all slon options actually settable from
 the command line?
In-Reply-To: <20160113235929.GA11754@fetter.org>
References: <20160113184351.GB28070@fetter.org>	<CANfbgbZT0w4FjiJQfo9xJvkyxOaQiMsmTTe6pkOGtQW_nti5cA@mail.gmail.com>
	<20160113235929.GA11754@fetter.org>
Message-ID: <5697B884.8080902@ssinger.info>

On 01/13/2016 06:59 PM, David Fetter wrote:
>
> I'm happy to add in some form of getopt_long, which I generally prefer
> for self-documentation purposes.  Do we need to do something special
> the way PostgreSQL does?  I presume simply mandating GNU wouldn't
> work.

If your hoping to have something that we could add for a 2.2.x then 
there I wouldn't want to introduce or change any dependencies.

In terms of requiring GNU getopt for 2.3.x I guess it depends on how 
common that is on other platforms:the bsd's, solaris and AIX.  I don't 
think slon uses command line options on win32 (but I could be wrong). I 
do expect slony to build on those platforms without requiring a lot more 
effort than what would be required to build PG.

My take is adding additional command line options is fine for a dot 
release as long as we don't change the names of any existing options. If 
anyone disagrees they should speak up.




>
>> If you're keen on patching in long-ish option names for everything,
>> I don't see a big reason to struggle against that.
>
> I'd be delighted to improve the clarity here.
>
> Cheers,
> David.
>


From ajs at crankycanuck.ca  Thu Jan 14 07:04:38 2016
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu, 14 Jan 2016 10:04:38 -0500
Subject: [Slony1-general] Slon blocked by an index on a non replicated
 table?? 2.2.3
In-Reply-To: <5697B508.3090503@ssinger.info>
References: <CAEaSS0aby6Gk868aoUmDUTvuLEQjMc=tHUO1-3CsGegP2ua2Tg@mail.gmail.com>
	<alpine.DEB.2.02.1601132116520.11931@mini.atlantida>
	<CAEaSS0airdGM18FVie0-84ys+Gh+3MFFcxFOhtziAToaJLJA-Q@mail.gmail.com>
	<5697B508.3090503@ssinger.info>
Message-ID: <20160114150438.GP34398@crankycanuck.ca>

On Thu, Jan 14, 2016 at 09:47:36AM -0500, Steve Singer wrote:
> ANY in progress transaction on the master blocks the initial copy even 
> if it hasn't yet touched a replicated table.

FWIW, I dimly recall that this restriction was something we learned
about by experience (at Afilias) either very early in the Slony work
or, more likely, with the previous (really broken) eRserver.  Age and
bitrot may be conflating this with a somewhat-related side effect of a
different bug, however.

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From vivek at khera.org  Thu Jan 14 08:29:04 2016
From: vivek at khera.org (Vick Khera)
Date: Thu, 14 Jan 2016 11:29:04 -0500
Subject: [Slony1-general] Make all slon options actually settable from
 the command line?
In-Reply-To: <5697B884.8080902@ssinger.info>
References: <20160113184351.GB28070@fetter.org>
	<CANfbgbZT0w4FjiJQfo9xJvkyxOaQiMsmTTe6pkOGtQW_nti5cA@mail.gmail.com>
	<20160113235929.GA11754@fetter.org> <5697B884.8080902@ssinger.info>
Message-ID: <CALd+dcemdaXG3w_E5Hir_QCQ-5hrm913=S8D0nfi5R44aB=ReA@mail.gmail.com>

On Thu, Jan 14, 2016 at 10:02 AM, Steve Singer <steve at ssinger.info> wrote:
> My take is adding additional command line options is fine for a dot
> release as long as we don't change the names of any existing options. If
> anyone disagrees they should speak up.

Agreed. If there are no new library dependencies and there are no
changes to existing command line then there is no problem adding more.

From fmcgeough at gmail.com  Thu Jan 21 07:42:17 2016
From: fmcgeough at gmail.com (Frank McGeough)
Date: Thu, 21 Jan 2016 10:42:17 -0500
Subject: [Slony1-general] duplicate key value violates unique constraint?
Message-ID: <E096D6E4-78DC-4964-A73D-8BA16498A77E@gmail.com>

I?m having intermittent issues that result in the message "duplicate key value violates unique constraint? in the slony logs. Here is my configuration :

1 master
2 slaves
postgresql 9.3.5 on all servers
slony2.2.2

All the servers are in the same network and running the same Linux o/s with the same file system. The issues just seem to happen randomly. They happen primarily on one table but have occurred on two different tables up to this point (the tables are logically related in the database - one is populated by events on the other one, the one that has most of the issues is the one with the trigger on it that populates the other table). When the error occurs it may occur on one slave or the other. Once on both.

These incidents have happened 6 times in the past week. Prior to this issue we?ve been running the same setup for many months without incident.

What I do to correct this is to actually remove the entry that is causing the duplicate key issue from the sl_log table. Replication picks back up again. When I compare the rows on all servers I see that they are the same. In all cases when the servers are back in sync the row is actually gone from all servers. 

There is no obvious corruption issue on the slave boxes. I?m not sure what else to check at this point. Are there suggestions from the slony developers on how to investigate what is actually going on to cause these issues? Any help is appreciated at this point. 

sample error :

2016-01-21 01:19:01 UTC ERROR  remoteWorkerThread_1_1: error at end of COPY IN: ERROR:  duplicate key value violates unique constraint "device_properties_pkey"
DETAIL:  Key (device_guid, property_guid, data_index)=(26464008, 39, 0) already exists.
CONTEXT:  SQL statement "INSERT INTO "device"."device_properties" ("device_guid", "property_guid", "data_index", "property_value", "tran_id", "dt_last_updated", "last_updated_by_userid", "version_id") VALUES ($1, $2, $3, $4, $5, $6, $7, $8);"
COPY sl_log_1, line 36: "1      6117236903      298     7797340846      device  device_properties       I       0       {device_guid,26464008,property_guid,39,data?"

thanks, 
Frank



From steve at ssinger.info  Thu Jan 21 08:50:23 2016
From: steve at ssinger.info (Steve Singer)
Date: Thu, 21 Jan 2016 11:50:23 -0500
Subject: [Slony1-general] duplicate key value violates unique constraint?
In-Reply-To: <E096D6E4-78DC-4964-A73D-8BA16498A77E@gmail.com>
References: <E096D6E4-78DC-4964-A73D-8BA16498A77E@gmail.com>
Message-ID: <56A10C4F.1050200@ssinger.info>

On 01/21/2016 10:42 AM, Frank McGeough wrote:
> I?m having intermittent issues that result in the message "duplicate key value violates unique constraint? in the slony logs. Here is my configuration :
>
> 1 master
> 2 slaves
> postgresql 9.3.5 on all servers
> slony2.2.2
>
> All the servers are in the same network and running the same Linux o/s with the same file system. The issues just seem to happen randomly. They happen primarily on one table but have occurred on two different tables up to this point (the tables are logically related in the database - one is populated by events on the other one, the one that has most of the issues is the one with the trigger on it that populates the other table). When the error occurs it may occur on one slave or the other. Once on both.
>
> These incidents have happened 6 times in the past week. Prior to this issue we?ve been running the same setup for many months without incident.
>
> What I do to correct this is to actually remove the entry that is causing the duplicate key issue from the sl_log table. Replication picks back up again. When I compare the rows on all servers I see that they are the same. In all cases when the servers are back in sync the row is actually gone from all servers.
>
> There is no obvious corruption issue on the slave boxes. I?m not sure what else to check at this point. Are there suggestions from the slony developers on how to investigate what is actually going on to cause these issues? Any help is appreciated at this point.

Above you say 'the one with the trigger on it that populates the other 
table'

You sort of imply you have triggers on tables but don't provide any details.

Do these triggers fire on replicas, the origin, everywhere?

Do the triggers insert data into a replicated table?

The conflicting row, you see the second insert for it in sl_log but do 
you know where the first instance comes from?  Is the row recently 
added? (if so you do you see the first insert in sl_log also) or has the 
row been there a long time.

Is the conflicting row otherwise identical to the existing row or is it 
only identical with respet to the primary key (ie the last_updated 
times, do they match?)




>
> sample error :
>
> 2016-01-21 01:19:01 UTC ERROR  remoteWorkerThread_1_1: error at end of COPY IN: ERROR:  duplicate key value violates unique constraint "device_properties_pkey"
> DETAIL:  Key (device_guid, property_guid, data_index)=(26464008, 39, 0) already exists.
> CONTEXT:  SQL statement "INSERT INTO "device"."device_properties" ("device_guid", "property_guid", "data_index", "property_value", "tran_id", "dt_last_updated", "last_updated_by_userid", "version_id") VALUES ($1, $2, $3, $4, $5, $6, $7, $8);"
> COPY sl_log_1, line 36: "1      6117236903      298     7797340846      device  device_properties       I       0       {device_guid,26464008,property_guid,39,data?"
>
> thanks,
> Frank
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From fmcgeough at gmail.com  Thu Jan 21 09:04:42 2016
From: fmcgeough at gmail.com (Frank McGeough)
Date: Thu, 21 Jan 2016 12:04:42 -0500
Subject: [Slony1-general] duplicate key value violates unique constraint?
In-Reply-To: <56A10C4F.1050200@ssinger.info>
References: <E096D6E4-78DC-4964-A73D-8BA16498A77E@gmail.com>
	<56A10C4F.1050200@ssinger.info>
Message-ID: <7B97C8FE-B946-4236-BB2A-7903BBA89693@gmail.com>

 On Jan 21, 2016, at 11:50 AM, Steve Singer <steve at ssinger.info> wrote:
>  
> On 01/21/2016 10:42 AM, Frank McGeough wrote:
>> I?m having intermittent issues that result in the message "duplicate key value violates unique constraint? in the slony logs. Here is my configuration :
>> 
>> 1 master
>> 2 slaves
>> postgresql 9.3.5 on all servers
>> slony2.2.2
>> 
>> All the servers are in the same network and running the same Linux o/s with the same file system. The issues just seem to happen randomly. They happen primarily on one table but have occurred on two different tables up to this point (the tables are logically related in the database - one is populated by events on the other one, the one that has most of the issues is the one with the trigger on it that populates the other table). When the error occurs it may occur on one slave or the other. Once on both.
>> 
>> These incidents have happened 6 times in the past week. Prior to this issue we?ve been running the same setup for many months without incident.
>> 
>> What I do to correct this is to actually remove the entry that is causing the duplicate key issue from the sl_log table. Replication picks back up again. When I compare the rows on all servers I see that they are the same. In all cases when the servers are back in sync the row is actually gone from all servers.
>> 
>> There is no obvious corruption issue on the slave boxes. I?m not sure what else to check at this point. Are there suggestions from the slony developers on how to investigate what is actually going on to cause these issues? Any help is appreciated at this point.
> 
> Above you say 'the one with the trigger on it that populates the other table'
> 
> You sort of imply you have triggers on tables but don't provide any details.
> 
> Do these triggers fire on replicas, the origin, everywhere?
> 
> Do the triggers insert data into a replicated table?
> 
> The conflicting row, you see the second insert for it in sl_log but do you know where the first instance comes from?  Is the row recently added? (if so you do you see the first insert in sl_log also) or has the row been there a long time.
> 
> Is the conflicting row otherwise identical to the existing row or is it only identical with respet to the primary key (ie the last_updated times, do they match?)
> 
> 

the triggers only fire on the origin. The two tables in our system are : device_properties and device_graph. device_properties has a slew of triggers :

device_properties_change_tg AFTER INSERT OR DELETE OR UPDATE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.device_properties_change()
device_properties_graph_tg AFTER INSERT OR DELETE OR UPDATE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.update_graph()
device_properties_location_id_change_tg AFTER INSERT OR DELETE OR UPDATE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.device_properties_change_location_id()
device_properties_tg AFTER INSERT OR DELETE OR UPDATE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.delete_cached_device_properties()
device_properties_ids_ondelete AFTER DELETE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.process_delete_device_ids()
fax_email_history_tg AFTER DELETE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.fax_email_history()
    t_dirty_device_property_sip_credentials AFTER INSERT OR DELETE OR UPDATE ON device.device_properties FOR EACH ROW EXECUTE PROCEDURE device.fn_t_dirty_device_property_sip_credentials()

all of these triggers are potentially writing data into other tables that are also replicated from origin. 

the conflicting row is identical to the existing row. 

> 
> 
>> 
>> sample error :
>> 
>> 2016-01-21 01:19:01 UTC ERROR  remoteWorkerThread_1_1: error at end of COPY IN: ERROR:  duplicate key value violates unique constraint "device_properties_pkey"
>> DETAIL:  Key (device_guid, property_guid, data_index)=(26464008, 39, 0) already exists.
>> CONTEXT:  SQL statement "INSERT INTO "device"."device_properties" ("device_guid", "property_guid", "data_index", "property_value", "tran_id", "dt_last_updated", "last_updated_by_userid", "version_id") VALUES ($1, $2, $3, $4, $5, $6, $7, $8);"
>> COPY sl_log_1, line 36: "1      6117236903      298     7797340846      device  device_properties       I       0       {device_guid,26464008,property_guid,39,data?"
>> 
>> thanks,
>> Frank
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info <mailto:Slony1-general at lists.slony.info>
>> http://lists.slony.info/mailman/listinfo/slony1-general <http://lists.slony.info/mailman/listinfo/slony1-general>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160121/d7aa8173/attachment-0001.htm 

From ttignor at akamai.com  Thu Jan 28 05:30:30 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 28 Jan 2016 13:30:30 +0000
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in progress
 - sl_log_1 not truncated
Message-ID: <D2CF8225.4C0CD%ttignor@akamai.com>


Hello slony folks,

From jan at wi3ck.info  Thu Jan 28 07:38:57 2016
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 28 Jan 2016 10:38:57 -0500
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <D2CF8225.4C0CD%ttignor@akamai.com>
References: <D2CF8225.4C0CD%ttignor@akamai.com>
Message-ID: <56AA3611.4000309@wi3ck.info>

On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>
> Hello slony folks,
>  From my reading I?m guessing (hoping) this isn?t a new problem. I have
> a simple cluster with one provider replicating to three subscribers. The
> provider?s changelog tables (sl_log_[1|2]) are fine, but the subscribers
> (with forwarding enabled) are all showing runaway growth. Looked through
> the FAQ and I don?t see the node I dropped or any idle transactions as
> viable culprits. Are there other thoughts on the cause? Can I safely
> manually delete/truncate some/all of the changelog tables? These
> replicas are all leaf nodes. I only have forwarding turned on to allow
> for failover, and my replication rate is the 2 sec default.
> Thanks in advance for any insights.

What is the output of the sl_status view "on those leaf nodes"?


>
> ams=# select
> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>
>   pg_size_pretty
>
> ----------------
>
>   75 MB
>
> (1 row)
>
>
> ams=# select
> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>
>   pg_size_pretty
>
> ----------------
>
>   34 GB
>
> (1 row)
>
>
> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
> (select no_id from _ams_cluster.sl_node) or con_received not in (select
> no_id from _ams_cluster.sl_node);
>
>   con_origin | con_received | con_seqno | con_timestamp
>
> ------------+--------------+-----------+---------------
>
> (0 rows)
>
>
> ams=# select * from pg_stat_activity where current_query like '%IDLE%';
>
>   datid | datname | procpid | usesysid |  usename   |
> application_name      |  client_addr   | client_hostname | client_port |
>          backend_start         |          xact_start           |
>    query_start          | waiting |
>
>                          current_query
>
> -------+---------+---------+----------+------------+---------------------------+----------------+-----------------+-------------+-------------------------------+-------------------------------+-------------------------------+---------+---
>
> ----------------------------------------------------------------
>
>   16393 | ams     |    2611 |   212995 | ams_viewer |
>          | 88.221.209.10  |                 |       43328 | 2016-01-28
> 12:24:49.706389+00 |                               | 2016-01-28
> 13:18:02.427848+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |   12894 |   212995 | ams_viewer |
>          | 88.221.209.10  |                 |       60112 | 2016-01-28
> 12:47:26.230681+00 |                               | 2016-01-28
> 13:15:27.744242+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |   12884 |   212995 | ams_viewer |
>          | 88.221.209.10  |                 |       44302 | 2016-01-28
> 12:47:25.100006+00 |                               | 2016-01-28
> 13:15:27.936059+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>          |                |                 |          -1 | 2016-01-28
> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
> 13:18:37.283992+00 | f       | se
>
> lect * from pg_stat_activity where current_query like '%IDLE%';
>
>   16393 | ams     |    6719 |   213867 | ams_slony  |
> slon.origin_2_provider_2  | 60.254.150.133 |                 |
> 61806 | 2016-01-22 01:59:14.800129+00 |                               |
> 2016-01-28 13:18:25.935111+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    6718 |   213867 | ams_slony  |
> slon.origin_3_provider_2  | 60.254.150.133 |                 |
> 61805 | 2016-01-22 01:59:14.797655+00 |                               |
> 2016-01-28 13:18:34.304475+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    5505 |   213867 | ams_slony  |
> slon.origin_4_provider_2  | 80.67.75.105   |                 |
> 36477 | 2016-01-22 01:56:25.637046+00 |                               |
> 2016-01-28 13:18:36.1348+00   | f       | <I
>
> DLE>
>
>   16393 | ams     |    5504 |   213867 | ams_slony  |
> slon.origin_3_provider_2  | 72.246.50.22   |                 |
> 51813 | 2016-01-22 01:56:25.240798+00 |                               |
> 2016-01-28 13:18:28.961629+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    5487 |   213867 | ams_slony  |
> slon.origin_4_provider_2  | 72.246.50.22   |                 |
> 51803 | 2016-01-22 01:56:22.896388+00 |                               |
> 2016-01-28 13:18:35.858913+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    5047 |   213867 | ams_slony  |
> slon.origin_2_provider_2  | 72.246.50.22   |                 |
> 51564 | 2016-01-22 01:55:23.600296+00 |                               |
> 2016-01-28 13:18:34.487192+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    5041 |   213867 | ams_slony  |
> slon.origin_2_provider_2  | 80.67.75.105   |                 |
> 36402 | 2016-01-22 01:55:22.964462+00 |                               |
> 2016-01-28 13:18:34.519066+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    6694 |   213867 | ams_slony  |
> slon.node_2_listen        | 60.254.150.133 |                 |
> 61795 | 2016-01-22 01:59:12.095052+00 |                               |
> 2016-01-28 13:18:27.928384+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4456 |   213867 | ams_slony  |
> slon.node_2_listen        | 72.246.50.22   |                 |
> 51238 | 2016-01-22 01:54:21.481355+00 |                               |
> 2016-01-28 13:18:36.766973+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4457 |   213867 | ams_slony  |
> slon.node_2_listen        | 80.67.75.105   |                 |
> 36333 | 2016-01-22 01:54:21.500456+00 |                               |
> 2016-01-28 13:18:36.204482+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4428 |   213867 | ams_slony  |
> slon.local_monitor        |                |                 |
> -1 | 2016-01-22 01:54:18.977015+00 |                               |
> 2016-01-28 13:18:36.652567+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4427 |   213867 | ams_slony  | slon.local_sync
>          |                |                 |          -1 | 2016-01-22
> 01:54:18.976932+00 |                               | 2016-01-28
> 13:18:36.151998+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4426 |   213867 | ams_slony  |
> slon.local_cleanup        |                |                 |
> -1 | 2016-01-22 01:54:18.976842+00 |                               |
> 2016-01-28 13:12:12.582921+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4425 |   213867 | ams_slony  |
> slon.remoteWorkerThread_4 |                |                 |
> -1 | 2016-01-22 01:54:18.976783+00 |                               |
> 2016-01-28 13:18:33.99715+00  | f       | <I
>
> DLE>
>
>   16393 | ams     |    4420 |   213867 | ams_slony  |
> slon.remoteWorkerThread_1 |                |                 |
> -1 | 2016-01-22 01:54:18.976548+00 |                               |
> 2016-01-28 13:18:33.561531+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4419 |   213867 | ams_slony  |
> slon.remoteWorkerThread_3 |                |                 |
> -1 | 2016-01-22 01:54:18.97647+00  |                               |
> 2016-01-28 13:18:34.808907+00 | f       | <I
>
> DLE>
>
>   16393 | ams     |    4413 |   213867 | ams_slony  | slon.local_listen
>          |                |                 |          -1 | 2016-01-22
> 01:54:18.965568+00 |                               | 2016-01-28
> 13:18:37.096159+00 | f       | <I
>
> DLE>
>
> (21 rows)
>
>
> ams=#
>
>
>
> Tom    :-)
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From ttignor at akamai.com  Thu Jan 28 08:11:01 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Thu, 28 Jan 2016 16:11:01 +0000
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <56AA3611.4000309@wi3ck.info>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
Message-ID: <D2CFA723.4C118%ttignor@akamai.com>


	Output below. They seem to be replicating normally, except for the sl_log
growth.


ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
 st_origin | st_received | st_last_event |       st_last_event_ts        |
st_last_received |      st_last_received_ts      |
st_last_received_event_ts   | st_lag_num_events |   st_lag_time
-----------+-------------+---------------+-------------------------------+-
-----------------+-------------------------------+-------------------------
------+-------------------+-----------------
         2 |           1 |    5000611610 | 2016-01-28 16:06:37.343826+00 |
      5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
16:06:37.343826+00 |                 0 | 00:00:09.201996
         2 |           3 |    5000611610 | 2016-01-28 16:06:37.343826+00 |
      5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
16:06:27.341894+00 |                 1 | 00:00:19.203928
         2 |           4 |    5000611610 | 2016-01-28 16:06:37.343826+00 |
      5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
16:06:37.343826+00 |                 0 | 00:00:09.201996
(3 rows)


ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
 st_origin | st_received | st_last_event |       st_last_event_ts        |
st_last_received |      st_last_received_ts      |
st_last_received_event_ts   | st_lag_num_events |   st_lag_time
-----------+-------------+---------------+-------------------------------+-
-----------------+-------------------------------+-------------------------
------+-------------------+-----------------
         3 |           4 |    5000654642 | 2016-01-28 16:07:05.493455+00 |
      5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
16:07:05.493455+00 |                 0 | 00:00:08.522529
         3 |           1 |    5000654642 | 2016-01-28 16:07:05.493455+00 |
      5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
16:07:05.493455+00 |                 0 | 00:00:08.522529
         3 |           2 |    5000654642 | 2016-01-28 16:07:05.493455+00 |
      5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
16:07:05.493455+00 |                 0 | 00:00:08.522529
(3 rows)


ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
 st_origin | st_received | st_last_event |       st_last_event_ts        |
st_last_received |      st_last_received_ts      |
st_last_received_event_ts   | st_lag_num_events |   st_lag_time
-----------+-------------+---------------+-------------------------------+-
-----------------+-------------------------------+-------------------------
------+-------------------+-----------------
         4 |           3 |    5000637483 | 2016-01-28 16:07:32.698809+00 |
      5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
16:07:22.695826+00 |                 1 | 00:00:19.077657
         4 |           1 |    5000637483 | 2016-01-28 16:07:32.698809+00 |
      5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
16:07:22.695826+00 |                 1 | 00:00:19.077657
         4 |           2 |    5000637483 | 2016-01-28 16:07:32.698809+00 |
      5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
16:07:22.695826+00 |                 1 | 00:00:19.077657
(3 rows)



	Tom    :-)



On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:

>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>
>> Hello slony folks,
>>  From my reading I?m guessing (hoping) this isn?t a new problem. I have
>> a simple cluster with one provider replicating to three subscribers. The
>> provider?s changelog tables (sl_log_[1|2]) are fine, but the subscribers
>> (with forwarding enabled) are all showing runaway growth. Looked through
>> the FAQ and I don?t see the node I dropped or any idle transactions as
>> viable culprits. Are there other thoughts on the cause? Can I safely
>> manually delete/truncate some/all of the changelog tables? These
>> replicas are all leaf nodes. I only have forwarding turned on to allow
>> for failover, and my replication rate is the 2 sec default.
>> Thanks in advance for any insights.
>
>What is the output of the sl_status view "on those leaf nodes"?
>
>
>>
>> ams=# select
>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>
>>   pg_size_pretty
>>
>> ----------------
>>
>>   75 MB
>>
>> (1 row)
>>
>>
>> ams=# select
>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>
>>   pg_size_pretty
>>
>> ----------------
>>
>>   34 GB
>>
>> (1 row)
>>
>>
>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>> (select no_id from _ams_cluster.sl_node) or con_received not in (select
>> no_id from _ams_cluster.sl_node);
>>
>>   con_origin | con_received | con_seqno | con_timestamp
>>
>> ------------+--------------+-----------+---------------
>>
>> (0 rows)
>>
>>
>> ams=# select * from pg_stat_activity where current_query like '%IDLE%';
>>
>>   datid | datname | procpid | usesysid |  usename   |
>> application_name      |  client_addr   | client_hostname | client_port |
>>          backend_start         |          xact_start           |
>>    query_start          | waiting |
>>
>>                          current_query
>>
>> 
>>-------+---------+---------+----------+------------+---------------------
>>------+----------------+-----------------+-------------+-----------------
>>--------------+-------------------------------+--------------------------
>>-----+---------+---
>>
>> ----------------------------------------------------------------
>>
>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>          | 88.221.209.10  |                 |       43328 | 2016-01-28
>> 12:24:49.706389+00 |                               | 2016-01-28
>> 13:18:02.427848+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>          | 88.221.209.10  |                 |       60112 | 2016-01-28
>> 12:47:26.230681+00 |                               | 2016-01-28
>> 13:15:27.744242+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>          | 88.221.209.10  |                 |       44302 | 2016-01-28
>> 12:47:25.100006+00 |                               | 2016-01-28
>> 13:15:27.936059+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>          |                |                 |          -1 | 2016-01-28
>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>> 13:18:37.283992+00 | f       | se
>>
>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>
>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>> 61806 | 2016-01-22 01:59:14.800129+00 |                               |
>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>> 61805 | 2016-01-22 01:59:14.797655+00 |                               |
>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>> 36477 | 2016-01-22 01:56:25.637046+00 |                               |
>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>> 51813 | 2016-01-22 01:56:25.240798+00 |                               |
>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>> 51803 | 2016-01-22 01:56:22.896388+00 |                               |
>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>> 51564 | 2016-01-22 01:55:23.600296+00 |                               |
>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>> 36402 | 2016-01-22 01:55:22.964462+00 |                               |
>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>> slon.node_2_listen        | 60.254.150.133 |                 |
>> 61795 | 2016-01-22 01:59:12.095052+00 |                               |
>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>> slon.node_2_listen        | 72.246.50.22   |                 |
>> 51238 | 2016-01-22 01:54:21.481355+00 |                               |
>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>> slon.node_2_listen        | 80.67.75.105   |                 |
>> 36333 | 2016-01-22 01:54:21.500456+00 |                               |
>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>> slon.local_monitor        |                |                 |
>> -1 | 2016-01-22 01:54:18.977015+00 |                               |
>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4427 |   213867 | ams_slony  | slon.local_sync
>>          |                |                 |          -1 | 2016-01-22
>> 01:54:18.976932+00 |                               | 2016-01-28
>> 13:18:36.151998+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>> slon.local_cleanup        |                |                 |
>> -1 | 2016-01-22 01:54:18.976842+00 |                               |
>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>> slon.remoteWorkerThread_4 |                |                 |
>> -1 | 2016-01-22 01:54:18.976783+00 |                               |
>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>> slon.remoteWorkerThread_1 |                |                 |
>> -1 | 2016-01-22 01:54:18.976548+00 |                               |
>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>> slon.remoteWorkerThread_3 |                |                 |
>> -1 | 2016-01-22 01:54:18.97647+00  |                               |
>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>
>> DLE>
>>
>>   16393 | ams     |    4413 |   213867 | ams_slony  | slon.local_listen
>>          |                |                 |          -1 | 2016-01-22
>> 01:54:18.965568+00 |                               | 2016-01-28
>> 13:18:37.096159+00 | f       | <I
>>
>> DLE>
>>
>> (21 rows)
>>
>>
>> ams=#
>>
>>
>>
>> Tom    :-)
>>
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
>-- 
>Jan Wieck
>Senior Software Engineer
>http://slony.info


From jan at wi3ck.info  Thu Jan 28 10:09:34 2016
From: jan at wi3ck.info (Jan Wieck)
Date: Thu, 28 Jan 2016 13:09:34 -0500
Subject: [Slony1-general] Slony-I: log switch to sl_log_2 still in
 progress - sl_log_1 not truncated
In-Reply-To: <D2CFA723.4C118%ttignor@akamai.com>
References: <D2CF8225.4C0CD%ttignor@akamai.com> <56AA3611.4000309@wi3ck.info>
	<D2CFA723.4C118%ttignor@akamai.com>
Message-ID: <56AA595E.9000706@wi3ck.info>

On 01/28/2016 11:11 AM, Tignor, Tom wrote:
>
> 	Output below. They seem to be replicating normally, except for the sl_log
> growth.

Indeed. Is there anything in the slon logs for those nodes that says why 
it doesn't finish the log switch?

Connect to the database as a the slony user.

To check if a log switch is indeed in progress, do

     SELECT last_value FROM _ams_cluster.sl_log_status;

It should be either 2 or 3. If it is 0 or 1, no log switch is in 
progress and you can start one with

     SELECCT _ams_cluster.logswitch_start();

If it is 2 or 3, then you can do

     SELECT _ams_cluster.logswitch_finish();

All these operations are harmless and will only do what is safely 
possible. Look at the code of logswitch_finish() to find out how it 
determines if the current log switch can be finished. In short, the 
cleanup thread is removing events from sl_event that have been confirmed 
by all nodes in the cluster. The function logswitch_finish() looks if 
there is anything left in sl_event, that belonged to that old log. If so 
it will not finish. Running those queries manually you can find out what 
that event is that is preventing the switch to finish.



>
>
> ams at ams-repl2.ams.netmgmt:~$ /a/third-party/postgresql/bin/psql -U
> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>   st_origin | st_received | st_last_event |       st_last_event_ts        |
> st_last_received |      st_last_received_ts      |
> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
> -----------+-------------+---------------+-------------------------------+-
> -----------------+-------------------------------+-------------------------
> ------+-------------------+-----------------
>           2 |           1 |    5000611610 | 2016-01-28 16:06:37.343826+00 |
>        5000611610 | 2016-01-28 16:06:38.843562+00 | 2016-01-28
> 16:06:37.343826+00 |                 0 | 00:00:09.201996
>           2 |           3 |    5000611610 | 2016-01-28 16:06:37.343826+00 |
>        5000611609 | 2016-01-28 16:06:29.851545+00 | 2016-01-28
> 16:06:27.341894+00 |                 1 | 00:00:19.203928
>           2 |           4 |    5000611610 | 2016-01-28 16:06:37.343826+00 |
>        5000611610 | 2016-01-28 16:06:38.710974+00 | 2016-01-28
> 16:06:37.343826+00 |                 0 | 00:00:09.201996
> (3 rows)
>
>
> ams at ams-repl3.lga.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>   st_origin | st_received | st_last_event |       st_last_event_ts        |
> st_last_received |      st_last_received_ts      |
> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
> -----------+-------------+---------------+-------------------------------+-
> -----------------+-------------------------------+-------------------------
> ------+-------------------+-----------------
>           3 |           4 |    5000654642 | 2016-01-28 16:07:05.493455+00 |
>        5000654642 | 2016-01-28 16:07:06.486539+00 | 2016-01-28
> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>           3 |           1 |    5000654642 | 2016-01-28 16:07:05.493455+00 |
>        5000654642 | 2016-01-28 16:07:08.040292+00 | 2016-01-28
> 16:07:05.493455+00 |                 0 | 00:00:08.522529
>           3 |           2 |    5000654642 | 2016-01-28 16:07:05.493455+00 |
>        5000654642 | 2016-01-28 16:07:08.472049+00 | 2016-01-28
> 16:07:05.493455+00 |                 0 | 00:00:08.522529
> (3 rows)
>
>
> ams at ams-repl4.blr.netmgmt:~$  /a/third-party/postgresql/bin/psql -U
> ams_slony -d ams -c 'select * from _ams_cluster.sl_status'
>   st_origin | st_received | st_last_event |       st_last_event_ts        |
> st_last_received |      st_last_received_ts      |
> st_last_received_event_ts   | st_lag_num_events |   st_lag_time
> -----------+-------------+---------------+-------------------------------+-
> -----------------+-------------------------------+-------------------------
> ------+-------------------+-----------------
>           4 |           3 |    5000637483 | 2016-01-28 16:07:32.698809+00 |
>        5000637482 | 2016-01-28 16:07:28.731404+00 | 2016-01-28
> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>           4 |           1 |    5000637483 | 2016-01-28 16:07:32.698809+00 |
>        5000637482 | 2016-01-28 16:07:24.839978+00 | 2016-01-28
> 16:07:22.695826+00 |                 1 | 00:00:19.077657
>           4 |           2 |    5000637483 | 2016-01-28 16:07:32.698809+00 |
>        5000637482 | 2016-01-28 16:07:22.926411+00 | 2016-01-28
> 16:07:22.695826+00 |                 1 | 00:00:19.077657
> (3 rows)
>
>
>
> 	Tom    :-)
>
>
>
> On 1/28/16, 10:38 AM, "Jan Wieck" <jan at wi3ck.info> wrote:
>
>>On 01/28/2016 08:30 AM, Tignor, Tom wrote:
>>>
>>> Hello slony folks,
>>>  From my reading I?m guessing (hoping) this isn?t a new problem. I have
>>> a simple cluster with one provider replicating to three subscribers. The
>>> provider?s changelog tables (sl_log_[1|2]) are fine, but the subscribers
>>> (with forwarding enabled) are all showing runaway growth. Looked through
>>> the FAQ and I don?t see the node I dropped or any idle transactions as
>>> viable culprits. Are there other thoughts on the cause? Can I safely
>>> manually delete/truncate some/all of the changelog tables? These
>>> replicas are all leaf nodes. I only have forwarding turned on to allow
>>> for failover, and my replication rate is the 2 sec default.
>>> Thanks in advance for any insights.
>>
>>What is the output of the sl_status view "on those leaf nodes"?
>>
>>
>>>
>>> ams=# select
>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_1'));
>>>
>>>   pg_size_pretty
>>>
>>> ----------------
>>>
>>>   75 MB
>>>
>>> (1 row)
>>>
>>>
>>> ams=# select
>>> pg_size_pretty(pg_total_relation_size('_ams_cluster.sl_log_2'));
>>>
>>>   pg_size_pretty
>>>
>>> ----------------
>>>
>>>   34 GB
>>>
>>> (1 row)
>>>
>>>
>>> ams=# select * from _ams_cluster.sl_confirm where con_origin not in
>>> (select no_id from _ams_cluster.sl_node) or con_received not in (select
>>> no_id from _ams_cluster.sl_node);
>>>
>>>   con_origin | con_received | con_seqno | con_timestamp
>>>
>>> ------------+--------------+-----------+---------------
>>>
>>> (0 rows)
>>>
>>>
>>> ams=# select * from pg_stat_activity where current_query like '%IDLE%';
>>>
>>>   datid | datname | procpid | usesysid |  usename   |
>>> application_name      |  client_addr   | client_hostname | client_port |
>>>          backend_start         |          xact_start           |
>>>    query_start          | waiting |
>>>
>>>                          current_query
>>>
>>>
>>>-------+---------+---------+----------+------------+---------------------
>>>------+----------------+-----------------+-------------+-----------------
>>>--------------+-------------------------------+--------------------------
>>>-----+---------+---
>>>
>>> ----------------------------------------------------------------
>>>
>>>   16393 | ams     |    2611 |   212995 | ams_viewer |
>>>          | 88.221.209.10  |                 |       43328 | 2016-01-28
>>> 12:24:49.706389+00 |                               | 2016-01-28
>>> 13:18:02.427848+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |   12894 |   212995 | ams_viewer |
>>>          | 88.221.209.10  |                 |       60112 | 2016-01-28
>>> 12:47:26.230681+00 |                               | 2016-01-28
>>> 13:15:27.744242+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |   12884 |   212995 | ams_viewer |
>>>          | 88.221.209.10  |                 |       44302 | 2016-01-28
>>> 12:47:25.100006+00 |                               | 2016-01-28
>>> 13:15:27.936059+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |   23466 |   213867 | ams_slony  | psql
>>>          |                |                 |          -1 | 2016-01-28
>>> 13:11:32.030343+00 | 2016-01-28 13:18:37.283992+00 | 2016-01-28
>>> 13:18:37.283992+00 | f       | se
>>>
>>> lect * from pg_stat_activity where current_query like '%IDLE%';
>>>
>>>   16393 | ams     |    6719 |   213867 | ams_slony  |
>>> slon.origin_2_provider_2  | 60.254.150.133 |                 |
>>> 61806 | 2016-01-22 01:59:14.800129+00 |                               |
>>> 2016-01-28 13:18:25.935111+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    6718 |   213867 | ams_slony  |
>>> slon.origin_3_provider_2  | 60.254.150.133 |                 |
>>> 61805 | 2016-01-22 01:59:14.797655+00 |                               |
>>> 2016-01-28 13:18:34.304475+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    5505 |   213867 | ams_slony  |
>>> slon.origin_4_provider_2  | 80.67.75.105   |                 |
>>> 36477 | 2016-01-22 01:56:25.637046+00 |                               |
>>> 2016-01-28 13:18:36.1348+00   | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    5504 |   213867 | ams_slony  |
>>> slon.origin_3_provider_2  | 72.246.50.22   |                 |
>>> 51813 | 2016-01-22 01:56:25.240798+00 |                               |
>>> 2016-01-28 13:18:28.961629+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    5487 |   213867 | ams_slony  |
>>> slon.origin_4_provider_2  | 72.246.50.22   |                 |
>>> 51803 | 2016-01-22 01:56:22.896388+00 |                               |
>>> 2016-01-28 13:18:35.858913+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    5047 |   213867 | ams_slony  |
>>> slon.origin_2_provider_2  | 72.246.50.22   |                 |
>>> 51564 | 2016-01-22 01:55:23.600296+00 |                               |
>>> 2016-01-28 13:18:34.487192+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    5041 |   213867 | ams_slony  |
>>> slon.origin_2_provider_2  | 80.67.75.105   |                 |
>>> 36402 | 2016-01-22 01:55:22.964462+00 |                               |
>>> 2016-01-28 13:18:34.519066+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    6694 |   213867 | ams_slony  |
>>> slon.node_2_listen        | 60.254.150.133 |                 |
>>> 61795 | 2016-01-22 01:59:12.095052+00 |                               |
>>> 2016-01-28 13:18:27.928384+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4456 |   213867 | ams_slony  |
>>> slon.node_2_listen        | 72.246.50.22   |                 |
>>> 51238 | 2016-01-22 01:54:21.481355+00 |                               |
>>> 2016-01-28 13:18:36.766973+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4457 |   213867 | ams_slony  |
>>> slon.node_2_listen        | 80.67.75.105   |                 |
>>> 36333 | 2016-01-22 01:54:21.500456+00 |                               |
>>> 2016-01-28 13:18:36.204482+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4428 |   213867 | ams_slony  |
>>> slon.local_monitor        |                |                 |
>>> -1 | 2016-01-22 01:54:18.977015+00 |                               |
>>> 2016-01-28 13:18:36.652567+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4427 |   213867 | ams_slony  | slon.local_sync
>>>          |                |                 |          -1 | 2016-01-22
>>> 01:54:18.976932+00 |                               | 2016-01-28
>>> 13:18:36.151998+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4426 |   213867 | ams_slony  |
>>> slon.local_cleanup        |                |                 |
>>> -1 | 2016-01-22 01:54:18.976842+00 |                               |
>>> 2016-01-28 13:12:12.582921+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4425 |   213867 | ams_slony  |
>>> slon.remoteWorkerThread_4 |                |                 |
>>> -1 | 2016-01-22 01:54:18.976783+00 |                               |
>>> 2016-01-28 13:18:33.99715+00  | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4420 |   213867 | ams_slony  |
>>> slon.remoteWorkerThread_1 |                |                 |
>>> -1 | 2016-01-22 01:54:18.976548+00 |                               |
>>> 2016-01-28 13:18:33.561531+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4419 |   213867 | ams_slony  |
>>> slon.remoteWorkerThread_3 |                |                 |
>>> -1 | 2016-01-22 01:54:18.97647+00  |                               |
>>> 2016-01-28 13:18:34.808907+00 | f       | <I
>>>
>>> DLE>
>>>
>>>   16393 | ams     |    4413 |   213867 | ams_slony  | slon.local_listen
>>>          |                |                 |          -1 | 2016-01-22
>>> 01:54:18.965568+00 |                               | 2016-01-28
>>> 13:18:37.096159+00 | f       | <I
>>>
>>> DLE>
>>>
>>> (21 rows)
>>>
>>>
>>> ams=#
>>>
>>>
>>>
>>> Tom    :-)
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>
>>
>>--
>>Jan Wieck
>>Senior Software Engineer
>>http://slony.info
>


-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From sungh.lei at gmail.com  Thu Jan 28 19:57:17 2016
From: sungh.lei at gmail.com (Sung Hsin Lei)
Date: Thu, 28 Jan 2016 22:57:17 -0500
Subject: [Slony1-general] Replication inexplicably stops
Message-ID: <CAHD_kv=0qJ+mXOgTQTXjb0tJ24kXk7yXv7H8==XMQ7gQU_DQRA@mail.gmail.com>

Hello guys,

So I have this setup that has already stopped on me 3 times the last 6
months. Each time it would replicate properly for 2-3 months and then it
would just stop. It currently is stopped since January 11, 2016. The only
way I can get replication back is to set everything up from scratch. I'm
wondering if anyone has an idea on the issue causing the stoppage. I'm
running 64-bit slony 2.2.4.

Currently, when I run slon on the replicated machine, I get the following:



C:\Program Files\PostgreSQL\9.3\bin>slon slony_Securithor2 "dbname =
Securithor2
  user = slonyuser password = securiTHOR971 port = 6234"
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: slon version 2.2.4
starting
 up
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
vac_frequenc
y = 3
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
log_level =
0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
sync_interva
l = 2000
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
sync_interva
l_timeout = 10000
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
sync_group_m
axsize = 20
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
quit_sync_pr
ovider = 0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
remote_liste
n_timeout = 300
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
monitor_inte
rval = 500
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
explain_inte
rval = 0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
tcp_keepaliv
e_idle = 0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
tcp_keepaliv
e_interval = 0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
tcp_keepaliv
e_count = 0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
apply_cache_
size = 100
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option log_pid
= 0
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
log_timestam
p = 1
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
tcp_keepaliv
e = 1
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
monitor_thre
ads = 1
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Real option
real_placeholde
r = 0.000000
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
cluster_name
= slony_Securithor2
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
conn_info = d
bname = Securithor2  user = slonyuser password = securiTHOR971 port = 6234
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option pid_file
= [N
ULL]
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
log_timestamp
_format = %Y-%m-%d %H:%M:%S %Z
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
archive_dir =
 [NULL]
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
sql_on_connec
tion = [NULL]
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
lag_interval
= [NULL]
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
command_on_lo
garchive = [NULL]
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
cleanup_inter
val = 10 minutes
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: local node id = 2
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   main: main process started
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: launching
sched_start_mainl
oop
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: loading current
cluster con
figuration
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeNode: no_id=1
no_comment='Ma
ster Node'
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storePath: pa_server=1
pa_client=
2 pa_conninfo="dbname=Securithor2 host=192.168.1.50 user=slonyuser password
= se
curiTHOR971  port = 6234" pa_connretry=10
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeListen: li_origin=1
li_recei
ver=2 li_provider=1
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeSet: set_id=1
set_origin=1 s
et_comment='All tables and sequences'
2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node 1
- no
worker thread
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeSubscribe: sub_set=1
sub_pro
vider=1 sub_forward='f'
2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node 1
- no
worker thread
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG enableSubscription: sub_set=1
2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node 1
- no
worker thread
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: last local event
sequence =
 5000462590
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: configuration complete
- st
arting threads
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   localListenThread: thread
starts
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
Securithor2
  user = slonyuser password = securiTHOR971 port = 6234" is 90310
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=5188
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG enableNode: no_id=1
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteWorkerThread_1: thread
star
ts
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteListenThread_1: thread
star
ts
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   main: running scheduler
mainloop
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG cleanupThread: thread starts
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   syncThread: thread starts
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   monitorThread: thread starts
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
Securithor2
  user = slonyuser password = securiTHOR971 port = 6234" is 90310
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG remoteWorkerThread_1: update
prov
ider configuration
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG remoteWorkerThread_1: added
activ
e set 1 to provider 1
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for
"dbname=Securithor2 h
ost=192.168.1.50 user=slonyuser password = securiTHOR971  port = 6234" is
90306
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
Securithor2
  user = slonyuser password = securiTHOR971 port = 6234" is 90310
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG cleanupThread: bias = 60
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
Securithor2
  user = slonyuser password = securiTHOR971 port = 6234" is 90310
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
Securithor2
  user = slonyuser password = securiTHOR971 port = 6234" is 90310
2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for
"dbname=Securithor2 h
ost=192.168.1.50 user=slonyuser password = securiTHOR971  port = 6234" is
90306
2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteWorkerThread_1:
syncing set
 1 with 59 table(s) from provider 1




It gets stuck at "syncing set 1 with 59 table(s) from provider 1" (the last
line) forever with the occasional messages that says something about
cleaning(threadcleaning I thing).


Checking the postgres logs, I see lots of:

2016-01-28 17:33:07 AST LOG:  n'a pas pu recevoir les donn??es du client :
unrecognized winsock error 10061

Which translates to:

2016-01-28 17:33:07 AST LOG:  was not able to receive the data from the
client : unrecognized winsock error 10061

I'm able to connect to the main db from the replicated machine no problem.
I have no idea how this error 10061 is caused.

Any ideas?

Appreciate the help.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160128/c0071c2a/attachment.htm 

From jan at wi3ck.info  Fri Jan 29 06:50:09 2016
From: jan at wi3ck.info (Jan Wieck)
Date: Fri, 29 Jan 2016 09:50:09 -0500
Subject: [Slony1-general] Replication inexplicably stops
In-Reply-To: <CAHD_kv=0qJ+mXOgTQTXjb0tJ24kXk7yXv7H8==XMQ7gQU_DQRA@mail.gmail.com>
References: <CAHD_kv=0qJ+mXOgTQTXjb0tJ24kXk7yXv7H8==XMQ7gQU_DQRA@mail.gmail.com>
Message-ID: <56AB7C21.9040800@wi3ck.info>

On 01/28/2016 10:57 PM, Sung Hsin Lei wrote:
> Hello guys,
>
> So I have this setup that has already stopped on me 3 times the last 6
> months. Each time it would replicate properly for 2-3 months and then it
> would just stop. It currently is stopped since January 11, 2016. The
> only way I can get replication back is to set everything up from
> scratch. I'm wondering if anyone has an idea on the issue causing the
> stoppage. I'm running 64-bit slony 2.2.4.
>
> Currently, when I run slon on the replicated machine, I get the following:
>
>
>
> C:\Program Files\PostgreSQL\9.3\bin>slon slony_Securithor2 "dbname =
> Securithor2
>    user = slonyuser password = securiTHOR971 port = 6234"
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: slon version 2.2.4
> starting
>   up
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> vac_frequenc
> y = 3
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> log_level =
> 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> sync_interva
> l = 2000
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> sync_interva
> l_timeout = 10000
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> sync_group_m
> axsize = 20
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> quit_sync_pr
> ovider = 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> remote_liste
> n_timeout = 300
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> monitor_inte
> rval = 500
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> explain_inte
> rval = 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> tcp_keepaliv
> e_idle = 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> tcp_keepaliv
> e_interval = 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> tcp_keepaliv
> e_count = 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
> apply_cache_
> size = 100
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
> log_pid = 0
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
> log_timestam
> p = 1
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
> tcp_keepaliv
> e = 1
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
> monitor_thre
> ads = 1
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Real option
> real_placeholde
> r = 0.000000
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> cluster_name
> = slony_Securithor2
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> conn_info = d
> bname = Securithor2  user = slonyuser password = securiTHOR971 port = 6234
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> pid_file = [N
> ULL]
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> log_timestamp
> _format = %Y-%m-%d %H:%M:%S %Z
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> archive_dir =
>   [NULL]
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> sql_on_connec
> tion = [NULL]
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> lag_interval
> = [NULL]
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> command_on_lo
> garchive = [NULL]
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
> cleanup_inter
> val = 10 minutes
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: local node id = 2
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   main: main process started
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: launching
> sched_start_mainl
> oop
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: loading current
> cluster con
> figuration
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeNode: no_id=1
> no_comment='Ma
> ster Node'
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storePath: pa_server=1
> pa_client=
> 2 pa_conninfo="dbname=Securithor2 host=192.168.1.50 user=slonyuser
> password = se
> curiTHOR971  port = 6234" pa_connretry=10
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeListen: li_origin=1
> li_recei
> ver=2 li_provider=1
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeSet: set_id=1
> set_origin=1 s
> et_comment='All tables and sequences'
> 2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node
> 1 - no
> worker thread
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeSubscribe: sub_set=1
> sub_pro
> vider=1 sub_forward='f'
> 2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node
> 1 - no
> worker thread
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG enableSubscription: sub_set=1
> 2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node
> 1 - no
> worker thread
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: last local event
> sequence =
>   5000462590
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: configuration
> complete - st
> arting threads
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   localListenThread: thread
> starts
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
> Securithor2
>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=5188
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG enableNode: no_id=1
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteWorkerThread_1:
> thread star
> ts
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteListenThread_1:
> thread star
> ts
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   main: running scheduler
> mainloop
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG cleanupThread: thread starts
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   syncThread: thread starts
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   monitorThread: thread starts
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
> Securithor2
>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG remoteWorkerThread_1:
> update prov
> ider configuration
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG remoteWorkerThread_1:
> added activ
> e set 1 to provider 1
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for
> "dbname=Securithor2 h
> ost=192.168.1.50 user=slonyuser password = securiTHOR971  port = 6234"
> is 90306
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
> Securithor2
>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG cleanupThread: bias = 60
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
> Securithor2
>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
> Securithor2
>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for
> "dbname=Securithor2 h
> ost=192.168.1.50 user=slonyuser password = securiTHOR971  port = 6234"
> is 90306
> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteWorkerThread_1:
> syncing set
>   1 with 59 table(s) from provider 1
>
>
>
>
> It gets stuck at "syncing set 1 with 59 table(s) from provider 1" (the
> last line) forever with the occasional messages that says something
> about cleaning(threadcleaning I thing).
>
>
> Checking the postgres logs, I see lots of:
>
> 2016-01-28 17:33:07 AST LOG:  n'a pas pu recevoir les donn??es du client
> : unrecognized winsock error 10061
>
> Which translates to:
>
> 2016-01-28 17:33:07 AST LOG:  was not able to receive the data from the
> client : unrecognized winsock error 10061
>
> I'm able to connect to the main db from the replicated machine no
> problem. I have no idea how this error 10061 is caused.

Winsock error 10061 is WSAECONNREFUSED

     Connection refused.

     No connection could be made because the target computer actively
     refused it. This usually results from trying to connect to a
     service that is inactive on the foreign host?that is, one with no
     server application running.

This might be a firewall issue. Can you use some network sniffer to find 
out what is happening on the TCP/IP level between the two machines?


Regards, Jan

-- 
Jan Wieck
Senior Software Engineer
http://slony.info

From sungh.lei at gmail.com  Sun Jan 31 16:14:41 2016
From: sungh.lei at gmail.com (Sung Hsin Lei)
Date: Sun, 31 Jan 2016 19:14:41 -0500
Subject: [Slony1-general] Replication inexplicably stops
In-Reply-To: <56AB7C21.9040800@wi3ck.info>
References: <CAHD_kv=0qJ+mXOgTQTXjb0tJ24kXk7yXv7H8==XMQ7gQU_DQRA@mail.gmail.com>
	<56AB7C21.9040800@wi3ck.info>
Message-ID: <CAHD_kv=Q8MQ9Qq+N3g2A=6GE-Tch4K3BU2rDy48CryK-5+EgkQ@mail.gmail.com>

It replicated for 2 months with the firewall and anti-virus on. Just in
case, I turned the firewall and anti virus off and it's still not
replicating. I used wiresharks to examine the packets and did not see
anything suspicious. Using pgadmin, I'm able to connect to the main server
from the replicated server and vice versa so the connection seems to be
accepted.

When connection cannot be established or is rejected, slon log usually
gives an error. In my case, it's just stuck on: INFO
remoteWorkerThread_1: syncing set
 1 with 59 table(s) from provider 1" with no errors.


Actually, it gets stuck for several minutes and then do some cleanup
operations. The following is the last few lines of the slon log. Sorry, the
person's windows is in French hence you see a mixture of English and French:


2016-01-31 19:44:24 Am?r. du Sud occid. INFO   remoteWorkerThread_1:
syncing set
 1 with 59 table(s) from provider 1
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=5388
CONTEXT:  instruction SQL ?? SELECT "_slony_Securithor2".cleanupNodelock()
??
fonction PL/pgsql "_slony_Securithor2".cleanupevent(interval), ligne 82 ??
PERFO
RM
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=1176
CONTEXT:  instruction SQL ?? SELECT "_slony_Securithor2".cleanupNodelock()
??
fonction PL/pgsql "_slony_Securithor2".cleanupevent(interval), ligne 82 ??
PERFO
RM
NOTICE:  Slony-I: log switch to sl_log_1 complete - truncate sl_log_2
CONTEXT:  fonction PL/pgsql "_slony_Securithor2".cleanupevent(interval),
ligne 9
5 ?? affectation
2016-01-31 19:54:24 Am?r. du Sud occid. INFO   cleanupThread:    0.062
seconds f
or cleanupEvent()
NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
CONTEXT:  instruction SQL ?? SELECT "_slony_Securithor2".logswitch_start()
??
fonction PL/pgsql "_slony_Securithor2".cleanupevent(interval), ligne 97 ??
PERFO
RM
2016-01-31 20:04:25 Am?r. du Sud occid. INFO   cleanupThread:    0.000
seconds f
or cleanupEvent()



What would cause no replication yet no error in the logs?

Thanks.

On Fri, Jan 29, 2016 at 9:50 AM, Jan Wieck <jan at wi3ck.info> wrote:

> On 01/28/2016 10:57 PM, Sung Hsin Lei wrote:
>
>> Hello guys,
>>
>> So I have this setup that has already stopped on me 3 times the last 6
>> months. Each time it would replicate properly for 2-3 months and then it
>> would just stop. It currently is stopped since January 11, 2016. The
>> only way I can get replication back is to set everything up from
>> scratch. I'm wondering if anyone has an idea on the issue causing the
>> stoppage. I'm running 64-bit slony 2.2.4.
>>
>> Currently, when I run slon on the replicated machine, I get the following:
>>
>>
>>
>> C:\Program Files\PostgreSQL\9.3\bin>slon slony_Securithor2 "dbname =
>> Securithor2
>>    user = slonyuser password = securiTHOR971 port = 6234"
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: slon version 2.2.4
>> starting
>>   up
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> vac_frequenc
>> y = 3
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> log_level =
>> 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> sync_interva
>> l = 2000
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> sync_interva
>> l_timeout = 10000
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> sync_group_m
>> axsize = 20
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> quit_sync_pr
>> ovider = 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> remote_liste
>> n_timeout = 300
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> monitor_inte
>> rval = 500
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> explain_inte
>> rval = 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> tcp_keepaliv
>> e_idle = 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> tcp_keepaliv
>> e_interval = 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> tcp_keepaliv
>> e_count = 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Integer option
>> apply_cache_
>> size = 100
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
>> log_pid = 0
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
>> log_timestam
>> p = 1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
>> tcp_keepaliv
>> e = 1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Boolean option
>> monitor_thre
>> ads = 1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: Real option
>> real_placeholde
>> r = 0.000000
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> cluster_name
>> = slony_Securithor2
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> conn_info = d
>> bname = Securithor2  user = slonyuser password = securiTHOR971 port = 6234
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> pid_file = [N
>> ULL]
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> log_timestamp
>> _format = %Y-%m-%d %H:%M:%S %Z
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> archive_dir =
>>   [NULL]
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> sql_on_connec
>> tion = [NULL]
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> lag_interval
>> = [NULL]
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> command_on_lo
>> garchive = [NULL]
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: String option
>> cleanup_inter
>> val = 10 minutes
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: local node id = 2
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   main: main process started
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: launching
>> sched_start_mainl
>> oop
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: loading current
>> cluster con
>> figuration
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeNode: no_id=1
>> no_comment='Ma
>> ster Node'
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storePath: pa_server=1
>> pa_client=
>> 2 pa_conninfo="dbname=Securithor2 host=192.168.1.50 user=slonyuser
>> password = se
>> curiTHOR971  port = 6234" pa_connretry=10
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeListen: li_origin=1
>> li_recei
>> ver=2 li_provider=1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeSet: set_id=1
>> set_origin=1 s
>> et_comment='All tables and sequences'
>> 2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node
>> 1 - no
>> worker thread
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG storeSubscribe: sub_set=1
>> sub_pro
>> vider=1 sub_forward='f'
>> 2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node
>> 1 - no
>> worker thread
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG enableSubscription:
>> sub_set=1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. WARN   remoteWorker_wakeup: node
>> 1 - no
>> worker thread
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: last local event
>> sequence =
>>   5000462590
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG main: configuration
>> complete - st
>> arting threads
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   localListenThread: thread
>> starts
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
>> Securithor2
>>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
>> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=5188
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG enableNode: no_id=1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteWorkerThread_1:
>> thread star
>> ts
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteListenThread_1:
>> thread star
>> ts
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   main: running scheduler
>> mainloop
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG cleanupThread: thread
>> starts
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   syncThread: thread starts
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   monitorThread: thread
>> starts
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
>> Securithor2
>>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG remoteWorkerThread_1:
>> update prov
>> ider configuration
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG remoteWorkerThread_1:
>> added activ
>> e set 1 to provider 1
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for
>> "dbname=Securithor2 h
>> ost=192.168.1.50 user=slonyuser password = securiTHOR971  port = 6234"
>> is 90306
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
>> Securithor2
>>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG cleanupThread: bias = 60
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
>> Securithor2
>>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for "dbname =
>> Securithor2
>>    user = slonyuser password = securiTHOR971 port = 6234" is 90310
>> 2016-01-28 17:41:00 Am?r. du Sud occid. CONFIG version for
>> "dbname=Securithor2 h
>> ost=192.168.1.50 user=slonyuser password = securiTHOR971  port = 6234"
>> is 90306
>> 2016-01-28 17:41:00 Am?r. du Sud occid. INFO   remoteWorkerThread_1:
>> syncing set
>>   1 with 59 table(s) from provider 1
>>
>>
>>
>>
>> It gets stuck at "syncing set 1 with 59 table(s) from provider 1" (the
>> last line) forever with the occasional messages that says something
>> about cleaning(threadcleaning I thing).
>>
>>
>> Checking the postgres logs, I see lots of:
>>
>> 2016-01-28 17:33:07 AST LOG:  n'a pas pu recevoir les donn??es du client
>> : unrecognized winsock error 10061
>>
>> Which translates to:
>>
>> 2016-01-28 17:33:07 AST LOG:  was not able to receive the data from the
>> client : unrecognized winsock error 10061
>>
>> I'm able to connect to the main db from the replicated machine no
>> problem. I have no idea how this error 10061 is caused.
>>
>
> Winsock error 10061 is WSAECONNREFUSED
>
>     Connection refused.
>
>     No connection could be made because the target computer actively
>     refused it. This usually results from trying to connect to a
>     service that is inactive on the foreign host?that is, one with no
>     server application running.
>
> This might be a firewall issue. Can you use some network sniffer to find
> out what is happening on the TCP/IP level between the two machines?
>
>
> Regards, Jan
>
> --
> Jan Wieck
> Senior Software Engineer
> http://slony.info
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160131/531600f3/attachment.htm 

