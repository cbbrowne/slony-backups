From ssinger at ca.afilias.info  Wed Nov  2 07:30:21 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 02 Nov 2011 10:30:21 -0400
Subject: [Slony1-general] Trouble installing Slony on Solaris
In-Reply-To: <1320056223.11405.YahooMailNeo@web121218.mail.ne1.yahoo.com>
References: <1320056223.11405.YahooMailNeo@web121218.mail.ne1.yahoo.com>
Message-ID: <4EB153FD.7020306@ca.afilias.info>

On 11-10-31 06:17 AM, Venkat Nag wrote:
> Hello Everyone,
>
> This is my first post to the Slony Community.
>
> Can someone please help me get a Slony build for Sun Solaris ?
>

What issues are you encountering?

I think I was successfully  able to build recent slony 2.0.7 versions on 
Solaris with both gcc and the sun compiler.



> Thanks
> VBN
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From wernergiam at yahoo.com  Wed Nov  2 15:59:07 2011
From: wernergiam at yahoo.com (chern wei giam)
Date: Wed, 2 Nov 2011 15:59:07 -0700 (PDT)
Subject: [Slony1-general] (no subject)
Message-ID: <1320274747.69887.yint-ygo-j2me@web110501.mail.gq1.yahoo.com>

http://ahlussunnah.cl/group/protect/markehjf.htm

From vitaliy.se at gmail.com  Mon Nov  7 13:54:45 2011
From: vitaliy.se at gmail.com (Vitaliy Semochkin)
Date: Tue, 8 Nov 2011 00:54:45 +0300
Subject: [Slony1-general] continue replication in after connection problem
	with slony
Message-ID: <CAHyKpfNs3bx524=GQE=fRq3qcSQrhyr6feiL60Fjs4DH=DCMCA@mail.gmail.com>

Hello,

I have 2 postgresql servers, ONE is used for read and write
operations, while the OTHER is used for read only operations.

I want all updates for several tables on  ONE to be transferred to the OTHER.
In case the ONE or the OTHER goes off line or there is a connection
problems, I want those updates to be transferred as soon as server
goes on line.

Is it possible to perform such replication using Slony?
If it is so, where to dig? ;-)

Thanks in Advance,
Vitaliy S

From vitaliy.se at gmail.com  Mon Nov  7 13:57:10 2011
From: vitaliy.se at gmail.com (Vitaliy Semochkin)
Date: Tue, 8 Nov 2011 00:57:10 +0300
Subject: [Slony1-general] continue replication in after connection problem
 with slony (sorry, if I duplicated the question)
Message-ID: <CAHyKpfNntvfm9PVeeTu9f1h1LNqyD1y7_R9eXT5SsFPHSs87LQ@mail.gmail.com>

Hello,

I have 2 postgresql servers, ONE is used for read and write
operations, while the OTHER is used for read only operations.

I want all updates for several tables on  ONE to be transferred to the OTHER.
In case the ONE or the OTHER goes off line or there is a connection
problems, I want those updates to be transferred as soon as server
goes on line.

Is it possible to perform such replication using Slony?
If it is so, where to dig? ;-)

Thanks in Advance,
Vitaliy S

From cbbrowne at afilias.info  Mon Nov  7 14:24:37 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Mon, 7 Nov 2011 17:24:37 -0500
Subject: [Slony1-general] continue replication in after connection
 problem with slony (sorry, if I duplicated the question)
In-Reply-To: <CAHyKpfNntvfm9PVeeTu9f1h1LNqyD1y7_R9eXT5SsFPHSs87LQ@mail.gmail.com>
References: <CAHyKpfNntvfm9PVeeTu9f1h1LNqyD1y7_R9eXT5SsFPHSs87LQ@mail.gmail.com>
Message-ID: <CANfbgbb4CurPTdmtWUoYyzvmv9iXJnquD7GJXsniQpd=kVwBbw@mail.gmail.com>

On Mon, Nov 7, 2011 at 4:57 PM, Vitaliy Semochkin <vitaliy.se at gmail.com> wrote:
> Hello,
>
> I have 2 postgresql servers, ONE is used for read and write
> operations, while the OTHER is used for read only operations.
>
> I want all updates for several tables on ?ONE to be transferred to the OTHER.
> In case the ONE or the OTHER goes off line or there is a connection
> problems, I want those updates to be transferred as soon as server
> goes on line.
>
> Is it possible to perform such replication using Slony?
> If it is so, where to dig? ;-)

Well, what Slony does sounds somewhat like an interpretation of this...

Supposing there are two servers:
 - Node #1, used for read and write, which is defined as the origin
 - Node #2, used only for read access, defined as subscriber/replica

That's a very standard Slony configuration.

If node #1 goes down, then there are two choices:

a) Wait until it comes back.  (e.g. - if the problem is a minor one).

In that case, updates will be transferred from #1 (origin) to #2
(subscriber) as soon as node #1 comes back on line.  (It may be
necessary to restart a slon process too; if it has some sort of
watchdog watching it, that's easy to make happen.  There's a script
for that...)

b) Give up on node #1 (e.g. - if the problem is a major one).

In that case, you need to submit a FAILOVER script (using
slony.info/documentation/2.1/stmtfailover.html) to turn node #2 into
the origin node.

At that point, node #1 is to be treated as destroyed.

There is no c)  :-)

Slony doesn't try to provide multimaster replication; while there are
ways of having *some* traffic going in both directions between nodes,
that needs to involve mutually disjoint sets of tables.

If case a) describes what you're thinking of, then Slony does what you
want.  If you're thinking of a "case c)," then perhaps not so much.

From vbnpg at yahoo.com  Tue Nov  8 03:01:52 2011
From: vbnpg at yahoo.com (Venkat Nag)
Date: Tue, 8 Nov 2011 03:01:52 -0800 (PST)
Subject: [Slony1-general] Trouble installing Slony on Solaris
In-Reply-To: <4EB153FD.7020306@ca.afilias.info>
References: <1320056223.11405.YahooMailNeo@web121218.mail.ne1.yahoo.com>
	<4EB153FD.7020306@ca.afilias.info>
Message-ID: <1320750112.38401.YahooMailNeo@web121214.mail.ne1.yahoo.com>

Hi Steven,

Thanks for the reply !

I faced issues in installing Slony on Solaris.

I tried compiling Slony binaries with Postgres binaries and was able to install it.

Thanks
VB


________________________________
From: Steve Singer <ssinger at ca.afilias.info>
To: Venkat Nag <vbnpg at yahoo.com>
Cc: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
Sent: Wednesday, November 2, 2011 8:00 PM
Subject: Re: [Slony1-general] Trouble installing Slony on Solaris

On 11-10-31 06:17 AM, Venkat Nag wrote:
> Hello Everyone,
>
> This is my first post to the Slony Community.
>
> Can someone please help me get a Slony build for Sun Solaris ?
>

What issues are you encountering?

I think I was successfully? able to build recent slony 2.0.7 versions on 
Solaris with both gcc and the sun compiler.



> Thanks
> VBN
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20111108/2af4c338/attachment.htm 

From vivek at khera.org  Tue Nov  8 08:05:02 2011
From: vivek at khera.org (Vick Khera)
Date: Tue, 8 Nov 2011 11:05:02 -0500
Subject: [Slony1-general] continue replication in after connection
 problem with slony
In-Reply-To: <CAHyKpfNs3bx524=GQE=fRq3qcSQrhyr6feiL60Fjs4DH=DCMCA@mail.gmail.com>
References: <CAHyKpfNs3bx524=GQE=fRq3qcSQrhyr6feiL60Fjs4DH=DCMCA@mail.gmail.com>
Message-ID: <CALd+dcd1sbad8qHqZT=yORU6TuT_DZQQeUqH31zCarNh7TEjhw@mail.gmail.com>

On Mon, Nov 7, 2011 at 4:54 PM, Vitaliy Semochkin <vitaliy.se at gmail.com> wrote:
> Is it possible to perform such replication using Slony?
> If it is so, where to dig? ;-)
>

That's how slony works out of the box.  The only issue you may have is
if there is a slow WAN link between the servers, and you have lots and
lots of updates.  If there is a significant time of disconnection
between the two, the backlog of change may be too great to apply.
Sometimes it is faster to start from scratch in those situations.

From l.rame at griensu.com  Tue Nov  8 12:21:02 2011
From: l.rame at griensu.com (Leonardo =?iso-8859-1?Q?M=2E_Ram=E9?=)
Date: Tue, 8 Nov 2011 17:21:02 -0300
Subject: [Slony1-general] New user question
Message-ID: <20111108202102.GB7215@leonardo-laptop.router31ff80.com>

Hi, we have one application connected to a local PostgreSql server and
we are facing the need to allow a remote site's users to use the
application. As a point-to-point connection is really expensive, we're
looking for other solutions, like replication.

On each site ~25 clients are connected to the server doing reads and
writes, and we would like to synchronize both servers, so, for example
if a new record is added on Site1, in about 15minutes, that record must
appear on Site2, and viceversa.

Is this possible with Slony-I?.

Thanks in advance,
-- 
Leonardo M. Ram?
Medical IT - Griensu S.A.
Av. Col?n 636 - Piso 8 Of. A
X5000EPT -- C?rdoba
Tel.: +54(351)4246924 +54(351)4247788 +54(351)4247979 int. 19
Cel.: +54(351)156629292


From cbbrowne at afilias.info  Tue Nov  8 14:18:29 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Tue, 8 Nov 2011 17:18:29 -0500
Subject: [Slony1-general] New user question
In-Reply-To: <20111108202102.GB7215@leonardo-laptop.router31ff80.com>
References: <20111108202102.GB7215@leonardo-laptop.router31ff80.com>
Message-ID: <CANfbgbamuyfFj6gTZPM+diA1YZbuR_Y+qBJ+xgfbFF4OOvSR-A@mail.gmail.com>

On Tue, Nov 8, 2011 at 3:21 PM, Leonardo M. Ram? <l.rame at griensu.com> wrote:
> Hi, we have one application connected to a local PostgreSql server and
> we are facing the need to allow a remote site's users to use the
> application. As a point-to-point connection is really expensive, we're
> looking for other solutions, like replication.
>
> On each site ~25 clients are connected to the server doing reads and
> writes, and we would like to synchronize both servers, so, for example
> if a new record is added on Site1, in about 15minutes, that record must
> appear on Site2, and viceversa.
>
> Is this possible with Slony-I?.

Unfortunately, the "vice versa" part is not possible.

Slony-I is a *single* master to multiple subscribers replication
system; it does not try to address the multimaster problem.

What reasonably works is for site #1 to be the "master," and for that
data to appear reasonably quickly at site #2.

There are ways of approximating multimaster replication with Slony-I,
but it quickly gets pretty involved, and you need to effectively
design that into your application.

From mfwilson at gmail.com  Wed Nov  9 10:19:41 2011
From: mfwilson at gmail.com (Mike Wilson)
Date: Wed, 9 Nov 2011 10:19:41 -0800
Subject: [Slony1-general] fetch 500 from LOG
Message-ID: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>

Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.

I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"

And how can I get rid of it if it's not an issue?    

Mike 






From l.rame at griensu.com  Wed Nov  9 11:09:12 2011
From: l.rame at griensu.com (Leonardo =?iso-8859-1?Q?M=2E_Ram=E9?=)
Date: Wed, 9 Nov 2011 16:09:12 -0300
Subject: [Slony1-general] New user question
In-Reply-To: <CANfbgbamuyfFj6gTZPM+diA1YZbuR_Y+qBJ+xgfbFF4OOvSR-A@mail.gmail.com>
References: <20111108202102.GB7215@leonardo-laptop.router31ff80.com>
	<CANfbgbamuyfFj6gTZPM+diA1YZbuR_Y+qBJ+xgfbFF4OOvSR-A@mail.gmail.com>
Message-ID: <20111109190912.GA2979@leonardo-laptop>

On 2011-11-08 17:18:29 -0500, Christopher Browne wrote:
> On Tue, Nov 8, 2011 at 3:21 PM, Leonardo M. Ram? <l.rame at griensu.com> wrote:
> > Hi, we have one application connected to a local PostgreSql server and
> > we are facing the need to allow a remote site's users to use the
> > application. As a point-to-point connection is really expensive, we're
> > looking for other solutions, like replication.
> >
> > On each site ~25 clients are connected to the server doing reads and
> > writes, and we would like to synchronize both servers, so, for example
> > if a new record is added on Site1, in about 15minutes, that record must
> > appear on Site2, and viceversa.
> >
> > Is this possible with Slony-I?.
> 
> Unfortunately, the "vice versa" part is not possible.
> 
> Slony-I is a *single* master to multiple subscribers replication
> system; it does not try to address the multimaster problem.
> 
> What reasonably works is for site #1 to be the "master," and for that
> data to appear reasonably quickly at site #2.
> 
> There are ways of approximating multimaster replication with Slony-I,
> but it quickly gets pretty involved, and you need to effectively
> design that into your application.

Thanks Christopher, I didn't know the types of replication. Now I'm sure
I need multiple-master replication, knowing this I'll take a look at
Burcado.

-- 
Leonardo M. Ram?
Medical IT - Griensu S.A.
Av. Col?n 636 - Piso 8 Of. A
X5000EPT -- C?rdoba
Tel.: +54(351)4246924 +54(351)4247788 +54(351)4247979 int. 19
Cel.: +54(351)156629292


From cbbrowne at afilias.info  Wed Nov  9 15:14:25 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 9 Nov 2011 18:14:25 -0500
Subject: [Slony1-general] Revision to DDL handling
Message-ID: <CANfbgbYTgrAq4S8GuEf=pqZdNK8phdpJB3L2T_7VDYC7L5FeMA@mail.gmail.com>

I have gotten around to poking at fairly old bug #137
<http://www.slony.info/bugzilla/show_bug.cgi?id=137>, which heads
towards a quite different implementation of DDL handling in Slony-I
version 2.2.

Where, in 2.0 and 2.1, the process of applying DDL looked like:

 - Start running a DDL event
    - Take the list of SQL statements out of the event, splitting them
into individual queries
    - For each query
      - Run the query

With some vagueness as to what happens if update activity is being
logged concurrently in sl_log_{1,2}.  (I'm not sure whether the
updates are applied *before* or *after* the DDL; neither is
necessarily the right answer!)

In 2.2, we add a new table, sl_log_script, which is used to capture
the SQL statements with exactly the same transaction and ordering
information used to control ordering of log application of log data in
sl_log_{1,2}.

This changes the semantics a bit, but, we think, in a pretty well
unambiguously better way.

It actually makes it easier to do DDL handling; we have a wrapper
function, ddlCapture(), which drops the DDL into the new table.  A
clever administrator might use exactly the same function to run their
own favorite bit of DDL and have it run exactly as it would have been
had they used slonik EXECUTE SCRIPT.

There is a change to EXECUTE SCRIPT; it becomes rather more meaningful
to use the EXECUTE ONLY ON option, and to have that be a list of
nodes, rather than just a single node.

And here's where a question opens...

The way I have initially implemented the new form of EXECUTE SCRIPT is
to request a specific list of nodes.

Thus:

EXECUTE SCRIPT (set id=1, filename='/tmp/my-ddl-script.sql', event
node=2, execute only on = '2,3,4');

My colleagues have suggested that perhaps we'd like to have the option
of a script running only on the subscribers to a single set.

I imagine this might be handled via a syntax like:
    EXECUTE SCRIPT (set id=1, filename='/tmp/my-ddl-script.sql', event
node=2, execute only on = set nodes);

But I also imagine that this may be overkill.  Creating a syntax
specifically for the case of running just on a certain set's nodes may
be adding a complication that no one really cares to use.

Does anyone feel strongly about this?  If not, then my inclination is
to have just two behaviours:
  a) Run the script on ALL nodes, as a default behaviour
  b) Run on a specified list of nodes, e.g. - EXECUTE ONLY ON='2,3,4'

If anyone badly wants an option c), I'd appreciate hearing so.

Though I'm ready to argue "but if you don't know what your set of
nodes are, I think you're in deep, deep trouble..."

From brianf at consistentstate.com  Wed Nov  9 16:49:08 2011
From: brianf at consistentstate.com (Brian Fehrle)
Date: Wed, 09 Nov 2011 17:49:08 -0700
Subject: [Slony1-general] Upgrade from 1.2 to 2.1 questions.
Message-ID: <4EBB1F84.1010904@consistentstate.com>

Hi all, I have a few questions about upgrading to a newer version of slony.

I have a group of systems that are on slony version 1.2.21, and postgres 
8.4. We'd like to upgrade to the latest slony release, 2.1.0.

First question, has slony 2.1.0 shown to be stable and work well for 
everyone? Would it be smarter to go with the latest of 2.0 series before 
jumping on 2.1, or is 2.1 less of a 'major release' type that I may be 
thinking it is?

Also, upgrading from 1.2.21 straight to 2.1, I see via this page 
(http://slony.info/documentation/2.1/slonyupgrade.html) it seems to 
suggest going the path of just dropping every node from the slony 
cluster, installing the new binaries, then setting it up again from 
scratch on the new version. Is this the best / most suggested upgrade 
path, and is upgrading from 1.2 to 2.1 such a major change that it's 
just not recommended to try to migrate an existing cluster? (it's pretty 
much answered in that page, but would like any other opinions from 
anyone who may have upgraded already).


These seem to be more of opinion based questions, what do you all think?

Thanks in advance,
- Brian F

From brianf at consistentstate.com  Wed Nov  9 17:07:09 2011
From: brianf at consistentstate.com (Brian Fehrle)
Date: Wed, 09 Nov 2011 18:07:09 -0700
Subject: [Slony1-general] Materialized view on replicated tables.
Message-ID: <4EBB23BD.1020804@consistentstate.com>

Hi all,

I've been testing the theory of having a materialized view set up to be 
on two replicated tables in slony. The purpose of the materialized view 
is to replace a standard view that is just dog slow. I got my test 
system working with a few hitches, and would like any feedback / 
thoughts / warnings to see if this is a bad idea to do in the first place.

First off, I'm currently testing in slony version 1.2.21 on postgres 
8.3, but the hope would to be eventually on slony 2.1 and postgres 8.4, 
so anything I do would hopefully be compatable with that setup also.

So in my test environment I set up two base tables, a users table and a 
services table. The users table has a column that is a foreign key 
linked to the services table. I then created a view table that retrieves 
two columns from each table, with a join on that foreign key. I then set 
up 3 separate triggers on the 'users' table that modify all data in my 
view table on any INSERTS, UPDATES, and DELETES.

I set up the DDL for these three tables, including the functions and 
triggers, on two boxes. I then set up a slony cluster on the two 
machines, designating one as the master, and one as the slave. he moment 
I start the two slon daemons, and the initial copy happens to bring the 
slave up to date to the master, the 3 triggers I created on the slave 
table are removed. These triggers are not removed on the slony master, 
but only the slave.

First question, is there a way to not let this happen. I know newer 
versions of slony have an OMIT COPY, but does that also apply to 
removing triggers too? If so, I would still need to get the slave table 
up to date with the master anyways, a whole different issue.

So after this happened, I re-applied my triggers to the slave table, and 
started inserting/updating/deleting data on the master. In this test 
that I set up, the materialized view on the slave was successfully 
updated as I would hope, so everything looks good in my test.

It was mentioned in the postgres IRC that slony handles updates 
differently than I may think. I turned log_min_duration statement to 0 
on the slave so I can catch it, but I actually couldn't find it. The 
update in my test environment worked, but the real environment that this 
may be pushed to will have dozens of triggers on 9 or more tables, so my 
test may not be very handy in confirming that this will even work.

So with his, I have two more questions:
1. My test materialized view is simple, but the real one would be 
complex (9 or more tables with 3 triggers on each possibly). Are there 
any 'gotchas' or 'warnings' that would make me even rethink attempting this?

2. If I preform a switchover from the master to the slave, would the act 
of switching over cause my triggers to disappear like they did when I 
set up the cluster (I haven't had a chance to test this yet on my test 
environment). And if I do a switchover, since the slave (now master) 
view is not technically part of replication, would there be chance of it 
becoming out of sync with the tables it's a view of? Also, when the 
master becomes the slave, the hope would be that the view continues to 
be updated then the tables are updated via slony.

thanks for any thoughts / opinions on this,
- Brian F

From ssinger at ca.afilias.info  Fri Nov 11 05:07:25 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 11 Nov 2011 08:07:25 -0500
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
Message-ID: <4EBD1E0D.30308@ca.afilias.info>

On 11-11-09 01:19 PM, Mike Wilson wrote:
> Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.
>
> I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
> slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"
>
> And how can I get rid of it if it's not an issue?
>
> Mike

What is causing the 'fetch 500' statements to show up in the server log? 
Are you only logging SQL that takes longer than x milliseconds? If so 
how long are your fetch 500 statements taking?  How many rows are in 
your sl_log_1 and sl_log_2?


>
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From ssinger at ca.afilias.info  Fri Nov 11 05:10:53 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 11 Nov 2011 08:10:53 -0500
Subject: [Slony1-general] Upgrade from 1.2 to 2.1 questions.
In-Reply-To: <4EBB1F84.1010904@consistentstate.com>
References: <4EBB1F84.1010904@consistentstate.com>
Message-ID: <4EBD1EDD.50407@ca.afilias.info>

On 11-11-09 07:49 PM, Brian Fehrle wrote:
> Hi all, I have a few questions about upgrading to a newer version of slony.
>

> I have a group of systems that are on slony version 1.2.21, and postgres
> 8.4. We'd like to upgrade to the latest slony release, 2.1.0.
> Also, upgrading from 1.2.21 straight to 2.1, I see via this page
> (http://slony.info/documentation/2.1/slonyupgrade.html) it seems to
> suggest going the path of just dropping every node from the slony
> cluster, installing the new binaries, then setting it up again from
> scratch on the new version. Is this the best / most suggested upgrade
> path, and is upgrading from 1.2 to 2.1 such a major change that it's
> just not recommended to try to migrate an existing cluster? (it's pretty
> much answered in that page, but would like any other opinions from
> anyone who may have upgraded already).

The *only* option when upgrading from 1.2 to 2.x is to drop slony and 
reinstall + reconfigure it.  If the documentation implies you have some 
other option then we need to make it more clear.

>
>
> These seem to be more of opinion based questions, what do you all think?
>
> Thanks in advance,
> - Brian F
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From ssinger at ca.afilias.info  Fri Nov 11 05:18:10 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 11 Nov 2011 08:18:10 -0500
Subject: [Slony1-general] Materialized view on replicated tables.
In-Reply-To: <4EBB23BD.1020804@consistentstate.com>
References: <4EBB23BD.1020804@consistentstate.com>
Message-ID: <4EBD2092.8030702@ca.afilias.info>

On 11-11-09 08:07 PM, Brian Fehrle wrote:
> Hi all,
>
> I've been testing the theory of having a materialized view set up to be
> on two replicated tables in slony. The purpose of the materialized view
> is to replace a standard view that is just dog slow. I got my test
> system working with a few hitches, and would like any feedback /
> thoughts / warnings to see if this is a bad idea to do in the first place.
>
> First off, I'm currently testing in slony version 1.2.21 on postgres
> 8.3, but the hope would to be eventually on slony 2.1 and postgres 8.4,
> so anything I do would hopefully be compatable with that setup also.
>
> So in my test environment I set up two base tables, a users table and a
> services table. The users table has a column that is a foreign key
> linked to the services table. I then created a view table that retrieves
> two columns from each table, with a join on that foreign key. I then set
> up 3 separate triggers on the 'users' table that modify all data in my
> view table on any INSERTS, UPDATES, and DELETES.
>
> I set up the DDL for these three tables, including the functions and
> triggers, on two boxes. I then set up a slony cluster on the two
> machines, designating one as the master, and one as the slave. he moment
> I start the two slon daemons, and the initial copy happens to bring the
> slave up to date to the master, the 3 triggers I created on the slave
> table are removed. These triggers are not removed on the slony master,
> but only the slave.
>
> First question, is there a way to not let this happen. I know newer
> versions of slony have an OMIT COPY, but does that also apply to
> removing triggers too? If so, I would still need to get the slave table
> up to date with the master anyways, a whole different issue.
>

Trigger handling is different in 1.2 compared with 2.x

For 1.2 http://www.slony.info/documentation/1.2/stmtstoretrigger.html 
will allow the trigger to run on the slaves.

In 2.x read the section in the manual 
http://www.slony.info/documentation/2.1/triggers.html

There is no OMIT_COPY in 1.2.     Either you replicate your materialized 
view table or you don't.  If you replicate your materialized view table 
then you want to disable the INSERT/UPDATE/DELETE trigger on the slave. 
  If you don't replicate that table you want to use 'STORE TRIGGER' in 
1.2 or make it a ALWAYS trigger in 2.x

> So after this happened, I re-applied my triggers to the slave table, and
> started inserting/updating/deleting data on the master. In this test
> that I set up, the materialized view on the slave was successfully
> updated as I would hope, so everything looks good in my test.
>
> It was mentioned in the postgres IRC that slony handles updates
> differently than I may think. I turned log_min_duration statement to 0
> on the slave so I can catch it, but I actually couldn't find it. The
> update in my test environment worked, but the real environment that this
> may be pushed to will have dozens of triggers on 9 or more tables, so my
> test may not be very handy in confirming that this will even work.
>
> So with his, I have two more questions:
> 1. My test materialized view is simple, but the real one would be
> complex (9 or more tables with 3 triggers on each possibly). Are there
> any 'gotchas' or 'warnings' that would make me even rethink attempting this?
>
> 2. If I preform a switchover from the master to the slave, would the act
> of switching over cause my triggers to disappear like they did when I
> set up the cluster (I haven't had a chance to test this yet on my test
> environment). And if I do a switchover, since the slave (now master)
> view is not technically part of replication, would there be chance of it
> becoming out of sync with the tables it's a view of? Also, when the
> master becomes the slave, the hope would be that the view continues to
> be updated then the tables are updated via slony.

In 2.x if you use 'ALWAYS' triggers then the trigger will fire on the 
server if it is a master or a slave.

>
> thanks for any thoughts / opinions on this,
> - Brian F
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From mfwilson at gmail.com  Fri Nov 11 11:04:43 2011
From: mfwilson at gmail.com (Mike Wilson)
Date: Fri, 11 Nov 2011 11:04:43 -0800
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <4EBD1E0D.30308@ca.afilias.info>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
	<4EBD1E0D.30308@ca.afilias.info>
Message-ID: <B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>


Mike Wilson
Predicate Logic
Cell: (310) 600-8777
SkypeID: lycovian


From my postgresql.log:
2011-11-11 11:03:15.237 PST db1.lax.jib(55096):LOG:  duration: 133.011 ms  statement: fetch 500 from LOG;
2011-11-11 11:03:17.241 PST db1.lax.jib(55096):LOG:  duration: 134.842 ms  statement: fetch 500 from LOG;
2011-11-11 11:03:19.239 PST db1.lax.jib(55096):LOG:  duration: 133.919 ms  statement: fetch 500 from LOG;
2011-11-11 11:03:21.240 PST db1.lax.jib(55096):LOG:  duration: 133.194 ms  statement: fetch 500 from LOG;
2011-11-11 11:03:23.241 PST db1.lax.jib(55096):LOG:  duration: 134.288 ms  statement: fetch 500 from LOG;
2011-11-11 11:03:25.241 PST db1.lax.jib(55096):LOG:  duration: 133.226 ms  statement: fetch 500 from LOG;

I'm only logging statements that take longer than 100ms to run.

Here is my output from sl_log1/2:
select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
 sl_log_1 | sl_log_2 
----------+----------
   119239 |    43685


On Nov 11, 2011, at 5:07 AM, Steve Singer wrote:

> On 11-11-09 01:19 PM, Mike Wilson wrote:
>> Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.
>> 
>> I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
>> slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"
>> 
>> And how can I get rid of it if it's not an issue?
>> 
>> Mike
> 
> What is causing the 'fetch 500' statements to show up in the server log? Are you only logging SQL that takes longer than x milliseconds? If so how long are your fetch 500 statements taking?  How many rows are in your sl_log_1 and sl_log_2?
> 
> 
>> 
>> 
>> 
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 


From ssinger at ca.afilias.info  Fri Nov 11 11:09:21 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 11 Nov 2011 14:09:21 -0500
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
	<4EBD1E0D.30308@ca.afilias.info>
	<B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>
Message-ID: <4EBD72E1.7000507@ca.afilias.info>

On 11-11-11 02:04 PM, Mike Wilson wrote:
>
> Mike Wilson
> Predicate Logic
> Cell: (310) 600-8777
> SkypeID: lycovian
>
>
>  From my postgresql.log:
> 2011-11-11 11:03:15.237 PST db1.lax.jib(55096):LOG:  duration: 133.011 ms  statement: fetch 500 from LOG;
> 2011-11-11 11:03:17.241 PST db1.lax.jib(55096):LOG:  duration: 134.842 ms  statement: fetch 500 from LOG;
> 2011-11-11 11:03:19.239 PST db1.lax.jib(55096):LOG:  duration: 133.919 ms  statement: fetch 500 from LOG;
> 2011-11-11 11:03:21.240 PST db1.lax.jib(55096):LOG:  duration: 133.194 ms  statement: fetch 500 from LOG;
> 2011-11-11 11:03:23.241 PST db1.lax.jib(55096):LOG:  duration: 134.288 ms  statement: fetch 500 from LOG;
> 2011-11-11 11:03:25.241 PST db1.lax.jib(55096):LOG:  duration: 133.226 ms  statement: fetch 500 from LOG;
>
> I'm only logging statements that take longer than 100ms to run.
>
> Here is my output from sl_log1/2:
> select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
>   sl_log_1 | sl_log_2
> ----------+----------
>     119239 |    43685

The fetch is taking a long time because sl_log_1 is big.  (The reason it 
takes so long is actually a bug that was fixed in 2.1)  sl_log_1 being 
that big probably means that log switching isn't happening.

Do you have any nodes that are behind?  (query sl_status on all your nodes)
Do you have any old nodes that are still listed in sl_node that you 
aren't using anymore?
Do (did) you have a long running transaction in your system that is 
preventing the log switch from taking place?





>
>
> On Nov 11, 2011, at 5:07 AM, Steve Singer wrote:
>
>> On 11-11-09 01:19 PM, Mike Wilson wrote:
>>> Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.
>>>
>>> I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
>>> slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"
>>>
>>> And how can I get rid of it if it's not an issue?
>>>
>>> Mike
>>
>> What is causing the 'fetch 500' statements to show up in the server log? Are you only logging SQL that takes longer than x milliseconds? If so how long are your fetch 500 statements taking?  How many rows are in your sl_log_1 and sl_log_2?
>>
>>
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>


From cbbrowne at afilias.info  Fri Nov 11 11:16:18 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Fri, 11 Nov 2011 14:16:18 -0500
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <4EBD72E1.7000507@ca.afilias.info>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
	<4EBD1E0D.30308@ca.afilias.info>
	<B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>
	<4EBD72E1.7000507@ca.afilias.info>
Message-ID: <CANfbgbam40jVVhpufFt+Fpx_X2Rv4YTueoaiMqm8Y62j=8HDZg@mail.gmail.com>

On Fri, Nov 11, 2011 at 2:09 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
>> ?From my postgresql.log:
>> 2011-11-11 11:03:15.237 PST db1.lax.jib(55096):LOG: ?duration: 133.011 ms ?statement: fetch 500 from LOG;
>> 2011-11-11 11:03:17.241 PST db1.lax.jib(55096):LOG: ?duration: 134.842 ms ?statement: fetch 500 from LOG;
>> 2011-11-11 11:03:19.239 PST db1.lax.jib(55096):LOG: ?duration: 133.919 ms ?statement: fetch 500 from LOG;
>> 2011-11-11 11:03:21.240 PST db1.lax.jib(55096):LOG: ?duration: 133.194 ms ?statement: fetch 500 from LOG;
>> 2011-11-11 11:03:23.241 PST db1.lax.jib(55096):LOG: ?duration: 134.288 ms ?statement: fetch 500 from LOG;
>> 2011-11-11 11:03:25.241 PST db1.lax.jib(55096):LOG: ?duration: 133.226 ms ?statement: fetch 500 from LOG;
>
> The fetch is taking a long time because sl_log_1 is big. ?(The reason it
> takes so long is actually a bug that was fixed in 2.1) ?sl_log_1 being
> that big probably means that log switching isn't happening.

Let me observe that 133ms is not a terribly long time.  It's possible
that everything's working just AOK.  If it was 133 seconds, that would
be one thing.  But 133ms isn't "obviously broken."

Perhaps the query would be somewhat faster if we had the query change
from 2.1 in place here, but I still don't imagine it would run without
*any* duration, not even if we were running this on a Cray :-).
-- 
Have you heard about the new Cray super computer? It's so fast, it
executes an infinite loop in 6 seconds.

From mfwilson at gmail.com  Fri Nov 11 12:46:55 2011
From: mfwilson at gmail.com (Mike Wilson)
Date: Fri, 11 Nov 2011 12:46:55 -0800
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <4EBD72E1.7000507@ca.afilias.info>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
	<4EBD1E0D.30308@ca.afilias.info>
	<B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>
	<4EBD72E1.7000507@ca.afilias.info>
Message-ID: <695E1D29-6677-4C20-8669-B8A817905C89@gmail.com>

General lag on the slave node (as recorded in sl_status) is less then 30 seconds.  This is a heavily transacted system running on very nice hardware so perhaps any problems are being masked by that.

I've read up on the issue and we don't appear to be experiencing any of the bugs related to this issue that I can find in the news groups.  No long running transactions, no old nodes in the sl_ tables.  In general, the system appears to be healthy (idle proc time ~95%), good buffer cache hit ratios, etc.

Thanks for the replies though.  I'll look into implementing 2.1 although we just did the upgrade to 2.0.7 and I'm not sure management will go for another down during the holiday season.  Just doing my due diligence as our load will rise steadily through the holiday season to very large load on these servers and I wanted to make sure the servers looked solid before we through 30 X the current load at them.

Mike Wilson
Predicate Logic
Cell: (310) 600-8777
SkypeID: lycovian




On Nov 11, 2011, at 11:09 AM, Steve Singer wrote:

> On 11-11-11 02:04 PM, Mike Wilson wrote:
>> 
>> Mike Wilson
>> Predicate Logic
>> Cell: (310) 600-8777
>> SkypeID: lycovian
>> 
>> 
>> From my postgresql.log:
>> 2011-11-11 11:03:15.237 PST db1.lax.jib(55096):LOG:  duration: 133.011 ms  statement: fetch 500 from LOG;
>> 2011-11-11 11:03:17.241 PST db1.lax.jib(55096):LOG:  duration: 134.842 ms  statement: fetch 500 from LOG;
>> 2011-11-11 11:03:19.239 PST db1.lax.jib(55096):LOG:  duration: 133.919 ms  statement: fetch 500 from LOG;
>> 2011-11-11 11:03:21.240 PST db1.lax.jib(55096):LOG:  duration: 133.194 ms  statement: fetch 500 from LOG;
>> 2011-11-11 11:03:23.241 PST db1.lax.jib(55096):LOG:  duration: 134.288 ms  statement: fetch 500 from LOG;
>> 2011-11-11 11:03:25.241 PST db1.lax.jib(55096):LOG:  duration: 133.226 ms  statement: fetch 500 from LOG;
>> 
>> I'm only logging statements that take longer than 100ms to run.
>> 
>> Here is my output from sl_log1/2:
>> select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
>>  sl_log_1 | sl_log_2
>> ----------+----------
>>    119239 |    43685
> 
> The fetch is taking a long time because sl_log_1 is big.  (The reason it takes so long is actually a bug that was fixed in 2.1)  sl_log_1 being that big probably means that log switching isn't happening.
> 
> Do you have any nodes that are behind?  (query sl_status on all your nodes)
> Do you have any old nodes that are still listed in sl_node that you aren't using anymore?
> Do (did) you have a long running transaction in your system that is preventing the log switch from taking place?
> 
> 
> 
> 
> 
>> 
>> 
>> On Nov 11, 2011, at 5:07 AM, Steve Singer wrote:
>> 
>>> On 11-11-09 01:19 PM, Mike Wilson wrote:
>>>> Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.
>>>> 
>>>> I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
>>>> slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"
>>>> 
>>>> And how can I get rid of it if it's not an issue?
>>>> 
>>>> Mike
>>> 
>>> What is causing the 'fetch 500' statements to show up in the server log? Are you only logging SQL that takes longer than x milliseconds? If so how long are your fetch 500 statements taking?  How many rows are in your sl_log_1 and sl_log_2?
>>> 
>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>> 
>> 
> 


From guillaume at lelarge.info  Sat Nov 12 10:54:50 2011
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Sat, 12 Nov 2011 19:54:50 +0100
Subject: [Slony1-general] Revision to DDL handling
In-Reply-To: <CANfbgbYTgrAq4S8GuEf=pqZdNK8phdpJB3L2T_7VDYC7L5FeMA@mail.gmail.com>
References: <CANfbgbYTgrAq4S8GuEf=pqZdNK8phdpJB3L2T_7VDYC7L5FeMA@mail.gmail.com>
Message-ID: <1321124090.2021.7.camel@localhost.localdomain>

On Wed, 2011-11-09 at 18:14 -0500, Christopher Browne wrote:
> [...]
> Does anyone feel strongly about this?  If not, then my inclination is
> to have just two behaviours:
>   a) Run the script on ALL nodes, as a default behaviour
>   b) Run on a specified list of nodes, e.g. - EXECUTE ONLY ON='2,3,4'
> 

I don't feel strongly about it, but I guess having a and b are enough. c
may be hard to code, will be cumbersome, and for no real value.

> If anyone badly wants an option c), I'd appreciate hearing so.
> 

I don't need that option.

> Though I'm ready to argue "but if you don't know what your set of
> nodes are, I think you're in deep, deep trouble..."

Exactly my thought.


-- 
Guillaume
  http://blog.guillaume.lelarge.info
  http://www.dalibo.com


From cbbrowne at afilias.info  Sat Nov 12 17:55:33 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Sat, 12 Nov 2011 20:55:33 -0500
Subject: [Slony1-general] Revision to DDL handling
In-Reply-To: <1321124090.2021.7.camel@localhost.localdomain>
References: <CANfbgbYTgrAq4S8GuEf=pqZdNK8phdpJB3L2T_7VDYC7L5FeMA@mail.gmail.com>
	<1321124090.2021.7.camel@localhost.localdomain>
Message-ID: <CANfbgbb6uA1x-X84SrY-OucBMR2pGEA5xP+nwSs4qevfuARXfw@mail.gmail.com>

On Nov 12, 2011 1:54 PM, "Guillaume Lelarge" <guillaume at lelarge.info> wrote:
>
> On Wed, 2011-11-09 at 18:14 -0500, Christopher Browne wrote:
> > [...]
> > Does anyone feel strongly about this?  If not, then my inclination is
> > to have just two behaviours:
> >   a) Run the script on ALL nodes, as a default behaviour
> >   b) Run on a specified list of nodes, e.g. - EXECUTE ONLY ON='2,3,4'
> >
>
> I don't feel strongly about it, but I guess having a and b are enough. c
> may be hard to code, will be cumbersome, and for no real value.

I'm most keenly concerned about the "cumbersome" part, actually.  That
would establish c) as a clear anti-feature.

But that's assuming clumsiness.

Let's consider (loosely, I'm not consulting docs to make this up)

Execute script (event node=5, script='/tmp/Foo.slonik', only on sets
='1,4');

Or
.... only on set=1);

That's neither necessarily *huge* implementation effort nor horribly clumsy.

Zero additional effort has its merits, but that's not enough to rule out
the couple ideas above.

I suppose I'm "against" c) more out of not wanting to add more options for
people to puzzle through than are necessary.

On reflection, there's some complication to log shipping tests, but I don't
think that depends on whether c) gets added or not.  Rather, we need a test
to validate that log shipping gets the DDL iff the node feeding log
shipping was on the executor list.  No real logic difference whether that
node was:
I. Subscribing to the set (c), or
II. In the "execute only on" node list.

That's not an argument pro or con, rather a "let's not forget needful
testing" aside.

> > Though I'm ready to argue "but if you don't know what your set of
> > nodes are, I think you're in deep, deep trouble..."
>
> Exactly my thought.

So I'm not crazy, always good to know!  :-)

Glad to get feedback, thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20111112/ee22384f/attachment.htm 

From guillaume at lelarge.info  Sun Nov 13 01:14:10 2011
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Sun, 13 Nov 2011 10:14:10 +0100
Subject: [Slony1-general] Revision to DDL handling
In-Reply-To: <CANfbgbb6uA1x-X84SrY-OucBMR2pGEA5xP+nwSs4qevfuARXfw@mail.gmail.com>
References: <CANfbgbYTgrAq4S8GuEf=pqZdNK8phdpJB3L2T_7VDYC7L5FeMA@mail.gmail.com>
	<1321124090.2021.7.camel@localhost.localdomain>
	<CANfbgbb6uA1x-X84SrY-OucBMR2pGEA5xP+nwSs4qevfuARXfw@mail.gmail.com>
Message-ID: <1321175650.2066.8.camel@localhost.localdomain>

On Sat, 2011-11-12 at 20:55 -0500, Christopher Browne wrote:
> 
> On Nov 12, 2011 1:54 PM, "Guillaume Lelarge" <guillaume at lelarge.info>
> wrote:
> >
> > On Wed, 2011-11-09 at 18:14 -0500, Christopher Browne wrote:
> > > [...]
> > > Does anyone feel strongly about this?  If not, then my inclination
> is
> > > to have just two behaviours:
> > >   a) Run the script on ALL nodes, as a default behaviour
> > >   b) Run on a specified list of nodes, e.g. - EXECUTE ONLY
> ON='2,3,4'
> > >
> >
> > I don't feel strongly about it, but I guess having a and b are
> enough. c
> > may be hard to code, will be cumbersome, and for no real value.
> 
> I'm most keenly concerned about the "cumbersome" part, actually.  That
> would establish c) as a clear anti-feature.
> 
> But that's assuming clumsiness.
> 
> Let's consider (loosely, I'm not consulting docs to make this up)
> 
> Execute script (event node=5, script='/tmp/Foo.slonik', only on sets
> ='1,4');
> 
> Or
> .... only on set=1);
> 
> That's neither necessarily *huge* implementation effort nor horribly
> clumsy.
> 
> Zero additional effort has its merits, but that's not enough to rule
> out the couple ideas above.
> 
> I suppose I'm "against" c) more out of not wanting to add more options
> for people to puzzle through than are necessary.
> 

More code means harder to maintain. People don't seem to want that
feature, so no need to add more complexity (even if it's low), more
code, etc, if people don't ask specifically for this feature (I'm not
going to say that this little survey gives a lot of informations from
users, but at least, it doesn't seem to attract many people).

> On reflection, there's some complication to log shipping tests, but I
> don't think that depends on whether c) gets added or not.  Rather, we
> need a test to validate that log shipping gets the DDL iff the node
> feeding log shipping was on the executor list.  No real logic
> difference whether that node was:
> I. Subscribing to the set (c), or
> II. In the "execute only on" node list. 
> 
> That's not an argument pro or con, rather a "let's not forget needful
> testing" aside. 
> 
> 
> > > Though I'm ready to argue "but if you don't know what your set of
> > > nodes are, I think you're in deep, deep trouble..."
> >
> > Exactly my thought.
> 
> So I'm not crazy, always good to know!  :-)
> 
> Glad to get feedback, thanks!
> 

You're welcome :)


-- 
Guillaume
  http://blog.guillaume.lelarge.info
  http://www.dalibo.com


From wernergiam at yahoo.com  Sun Nov 13 15:05:35 2011
From: wernergiam at yahoo.com (chern wei giam)
Date: Sun, 13 Nov 2011 15:05:35 -0800 (PST)
Subject: [Slony1-general] (no subject)
Message-ID: <1321225535.29428.yint-ygo-j2me@web110506.mail.gq1.yahoo.com>

http://seancondron.com/blog/wp-content/themes/uchilla1.0/images/backgrounds/idmaild.htm?wlel=wlel

From ssinger at ca.afilias.info  Mon Nov 14 07:10:40 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 14 Nov 2011 10:10:40 -0500
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <695E1D29-6677-4C20-8669-B8A817905C89@gmail.com>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
	<4EBD1E0D.30308@ca.afilias.info>
	<B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>
	<4EBD72E1.7000507@ca.afilias.info>
	<695E1D29-6677-4C20-8669-B8A817905C89@gmail.com>
Message-ID: <4EC12F70.7070801@ca.afilias.info>

On 11-11-11 03:46 PM, Mike Wilson wrote:
> General lag on the slave node (as recorded in sl_status) is less then 30 seconds.  This is a heavily transacted system running on very nice hardware so perhaps any problems are being masked by that.
>
> I've read up on the issue and we don't appear to be experiencing any of the bugs related to this issue that I can find in the news groups.  No long running transactions, no old nodes in the sl_ tables.  In general, the system appears to be healthy (idle proc time ~95%), good buffer cache hit ratios, etc.
>
> Thanks for the replies though.  I'll look into implementing 2.1 although we just did the upgrade to 2.0.7 and I'm not sure management will go for another down during the holiday season.  Just doing my due diligence as our load will rise steadily through the holiday season to very large load on these servers and I wanted to make sure the servers looked solid before we through 30 X the current load at them.

Now that a few days have passed,

is your sl_log_1 count still growing or has it dropped?  If your sl_log 
tables keep growing and aren't being truncated by the cleanup thread 
then you have a problem that will eventually get worse.  If the 119,239 
rows in sl_log_1 was a temporary thing due to your application doing 
lots of updates then that might be normal for your system.

>
> Mike Wilson
> Predicate Logic
> Cell: (310) 600-8777
> SkypeID: lycovian
>
>
>
>
> On Nov 11, 2011, at 11:09 AM, Steve Singer wrote:
>
>> On 11-11-11 02:04 PM, Mike Wilson wrote:
>>>
>>> Mike Wilson
>>> Predicate Logic
>>> Cell: (310) 600-8777
>>> SkypeID: lycovian
>>>
>>>
>>>  From my postgresql.log:
>>> 2011-11-11 11:03:15.237 PST db1.lax.jib(55096):LOG:  duration: 133.011 ms  statement: fetch 500 from LOG;
>>> 2011-11-11 11:03:17.241 PST db1.lax.jib(55096):LOG:  duration: 134.842 ms  statement: fetch 500 from LOG;
>>> 2011-11-11 11:03:19.239 PST db1.lax.jib(55096):LOG:  duration: 133.919 ms  statement: fetch 500 from LOG;
>>> 2011-11-11 11:03:21.240 PST db1.lax.jib(55096):LOG:  duration: 133.194 ms  statement: fetch 500 from LOG;
>>> 2011-11-11 11:03:23.241 PST db1.lax.jib(55096):LOG:  duration: 134.288 ms  statement: fetch 500 from LOG;
>>> 2011-11-11 11:03:25.241 PST db1.lax.jib(55096):LOG:  duration: 133.226 ms  statement: fetch 500 from LOG;
>>>
>>> I'm only logging statements that take longer than 100ms to run.
>>>
>>> Here is my output from sl_log1/2:
>>> select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
>>>   sl_log_1 | sl_log_2
>>> ----------+----------
>>>     119239 |    43685
>>
>> The fetch is taking a long time because sl_log_1 is big.  (The reason it takes so long is actually a bug that was fixed in 2.1)  sl_log_1 being that big probably means that log switching isn't happening.
>>
>> Do you have any nodes that are behind?  (query sl_status on all your nodes)
>> Do you have any old nodes that are still listed in sl_node that you aren't using anymore?
>> Do (did) you have a long running transaction in your system that is preventing the log switch from taking place?
>>
>>
>>
>>
>>
>>>
>>>
>>> On Nov 11, 2011, at 5:07 AM, Steve Singer wrote:
>>>
>>>> On 11-11-09 01:19 PM, Mike Wilson wrote:
>>>>> Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.
>>>>>
>>>>> I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
>>>>> slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"
>>>>>
>>>>> And how can I get rid of it if it's not an issue?
>>>>>
>>>>> Mike
>>>>
>>>> What is causing the 'fetch 500' statements to show up in the server log? Are you only logging SQL that takes longer than x milliseconds? If so how long are your fetch 500 statements taking?  How many rows are in your sl_log_1 and sl_log_2?
>>>>
>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Slony1-general mailing list
>>>>> Slony1-general at lists.slony.info
>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>
>>>
>>
>


From mfwilson at gmail.com  Mon Nov 14 10:25:17 2011
From: mfwilson at gmail.com (Mike Wilson)
Date: Mon, 14 Nov 2011 10:25:17 -0800
Subject: [Slony1-general] fetch 500 from LOG
In-Reply-To: <4EC12F70.7070801@ca.afilias.info>
References: <5123FC12-8F82-49E7-AEC5-29BAE65C758F@gmail.com>
	<4EBD1E0D.30308@ca.afilias.info>
	<B3045567-92CD-47B5-B7F8-528F34C74A38@gmail.com>
	<4EBD72E1.7000507@ca.afilias.info>
	<695E1D29-6677-4C20-8669-B8A817905C89@gmail.com>
	<4EC12F70.7070801@ca.afilias.info>
Message-ID: <27776043-D957-41F5-A003-C857C26C913A@gmail.com>

The sl_log1/2 values fluctuate on a continuous basis.  Here are three queries that show this run over about an hour of time:
select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
 sl_log_1 | sl_log_2 
----------+----------
    80179 |    31451
(1 row)

Time: 36.624 ms
(slony at db2:5432) [c0] 
=# select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
 sl_log_1 | sl_log_2 
----------+----------
    82948 |    37270
(1 row)

Time: 42.952 ms
(slony at db2:5432) [c0] 
=# select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
 sl_log_1 | sl_log_2 
----------+----------
        0 |    54862

I believe this would indicate that the logs are being filled, processed, and ultimately cleared as part of Slony's regular processes.  I know there was a bug related to these logs never getting cleared that threw the log message I was concerned with but my servers aren't apparently experiencing this issue.

I think there was some mention of long running queries.  I do have a few of these listed as "<IDLE>" or long running "vacuum analyze" in pg_stat_activitiy though:
 select usename, current_query, waiting, xact_start, query_start from pg_stat_activity order by query_start desc;
 ...
 barfoo  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:16:23.744028-08
 barfoo  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:16:19.477108-08
 barfoo  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:16:08.787367-08
 books   | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:15:28.842382-08
 foobar  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:14:39.039847-08
 foobar  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:13:34.771573-08
 slony   | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:12:51.091648-08
 barfoo  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:08:06.93106-08
 barfoo  | <IDLE>                                                                                                           | f       |                               | 2011-11-14 10:08:06.803036-08
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-11-13 22:00:00.759804-08 | 2011-11-13 22:00:00.759804-08
 foobar  | <IDLE>                                                                                                           | f       |                               | 2011-11-08 22:08:32.807418-08
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-11-06 22:00:00.848159-08 | 2011-11-06 22:00:00.848159-08
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-10-30 22:00:01.009049-07 | 2011-10-30 22:00:01.009049-07
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-10-23 22:00:00.372459-07 | 2011-10-23 22:00:00.372459-07
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-10-16 22:00:00.525067-07 | 2011-10-16 22:00:00.525067-07
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-10-09 22:00:00.091914-07 | 2011-10-09 22:00:00.091914-07
 foobar  | vacuum analyze;                                                                                                  | t       | 2011-10-02 22:00:00.85743-07  | 2011-10-02 22:00:00.85743-07
 foobar  | <IDLE>                                                                                                           | f       |                               | 2011-10-01 17:23:05.776958-07
 foobar  | <IDLE>                                                                                                           | f       |                               | 2011-10-01 17:23:05.755754-07
 foobar  | <IDLE>                                                                                                           | f       |                               | 2011-10-01 17:23:05.743447-07
 foobar  | autovacuum: VACUUM c0.assets (to prevent wraparound)                                                             | f       | 2011-09-29 22:12:30.92651-07  | 2011-09-29 22:12:30.92651-07
(766 rows)

I hadn't noticed the hung vacuum's before.  These are part of a weekly vacuum process that appears to be getting hung up every week.  They aren't apparently affecting the DB's performance but they concern me.  I think I should probably kill their background processes to clear them out.


Mike Wilson
Predicate Logic
Cell: (310) 600-8777
SkypeID: lycovian




On Nov 14, 2011, at 7:10 AM, Steve Singer wrote:

> On 11-11-11 03:46 PM, Mike Wilson wrote:
>> General lag on the slave node (as recorded in sl_status) is less then 30 seconds.  This is a heavily transacted system running on very nice hardware so perhaps any problems are being masked by that.
>> 
>> I've read up on the issue and we don't appear to be experiencing any of the bugs related to this issue that I can find in the news groups.  No long running transactions, no old nodes in the sl_ tables.  In general, the system appears to be healthy (idle proc time ~95%), good buffer cache hit ratios, etc.
>> 
>> Thanks for the replies though.  I'll look into implementing 2.1 although we just did the upgrade to 2.0.7 and I'm not sure management will go for another down during the holiday season.  Just doing my due diligence as our load will rise steadily through the holiday season to very large load on these servers and I wanted to make sure the servers looked solid before we through 30 X the current load at them.
> 
> Now that a few days have passed,
> 
> is your sl_log_1 count still growing or has it dropped?  If your sl_log tables keep growing and aren't being truncated by the cleanup thread then you have a problem that will eventually get worse.  If the 119,239 rows in sl_log_1 was a temporary thing due to your application doing lots of updates then that might be normal for your system.
> 
>> 
>> Mike Wilson
>> Predicate Logic
>> Cell: (310) 600-8777
>> SkypeID: lycovian
>> 
>> 
>> 
>> 
>> On Nov 11, 2011, at 11:09 AM, Steve Singer wrote:
>> 
>>> On 11-11-11 02:04 PM, Mike Wilson wrote:
>>>> 
>>>> Mike Wilson
>>>> Predicate Logic
>>>> Cell: (310) 600-8777
>>>> SkypeID: lycovian
>>>> 
>>>> 
>>>> From my postgresql.log:
>>>> 2011-11-11 11:03:15.237 PST db1.lax.jib(55096):LOG:  duration: 133.011 ms  statement: fetch 500 from LOG;
>>>> 2011-11-11 11:03:17.241 PST db1.lax.jib(55096):LOG:  duration: 134.842 ms  statement: fetch 500 from LOG;
>>>> 2011-11-11 11:03:19.239 PST db1.lax.jib(55096):LOG:  duration: 133.919 ms  statement: fetch 500 from LOG;
>>>> 2011-11-11 11:03:21.240 PST db1.lax.jib(55096):LOG:  duration: 133.194 ms  statement: fetch 500 from LOG;
>>>> 2011-11-11 11:03:23.241 PST db1.lax.jib(55096):LOG:  duration: 134.288 ms  statement: fetch 500 from LOG;
>>>> 2011-11-11 11:03:25.241 PST db1.lax.jib(55096):LOG:  duration: 133.226 ms  statement: fetch 500 from LOG;
>>>> 
>>>> I'm only logging statements that take longer than 100ms to run.
>>>> 
>>>> Here is my output from sl_log1/2:
>>>> select (select count(*) from sl_log_1) sl_log_1, (select count(*) from sl_log_2) sl_log_2;
>>>>  sl_log_1 | sl_log_2
>>>> ----------+----------
>>>>    119239 |    43685
>>> 
>>> The fetch is taking a long time because sl_log_1 is big.  (The reason it takes so long is actually a bug that was fixed in 2.1)  sl_log_1 being that big probably means that log switching isn't happening.
>>> 
>>> Do you have any nodes that are behind?  (query sl_status on all your nodes)
>>> Do you have any old nodes that are still listed in sl_node that you aren't using anymore?
>>> Do (did) you have a long running transaction in your system that is preventing the log switch from taking place?
>>> 
>>> 
>>> 
>>> 
>>> 
>>>> 
>>>> 
>>>> On Nov 11, 2011, at 5:07 AM, Steve Singer wrote:
>>>> 
>>>>> On 11-11-09 01:19 PM, Mike Wilson wrote:
>>>>>> Seeing "fetch 500 from LOG" almost continuously in my PG logs for a new Slony 2.0.7 install.  The previous version (2.0.3?) didn't show these messages in the PG log.  Researching the issue, historically, this message was usually accompanied by a performance issue.  This isn't the case with my databases though, they appear to be running just as well as ever and the lag between replicated nodes appears to be about the same as the previous version.
>>>>>> 
>>>>>> I guess my question is what does this message mean in this version of Slony?  Is it an indication of sub-optimal slon parameters?
>>>>>> slon -g 20 $SLON_CLUSTER "host=$HOSTNAME port=$PORT dbname=$DB user=$USER"
>>>>>> 
>>>>>> And how can I get rid of it if it's not an issue?
>>>>>> 
>>>>>> Mike
>>>>> 
>>>>> What is causing the 'fetch 500' statements to show up in the server log? Are you only logging SQL that takes longer than x milliseconds? If so how long are your fetch 500 statements taking?  How many rows are in your sl_log_1 and sl_log_2?
>>>>> 
>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Slony1-general mailing list
>>>>>> Slony1-general at lists.slony.info
>>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>> 
>>>> 
>>> 
>> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20111114/06fb62d3/attachment-0001.htm 

From kyp404 at gmail.com  Tue Nov 15 04:39:27 2011
From: kyp404 at gmail.com (kyp404)
Date: Tue, 15 Nov 2011 13:39:27 +0100
Subject: [Slony1-general] Transaction error on slave node?
Message-ID: <CAJuhuhJfY-2QeeHaHz=t7yruGKEJMLV=Ko9MhL44d-QjZ=8Zyg@mail.gmail.com>

Hi all,

We have a master and a slave DB server (both PostgreSQL 8.3, Slony 2.0.1).
Maybe we were hasty, because we delete ~18 million rows from a table on
master with one SQL command. Slony log and transfer jobs/transactions to
the slave node, but on the slave node the slony can't do this job. Slony
starts the transaction, but after ~7 million delete commands the server
close the connection.

We found this in the Slony log:
2011-11-15 11:39:25 CET DEBUG4 remoteHelperThread_1_1: fetch from cursor
2011-11-15 11:39:25 CET ERROR  remoteHelperThread_1_1: "fetch 500 from LOG;
" server closed the connection unexpectedly
        This probably means the server terminated abnormally
        before or while processing the request.
2011-11-15 11:39:25 CET DEBUG4 remoteHelperThread_1_1: return 50 unused
line buffers
2011-11-15 11:39:25 CET ERROR  remoteWorkerThread_1: "close LOG; "
PGRES_FATAL_ERROR 2011-11-15 11:39:25 CET ERROR  remoteWorkerThread_1:
"rollback transaction; set enable_seqsca
n = default; set enable_indexscan = default; " PGRES_FATAL_ERROR 2011-11-15
11:39:25 CET DEBUG1 remoteHelperThread_1_1: 2850.992 seconds until close
cursor
2011-11-15 11:39:25 CET INFO   remoteHelperThread_1_1: inserts=0 updates=0
deletes=7220000

We tried to tuning the PostgreSQL and now two times faster, but every
transaction stop after ~ 7 million rows. We couldn't find error message in
the PostgreSQL log.

Why stop the transaction? What should we do?

Thank you in advance,
Kyp
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20111115/a5272fbc/attachment.htm 

From vividy at justware.co.jp  Tue Nov 15 18:12:30 2011
From: vividy at justware.co.jp (Michael Cheung)
Date: Wed, 16 Nov 2011 11:12:30 +0900
Subject: [Slony1-general] drop table error
Message-ID: <20111116111230.E947.8B406A0E@justware.co.jp>

Hi, all;

I use pg 8.3.16+slony1 2.1.0 now.

I upgrade slony1 from 2.0.2 to 2.1.0 yesterday , anything seems well, 
but this morning when I do drop table, it retun errors as below.

$ ./slonik_drop_table 139 1 | ./slonik 
<stdin>:12: Error: exitcode was -1 - must be in range [0-255]

and I confirm my version is 
$ ./slonik -v
slonik version 2.1.0

and I do upgrade as below yesterday, 
$ ./slonik_update_nodes | ./slonik
it complained something like you can't use truncate trigger, but everything
seems ok.

Any idea about the drop table error?

Thanks a lot.

Micheal


From ssinger_pg at sympatico.ca  Tue Nov 15 19:03:01 2011
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Tue, 15 Nov 2011 22:03:01 -0500
Subject: [Slony1-general] drop table error
In-Reply-To: <20111116111230.E947.8B406A0E@justware.co.jp>
References: <20111116111230.E947.8B406A0E@justware.co.jp>
Message-ID: <BLU0-SMTP13CFD06AF5FC3387D797CFACC60@phx.gbl>

On Wed, 16 Nov 2011, Michael Cheung wrote:

> Hi, all;
>
> I use pg 8.3.16+slony1 2.1.0 now.
>
> I upgrade slony1 from 2.0.2 to 2.1.0 yesterday , anything seems well,
> but this morning when I do drop table, it retun errors as below.

This would be a bug introduced in Slony 2.1.0 (that we weren't aware of 
until now).

Some of the altperl scripts on an error return -1 but the slonik exit 
statement was changed to only return values >=0.

If you change the third line from the bottom in slonik_drop_table script 
line

$slonik .= "    exit -1;\n";

to instead be

$slonik .= "    exit 1;\n";

You should avoid that error.  The slonik_drop_sequence script seems to have 
the same issue.

I will try to write up a patch later this week.

>
> $ ./slonik_drop_table 139 1 | ./slonik
> <stdin>:12: Error: exitcode was -1 - must be in range [0-255]
>
> and I confirm my version is
> $ ./slonik -v
> slonik version 2.1.0
>
> and I do upgrade as below yesterday,
> $ ./slonik_update_nodes | ./slonik
> it complained something like you can't use truncate trigger, but everything
> seems ok.
>
> Any idea about the drop table error?
>
> Thanks a lot.
>
> Micheal
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From vividy at justware.co.jp  Tue Nov 15 19:43:45 2011
From: vividy at justware.co.jp (Michael Cheung)
Date: Wed, 16 Nov 2011 12:43:45 +0900
Subject: [Slony1-general] drop table error
In-Reply-To: <BLU0-SMTP13CFD06AF5FC3387D797CFACC60@phx.gbl>
References: <20111116111230.E947.8B406A0E@justware.co.jp>
	<BLU0-SMTP13CFD06AF5FC3387D797CFACC60@phx.gbl>
Message-ID: <20111116124345.E956.8B406A0E@justware.co.jp>

Steve;

Thanks for your reply.

I will patch slonik_drop_table and slonik_drop_sequence, and wait for your
official update.

Regards;
Michael


On Tue, 15 Nov 2011 22:03:01 -0500
"Steve Singer" <ssinger_pg at sympatico.ca> wrote:

> On Wed, 16 Nov 2011, Michael Cheung wrote:
> 
> > Hi, all;
> >
> > I use pg 8.3.16+slony1 2.1.0 now.
> >
> > I upgrade slony1 from 2.0.2 to 2.1.0 yesterday , anything seems well,
> > but this morning when I do drop table, it retun errors as below.
> 
> This would be a bug introduced in Slony 2.1.0 (that we weren't aware of 
> until now).
> 
> Some of the altperl scripts on an error return -1 but the slonik exit 
> statement was changed to only return values >=0.
> 
> If you change the third line from the bottom in slonik_drop_table script 
> line
> 
> $slonik .= "    exit -1;\n";
> 
> to instead be
> 
> $slonik .= "    exit 1;\n";
> 
> You should avoid that error.  The slonik_drop_sequence script seems to have 
> the same issue.
> 
> I will try to write up a patch later this week.
> 
> >
> > $ ./slonik_drop_table 139 1 | ./slonik
> > <stdin>:12: Error: exitcode was -1 - must be in range [0-255]
> >
> > and I confirm my version is
> > $ ./slonik -v
> > slonik version 2.1.0
> >
> > and I do upgrade as below yesterday,
> > $ ./slonik_update_nodes | ./slonik
> > it complained something like you can't use truncate trigger, but everything
> > seems ok.
> >
> > Any idea about the drop table error?
> >
> > Thanks a lot.
> >
> > Micheal
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> >
> 


From wernergiam at yahoo.com  Wed Nov 16 07:31:03 2011
From: wernergiam at yahoo.com (chern wei giam)
Date: Wed, 16 Nov 2011 07:31:03 -0800 (PST)
Subject: [Slony1-general] (no subject)
Message-ID: <1321457463.25761.YahooMailMobile@web110503.mail.gq1.yahoo.com>

<a tabindex="1" title="" name="ciwaakirjk" href="http://ma-consultation-juridique.com/admin/phpexcel/Documentation/API/PHPExcel_CachedObjectStorage/doprds.htm">http://ma-consultation-juridique.com/admin/phpexcel/Documentation/API/PHPExcel_CachedObjectStorage/doprds.htm</a>?hnho=hnho
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20111116/a68b7efc/attachment.htm 

From brianf at consistentstate.com  Thu Nov 17 09:51:55 2011
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 17 Nov 2011 10:51:55 -0700
Subject: [Slony1-general] --exclude-schema skipping operators?
Message-ID: <4EC549BB.1040205@consistentstate.com>

Hi all,
(postgres version 8.3, slony version 1.2.21)

     Quick question. I want to create a schema only dump of a database 
(pg_dump -s), but I don't want any slony elements in the dump. I 
excluded the slony schema via --exclude-schema="_slony", and nothing in 
that schema was dumped.

I do however, get some operators dumped that were not in the slony 
schema, but reference it.

Example:
--
-- Name: <=; Type: OPERATOR; Schema: public; Owner: slony
--

CREATE OPERATOR <= (
     PROCEDURE = _slony.xxidle,
     LEFTARG = _slony.xxid,
     RIGHTARG = _slony.xxid,
     COMMUTATOR = >=,
     NEGATOR = >,
     RESTRICT = scalarltsel,
     JOIN = scalarltjoinsel
);
ALTER OPERATOR public.<= (_slony.xxid, _slony.xxid) OWNER TO slony;


This operator lives in the public schema, but since it references things 
in _slony, when applying the dump to a new database, it fails (because 
the _slony schema isn't created, as intended).

I don't remember ever seeing anything put into the public schema by 
slony before, am I mistaken? Or was my original setup of this slony 
cluster messed up where it put operators in the wrong schema in the 
first place? Does slony 2.x have the same behavior or has it changed?

Thanks,
- Brian F

From maxim.boguk at gmail.com  Thu Nov 17 20:56:34 2011
From: maxim.boguk at gmail.com (Maxim Boguk)
Date: Fri, 18 Nov 2011 15:56:34 +1100
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql 9.1.1
Message-ID: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>

Slony 2.0.7 replication just stalled on big update on master (like
300.000 in batch.. nothing crazy).

Slony log on slave show repeatable (like each 5 minute) error:

2011-11-18 08:46:03 MSKERROR  remoteWorkerThread_4: "insert into
"_sports".sl_event     (ev_origin, ev_seqno, ev_timestamp,
ev_snapshot, ev_type     ) values ('4', '5000054127', '2011-11-18
06:18:44.863299', '922722:922722:', 'SYNC'); insert into
"_sports".sl_confirm     (con_origin, con_received, con_seqno,
con_timestamp)    values (4, 3, '5000054127', now()); insert into
"_sports".sl_event     (ev_origin, ev_seqno, ev_timestamp,
ev_snapshot, ev_type     ) values ('4', '5000054128', '2011-11-18
06:18:45.890297', '922731:922731:', 'SYNC'); insert into
"_sports".sl_confirm   (con_origin, con_received, con_seqno,
con_timestamp)    values (4, 3, '5000054128', now()); insert into
"_sports".sl_event     (ev_origin, ev_seqno, ev_timestamp,
ev_snapshot, ev_type     ) values ('4', '5000054129', '2011-11-18
06:18:46.921278', '922736:922736:', 'SYNC'); insert into
"_sports".sl_confirm      (con_origin, con_received, con_seqno,
con_timestamp)    values (4, 3, '5000054129', now()); commit
transaction;"
PGRES_FATAL_ERROR ERROR:  could not serialize access due to read/write
dependencies among transactions
DETAIL:  Reason code: Canceled on identification as a pivot, during write.
HINT:  The transaction might succeed if retried.
In that case slony just terminate without commiting batch (and
returned to the life only by watchdog).


Or (more rarely and seems harmless):
2011-11-18 08:49:42 MSKERROR  remoteWorkerThread_4: "update
"_sports".sl_setsync set     ssy_seqno = '5000054129', ssy_snapshot =
'922736:922736:',     ssy_action_list = '' where ssy_setid in
(2,3,5,4,7,6,8) and ssy_seqno < '5000054129'; "
ERROR:  could not serialize access due to read/write dependencies
among transactions
DETAIL:  Reason code: Canceled on identification as a pivot, during write.
HINT:  The transaction might succeed if retried.

Seem that new in 9.1 implementation of true serializable isolation
play here (and produce more serializability errors) and Slony not
ready to deal with serializability errors at that stage.

-- 
Maxim Boguk
Senior Postgresql DBA.

Phone RU: +7 910 405 4718
Phone AU: +61 45 218 5678

Skype: maxim.boguk
Jabber: maxim.boguk at gmail.com

LinkedIn profile: http://nz.linkedin.com/in/maximboguk
If they can send one man to the moon... why can't they send them all?

???????: http://mboguk.moikrug.ru/
???? ?????? ?????, ?? ?? ??? ? ????? ????? - ??????, ?? ? ???? ?????? ?? ???.

From Ger.Timmens at adyen.com  Fri Nov 18 06:13:55 2011
From: Ger.Timmens at adyen.com (Ger Timmens)
Date: Fri, 18 Nov 2011 15:13:55 +0100
Subject: [Slony1-general] --exclude-schema skipping operators?
In-Reply-To: <mailman.1.1321560002.18770.slony1-general@lists.slony.info>
References: <mailman.1.1321560002.18770.slony1-general@lists.slony.info>
Message-ID: <4EC66823.1060602@adyen.com>

If you parse your schema to perl script below, it will be 'slony' clean:

#!/usr/bin/perl
#
my $TriggerFound=0;
my $SchemaToRemove="_replication";

while (<>)
{
  if ( /$SchemaToRemove/ )
  {
    $TriggerFound=1;
  }
  if ($TriggerFound == 0)
  {
   print $_;
  }
  if (( /;/ ) && ($TriggerFound == 1))
  {
    $TriggerFound = 0;
  }
}

Regards,

Ger Timmens

On 11/17/2011 09:00 PM, slony1-general-request at lists.slony.info wrote:
> Send Slony1-general mailing list submissions to
> 	slony1-general at lists.slony.info
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.slony.info/mailman/listinfo/slony1-general
> or, via email, send a message with subject or body 'help' to
> 	slony1-general-request at lists.slony.info
> 
> You can reach the person managing the list at
> 	slony1-general-owner at lists.slony.info
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Slony1-general digest..."
> 
> 
> Today's Topics:
> 
>    1. --exclude-schema skipping operators? (Brian Fehrle)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Thu, 17 Nov 2011 10:51:55 -0700
> From: Brian Fehrle <brianf at consistentstate.com>
> Subject: [Slony1-general] --exclude-schema skipping operators?
> To: slony1-general at lists.slony.info
> Message-ID: <4EC549BB.1040205 at consistentstate.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> Hi all,
> (postgres version 8.3, slony version 1.2.21)
> 
>      Quick question. I want to create a schema only dump of a database 
> (pg_dump -s), but I don't want any slony elements in the dump. I 
> excluded the slony schema via --exclude-schema="_slony", and nothing in 
> that schema was dumped.
> 
> I do however, get some operators dumped that were not in the slony 
> schema, but reference it.
> 
> Example:
> --
> -- Name: <=; Type: OPERATOR; Schema: public; Owner: slony
> --
> 
> CREATE OPERATOR <= (
>      PROCEDURE = _slony.xxidle,
>      LEFTARG = _slony.xxid,
>      RIGHTARG = _slony.xxid,
>      COMMUTATOR = >=,
>      NEGATOR = >,
>      RESTRICT = scalarltsel,
>      JOIN = scalarltjoinsel
> );
> ALTER OPERATOR public.<= (_slony.xxid, _slony.xxid) OWNER TO slony;
> 
> 
> This operator lives in the public schema, but since it references things 
> in _slony, when applying the dump to a new database, it fails (because 
> the _slony schema isn't created, as intended).
> 
> I don't remember ever seeing anything put into the public schema by 
> slony before, am I mistaken? Or was my original setup of this slony 
> cluster messed up where it put operators in the wrong schema in the 
> first place? Does slony 2.x have the same behavior or has it changed?
> 
> Thanks,
> - Brian F
> 
> 
> ------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> End of Slony1-general Digest, Vol 57, Issue 13
> **********************************************

From cbbrowne at afilias.info  Fri Nov 18 10:18:54 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Fri, 18 Nov 2011 13:18:54 -0500
Subject: [Slony1-general] --exclude-schema skipping operators?
In-Reply-To: <4EC549BB.1040205@consistentstate.com>
References: <4EC549BB.1040205@consistentstate.com>
Message-ID: <CANfbgbYNZjTFwwp0Nr8nO_rqby93eQ=1F=yc2xBn8R+bU8PL5w@mail.gmail.com>

On Thu, Nov 17, 2011 at 12:51 PM, Brian Fehrle
<brianf at consistentstate.com> wrote:
> Hi all,
> (postgres version 8.3, slony version 1.2.21)
>
> ? ? Quick question. I want to create a schema only dump of a database
> (pg_dump -s), but I don't want any slony elements in the dump. I
> excluded the slony schema via --exclude-schema="_slony", and nothing in
> that schema was dumped.
>
> I do however, get some operators dumped that were not in the slony
> schema, but reference it.
>
> Example:
> --
> -- Name: <=; Type: OPERATOR; Schema: public; Owner: slony
> --
>
> CREATE OPERATOR <= (
> ? ? PROCEDURE = _slony.xxidle,
> ? ? LEFTARG = _slony.xxid,
> ? ? RIGHTARG = _slony.xxid,
> ? ? COMMUTATOR = >=,
> ? ? NEGATOR = >,
> ? ? RESTRICT = scalarltsel,
> ? ? JOIN = scalarltjoinsel
> );
> ALTER OPERATOR public.<= (_slony.xxid, _slony.xxid) OWNER TO slony;
>
>
> This operator lives in the public schema, but since it references things
> in _slony, when applying the dump to a new database, it fails (because
> the _slony schema isn't created, as intended).
>
> I don't remember ever seeing anything put into the public schema by
> slony before, am I mistaken? Or was my original setup of this slony
> cluster messed up where it put operators in the wrong schema in the
> first place? Does slony 2.x have the same behavior or has it changed?

That's arguably a bug; I haven't reviewed the code, but it seems not
overly appropriate for Slony to be creating operators in other than
its own schema.

After all, someone might not *have* a public schema :-(.

At any rate, that operator is gone as of version 2.  The kind folks
from Skype got XID functions and such added into core in PostgreSQL
8.3, and as of Slony 2.0, we use the built-in functions rather than
having our own XXID type.

From cbbrowne at afilias.info  Fri Nov 18 10:26:18 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Fri, 18 Nov 2011 13:26:18 -0500
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
Message-ID: <CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>

On Thu, Nov 17, 2011 at 11:56 PM, Maxim Boguk <maxim.boguk at gmail.com> wrote:
> PGRES_FATAL_ERROR ERROR: ?could not serialize access due to read/write
> dependencies among transactions
> DETAIL: ?Reason code: Canceled on identification as a pivot, during write.
> HINT: ?The transaction might succeed if retried.
> In that case slony just terminate without commiting batch (and
> returned to the life only by watchdog).

That seems like one of the sorts of cases that we could expect there
being with the new "full serialization" support in 9.1.

Terminating the batch is not notably disastrous; a retry should work fine.

The case that would be troublesome would be if you were trying to set
up a subscription, and get serialization interruptions often enough
that you can't get the subscription to "take."

But this seems like the sort of thing that you have to be prepared for
when you adopt full serialization support.

My inclination is actually to do a bit of a "push back"; I don't think
it's quite right that Postgres is responding, in this case, by
indicating that this is a FATAL error.  If we can expect a retry to
work, that surely isn't FATAL, is it?

From maxim.boguk at gmail.com  Fri Nov 18 14:31:30 2011
From: maxim.boguk at gmail.com (Maxim Boguk)
Date: Sat, 19 Nov 2011 09:31:30 +1100
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
Message-ID: <CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>

On Sat, Nov 19, 2011 at 5:26 AM, Christopher Browne
<cbbrowne at afilias.info> wrote:
> On Thu, Nov 17, 2011 at 11:56 PM, Maxim Boguk <maxim.boguk at gmail.com> wrote:
>> PGRES_FATAL_ERROR ERROR: ?could not serialize access due to read/write
>> dependencies among transactions
>> DETAIL: ?Reason code: Canceled on identification as a pivot, during write.
>> HINT: ?The transaction might succeed if retried.
>> In that case slony just terminate without commiting batch (and
>> returned to the life only by watchdog).
>
> That seems like one of the sorts of cases that we could expect there
> being with the new "full serialization" support in 9.1.
>
> Terminating the batch is not notably disastrous; a retry should work fine.

After replica got lagged more then 3 hours (and slony getting
deadlocked every 5 minutes or so,
 as a result - large batch just do not go to through) I decided that
way don't work.

It seems chance getting into deadlock in my case very close to 100%.

After some attempts I found only one way to deal with that issue:
stop slony on second replica and on the master db for time sufficient
to the db chew that large batch (stopping only on master or only on
second replica was not enough).

So that problem can be really painful and require manual DBA actions.


-- 
Maxim Boguk
Senior Postgresql DBA.

Phone RU: +7 910 405 4718
Phone AU: +61 45 218 5678

Skype: maxim.boguk
Jabber: maxim.boguk at gmail.com

LinkedIn profile: http://nz.linkedin.com/in/maximboguk
If they can send one man to the moon... why can't they send them all?

???????: http://mboguk.moikrug.ru/
???? ?????? ?????, ?? ?? ??? ? ????? ????? - ??????, ?? ? ???? ?????? ?? ???.

From maxim.boguk at gmail.com  Sat Nov 19 00:05:23 2011
From: maxim.boguk at gmail.com (Maxim Boguk)
Date: Sat, 19 Nov 2011 19:05:23 +1100
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
Message-ID: <CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>

On Sat, Nov 19, 2011 at 9:31 AM, Maxim Boguk <maxim.boguk at gmail.com> wrote:
> On Sat, Nov 19, 2011 at 5:26 AM, Christopher Browne
> <cbbrowne at afilias.info> wrote:
>> On Thu, Nov 17, 2011 at 11:56 PM, Maxim Boguk <maxim.boguk at gmail.com> wrote:
>>> PGRES_FATAL_ERROR ERROR: ?could not serialize access due to read/write
>>> dependencies among transactions
>>> DETAIL: ?Reason code: Canceled on identification as a pivot, during write.
>>> HINT: ?The transaction might succeed if retried.
>>> In that case slony just terminate without commiting batch (and
>>> returned to the life only by watchdog).
>>
>> That seems like one of the sorts of cases that we could expect there
>> being with the new "full serialization" support in 9.1.
>>
>> Terminating the batch is not notably disastrous; a retry should work fine.
>
> After replica got lagged more then 3 hours (and slony getting
> deadlocked every 5 minutes or so,
> ?as a result - large batch just do not go to through) I decided that
> way don't work.
>
> It seems chance getting into deadlock in my case very close to 100%.
>
> After some attempts I found only one way to deal with that issue:
> stop slony on second replica and on the master db for time sufficient
> to the db chew that large batch (stopping only on master or only on
> second replica was not enough).
>
> So that problem can be really painful and require manual DBA actions.
>

Now replication lagging for 9 hours with errors every 40-50 minutes:

2011-11-19 11:01:31 MSKERROR  remoteWorkerThread_1: "update
"_sports".sl_setsync set     ssy_seqno = '5016038207', ssy_snapshot =
'501051321:501051321:',     ssy_action_list = '' where ssy_setid in
(1,9) and ssy_seqno < '5016038207'; " ERROR:  could not serialize
access due to read/write dependencies among transactions
DETAIL:  Reason code: Canceled on identification as a pivot, during write.
HINT:  The transaction might succeed if retried.

again on medium size (500.000) batch update.

It seems 3x-node configuration with slony 2.0.7 + postgresql 9.1 quite
unstable with batch updates.

Is here everything that I can use to work it more smooth?


-- 
Maxim Boguk
Senior Postgresql DBA.

Phone RU: +7 910 405 4718
Phone AU: +61 45 218 5678

Skype: maxim.boguk
Jabber: maxim.boguk at gmail.com

LinkedIn profile: http://nz.linkedin.com/in/maximboguk
If they can send one man to the moon... why can't they send them all?

???????: http://mboguk.moikrug.ru/
???? ?????? ?????, ?? ?? ??? ? ????? ????? - ??????, ?? ? ???? ?????? ?? ???.

From ssinger at ca.afilias.info  Sat Nov 19 18:02:14 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Sat, 19 Nov 2011 21:02:14 -0500
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
 9.1.1
In-Reply-To: <CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
Message-ID: <4EC85FA6.8090104@ca.afilias.info>

On 11-11-19 03:05 AM, Maxim Boguk wrote:
> On Sat, Nov 19, 2011 at 9:31 AM, Maxim Boguk<maxim.boguk at gmail.com>  wrote:
>
> Now replication lagging for 9 hours with errors every 40-50 minutes:
>
> 2011-11-19 11:01:31 MSKERROR  remoteWorkerThread_1: "update
> "_sports".sl_setsync set     ssy_seqno = '5016038207', ssy_snapshot =
> '501051321:501051321:',     ssy_action_list = '' where ssy_setid in
> (1,9) and ssy_seqno<  '5016038207'; " ERROR:  could not serialize
> access due to read/write dependencies among transactions
> DETAIL:  Reason code: Canceled on identification as a pivot, during write.
> HINT:  The transaction might succeed if retried.
>
> again on medium size (500.000) batch update.
>
> It seems 3x-node configuration with slony 2.0.7 + postgresql 9.1 quite
> unstable with batch updates.
>
> Is here everything that I can use to work it more smooth?
>
>

I  wonder out load if the slony remote worker needs to be in a 
serializable transaction or if READ COMMITTED is good enough.

I also wonder if making the remote listener threads explicit read only 
transactions would help.

The disorder tests against a 9.1 machine hit many of these read/write 
dependencies in a test run.   Slony just retries things and it 
eventually works but I can see how a 500,000 item batch means that you 
might often encounter an issue before finishing a SYNC.

My unverified guess is that changing this won't break slony but might 
effect applications querying the slave that depend on the serializable 
behaviour and maybe it should be a configuration parameter.

There are two places in remote_worker.c where it places the local 
connection to serializable.





From cedric.villemain.debian at gmail.com  Sun Nov 20 02:47:48 2011
From: cedric.villemain.debian at gmail.com (=?ISO-8859-1?Q?C=E9dric_Villemain?=)
Date: Sun, 20 Nov 2011 11:47:48 +0100
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <4EC85FA6.8090104@ca.afilias.info>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
	<4EC85FA6.8090104@ca.afilias.info>
Message-ID: <CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>

2011/11/20 Steve Singer <ssinger at ca.afilias.info>:
> On 11-11-19 03:05 AM, Maxim Boguk wrote:
>> On Sat, Nov 19, 2011 at 9:31 AM, Maxim Boguk<maxim.boguk at gmail.com> ?wrote:
>>
>> Now replication lagging for 9 hours with errors every 40-50 minutes:
>>
>> 2011-11-19 11:01:31 MSKERROR ?remoteWorkerThread_1: "update
>> "_sports".sl_setsync set ? ? ssy_seqno = '5016038207', ssy_snapshot =
>> '501051321:501051321:', ? ? ssy_action_list = '' where ssy_setid in
>> (1,9) and ssy_seqno< ?'5016038207'; " ERROR: ?could not serialize
>> access due to read/write dependencies among transactions
>> DETAIL: ?Reason code: Canceled on identification as a pivot, during write.
>> HINT: ?The transaction might succeed if retried.
>>
>> again on medium size (500.000) batch update.
>>
>> It seems 3x-node configuration with slony 2.0.7 + postgresql 9.1 quite
>> unstable with batch updates.
>>
>> Is here everything that I can use to work it more smooth?
>>
>>
>
> I ?wonder out load if the slony remote worker needs to be in a
> serializable transaction or if READ COMMITTED is good enough.
>
> I also wonder if making the remote listener threads explicit read only
> transactions would help.

if it is a read only transaction, then it is preferable to set it explicitely.
If I well understood Heikki, it should remove 'pivot' situation. See
http://wiki.postgresql.org/images/4/4f/SSI-PGConfEU2011.pdf (slide 22
for the immediate suggestions)

>
> The disorder tests against a 9.1 machine hit many of these read/write
> dependencies in a test run. ? Slony just retries things and it
> eventually works but I can see how a 500,000 item batch means that you
> might often encounter an issue before finishing a SYNC.
>
> My unverified guess is that changing this won't break slony but might
> effect applications querying the slave that depend on the serializable
> behaviour and maybe it should be a configuration parameter.
>
> There are two places in remote_worker.c where it places the local
> connection to serializable.
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>



-- 
C?dric Villemain +33 (0)6 20 30 22 52
http://2ndQuadrant.fr/
PostgreSQL: Support 24x7 - D?veloppement, Expertise et Formation

From vividy at justware.co.jp  Tue Nov 22 05:00:34 2011
From: vividy at justware.co.jp (Michael Cheung)
Date: Tue, 22 Nov 2011 22:00:34 +0900
Subject: [Slony1-general] error while slonik_execute_script(slony
	2.1.0+postgresql 8.3.16)
Message-ID: <20111122220034.67D5.8B406A0E@justware.co.jp>

Hi all;
(postgres version 8.3.16, slony version 2.1.0)

While I run sql to create index on a huge table(named tableA) by slonik_execute_script,
(actually this operation cost me almost 20 min.)
And slonik_execute_script command end with following error message.

<stdin>:5: NOTICE:  public.tableB has an invalid configuration on the log trigger. This was not corrected because
only_lock is true and the table is not locked.
CONTEXT:  SQL statement "SELECT  "_apl_repl1".repair_log_triggers(true)"
PL/pgSQL function "ddlscript_complete_int" line 5 at PERFORM
SQL statement "SELECT  "_apl_repl1".ddlScript_complete_int( $1 , $2 )"
PL/pgSQL function "ddlscript_complete" line 7 at PERFORM

And I have check the new index on table A on every node, it is created and seems fine.
And record count on tableB is same on every node. Everything seems well.

But anyone can tell me what happened with this message? And is it safe to just ignore it?

Thanks a lot.

Michael


From ssinger at ca.afilias.info  Tue Nov 22 07:25:57 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 22 Nov 2011 10:25:57 -0500
Subject: [Slony1-general] error while slonik_execute_script(slony
 2.1.0+postgresql 8.3.16)
In-Reply-To: <20111122220034.67D5.8B406A0E@justware.co.jp>
References: <20111122220034.67D5.8B406A0E@justware.co.jp>
Message-ID: <4ECBBF05.4080109@ca.afilias.info>

On 11-11-22 08:00 AM, Michael Cheung wrote:
> Hi all;
> (postgres version 8.3.16, slony version 2.1.0)
>
> While I run sql to create index on a huge table(named tableA) by slonik_execute_script,
> (actually this operation cost me almost 20 min.)
> And slonik_execute_script command end with following error message.
>
> <stdin>:5: NOTICE:  public.tableB has an invalid configuration on the log trigger. This was not corrected because
> only_lock is true and the table is not locked.
> CONTEXT:  SQL statement "SELECT  "_apl_repl1".repair_log_triggers(true)"
> PL/pgSQL function "ddlscript_complete_int" line 5 at PERFORM
> SQL statement "SELECT  "_apl_repl1".ddlScript_complete_int( $1 , $2 )"
> PL/pgSQL function "ddlscript_complete" line 7 at PERFORM
>
> And I have check the new index on table A on every node, it is created and seems fine.
> And record count on tableB is same on every node. Everything seems well.
>
> But anyone can tell me what happened with this message? And is it safe to just ignore it?
>

If you look at the Slony log trigger on when of your tables it will look 
something like this:


_test_logtrigger AFTER INSERT OR DELETE OR UPDATE ON a FOR EACH ROW 
EXECUTE PROCEDURE _test.logtrigger('_test', '1', 'kvk')

The 'kvk' means that the first column in the table is part of the key, 
the second column is not part of a key(primary key or unique 
constraint), and the third column makes up the second component of the 
unique key.

If you do something to the table to change this, ie drop the second 
column.  Then this argument order is no longer correct and slony needs 
to recreate the logtrigger to have the correct arguments.   Doing that 
will require an exclusive lock.  If EXECUTE SCRIPT in 2.1 determines 
that the script has already taken out an exclusive lock on the table 
then it will just fix the trigger.   However it won't go and obtain the 
lock.

The function select _test.determineAttKindUnique('public.a','a_pkey'); 
(where _test is your slony schema name) will tell you want the arguments 
should be for a particular function.  Compare the output of that for 
your tableB with the arguments on the trigger (\d in psql).

If you call _test.repair_log_trigger(false) it will take an exclusive 
lock on the tables it requires and repair the trigger arguments.  (You 
would need to do that on all nodes).






> Thanks a lot.
>
> Michael
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From ssinger at ca.afilias.info  Tue Nov 22 14:26:46 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 22 Nov 2011 17:26:46 -0500
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
 9.1.1
In-Reply-To: <CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
	<4EC85FA6.8090104@ca.afilias.info>
	<CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>
Message-ID: <4ECC21A6.5070805@ca.afilias.info>

On 11-11-20 05:47 AM, C?dric Villemain wrote:
> 2011/11/20 Steve Singer<ssinger at ca.afilias.info>:
>>
>> I  wonder out load if the slony remote worker needs to be in a
>> serializable transaction or if READ COMMITTED is good enough.
>>
>> I also wonder if making the remote listener threads explicit read only
>> transactions would help.
>
> if it is a read only transaction, then it is preferable to set it explicitely.
> If I well understood Heikki, it should remove 'pivot' situation. See
> http://wiki.postgresql.org/images/4/4f/SSI-PGConfEU2011.pdf (slide 22
> for the immediate suggestions)

The attached patch makes the remoteListener read only and seems to 
eliminate the pivot situations involving sl_event and sl_confirm.

Making the remote worker READ COMMITTED seemed to break things.

Maxim do you have a test environment that you can reproduce this 
situation in, does the attach patch help?



>
>>
>> The disorder tests against a 9.1 machine hit many of these read/write
>> dependencies in a test run.   Slony just retries things and it
>> eventually works but I can see how a 500,000 item batch means that you
>> might often encounter an issue before finishing a SYNC.
>>
>> My unverified guess is that changing this won't break slony but might
>> effect applications querying the slave that depend on the serializable
>> behaviour and maybe it should be a configuration parameter.
>>
>> There are two places in remote_worker.c where it places the local
>> connection to serializable.
>>
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: read_only_patch.diff
Type: text/x-patch
Size: 1116 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20111122/45924ada/attachment.bin 

From simon at 2ndQuadrant.com  Wed Nov 23 01:28:35 2011
From: simon at 2ndQuadrant.com (Simon Riggs)
Date: Wed, 23 Nov 2011 09:28:35 +0000
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <4ECC21A6.5070805@ca.afilias.info>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
	<4EC85FA6.8090104@ca.afilias.info>
	<CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>
	<4ECC21A6.5070805@ca.afilias.info>
Message-ID: <CA+U5nMKRkdBLb1RQFRu45msPSNNoqqd-BOso2K6f+d61+Vzzzw@mail.gmail.com>

On Tue, Nov 22, 2011 at 10:26 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> On 11-11-20 05:47 AM, C?dric Villemain wrote:
>>
>> 2011/11/20 Steve Singer<ssinger at ca.afilias.info>:
>>>
>>> I ?wonder out load if the slony remote worker needs to be in a
>>> serializable transaction or if READ COMMITTED is good enough.
>>>
>>> I also wonder if making the remote listener threads explicit read only
>>> transactions would help.
>>
>> if it is a read only transaction, then it is preferable to set it
>> explicitely.
>> If I well understood Heikki, it should remove 'pivot' situation. See
>> http://wiki.postgresql.org/images/4/4f/SSI-PGConfEU2011.pdf (slide 22
>> for the immediate suggestions)
>
> The attached patch makes the remoteListener read only and seems to eliminate
> the pivot situations involving sl_event and sl_confirm.

The attached patch doesn't work because DEFERRABLE only works in
conjunction with serializable mode.
New version of patch attached to do this.

> Making the remote worker READ COMMITTED seemed to break things.

The error message generated by serialization is
ERRCODE_T_R_SERIALIZATION_FAILURE, raised at level ERROR. So the
"FATAL" error is being generated by Slony not by Postgres.

ISTM that setting the remote worker to REPEATABLE READ would work well
for this case. Patch attached.

-- 
?Simon Riggs?????????????????? http://www.2ndQuadrant.com/
?PostgreSQL Development, 24x7 Support, Training & Services

From ssinger at ca.afilias.info  Wed Nov 23 05:51:14 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 23 Nov 2011 08:51:14 -0500
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
 9.1.1
In-Reply-To: <CA+U5nMKRkdBLb1RQFRu45msPSNNoqqd-BOso2K6f+d61+Vzzzw@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>	<4EC85FA6.8090104@ca.afilias.info>	<CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>	<4ECC21A6.5070805@ca.afilias.info>
	<CA+U5nMKRkdBLb1RQFRu45msPSNNoqqd-BOso2K6f+d61+Vzzzw@mail.gmail.com>
Message-ID: <4ECCFA52.9030109@ca.afilias.info>

On 11-11-23 04:28 AM, Simon Riggs wrote:
> On Tue, Nov 22, 2011 at 10:26 PM, Steve Singer<ssinger at ca.afilias.info>  wrote:
>> On 11-11-20 05:47 AM, C?dric Villemain wrote:
>>>

>
> ISTM that setting the remote worker to REPEATABLE READ would work well
> for this case. Patch attached.
>

Simon, Did you forget to attach the patch? I don't see it.



From simon at 2ndQuadrant.com  Wed Nov 23 06:27:47 2011
From: simon at 2ndQuadrant.com (Simon Riggs)
Date: Wed, 23 Nov 2011 14:27:47 +0000
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <4ECCFA52.9030109@ca.afilias.info>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
	<4EC85FA6.8090104@ca.afilias.info>
	<CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>
	<4ECC21A6.5070805@ca.afilias.info>
	<CA+U5nMKRkdBLb1RQFRu45msPSNNoqqd-BOso2K6f+d61+Vzzzw@mail.gmail.com>
	<4ECCFA52.9030109@ca.afilias.info>
Message-ID: <CA+U5nMJJi2rFO0BEgUUx12mzPO8OwVQ_hpFYsvJeZrjBnQFr_Q@mail.gmail.com>

On Wed, Nov 23, 2011 at 1:51 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> On 11-11-23 04:28 AM, Simon Riggs wrote:
>>
>> On Tue, Nov 22, 2011 at 10:26 PM, Steve Singer<ssinger at ca.afilias.info>
>> ?wrote:
>>>
>>> On 11-11-20 05:47 AM, C?dric Villemain wrote:
>>>>
>
>>
>> ISTM that setting the remote worker to REPEATABLE READ would work well
>> for this case. Patch attached.
>>
>
> Simon, Did you forget to attach the patch? I don't see it.

Looks that way.

-- 
?Simon Riggs?????????????????? http://www.2ndQuadrant.com/
?PostgreSQL Development, 24x7 Support, Training & Services
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rem_worker_repeatable_read.v1.patch
Type: application/octet-stream
Size: 575 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20111123/ac08e246/attachment.obj 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: read_only_patch.v2.diff
Type: application/octet-stream
Size: 1071 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20111123/ac08e246/attachment-0001.obj 

From ssinger at ca.afilias.info  Wed Nov 23 13:09:13 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 23 Nov 2011 16:09:13 -0500
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
 9.1.1
In-Reply-To: <CA+U5nMJJi2rFO0BEgUUx12mzPO8OwVQ_hpFYsvJeZrjBnQFr_Q@mail.gmail.com>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>	<4EC85FA6.8090104@ca.afilias.info>	<CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>	<4ECC21A6.5070805@ca.afilias.info>	<CA+U5nMKRkdBLb1RQFRu45msPSNNoqqd-BOso2K6f+d61+Vzzzw@mail.gmail.com>	<4ECCFA52.9030109@ca.afilias.info>
	<CA+U5nMJJi2rFO0BEgUUx12mzPO8OwVQ_hpFYsvJeZrjBnQFr_Q@mail.gmail.com>
Message-ID: <4ECD60F9.5040806@ca.afilias.info>

On 11-11-23 09:27 AM, Simon Riggs wrote:
> On Wed, Nov 23, 2011 at 1:51 PM, Steve Singer<ssinger at ca.afilias.info>  wrote:
>> On 11-11-23 04:28 AM, Simon Riggs wrote:
>>>
>>> On Tue, Nov 22, 2011 at 10:26 PM, Steve Singer<ssinger at ca.afilias.info>
>>>   wrote:
>>>>
>>>> On 11-11-20 05:47 AM, C?dric Villemain wrote:
>>>>>
>>
>>>
>>> ISTM that setting the remote worker to REPEATABLE READ would work well
>>> for this case. Patch attached.
>>>
>>
>> Simon, Did you forget to attach the patch? I don't see it.
>
> Looks that way.
>

Thanks for the patch.

So why would a SERIALIZABLE READ ONLY DEFERRED transaction produce fewer 
conflicts than a READ COMMITTED transaction?  Currently the 
remote_listener gets the default isolation level (READ COMMITTED).

The attached patch combines your two patches plus performs the same 
change to other places in remote_worker (there are places in 
remote_worker.c where that initial transaction is rolledback and 
restarted, this version also makes sure that those transactions are 
started as READ COMMITTED).   It also makes the connections that the 
remote helpers do to the remote database for querying sl_log_x READ ONLY 
DEFERRED.

When I run this patch through the test suite on 9.1 I don't see any 
serialization pivot failures (at least not yet).

-------------- next part --------------
A non-text attachment was scrubbed...
Name: read_only_patch.v3.diff
Type: text/x-patch
Size: 6342 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20111123/d6eb12cc/attachment.bin 

From simon at 2ndQuadrant.com  Wed Nov 23 13:45:43 2011
From: simon at 2ndQuadrant.com (Simon Riggs)
Date: Wed, 23 Nov 2011 21:45:43 +0000
Subject: [Slony1-general] Strange bug with slony 2.0.7 and postgresql
	9.1.1
In-Reply-To: <4ECD60F9.5040806@ca.afilias.info>
References: <CAK-MWwR-_8yXrNAHEDDKpnOid0zBUuDjg567t1yRC8a=9NjmoQ@mail.gmail.com>
	<CANfbgbb4K+E5M7mfn1RpsqE44q5jbgCrh-Nuy1x0cMzDhHvcPA@mail.gmail.com>
	<CAK-MWwTAuXD+j1b9g1XZ8TRGQ5H3rkau6_kG6V5D1fHc32EteA@mail.gmail.com>
	<CAK-MWwQxnfDy3F8TtG63L9MTLL3mXbwCHXZXs3kq7+SddL6TfQ@mail.gmail.com>
	<4EC85FA6.8090104@ca.afilias.info>
	<CAF6yO=3sgL91UarQZyufo9r6o4qzoXLGKXoWFomraCTveacp5w@mail.gmail.com>
	<4ECC21A6.5070805@ca.afilias.info>
	<CA+U5nMKRkdBLb1RQFRu45msPSNNoqqd-BOso2K6f+d61+Vzzzw@mail.gmail.com>
	<4ECCFA52.9030109@ca.afilias.info>
	<CA+U5nMJJi2rFO0BEgUUx12mzPO8OwVQ_hpFYsvJeZrjBnQFr_Q@mail.gmail.com>
	<4ECD60F9.5040806@ca.afilias.info>
Message-ID: <CA+U5nMK-OhOfadAK3QVTuCk9Hz6VdxXwpKUuP6z8V=JeFxZg+w@mail.gmail.com>

On Wed, Nov 23, 2011 at 9:09 PM, Steve Singer <ssinger at ca.afilias.info> wrote:

> So why would a SERIALIZABLE READ ONLY DEFERRED transaction produce fewer
> conflicts than a READ COMMITTED transaction? ?Currently the remote_listener
> gets the default isolation level (READ COMMITTED).

The setting of READ ONLY DEFERRED as you had it doesn't make sense.
DEFERRED only does something if you ask for SERIALIZABLE as well.

Marking things READ ONLY makes sense because it could prevent
serializable errors in other transactions.

As to whether you need SERIALIZABLE, deferred or otherwise in the
listener is not for me to say.

-- 
?Simon Riggs?????????????????? http://www.2ndQuadrant.com/
?PostgreSQL Development, 24x7 Support, Training & Services

From vividy at justware.co.jp  Wed Nov 23 18:41:42 2011
From: vividy at justware.co.jp (Michael Cheung)
Date: Thu, 24 Nov 2011 11:41:42 +0900
Subject: [Slony1-general] error while slonik_execute_script(slony
	2.1.0+postgresql 8.3.16)
In-Reply-To: <4ECBBF05.4080109@ca.afilias.info>
References: <20111122220034.67D5.8B406A0E@justware.co.jp>
	<4ECBBF05.4080109@ca.afilias.info>
Message-ID: <20111124114142.1AB0.8B406A0E@justware.co.jp>

On Tue, 22 Nov 2011 10:25:57 -0500
"Steve Singer" <ssinger at ca.afilias.info> wrote:

> On 11-11-22 08:00 AM, Michael Cheung wrote:
> > Hi all;
> > (postgres version 8.3.16, slony version 2.1.0)
> >
> > While I run sql to create index on a huge table(named tableA) by slonik_execute_script,
> > (actually this operation cost me almost 20 min.)
> > And slonik_execute_script command end with following error message.
> >
> > <stdin>:5: NOTICE:  public.tableB has an invalid configuration on the log trigger. This was not corrected because
> > only_lock is true and the table is not locked.
> > CONTEXT:  SQL statement "SELECT  "_apl_repl1".repair_log_triggers(true)"
> > PL/pgSQL function "ddlscript_complete_int" line 5 at PERFORM
> > SQL statement "SELECT  "_apl_repl1".ddlScript_complete_int( $1 , $2 )"
> > PL/pgSQL function "ddlscript_complete" line 7 at PERFORM
> >
> > And I have check the new index on table A on every node, it is created and seems fine.
> > And record count on tableB is same on every node. Everything seems well.
> >
> > But anyone can tell me what happened with this message? And is it safe to just ignore it?
> >
> 
> If you look at the Slony log trigger on when of your tables it will look 
> something like this:
> 
> 
> _test_logtrigger AFTER INSERT OR DELETE OR UPDATE ON a FOR EACH ROW 
> EXECUTE PROCEDURE _test.logtrigger('_test', '1', 'kvk')
> 
> The 'kvk' means that the first column in the table is part of the key, 
> the second column is not part of a key(primary key or unique 
> constraint), and the third column makes up the second component of the 
> unique key.
> 
> If you do something to the table to change this, ie drop the second 
> column.  Then this argument order is no longer correct and slony needs 
> to recreate the logtrigger to have the correct arguments.   Doing that 
> will require an exclusive lock.  If EXECUTE SCRIPT in 2.1 determines 
> that the script has already taken out an exclusive lock on the table 
> then it will just fix the trigger.   However it won't go and obtain the 
> lock.
> 
> The function select _test.determineAttKindUnique('public.a','a_pkey'); 
> (where _test is your slony schema name) will tell you want the arguments 
> should be for a particular function.  Compare the output of that for 
> your tableB with the arguments on the trigger (\d in psql).
> 
> If you call _test.repair_log_trigger(false) it will take an exclusive 
> lock on the tables it requires and repair the trigger arguments.  (You 
> would need to do that on all nodes).
> 
Thanks for your guide in detail.
I do change the pkey for tableA before , and now I have repaired trigger on tableA
by repair_log_triggers as you said.

Thanks for your help.



From khizer at srishtisoft.com  Thu Nov 24 22:06:51 2011
From: khizer at srishtisoft.com (khizer)
Date: Fri, 25 Nov 2011 11:36:51 +0530
Subject: [Slony1-general] Slony err
Message-ID: <4ECF307B.4000906@srishtisoft.com>

Hello Everyone,

              I am getting the following err while running the below command

:~# slonik replconfig.conf
replconfig.conf:14: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  
could not access file "$libdir/xxid": No such file or directory
replconfig.conf:14: Error: the extension for the xxid data type cannot 
be loaded in database 'dbname=repltestdb host=192.168.4.145 port=5432 
user=postgres'


how can i resolve this????????

Mehdi


From ian.lea at gmail.com  Fri Nov 25 05:58:32 2011
From: ian.lea at gmail.com (Ian Lea)
Date: Fri, 25 Nov 2011 13:58:32 +0000
Subject: [Slony1-general] Slony err
In-Reply-To: <4ECF307B.4000906@srishtisoft.com>
References: <4ECF307B.4000906@srishtisoft.com>
Message-ID: <CAEY5pxWZs3DAi-9289Hb8eTNeFFz-=3TkeB4kv7nTSKxuGkjpw@mail.gmail.com>

> ? ? ? ? ? ? ?I am getting the following err while running the below command
>
> :~# slonik replconfig.conf
> replconfig.conf:14: PGRES_FATAL_ERROR load '$libdir/xxid'; ?- ERROR:
> could not access file "$libdir/xxid": No such file or directory
> replconfig.conf:14: Error: the extension for the xxid data type cannot
> be loaded in database 'dbname=repltestdb host=192.168.4.145 port=5432
> user=postgres'
>
>
> how can i resolve this????????

Read the FAQ? http://slony.info/documentation/faq.html

If you need to repost, don't forget to include useful information such
as the versions of slony and postgres.

--
Ian.

From vitaliy.se at gmail.com  Fri Nov 25 13:05:11 2011
From: vitaliy.se at gmail.com (Vitaliy Semochkin)
Date: Sat, 26 Nov 2011 00:05:11 +0300
Subject: [Slony1-general] slony centos 5.7 RPM installation setup
Message-ID: <CAHyKpfPtnbyLZ+KLwKqG0mdSbmtkuEFG8qbjYUGxeOn8NkLKfg@mail.gmail.com>

Hello,

I've installed slony on centos 5.7 using postgre yum repository slony1-91

Can someone please tell me where I'm supposed to store place slony
shell variables such as:
CLUSTERNAME
MASTERDBNAME
SLAVEDBNAME
MASTERHOST
SLAVEHOST
REPLICATIONUSER

I can create a shell file and place it to /etc/profile.d but it came
to my mind that an rpm distribution might already have such file.


when I start slon manually I use something like
slon $CLUSTERNAME "dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST"


also how to pass paramets to /etc/init.d/slony1-91

Regards,
Vitaliy S

From ssinger at ca.afilias.info  Fri Nov 25 14:05:33 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 25 Nov 2011 17:05:33 -0500
Subject: [Slony1-general] slony centos 5.7 RPM installation setup
In-Reply-To: <CAHyKpfPtnbyLZ+KLwKqG0mdSbmtkuEFG8qbjYUGxeOn8NkLKfg@mail.gmail.com>
References: <CAHyKpfPtnbyLZ+KLwKqG0mdSbmtkuEFG8qbjYUGxeOn8NkLKfg@mail.gmail.com>
Message-ID: <4ED0112D.2060201@ca.afilias.info>

On 11-11-25 04:05 PM, Vitaliy Semochkin wrote:
> Hello,
>
> I've installed slony on centos 5.7 using postgre yum repository slony1-91
>
> Can someone please tell me where I'm supposed to store place slony
> shell variables such as:
> CLUSTERNAME
> MASTERDBNAME
> SLAVEDBNAME
> MASTERHOST
> SLAVEHOST
> REPLICATIONUSER
>
> I can create a shell file and place it to /etc/profile.d but it came
> to my mind that an rpm distribution might already have such file.
>
>
> when I start slon manually I use something like
> slon $CLUSTERNAME "dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST"
>
>
> also how to pass paramets to /etc/init.d/slony1-91
>

That script will source the file /etc/sysconfig/slony1  so if you add 
things into /etc/sysconfig/slony1 it will get added to the environment 
of the init script.

It also passes the argument to slon when it starts slon.

SLONCONF=/etc/slon.conf

You can set the slon parameters in that slon.conf file

See http://www.slony.info/documentation/2.1/runtime-config.html for the 
format of the conf file.



> Regards,
> Vitaliy S
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From devrim at gunduz.org  Fri Nov 25 21:11:15 2011
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Sat, 26 Nov 2011 07:11:15 +0200
Subject: [Slony1-general] slony centos 5.7 RPM installation setup
In-Reply-To: <CAHyKpfPtnbyLZ+KLwKqG0mdSbmtkuEFG8qbjYUGxeOn8NkLKfg@mail.gmail.com>
References: <CAHyKpfPtnbyLZ+KLwKqG0mdSbmtkuEFG8qbjYUGxeOn8NkLKfg@mail.gmail.com>
Message-ID: <1322284275.25669.13.camel@lenovo01-laptop03.gunduz.org>

On Sat, 2011-11-26 at 00:05 +0300, Vitaliy Semochkin wrote:
> also how to pass paramets to /etc/init.d/slony1-91

I am not expecting this script to work. For example, it currently
overrides everything in /etc/sysconfig/slony1/* , which is clearly a
bug.

I hit this while working on a customer this week. I will try to push a
fix soon.

Regards,
-- 
Devrim G?ND?Z
Principal Systems Engineer @ EnterpriseDB: http://www.enterprisedb.com
PostgreSQL Dan??man?/Consultant, Red Hat Certified Engineer
Community: devrim~PostgreSQL.org, devrim.gunduz~linux.org.tr
http://www.gunduz.org  Twitter: http://twitter.com/devrimgunduz
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20111126/49d680e1/attachment.pgp 

From drees76 at gmail.com  Mon Nov 28 22:42:45 2011
From: drees76 at gmail.com (David Rees)
Date: Mon, 28 Nov 2011 22:42:45 -0800
Subject: [Slony1-general] 1.2.23 and 9.0 compatibility?
Message-ID: <CAHtT9RvJmRYuM8b2RL-wZfpJaPnA=GJU00RSnx7zkd4MuAMvJQ@mail.gmail.com>

Is a 1.2.23 release planned? ?I am testing a cluster with Pg 9.0 and
ran across messages related to bug #239 (
http://www.slony.info/bugzilla/show_bug.cgi?id=239 ) where slony is
expecting pg_listener to exist, but it doesn't.

It doesn't appear to affect functionality at this point, but I assume
unless I'm using the latest from git at least #239 will bite me at
some point in time.

FWIW, I've attached here's what I'm seeing - should I open a bug? ?It
appears to be the same as bug #148 which hasn't been backported to the
1.2 branch.

If I wish to stay with Slony 1.2 for now to avoid rebuilding my
cluster is it recommended to stick with Pg <= 8.4?

-Dave
-------------- next part --------------
A non-text attachment was scrubbed...
Name: slony.log
Type: application/octet-stream
Size: 467 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20111128/d5d612bf/attachment.obj 

