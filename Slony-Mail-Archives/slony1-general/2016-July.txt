From ttignor at akamai.com  Fri Jul  8 12:27:15 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Fri, 8 Jul 2016 19:27:15 +0000
Subject: [Slony1-general] drop node error
Message-ID: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>


                Hello slony group,
                I?m testing now with slony1-2.2.4. I have just recently produced an error which effectively stops slon processing on some node A due to some node B being dropped. The event reproduces only infrequently. As some will know, a slon daemon for a given node which becomes aware its node has been dropped will respond by dropping its cluster schema. There appears to be a race condition between the node B schema drop and the (surviving) node A receipt of the disableNode (drop node) event. If the former occurs before the latter, all the remote worker threads on node A enter an error state. See the log samples below. I resolved this the first time by deleting all the recent non-SYNC events from the sl_event tables, and more recently with a simple node A slon restart.
                Please advise if there is any ticket I should provide this info to, or if I should create a new one. Thanks.


---- node 1 log ----
2016-07-08 18:06:31 UTC [30382] INFO   remoteWorkerThread_999999: SYNC 5000000008 done in 0.002 seconds
2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_999999: SYNC 5000000009 done in 0.002 seconds
2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_2: SYNC 5000017869 done in 0.002 seconds
2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_3: SYNC 5000018148 done in 0.004 seconds
2016-07-08 18:06:45 UTC [30382] CONFIG remoteWorkerThread_2: update provider configuration
2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: "select last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:  schema "_ams_clu\
ster" does not exist
LINE 1: select last_value from "_ams_cluster".sl_log_status
                               ^

2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
2016-07-08 18:06:45 UTC [30382] CONFIG version for "dbname=ams
      host=198.18.102.45
      user=ams_slony
      sslmode=verify-ca
      sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
      sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
      sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem" is 90119
2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: "select last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:  schema "_ams_clu\
ster" does not exist
LINE 1: select last_value from "_ams_cluster".sl_log_status
                               ^

2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
2016-07-08 18:06:45 UTC [30382] ERROR  remoteListenThread_999999: "select ev_origin, ev_seqno, ev_timestamp,        ev_snapshot,        "pg_catalog".txid_sna\
pshot_xmin(ev_snapshot),        "pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,        ev_data1, ev_data2,        ev_data3, ev_data4,        ev\
_data5, ev_data6,        ev_data7, ev_data8 from "_ams_cluster".sl_event e where (e.ev_origin = '999999' and e.ev_seqno > '5000000009') or (e.ev_origin = '2'\
and e.ev_seqno > '5000017870') or (e.ev_origin = '3' and e.ev_seqno > '5000018151') order by e.ev_origin, e.ev_seqno limit 40" - ERROR:  schema "_ams_cluste\
r" does not exist
LINE 1: ...v_data5, ev_data6,        ev_data7, ev_data8 from "_ams_clus...
                                                             ^
2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: "start transaction; set enable_seqscan = off; set enable_indexscan = on; " PGRES_FATAL_ERROR ERR\
OR:  current transaction is aborted, commands ignored until end of transaction block
2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: "start transaction; set enable_seqscan = off; set enable_indexscan = on; " PGRES_FATAL_ERROR ERR\
OR:  current transaction is aborted, commands ignored until end of transaction block
2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
----


---- node 999999 log ----
2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC 5000081216 done in 0.004 seconds
2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_2: SYNC 5000017870 done in 0.004 seconds
2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_3: SYNC 5000018150 done in 0.004 seconds
2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC 5000081217 done in 0.003 seconds
2016-07-08 18:06:44 UTC [558] WARN   remoteWorkerThread_3: got DROP NODE for local node ID
NOTICE:  Slony-I: Please drop schema "_ams_cluster"
NOTICE:  drop cascades to 171 other objects
DETAIL:  drop cascades to table _ams_cluster.sl_node
drop cascades to table _ams_cluster.sl_nodelock
drop cascades to table _ams_cluster.sl_set
drop cascades to table _ams_cluster.sl_setsync
drop cascades to table _ams_cluster.sl_table
----

            Tom    ?



-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160708/bd39c8a5/attachment.htm 

From steve at ssinger.info  Tue Jul 12 05:23:10 2016
From: steve at ssinger.info (Steve Singer)
Date: Tue, 12 Jul 2016 08:23:10 -0400
Subject: [Slony1-general] drop node error
In-Reply-To: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>
References: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>
Message-ID: <5784E12E.70500@ssinger.info>

On 07/08/2016 03:27 PM, Tignor, Tom wrote:
>                  Hello slony group,
>
>                  I?m testing now with slony1-2.2.4. I have just recently
> produced an error which effectively stops slon processing on some node A
> due to some node B being dropped. The event reproduces only
> infrequently. As some will know, a slon daemon for a given node which
> becomes aware its node has been dropped will respond by dropping its
> cluster schema. There appears to be a race condition between the node B
> schema drop and the (surviving) node A receipt of the disableNode (drop
> node) event. If the former occurs before the latter, all the remote
> worker threads on node A enter an error state. See the log samples
> below. I resolved this the first time by deleting all the recent
> non-SYNC events from the sl_event tables, and more recently with a
> simple node A slon restart.
>
>                  Please advise if there is any ticket I should provide
> this info to, or if I should create a new one. Thanks.
>

The Slony bug tracker is at
http://bugs.slony.info/bugzilla/


I assume your saying that when the slon restart it keeps hitting this 
error and keeps restarting.



> ---- node 1 log ----
>
> 2016-07-08 18:06:31 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
> 5000000008 done in 0.002 seconds
>
> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
> 5000000009 done in 0.002 seconds
>
> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_2: SYNC
> 5000017869 done in 0.002 seconds
>
> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_3: SYNC
> 5000018148 done in 0.004 seconds
>
> 2016-07-08 18:06:45 UTC [30382] CONFIG remoteWorkerThread_2: update
> provider configuration
>
> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: "select
> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
> schema "_ams_clu\
>
> ster" does not exist
>
> LINE 1: select last_value from "_ams_cluster".sl_log_status
>
>                                 ^
>
> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>
> 2016-07-08 18:06:45 UTC [30382] CONFIG version for "dbname=ams
>
>        host=198.18.102.45
>
>        user=ams_slony
>
>        sslmode=verify-ca
>
>        sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
>
>        sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
>
>        sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem"
> is 90119
>
> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: "select
> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
> schema "_ams_clu\
>
> ster" does not exist
>
> LINE 1: select last_value from "_ams_cluster".sl_log_status
>
>                                 ^
>
> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>
> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteListenThread_999999:
> "select ev_origin, ev_seqno, ev_timestamp,        ev_snapshot,
> "pg_catalog".txid_sna\
>
> pshot_xmin(ev_snapshot),
> "pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,
>   ev_data1, ev_data2,        ev_data3, ev_data4,        ev\
>
> _data5, ev_data6,        ev_data7, ev_data8 from "_ams_cluster".sl_event
> e where (e.ev_origin = '999999' and e.ev_seqno > '5000000009') or
> (e.ev_origin = '2'\
>
> and e.ev_seqno > '5000017870') or (e.ev_origin = '3' and e.ev_seqno >
> '5000018151') order by e.ev_origin, e.ev_seqno limit 40" - ERROR:
> schema "_ams_cluste\
>
> r" does not exist
>
> LINE 1: ...v_data5, ev_data6,        ev_data7, ev_data8 from "_ams_clus...
>
>                                                               ^
>
> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: "start
> transaction; set enable_seqscan = off; set enable_indexscan = on; "
> PGRES_FATAL_ERROR ERR\
>
> OR:  current transaction is aborted, commands ignored until end of
> transaction block
>
> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>
> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: "start
> transaction; set enable_seqscan = off; set enable_indexscan = on; "
> PGRES_FATAL_ERROR ERR\
>
> OR:  current transaction is aborted, commands ignored until end of
> transaction block
>
> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>
> ----
>
> ---- node 999999 log ----
>
> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
> 5000081216 done in 0.004 seconds
>
> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_2: SYNC
> 5000017870 done in 0.004 seconds
>
> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_3: SYNC
> 5000018150 done in 0.004 seconds
>
> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
> 5000081217 done in 0.003 seconds
>
> 2016-07-08 18:06:44 UTC [558] WARN   remoteWorkerThread_3: got DROP NODE
> for local node ID
>
> NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>
> NOTICE:  drop cascades to 171 other objects
>
> DETAIL:  drop cascades to table _ams_cluster.sl_node
>
> drop cascades to table _ams_cluster.sl_nodelock
>
> drop cascades to table _ams_cluster.sl_set
>
> drop cascades to table _ams_cluster.sl_setsync
>
> drop cascades to table _ams_cluster.sl_table
>
> ----
>
>              Tom J
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From steve at ssinger.info  Sun Jul 17 11:16:15 2016
From: steve at ssinger.info (Steve Singer)
Date: Sun, 17 Jul 2016 14:16:15 -0400
Subject: [Slony1-general] drop node error
In-Reply-To: <5784E12E.70500@ssinger.info>
References: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>
	<5784E12E.70500@ssinger.info>
Message-ID: <578BCB6F.2070500@ssinger.info>

On 07/12/2016 08:23 AM, Steve Singer wrote:
> On 07/08/2016 03:27 PM, Tignor, Tom wrote:
>>                   Hello slony group,
>>
>>                   I?m testing now with slony1-2.2.4. I have just recently
>> produced an error which effectively stops slon processing on some node A
>> due to some node B being dropped. The event reproduces only
>> infrequently. As some will know, a slon daemon for a given node which
>> becomes aware its node has been dropped will respond by dropping its
>> cluster schema. There appears to be a race condition between the node B
>> schema drop and the (surviving) node A receipt of the disableNode (drop
>> node) event. If the former occurs before the latter, all the remote
>> worker threads on node A enter an error state. See the log samples
>> below. I resolved this the first time by deleting all the recent
>> non-SYNC events from the sl_event tables, and more recently with a
>> simple node A slon restart.
>>
>>                   Please advise if there is any ticket I should provide
>> this info to, or if I should create a new one. Thanks.
>>
>
> The Slony bug tracker is at
> http://bugs.slony.info/bugzilla/
>
>
> I assume your saying that when the slon restart it keeps hitting this
> error and keeps restarting.
>

Any more hints on how you reproduce this would be helpful.
I've been trying to reproduce this with no luck.

At the time you issue the drop node, are all other nodes caught up to 
the drop'd node (and the event node) with respect to configuration events?

Ie if you do
sync(id=$NODE_ABOUT_TO_DROP);
wait for event(wait on=$NODE_ABOUT_TO_DROP, origin=$NODE_ABOUT_TO_DROP, 
confirmed=all);

sync(id=$EVENT_NODE_FOR_DROP);
wait for event(wait on=$EVENT_NODE_FOR_DROP, 
origin=$EVENT_NODE_FOR_DROP, confirmed=all);


(notice that I am NOT passing any timeout to the wait for).





>
>
>> ---- node 1 log ----
>>
>> 2016-07-08 18:06:31 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>> 5000000008 done in 0.002 seconds
>>
>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>> 5000000009 done in 0.002 seconds
>>
>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_2: SYNC
>> 5000017869 done in 0.002 seconds
>>
>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_3: SYNC
>> 5000018148 done in 0.004 seconds
>>
>> 2016-07-08 18:06:45 UTC [30382] CONFIG remoteWorkerThread_2: update
>> provider configuration
>>
>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: "select
>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>> schema "_ams_clu\
>>
>> ster" does not exist
>>
>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>
>>                                  ^
>>
>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>
>> 2016-07-08 18:06:45 UTC [30382] CONFIG version for "dbname=ams
>>
>>         host=198.18.102.45
>>
>>         user=ams_slony
>>
>>         sslmode=verify-ca
>>
>>         sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
>>
>>         sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
>>
>>         sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem"
>> is 90119
>>
>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: "select
>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>> schema "_ams_clu\
>>
>> ster" does not exist
>>
>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>
>>                                  ^
>>
>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>
>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteListenThread_999999:
>> "select ev_origin, ev_seqno, ev_timestamp,        ev_snapshot,
>> "pg_catalog".txid_sna\
>>
>> pshot_xmin(ev_snapshot),
>> "pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,
>>    ev_data1, ev_data2,        ev_data3, ev_data4,        ev\
>>
>> _data5, ev_data6,        ev_data7, ev_data8 from "_ams_cluster".sl_event
>> e where (e.ev_origin = '999999' and e.ev_seqno > '5000000009') or
>> (e.ev_origin = '2'\
>>
>> and e.ev_seqno > '5000017870') or (e.ev_origin = '3' and e.ev_seqno >
>> '5000018151') order by e.ev_origin, e.ev_seqno limit 40" - ERROR:
>> schema "_ams_cluste\
>>
>> r" does not exist
>>
>> LINE 1: ...v_data5, ev_data6,        ev_data7, ev_data8 from "_ams_clus...
>>
>>                                                                ^
>>
>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: "start
>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>> PGRES_FATAL_ERROR ERR\
>>
>> OR:  current transaction is aborted, commands ignored until end of
>> transaction block
>>
>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>
>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: "start
>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>> PGRES_FATAL_ERROR ERR\
>>
>> OR:  current transaction is aborted, commands ignored until end of
>> transaction block
>>
>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>
>> ----
>>
>> ---- node 999999 log ----
>>
>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>> 5000081216 done in 0.004 seconds
>>
>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_2: SYNC
>> 5000017870 done in 0.004 seconds
>>
>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_3: SYNC
>> 5000018150 done in 0.004 seconds
>>
>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>> 5000081217 done in 0.003 seconds
>>
>> 2016-07-08 18:06:44 UTC [558] WARN   remoteWorkerThread_3: got DROP NODE
>> for local node ID
>>
>> NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>>
>> NOTICE:  drop cascades to 171 other objects
>>
>> DETAIL:  drop cascades to table _ams_cluster.sl_node
>>
>> drop cascades to table _ams_cluster.sl_nodelock
>>
>> drop cascades to table _ams_cluster.sl_set
>>
>> drop cascades to table _ams_cluster.sl_setsync
>>
>> drop cascades to table _ams_cluster.sl_table
>>
>> ----
>>
>>               Tom J
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From ttignor at akamai.com  Mon Jul 18 04:58:08 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 18 Jul 2016 11:58:08 +0000
Subject: [Slony1-general] drop node error
In-Reply-To: <578BCB6F.2070500@ssinger.info>
References: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>
	<5784E12E.70500@ssinger.info> <578BCB6F.2070500@ssinger.info>
Message-ID: <2BD237A6-E0A0-488A-9555-E14191B7FB09@akamai.com>


	Hi Steve,
	Thanks for looking into this. The context in which this occurs is in an effort to move a node. The specifics are somewhat involved, but the key points are that a node is failed over, then dropped and a new node with the same node ID created elsewhere (another DB on another host). The recent work makes a best effort to automatically reverse the steps if a failure is encountered, so failover and drop the new node, then recreate the original. There are certainly ?wait for event?s along the way, but it seems likely all nodes aren?t fully caught up before an error. If you?re trying to reproduce, recurringly dropping and recreating a node with the same node ID could help.
	For the specific problem of the race, though, it might be simpler to just ?kill ?STOP? the slon daemons on node A and then wait for node B to drop its schema (then resume with ?kill ?CONT? to node A.)

	Tom    ?


On 7/17/16, 2:16 PM, "Steve Singer" <steve at ssinger.info> wrote:

>On 07/12/2016 08:23 AM, Steve Singer wrote:
>> On 07/08/2016 03:27 PM, Tignor, Tom wrote:
>>>                   Hello slony group,
>>>
>>>                   I?m testing now with slony1-2.2.4. I have just recently
>>> produced an error which effectively stops slon processing on some node A
>>> due to some node B being dropped. The event reproduces only
>>> infrequently. As some will know, a slon daemon for a given node which
>>> becomes aware its node has been dropped will respond by dropping its
>>> cluster schema. There appears to be a race condition between the node B
>>> schema drop and the (surviving) node A receipt of the disableNode (drop
>>> node) event. If the former occurs before the latter, all the remote
>>> worker threads on node A enter an error state. See the log samples
>>> below. I resolved this the first time by deleting all the recent
>>> non-SYNC events from the sl_event tables, and more recently with a
>>> simple node A slon restart.
>>>
>>>                   Please advise if there is any ticket I should provide
>>> this info to, or if I should create a new one. Thanks.
>>>
>>
>> The Slony bug tracker is at
>> http://bugs.slony.info/bugzilla/
>>
>>
>> I assume your saying that when the slon restart it keeps hitting this
>> error and keeps restarting.
>>
>
>Any more hints on how you reproduce this would be helpful.
>I've been trying to reproduce this with no luck.
>
>At the time you issue the drop node, are all other nodes caught up to 
>the drop'd node (and the event node) with respect to configuration events?
>
>Ie if you do
>sync(id=$NODE_ABOUT_TO_DROP);
>wait for event(wait on=$NODE_ABOUT_TO_DROP, origin=$NODE_ABOUT_TO_DROP, 
>confirmed=all);
>
>sync(id=$EVENT_NODE_FOR_DROP);
>wait for event(wait on=$EVENT_NODE_FOR_DROP, 
>origin=$EVENT_NODE_FOR_DROP, confirmed=all);
>
>
>(notice that I am NOT passing any timeout to the wait for).
>
>
>
>
>
>>
>>
>>> ---- node 1 log ----
>>>
>>> 2016-07-08 18:06:31 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>>> 5000000008 done in 0.002 seconds
>>>
>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>>> 5000000009 done in 0.002 seconds
>>>
>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_2: SYNC
>>> 5000017869 done in 0.002 seconds
>>>
>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_3: SYNC
>>> 5000018148 done in 0.004 seconds
>>>
>>> 2016-07-08 18:06:45 UTC [30382] CONFIG remoteWorkerThread_2: update
>>> provider configuration
>>>
>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: "select
>>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>>> schema "_ams_clu\
>>>
>>> ster" does not exist
>>>
>>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>>
>>>                                  ^
>>>
>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>>
>>> 2016-07-08 18:06:45 UTC [30382] CONFIG version for "dbname=ams
>>>
>>>         host=198.18.102.45
>>>
>>>         user=ams_slony
>>>
>>>         sslmode=verify-ca
>>>
>>>         sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
>>>
>>>         sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
>>>
>>>         sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem"
>>> is 90119
>>>
>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: "select
>>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>>> schema "_ams_clu\
>>>
>>> ster" does not exist
>>>
>>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>>
>>>                                  ^
>>>
>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>>
>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteListenThread_999999:
>>> "select ev_origin, ev_seqno, ev_timestamp,        ev_snapshot,
>>> "pg_catalog".txid_sna\
>>>
>>> pshot_xmin(ev_snapshot),
>>> "pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,
>>>    ev_data1, ev_data2,        ev_data3, ev_data4,        ev\
>>>
>>> _data5, ev_data6,        ev_data7, ev_data8 from "_ams_cluster".sl_event
>>> e where (e.ev_origin = '999999' and e.ev_seqno > '5000000009') or
>>> (e.ev_origin = '2'\
>>>
>>> and e.ev_seqno > '5000017870') or (e.ev_origin = '3' and e.ev_seqno >
>>> '5000018151') order by e.ev_origin, e.ev_seqno limit 40" - ERROR:
>>> schema "_ams_cluste\
>>>
>>> r" does not exist
>>>
>>> LINE 1: ...v_data5, ev_data6,        ev_data7, ev_data8 from "_ams_clus...
>>>
>>>                                                                ^
>>>
>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: "start
>>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>>> PGRES_FATAL_ERROR ERR\
>>>
>>> OR:  current transaction is aborted, commands ignored until end of
>>> transaction block
>>>
>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>>
>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: "start
>>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>>> PGRES_FATAL_ERROR ERR\
>>>
>>> OR:  current transaction is aborted, commands ignored until end of
>>> transaction block
>>>
>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>>
>>> ----
>>>
>>> ---- node 999999 log ----
>>>
>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>>> 5000081216 done in 0.004 seconds
>>>
>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_2: SYNC
>>> 5000017870 done in 0.004 seconds
>>>
>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_3: SYNC
>>> 5000018150 done in 0.004 seconds
>>>
>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>>> 5000081217 done in 0.003 seconds
>>>
>>> 2016-07-08 18:06:44 UTC [558] WARN   remoteWorkerThread_3: got DROP NODE
>>> for local node ID
>>>
>>> NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>>>
>>> NOTICE:  drop cascades to 171 other objects
>>>
>>> DETAIL:  drop cascades to table _ams_cluster.sl_node
>>>
>>> drop cascades to table _ams_cluster.sl_nodelock
>>>
>>> drop cascades to table _ams_cluster.sl_set
>>>
>>> drop cascades to table _ams_cluster.sl_setsync
>>>
>>> drop cascades to table _ams_cluster.sl_table
>>>
>>> ----
>>>
>>>               Tom J
>>>
>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>


From steve at ssinger.info  Mon Jul 18 06:13:37 2016
From: steve at ssinger.info (Steve Singer)
Date: Mon, 18 Jul 2016 09:13:37 -0400 (EDT)
Subject: [Slony1-general] drop node error
In-Reply-To: <2BD237A6-E0A0-488A-9555-E14191B7FB09@akamai.com>
References: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>
	<5784E12E.70500@ssinger.info> <578BCB6F.2070500@ssinger.info>
	<2BD237A6-E0A0-488A-9555-E14191B7FB09@akamai.com>
Message-ID: <alpine.DEB.2.11.1607180905490.4971@opti.atlantida>

On Mon, 18 Jul 2016, Tignor, Tom wrote:

One thing that I stress is that it is a good idea (and maybe very important) 
is that all nodes be caught up (or at least with respect to configuration 
events) before you issue the drop node command.  The automatic waitfor 
support in slonik is supposed to ensure this before submitting the drop 
node but maybe it isn't working as I expect or you might be doing something 
to subvert this.

Your error message looked like the remoteWorker_$droppedNode was trying to 
connect to the dropped node and pull data.  I THINK the remoteWorker would 
only be trying to pull data from the $dropped node if the local node thought 
that the dropped node was a set origin.  This makes me think that the 
failover wasn't confirmed by all nodes before issuing the drop node.  If 
your drop node is in the same slonik script as the failover then slonik 
'should' be doing this, but maybe there is a bug, or maybe your doing 
something to make the drop node get submitted too early.

Without more details it is hard to say.


>
> 	Hi Steve,
> 	Thanks for looking into this. The context in which this occurs is in 
> an effort to move a node. The specifics are somewhat involved, but the key 
> points are that a node is failed over, then dropped and a new node with 
> the same node ID created elsewhere (another DB on another host). The 
> recent work makes a best effort to automatically reverse the steps if a 
> failure is encountered, so failover and drop the new node, then recreate 
> the original. There are certainly ?wait for event?s along the way, but it 
> seems likely all nodes aren?t fully caught up before an error. If you?re 
> trying to reproduce, recurringly dropping and recreating a node with the 
> same node ID could help.
> 	For the specific problem of the race, though, it might be simpler to 
> just ?kill ?STOP? the slon daemons on node A and then wait for node B to 
> drop its schema (then resume with ?kill ?CONT? to node A.)
>
> 	Tom    ?
>
>
> On 7/17/16, 2:16 PM, "Steve Singer" <steve at ssinger.info> wrote:
>
>> On 07/12/2016 08:23 AM, Steve Singer wrote:
>>> On 07/08/2016 03:27 PM, Tignor, Tom wrote:
>>>>                   Hello slony group,
>>>>
>>>>                   I?m testing now with slony1-2.2.4. I have just recently
>>>> produced an error which effectively stops slon processing on some node A
>>>> due to some node B being dropped. The event reproduces only
>>>> infrequently. As some will know, a slon daemon for a given node which
>>>> becomes aware its node has been dropped will respond by dropping its
>>>> cluster schema. There appears to be a race condition between the node B
>>>> schema drop and the (surviving) node A receipt of the disableNode (drop
>>>> node) event. If the former occurs before the latter, all the remote
>>>> worker threads on node A enter an error state. See the log samples
>>>> below. I resolved this the first time by deleting all the recent
>>>> non-SYNC events from the sl_event tables, and more recently with a
>>>> simple node A slon restart.
>>>>
>>>>                   Please advise if there is any ticket I should provide
>>>> this info to, or if I should create a new one. Thanks.
>>>>
>>>
>>> The Slony bug tracker is at
>>> http://bugs.slony.info/bugzilla/
>>>
>>>
>>> I assume your saying that when the slon restart it keeps hitting this
>>> error and keeps restarting.
>>>
>>
>> Any more hints on how you reproduce this would be helpful.
>> I've been trying to reproduce this with no luck.
>>
>> At the time you issue the drop node, are all other nodes caught up to
>> the drop'd node (and the event node) with respect to configuration events?
>>
>> Ie if you do
>> sync(id=$NODE_ABOUT_TO_DROP);
>> wait for event(wait on=$NODE_ABOUT_TO_DROP, origin=$NODE_ABOUT_TO_DROP,
>> confirmed=all);
>>
>> sync(id=$EVENT_NODE_FOR_DROP);
>> wait for event(wait on=$EVENT_NODE_FOR_DROP,
>> origin=$EVENT_NODE_FOR_DROP, confirmed=all);
>>
>>
>> (notice that I am NOT passing any timeout to the wait for).
>>
>>
>>
>>
>>
>>>
>>>
>>>> ---- node 1 log ----
>>>>
>>>> 2016-07-08 18:06:31 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>>>> 5000000008 done in 0.002 seconds
>>>>
>>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>>>> 5000000009 done in 0.002 seconds
>>>>
>>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_2: SYNC
>>>> 5000017869 done in 0.002 seconds
>>>>
>>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_3: SYNC
>>>> 5000018148 done in 0.004 seconds
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] CONFIG remoteWorkerThread_2: update
>>>> provider configuration
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: "select
>>>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>>>> schema "_ams_clu\
>>>>
>>>> ster" does not exist
>>>>
>>>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>>>
>>>>                                  ^
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] CONFIG version for "dbname=ams
>>>>
>>>>         host=198.18.102.45
>>>>
>>>>         user=ams_slony
>>>>
>>>>         sslmode=verify-ca
>>>>
>>>>         sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
>>>>
>>>>         sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
>>>>
>>>>         sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem"
>>>> is 90119
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: "select
>>>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>>>> schema "_ams_clu\
>>>>
>>>> ster" does not exist
>>>>
>>>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>>>
>>>>                                  ^
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>>>
>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteListenThread_999999:
>>>> "select ev_origin, ev_seqno, ev_timestamp,        ev_snapshot,
>>>> "pg_catalog".txid_sna\
>>>>
>>>> pshot_xmin(ev_snapshot),
>>>> "pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,
>>>>    ev_data1, ev_data2,        ev_data3, ev_data4,        ev\
>>>>
>>>> _data5, ev_data6,        ev_data7, ev_data8 from "_ams_cluster".sl_event
>>>> e where (e.ev_origin = '999999' and e.ev_seqno > '5000000009') or
>>>> (e.ev_origin = '2'\
>>>>
>>>> and e.ev_seqno > '5000017870') or (e.ev_origin = '3' and e.ev_seqno >
>>>> '5000018151') order by e.ev_origin, e.ev_seqno limit 40" - ERROR:
>>>> schema "_ams_cluste\
>>>>
>>>> r" does not exist
>>>>
>>>> LINE 1: ...v_data5, ev_data6,        ev_data7, ev_data8 from "_ams_clus...
>>>>
>>>>                                                                ^
>>>>
>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: "start
>>>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>>>> PGRES_FATAL_ERROR ERR\
>>>>
>>>> OR:  current transaction is aborted, commands ignored until end of
>>>> transaction block
>>>>
>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>>>
>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: "start
>>>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>>>> PGRES_FATAL_ERROR ERR\
>>>>
>>>> OR:  current transaction is aborted, commands ignored until end of
>>>> transaction block
>>>>
>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>>>
>>>> ----
>>>>
>>>> ---- node 999999 log ----
>>>>
>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>>>> 5000081216 done in 0.004 seconds
>>>>
>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_2: SYNC
>>>> 5000017870 done in 0.004 seconds
>>>>
>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_3: SYNC
>>>> 5000018150 done in 0.004 seconds
>>>>
>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>>>> 5000081217 done in 0.003 seconds
>>>>
>>>> 2016-07-08 18:06:44 UTC [558] WARN   remoteWorkerThread_3: got DROP NODE
>>>> for local node ID
>>>>
>>>> NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>>>>
>>>> NOTICE:  drop cascades to 171 other objects
>>>>
>>>> DETAIL:  drop cascades to table _ams_cluster.sl_node
>>>>
>>>> drop cascades to table _ams_cluster.sl_nodelock
>>>>
>>>> drop cascades to table _ams_cluster.sl_set
>>>>
>>>> drop cascades to table _ams_cluster.sl_setsync
>>>>
>>>> drop cascades to table _ams_cluster.sl_table
>>>>
>>>> ----
>>>>
>>>>               Tom J
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>
>
>

From ttignor at akamai.com  Mon Jul 18 06:54:57 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 18 Jul 2016 13:54:57 +0000
Subject: [Slony1-general] drop node error
In-Reply-To: <alpine.DEB.2.11.1607180905490.4971@opti.atlantida>
References: <72A9DE45-16C6-49B2-83EE-B53D95A89906@akamai.com>
	<5784E12E.70500@ssinger.info> <578BCB6F.2070500@ssinger.info>
	<2BD237A6-E0A0-488A-9555-E14191B7FB09@akamai.com>
	<alpine.DEB.2.11.1607180905490.4971@opti.atlantida>
Message-ID: <2B21225D-11CE-45E1-BB12-74F62CF58D2B@akamai.com>


	I actually have the failover in one script and then a ?drop node, wait for event? sequence (in that order) in a subsequent script. So there actually isn?t an explicit wait between the failover and the node drop. In all my testing, the failover has proven to be the most reliable slony op, but given what you?ve said, it looks like adding a wait is in order.

	Tom    ?


On 7/18/16, 9:13 AM, "Steve Singer" <steve at ssinger.info> wrote:

>On Mon, 18 Jul 2016, Tignor, Tom wrote:
>
>One thing that I stress is that it is a good idea (and maybe very important) 
>is that all nodes be caught up (or at least with respect to configuration 
>events) before you issue the drop node command.  The automatic waitfor 
>support in slonik is supposed to ensure this before submitting the drop 
>node but maybe it isn't working as I expect or you might be doing something 
>to subvert this.
>
>Your error message looked like the remoteWorker_$droppedNode was trying to 
>connect to the dropped node and pull data.  I THINK the remoteWorker would 
>only be trying to pull data from the $dropped node if the local node thought 
>that the dropped node was a set origin.  This makes me think that the 
>failover wasn't confirmed by all nodes before issuing the drop node.  If 
>your drop node is in the same slonik script as the failover then slonik 
>'should' be doing this, but maybe there is a bug, or maybe your doing 
>something to make the drop node get submitted too early.
>
>Without more details it is hard to say.
>
>
>>
>> 	Hi Steve,
>> 	Thanks for looking into this. The context in which this occurs is in 
>> an effort to move a node. The specifics are somewhat involved, but the key 
>> points are that a node is failed over, then dropped and a new node with 
>> the same node ID created elsewhere (another DB on another host). The 
>> recent work makes a best effort to automatically reverse the steps if a 
>> failure is encountered, so failover and drop the new node, then recreate 
>> the original. There are certainly ?wait for event?s along the way, but it 
>> seems likely all nodes aren?t fully caught up before an error. If you?re 
>> trying to reproduce, recurringly dropping and recreating a node with the 
>> same node ID could help.
>> 	For the specific problem of the race, though, it might be simpler to 
>> just ?kill ?STOP? the slon daemons on node A and then wait for node B to 
>> drop its schema (then resume with ?kill ?CONT? to node A.)
>>
>> 	Tom    ?
>>
>>
>> On 7/17/16, 2:16 PM, "Steve Singer" <steve at ssinger.info> wrote:
>>
>>> On 07/12/2016 08:23 AM, Steve Singer wrote:
>>>> On 07/08/2016 03:27 PM, Tignor, Tom wrote:
>>>>>                   Hello slony group,
>>>>>
>>>>>                   I?m testing now with slony1-2.2.4. I have just recently
>>>>> produced an error which effectively stops slon processing on some node A
>>>>> due to some node B being dropped. The event reproduces only
>>>>> infrequently. As some will know, a slon daemon for a given node which
>>>>> becomes aware its node has been dropped will respond by dropping its
>>>>> cluster schema. There appears to be a race condition between the node B
>>>>> schema drop and the (surviving) node A receipt of the disableNode (drop
>>>>> node) event. If the former occurs before the latter, all the remote
>>>>> worker threads on node A enter an error state. See the log samples
>>>>> below. I resolved this the first time by deleting all the recent
>>>>> non-SYNC events from the sl_event tables, and more recently with a
>>>>> simple node A slon restart.
>>>>>
>>>>>                   Please advise if there is any ticket I should provide
>>>>> this info to, or if I should create a new one. Thanks.
>>>>>
>>>>
>>>> The Slony bug tracker is at
>>>> http://bugs.slony.info/bugzilla/
>>>>
>>>>
>>>> I assume your saying that when the slon restart it keeps hitting this
>>>> error and keeps restarting.
>>>>
>>>
>>> Any more hints on how you reproduce this would be helpful.
>>> I've been trying to reproduce this with no luck.
>>>
>>> At the time you issue the drop node, are all other nodes caught up to
>>> the drop'd node (and the event node) with respect to configuration events?
>>>
>>> Ie if you do
>>> sync(id=$NODE_ABOUT_TO_DROP);
>>> wait for event(wait on=$NODE_ABOUT_TO_DROP, origin=$NODE_ABOUT_TO_DROP,
>>> confirmed=all);
>>>
>>> sync(id=$EVENT_NODE_FOR_DROP);
>>> wait for event(wait on=$EVENT_NODE_FOR_DROP,
>>> origin=$EVENT_NODE_FOR_DROP, confirmed=all);
>>>
>>>
>>> (notice that I am NOT passing any timeout to the wait for).
>>>
>>>
>>>
>>>
>>>
>>>>
>>>>
>>>>> ---- node 1 log ----
>>>>>
>>>>> 2016-07-08 18:06:31 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>>>>> 5000000008 done in 0.002 seconds
>>>>>
>>>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_999999: SYNC
>>>>> 5000000009 done in 0.002 seconds
>>>>>
>>>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_2: SYNC
>>>>> 5000017869 done in 0.002 seconds
>>>>>
>>>>> 2016-07-08 18:06:33 UTC [30382] INFO   remoteWorkerThread_3: SYNC
>>>>> 5000018148 done in 0.004 seconds
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] CONFIG remoteWorkerThread_2: update
>>>>> provider configuration
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: "select
>>>>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>>>>> schema "_ams_clu\
>>>>>
>>>>> ster" does not exist
>>>>>
>>>>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>>>>
>>>>>                                  ^
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] CONFIG version for "dbname=ams
>>>>>
>>>>>         host=198.18.102.45
>>>>>
>>>>>         user=ams_slony
>>>>>
>>>>>         sslmode=verify-ca
>>>>>
>>>>>         sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
>>>>>
>>>>>         sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
>>>>>
>>>>>         sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem"
>>>>> is 90119
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: "select
>>>>> last_value from "_ams_cluster".sl_log_status" PGRES_FATAL_ERROR ERROR:
>>>>> schema "_ams_clu\
>>>>>
>>>>> ster" does not exist
>>>>>
>>>>> LINE 1: select last_value from "_ams_cluster".sl_log_status
>>>>>
>>>>>                                  ^
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>>>>
>>>>> 2016-07-08 18:06:45 UTC [30382] ERROR  remoteListenThread_999999:
>>>>> "select ev_origin, ev_seqno, ev_timestamp,        ev_snapshot,
>>>>> "pg_catalog".txid_sna\
>>>>>
>>>>> pshot_xmin(ev_snapshot),
>>>>> "pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,
>>>>>    ev_data1, ev_data2,        ev_data3, ev_data4,        ev\
>>>>>
>>>>> _data5, ev_data6,        ev_data7, ev_data8 from "_ams_cluster".sl_event
>>>>> e where (e.ev_origin = '999999' and e.ev_seqno > '5000000009') or
>>>>> (e.ev_origin = '2'\
>>>>>
>>>>> and e.ev_seqno > '5000017870') or (e.ev_origin = '3' and e.ev_seqno >
>>>>> '5000018151') order by e.ev_origin, e.ev_seqno limit 40" - ERROR:
>>>>> schema "_ams_cluste\
>>>>>
>>>>> r" does not exist
>>>>>
>>>>> LINE 1: ...v_data5, ev_data6,        ev_data7, ev_data8 from "_ams_clus...
>>>>>
>>>>>                                                                ^
>>>>>
>>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: "start
>>>>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>>>>> PGRES_FATAL_ERROR ERR\
>>>>>
>>>>> OR:  current transaction is aborted, commands ignored until end of
>>>>> transaction block
>>>>>
>>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_3: SYNC aborted
>>>>>
>>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: "start
>>>>> transaction; set enable_seqscan = off; set enable_indexscan = on; "
>>>>> PGRES_FATAL_ERROR ERR\
>>>>>
>>>>> OR:  current transaction is aborted, commands ignored until end of
>>>>> transaction block
>>>>>
>>>>> 2016-07-08 18:06:55 UTC [30382] ERROR  remoteWorkerThread_2: SYNC aborted
>>>>>
>>>>> ----
>>>>>
>>>>> ---- node 999999 log ----
>>>>>
>>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>>>>> 5000081216 done in 0.004 seconds
>>>>>
>>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_2: SYNC
>>>>> 5000017870 done in 0.004 seconds
>>>>>
>>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_3: SYNC
>>>>> 5000018150 done in 0.004 seconds
>>>>>
>>>>> 2016-07-08 18:06:44 UTC [558] INFO   remoteWorkerThread_1: SYNC
>>>>> 5000081217 done in 0.003 seconds
>>>>>
>>>>> 2016-07-08 18:06:44 UTC [558] WARN   remoteWorkerThread_3: got DROP NODE
>>>>> for local node ID
>>>>>
>>>>> NOTICE:  Slony-I: Please drop schema "_ams_cluster"
>>>>>
>>>>> NOTICE:  drop cascades to 171 other objects
>>>>>
>>>>> DETAIL:  drop cascades to table _ams_cluster.sl_node
>>>>>
>>>>> drop cascades to table _ams_cluster.sl_nodelock
>>>>>
>>>>> drop cascades to table _ams_cluster.sl_set
>>>>>
>>>>> drop cascades to table _ams_cluster.sl_setsync
>>>>>
>>>>> drop cascades to table _ams_cluster.sl_table
>>>>>
>>>>> ----
>>>>>
>>>>>               Tom J
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Slony1-general mailing list
>>>>> Slony1-general at lists.slony.info
>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>
>>>>
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>
>>>
>>
>>


From mark.steben at drivedominion.com  Tue Jul 26 14:01:30 2016
From: mark.steben at drivedominion.com (Mark Steben)
Date: Tue, 26 Jul 2016 17:01:30 -0400
Subject: [Slony1-general] upgrading with pg_upgrade
Message-ID: <CADyzmyzh78M9QMoUUW2_OOWWAU2HBGQCNXgg-pQEgC2sOY9ErA@mail.gmail.com>

Good afternoon,

I am attempting to run pg_upgrade with the link option to upgrade from
postgres 9.2.12 to 9.4.8. Within the current 9.2.12 setup I have
slony1-2.2.3 running.

This is what I have attempted so far:
 1. installed postgres 9.4.8 on the same file system as 9.2 from tarball
source
 1. killed slony (on 9.2)
 2. stopped postgres
 3. ran 9.4.8 initdb
 4. installed slony1-2.2.3 on the new 9.4.8 setup from tarball source (here
is the command)
    ./configure --prefix=/usr/local/postgresql-9.4.8/bin
--with-pgconfigdir=/usr/local/postgresql-9.4.8/bin
--with-perltools=/usr/local/postgresql-9.4.8/bin
--with-pglibdir=/usr/local/postgresql-9.4.8/lib
--with-pgpkglibdir=/usr/local/postgresql-9.4.8/lib
 make
 make install

 5. attempted pg_upgrade (here is the command)
   /usr/local/postgresql-9.4.8/bin/pg_upgrade  -b
/usr/local/postgresql-9.2.11/bin -B /usr/local/postgresql-9.4.8/bin -d
/data/PSQL_9.2 -D /data/PSQL_9.4 -k -v &

I am now getting an error from this indicating a version mismatch:
   Could not load library "$libdir/slony1_funcs.2.2.3"
ERROR:  incompatible library "/usr/local/postgresql-9.4.8/lib/
slony1_funcs.2.2.3.so": version mismatch
DETAIL:  Server is version 9.4, library is version 9.2.

I then installed slony1-2.2.5 with the above command, ran the same
pg_upgrade command and got the exact same error. Then I deleted the
slony1_funcs.2.2.3.so file from my library directory leaving the
slony1_funcs.2.2.5.so and attempted the upgrade.  This time it errored out
with a NOT FOUND condition on the slony1_funcs.2.2.3.so.  So I'm not sure
what to do next.  I looked at slony.info to see if it shed any light on
compatibilities between slony versions and postgres versions and didn't
find anything.

A note: without slony installed in either instance the upgrade is
successful.

Any help / insight appreciated.  Thank you



-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase
<http://www.autobase.net/>
  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160726/4623cb29/attachment.htm 

From cbbrowne at afilias.info  Wed Jul 27 13:34:07 2016
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 27 Jul 2016 16:34:07 -0400
Subject: [Slony1-general] upgrading with pg_upgrade
In-Reply-To: <CADyzmyzh78M9QMoUUW2_OOWWAU2HBGQCNXgg-pQEgC2sOY9ErA@mail.gmail.com>
References: <CADyzmyzh78M9QMoUUW2_OOWWAU2HBGQCNXgg-pQEgC2sOY9ErA@mail.gmail.com>
Message-ID: <CANfbgbYDZA12rKiTOto77-Q-XUW_MJ_g31t9168MyO-Ka1A7ww@mail.gmail.com>

What you'll need to have is Slony {of whatever version you want, 2.2.3, or
2.2.5}, compiled against the *new* version of PostgreSQL, which is
presumably 9.4.8.

It seems that you're using Slony compiled against the old version
(9.2.whatever), and it shouldn't be *too* surprising that there's a problem
running the resultant .so binaries against a 9.4.8 server.

You should pick a Slony version; for minimal initial difference, I'd go
with 2.2.3, at migration time, as that's the version you were running
against Postgres 9.2.

Compile 2.2.3 against 9.4.8; that'll put slony1_funcs.2.2.3.so in the lib
directory for PG 9.4.8.  That should resolve both issues ("NOT FOUND" and
"incompatible version").

There is a caveat to that "should"...  I am not sure that anyone has
actually run such a case.  I expect it "ought to work," but that's not
certainty.  (I had a chat with Steve Singer, and he had similar thoughts.)

Hopefully you're testing that against a "test" database, initially, and not
production :-)

In principle, you could compile Slony 2.2.5 against 9.4.8, but that's going
to require more complexity, and that you replace more things by hand all at
once.

I'd rather upgrade and keep 2.2.3 in place during the Postgres upgrade.
And then, after the database upgrade, install slony 2.2.5 and use UPDATE
FUNCTIONS to perform the upgrade of Slony.
There's no facilities in Slony to help do both upgrades (DB and of Slony)
simultaneously.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160727/98cd3b70/attachment.htm 

