From tmblue at gmail.com  Sat Sep 10 17:44:50 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sat, 10 Sep 2016 17:44:50 -0700
Subject: [Slony1-general] value violates unique constraint "sl_event-pkey"
Message-ID: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>

Running into an issue, working with a cluster on another network, so I
added node 11 (insert secondary in site2) from node 2 (which is the insert
secondary of site 1). Set 2 kept failing, so I went in and dropped node 11
from the cluster config, but node 2, which is the seecondary insert server
continues with this error.. I've not inserted anything into node 2, nor can
you, I'm really confused how my primary cluster got in this state..


Any assistance to help clean it up? Dropping node 2 and re-adding is a 24
hour ordeal.

2016-09-10 17:41:50 PDT ERROR  remoteWorkerThread_11: "insert into
"_cls".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot,
ev_type     ) values ('11', '5000005698', '2016-09-10 08:34:48.691123-07',
'6422065:6530984:6422065', 'SYNC'); insert into "_cls".sl_confirm (con_origin,
con_received, con_seqno, con_timestamp)    values (11, 2, '5000005698',
now()); select "_cls".logApplySaveStats('_cls', 11, '0.047 s'::interval);
commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates
unique constraint "sl_event-pkey"

DETAIL:  Key (ev_origin, ev_seqno)=(11, 5000005698) already exists.


thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160910/fd433343/attachment.htm 

From tmblue at gmail.com  Sat Sep 10 17:50:12 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sat, 10 Sep 2016 17:50:12 -0700
Subject: [Slony1-general] value violates unique constraint
	"sl_event-pkey"
In-Reply-To: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>
References: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>
Message-ID: <CAEaSS0Yq0Txr9Gy1LzeojpEJdpoTtRHDsUqCP0RW_3wg0vK4Rg@mail.gmail.com>

On Sat, Sep 10, 2016 at 5:44 PM, Tory M Blue <tmblue at gmail.com> wrote:

> Running into an issue, working with a cluster on another network, so I
> added node 11 (insert secondary in site2) from node 2 (which is the insert
> secondary of site 1). Set 2 kept failing, so I went in and dropped node 11
> from the cluster config, but node 2, which is the seecondary insert server
> continues with this error.. I've not inserted anything into node 2, nor can
> you, I'm really confused how my primary cluster got in this state..
>
>
> Any assistance to help clean it up? Dropping node 2 and re-adding is a 24
> hour ordeal.
>
> 2016-09-10 17:41:50 PDT ERROR  remoteWorkerThread_11: "insert into
> "_cls".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot,
> ev_type     ) values ('11', '5000005698', '2016-09-10 08:34:48.691123-07',
> '6422065:6530984:6422065', 'SYNC'); insert into "_cls".sl_confirm (con_origin,
> con_received, con_seqno, con_timestamp)    values (11, 2, '5000005698',
> now()); select "_cls".logApplySaveStats('_cls', 11, '0.047 s'::interval);
> commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key value violates
> unique constraint "sl_event-pkey"
>
> DETAIL:  Key (ev_origin, ev_seqno)=(11, 5000005698) already exists.
>
>
Also interesting node 2 seems to think node 11 is still legit

idb01.prod.ca is node 11:

idb01.prod.ca.domain.net is not active. Make sure that it is already part
of the slon configuration currently running

select * from _cls.sl_node;

 no_id | no_active | no_comment | no_failed

-------+-----------+------------+-----------

     3 | t         | Node  3    | f

     4 | t         | Node  4    | f

     5 | t         | Node  5    | f

     1 | t         | Node  1    | f

     2 | t         | Node  2    | f

 *   11 | t         | Node  11   | f*


Paths are there as well..


Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160910/b3412e4d/attachment.htm 

From tmblue at gmail.com  Sat Sep 10 18:26:21 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sat, 10 Sep 2016 18:26:21 -0700
Subject: [Slony1-general] value violates unique constraint
	"sl_event-pkey"
In-Reply-To: <CAEaSS0Yq0Txr9Gy1LzeojpEJdpoTtRHDsUqCP0RW_3wg0vK4Rg@mail.gmail.com>
References: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>
	<CAEaSS0Yq0Txr9Gy1LzeojpEJdpoTtRHDsUqCP0RW_3wg0vK4Rg@mail.gmail.com>
Message-ID: <CAEaSS0ae8cmMeiNd=trq4V2ZgXUXZaezs4nNTms5F9A7JqdDvQ@mail.gmail.com>

On Sat, Sep 10, 2016 at 5:50 PM, Tory M Blue <tmblue at gmail.com> wrote:

>
>
> On Sat, Sep 10, 2016 at 5:44 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
>> Running into an issue, working with a cluster on another network, so I
>> added node 11 (insert secondary in site2) from node 2 (which is the insert
>> secondary of site 1). Set 2 kept failing, so I went in and dropped node 11
>> from the cluster config, but node 2, which is the seecondary insert server
>> continues with this error.. I've not inserted anything into node 2, nor can
>> you, I'm really confused how my primary cluster got in this state..
>>
>>
>> Any assistance to help clean it up? Dropping node 2 and re-adding is a 24
>> hour ordeal.
>>
>> 2016-09-10 17:41:50 PDT ERROR  remoteWorkerThread_11: "insert into
>> "_cls".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_snapshot,
>> ev_type     ) values ('11', '5000005698', '2016-09-10 08:34:48.691123-07',
>> '6422065:6530984:6422065', 'SYNC'); insert into "_cls".sl_confirm (con_origin,
>> con_received, con_seqno, con_timestamp)    values (11, 2, '5000005698',
>> now()); select "_cls".logApplySaveStats('_cls', 11, '0.047
>> s'::interval); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate key
>> value violates unique constraint "sl_event-pkey"
>>
>> DETAIL:  Key (ev_origin, ev_seqno)=(11, 5000005698) already exists.
>>
>>
> Also interesting node 2 seems to think node 11 is still legit
>
> idb01.prod.ca is node 11:
>
> idb01.prod.ca.domain.net is not active. Make sure that it is already part
> of the slon configuration currently running
>
> select * from _cls.sl_node;
>
>  no_id | no_active | no_comment | no_failed
>
> -------+-----------+------------+-----------
>
>      3 | t         | Node  3    | f
>
>      4 | t         | Node  4    | f
>
>      5 | t         | Node  5    | f
>
>      1 | t         | Node  1    | f
>
>      2 | t         | Node  2    | f
>
>  *   11 | t         | Node  11   | f*
>
>
> Paths are there as well..
>
>
> After manually removing node 11 from all the servers in site 1, node 2
slon started up on it's own and caught up. So not sure what is happening
and why node 11 got into that state

Will look around and make sure that all hosts, know about all hosts

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160910/26ca5c46/attachment.htm 

From steve at ssinger.info  Sat Sep 10 19:57:07 2016
From: steve at ssinger.info (Steve Singer)
Date: Sat, 10 Sep 2016 22:57:07 -0400 (EDT)
Subject: [Slony1-general] value violates unique constraint
 "sl_event-pkey"
In-Reply-To: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>
References: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>
Message-ID: <alpine.DEB.2.11.1609102252570.4243@opti.atlantida>

On Sat, 10 Sep 2016, Tory M Blue wrote:

> 
> Running into an issue, working with a cluster on another network, so I added node 11 (insert secondary in site2) from
> node 2 (which is the insert secondary of site 1). Set 2 kept failing, so I went in and dropped node 11 from the
> cluster config, but node 2, which is the seecondary insert server continues with this error.. I've not inserted
> anything into node 2, nor can you, I'm really confused how my primary cluster got in this state..
> 
> 
> Any assistance to help clean it up? Dropping node 2 and re-adding is a 24 hour ordeal.
> 
> 2016-09-10 17:41:50 PDT ERROR? remoteWorkerThread_11: "insert into "_cls".sl_event ? ? (ev_origin, ev_seqno,
> ev_timestamp,? ? ? ev_snapshot, ev_type ? ? ) values ('11', '5000005698', '2016-09-10 08:34:48.691123-07',
> '6422065:6530984:6422065', 'SYNC'); insert into "_cls".sl_confirm (con_origin, con_received, con_seqno,
> con_timestamp)? ? values (11, 2, '5000005698', now()); select "_cls".logApplySaveStats('_cls', 11, '0.047
> s'::interval); commit transaction;" PGRES_FATAL_ERROR ERROR:? duplicate key value violates unique constraint
> "sl_event-pkey"
> 
> DETAIL:? Key (ev_origin, ev_seqno)=(11, 5000005698) already exists.
>

So when slon 2 (I assume this is slon 2) is stopped, is/was there a row in 
sl_event with ev_origin=11 and ev_seqno=5000005698

?

You also didn't you say when things were failing before dropping the node.


> 
> thanks
> 
> Tory
> 
> 
>

From tmblue at gmail.com  Sat Sep 10 20:19:07 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sat, 10 Sep 2016 20:19:07 -0700
Subject: [Slony1-general] value violates unique constraint
	"sl_event-pkey"
In-Reply-To: <alpine.DEB.2.11.1609102252570.4243@opti.atlantida>
References: <CAEaSS0ZopkHupKEWuO6Xa5w=efy8mOCe+TSjuO7wrWho8+B8Vw@mail.gmail.com>
	<alpine.DEB.2.11.1609102252570.4243@opti.atlantida>
Message-ID: <CAEaSS0b2LydwTwbLv1-6iZppz+1yJiNqWsAfFV4Pb758ykvtng@mail.gmail.com>

On Sat, Sep 10, 2016 at 7:57 PM, Steve Singer <steve at ssinger.info> wrote:

> On Sat, 10 Sep 2016, Tory M Blue wrote:
>
>
>> Running into an issue, working with a cluster on another network, so I
>> added node 11 (insert secondary in site2) from
>> node 2 (which is the insert secondary of site 1). Set 2 kept failing, so
>> I went in and dropped node 11 from the
>> cluster config, but node 2, which is the seecondary insert server
>> continues with this error.. I've not inserted
>> anything into node 2, nor can you, I'm really confused how my primary
>> cluster got in this state..
>>
>>
>> Any assistance to help clean it up? Dropping node 2 and re-adding is a 24
>> hour ordeal.
>>
>> 2016-09-10 17:41:50 PDT ERROR  remoteWorkerThread_11: "insert into
>> "_cls".sl_event     (ev_origin, ev_seqno,
>> ev_timestamp,      ev_snapshot, ev_type     ) values ('11', '5000005698',
>> '2016-09-10 08:34:48.691123-07',
>> '6422065:6530984:6422065', 'SYNC'); insert into "_cls".sl_confirm
>> (con_origin, con_received, con_seqno,
>> con_timestamp)    values (11, 2, '5000005698', now()); select
>> "_cls".logApplySaveStats('_cls', 11, '0.047
>> s'::interval); commit transaction;" PGRES_FATAL_ERROR ERROR:  duplicate
>> key value violates unique constraint
>> "sl_event-pkey"
>>
>> DETAIL:  Key (ev_origin, ev_seqno)=(11, 5000005698) already exists.
>>
>>
> So when slon 2 (I assume this is slon 2) is stopped, is/was there a row in
> sl_event with ev_origin=11 and ev_seqno=5000005698
>
> ?
>
> You also didn't you say when things were failing before dropping the node.
>
>
>
>> thanks
>>
>> Tory
>>
>>
>>
>> Sorry Steve

Slon stopped functioning on node 2, and thus that error. Got the normal,
will retry in 10 seconds. Meaning slon was in a bad spot, and typically I
see that when it's not initialized, however that wasn't the case and I got
the error I included 100s of times with the following, will retry in 10
seconds.

What I did was manually removed all node 11 information , since the
dropnode that I ran earlier seemed to make slon think that node 11 was not
configured, yet all the tables , sl_listen:subscribe, path, node etc was
still there on all 5 nodes in the primary cluster.

I'm fully replicated again, and did not have to drop/add node 2 (whew).
Just took some manual tweaking inside the slony schema.

will try to add the node again later to see what happens...

What I'm attempting is

Site 1:
Nodes 1-5
Nodes 1-2 are insert  (obviously primary/secondary)
Nodesl 3-5 are query only

Site 2
Nodes 11-15
Nodes 11-12 are insert (initially will be secondary/secondary, until we
switch from site 1 to site 2)
Nodes 13-15 are query only.

I'm attempting to (which we have done in the past), is keep the load off
node 1 (primary insert DB) and have node 2 handle replication (provide) to
Node 11 (only this step at first).

I will then add Node 12 from node 11 and nodes 13-15 to grab their data
from node 12.

I somehow hacked my way out of the issue :) YAY! But am worried that I
can't replicate to node 11 yet, and while it's wide area network, the delay
is a whopping 2ms and I've dropped all the big indexes so that slon
/network is not waiting for those to complete..

Will update later, but it was odd this host got in this state, weird that
the dropnode failed (it's possible because it was still trying to complete
the initial addnode and thus was busy with loading all the tables, but I
figured that a drop notice would stop all types of replication events, but
it does not. Stopping slon even with the drop waiting on node 11, node 11
came back and instantly started truncating tables again and attempting to
replicate, ignoring the recent Drop instructions.

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160910/c5a58a76/attachment-0001.htm 

From tmblue at gmail.com  Sun Sep 11 17:56:27 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sun, 11 Sep 2016 17:56:27 -0700
Subject: [Slony1-general] Why won't slon respond to a drop command when it's
	failing on add command?
Message-ID: <CAEaSS0aiuv2VD36wCrAAVN=kodNR9pyKEtjT7p5J7ubLd6DrdQ@mail.gmail.com>

707680-2016-09-11 16:37:50 PDT INFO   remoteWorkerThread_2: SYNC 5002145332
done in 0.018 seconds

*707771:2016-09-11 16:37:50 PDT WARN   remoteWorkerThread_2: got DROP NODE
for local node ID*

707856-2016-09-11 16:38:21 PDT INFO   remoteWorkerThread_5: SYNC 5002157926
done in 0.021 seconds

At this point it continues on it's voyage of trying to replicate (from an
add node), but I'm getting a failure so I'm trying to drop so that my
origin can truncate it's logs.

Due to the failure listed below:

NOTICE:  truncate of "torque"."impressions" succeeded

  2016-09-11 16:35:22 PDT CONFIG remoteWorkerThread_1: 5500.917 seconds to
copy table "torque"."impressions"

2016-09-11 16:35:22 PDT CONFIG remoteWorkerThread_1: copy table
"torque"."impressions_archive"

2016-09-11 16:35:23 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"torque"."impressions_archive"

*2016-09-11 16:35:23 PDT ERROR  remoteWorkerThread_1: "select
"_cls".copyFields(237);"*

2016-09-11 16:35:23 PDT WARN   remoteWorkerThread_1: data copy for set 2
failed 2 times - sleep 30 seconds


I've seen the behavior with the failing set and I think we determined that
the network between the origin and the new node, was somehow killing the
connection, so we decided to remove the index creation step on the new
node, so that it was just doing a postgres copy and would move on to the
next, not leaving enough idle time for any idle timeouts or other to kick
in and kill the connection.. But I am not seeing that happen in this
situation, but the set 2 continues to fail with the erro*r 237*

The commands continue after seeing the DROP command indicated earlier

2016-09-11 16:56:48 PDT CONFIG remoteWorkerThread_1: 25955276171 bytes
copied for table "torque"."impressions_daily"

2016-09-11 17:03:52 PDT CONFIG remoteWorkerThread_1: 1620.812 seconds to
copy table "torque"."impressions_daily"

2016-09-11 17:03:52 PDT CONFIG remoteWorkerThread_1: copy table
"torque"."impressions"

2016-09-11 17:03:52 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"torque"."impressions"

At this point it has ignored the DROP :) and is attempting to copy the
torque.impressions table , which it completed the _daily table previously,
but started , or kept on with the data copy's even after the DROP was
signaled I expect a Set 2 fail with the same error in about an hour.. But I
need it to stop and not ignore the DROP command.

*Thanks*

*Tory*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160911/ec7b5987/attachment.htm 

From tmblue at gmail.com  Sun Sep 11 23:41:22 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sun, 11 Sep 2016 23:41:22 -0700
Subject: [Slony1-general] Need some help again, keep alives,
	wide area replication , set failure
Message-ID: <CAEaSS0aGvURPzFucvwfJrzWhFwPgzPa=qVWa2uMrAR23MctQkQ@mail.gmail.com>

Jan has helped me before, giving me ideas to help with wide area
replication where it seems that the connection drops between a large copy
set and/or an index creation,  when there is no bits crossing the wire and
the connections are dropped by the FW or other so Slony finishes up a
table, index creation and attempts to grab the next table, but the
connection is no longer there, so Slony says failed and attempts again.

I think I'm running into this between my Colo and Amazon, using their VPN
gateway.

Here is the snippet of logs, there is no index here, we dropped it on the
new node, so that it would not fail, but what's odd here is that it copies
all the data and 35 minutes later it reports the time, which tells me it's
doing something, but I'm not sure what, if there is no index on that table.
(there is a primary key with maintains integrity, and we didn't think we
should drop that). but there are no other indexes, so the 35 minutes or
whatever is a mystery..


2016-09-11 21:32:24 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"torque"."adimpressions"
2016-09-11 *22:39:39 *PDT CONFIG remoteWorkerThread_1: 76955497834 bytes
copied for table "torque"."adimpressions"
916499:2016-09-11 *23:14:25 *PDT CONFIG remoteWorkerThread_1: 6121.393
seconds to copy table "torque"."impressions"
916608:2016-09-11 23:14:25 PDT CONFIG remoteWorkerThread_1: copy table
"torque".impressions_archive"
916705:2016-09-11 23:14:25 PDT CONFIG remoteWorkerThread_1: Begin COPY of
table "torque"."impressions_archive"
916811:2016-09-11 23:14:25 PDT ERROR  remoteWorkerThread_1: "select
"_cls".copyFields(237);"
916907:2016-09-11 23:14:25 PDT WARN   remoteWorkerThread_1: data copy for
set 2 failed 1 times - sleep 15 seconds
917014:2016-09-11 23:14:25 PDT INFO   cleanupThread: 7606.655 seconds for
cleanupEvent()

This run,  I added keep-alives by the following method. (and the timing and
results are the same without them, set 2 fails with error 237).

Adding the following to both slon commands on the origin and the new node

tcp_keepalive_idle 300 tcp_keepalive_count 5 tcp_keepalive_interval 300

Now not entirely sure how this is suppose to work and did I not tune this
right. It obviously fails at the 30 minute mark, this is 25 minutes,
however the servers never loses connection (I have a ping (not quite the
same), but it has zero packet loss over the 2+ hours that these attempts to
get things replicated take)). So maybe someone smarter then me can advice
how I should tune the keep alives if that's what is happening.

I thought it would only use the keep-alives if it felt the partner was no
longer there, but since i know pings show there is no connectivity issues,
I'm at a loss. AGAIN :)

Thanks for the assist

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160911/5b0b9805/attachment.htm 

From tmblue at gmail.com  Sun Sep 11 23:57:48 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Sun, 11 Sep 2016 23:57:48 -0700
Subject: [Slony1-general] Need some help again, keep alives,
	wide area replication , set failure
In-Reply-To: <CAEaSS0aGvURPzFucvwfJrzWhFwPgzPa=qVWa2uMrAR23MctQkQ@mail.gmail.com>
References: <CAEaSS0aGvURPzFucvwfJrzWhFwPgzPa=qVWa2uMrAR23MctQkQ@mail.gmail.com>
Message-ID: <CAEaSS0Y56AJV_x=3aHCVL8CPdf0y37N2CAqy9o4kMzqte5wBSg@mail.gmail.com>

Sorry I'm a bit sleep deprived, this is almost the exact thing I asked for
help on in 2014.. Jan and Jeff both came in and gave me suggestions for
keep alives which are much more aggressive than I have it set to.

So I'm going to test with the more aggressive settings from this thread in
2014 "
https://www.mail-archive.com/slony1-general at lists.slony.info/msg06967.html"

How lame I spaced, I knew Jan had been helpful, but totally spaced this
thread.. UUGH! sorry

And yes double bad, top posting!!

Tory

On Sun, Sep 11, 2016 at 11:41 PM, Tory M Blue <tmblue at gmail.com> wrote:

> Jan has helped me before, giving me ideas to help with wide area
> replication where it seems that the connection drops between a large copy
> set and/or an index creation,  when there is no bits crossing the wire and
> the connections are dropped by the FW or other so Slony finishes up a
> table, index creation and attempts to grab the next table, but the
> connection is no longer there, so Slony says failed and attempts again.
>
> I think I'm running into this between my Colo and Amazon, using their VPN
> gateway.
>
> Here is the snippet of logs, there is no index here, we dropped it on the
> new node, so that it would not fail, but what's odd here is that it copies
> all the data and 35 minutes later it reports the time, which tells me it's
> doing something, but I'm not sure what, if there is no index on that table.
> (there is a primary key with maintains integrity, and we didn't think we
> should drop that). but there are no other indexes, so the 35 minutes or
> whatever is a mystery..
>
>
> 2016-09-11 21:32:24 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
> "torque"."adimpressions"
> 2016-09-11 *22:39:39 *PDT CONFIG remoteWorkerThread_1: 76955497834 bytes
> copied for table "torque"."adimpressions"
> 916499:2016-09-11 *23:14:25 *PDT CONFIG remoteWorkerThread_1: 6121.393
> seconds to copy table "torque"."impressions"
> 916608:2016-09-11 23:14:25 PDT CONFIG remoteWorkerThread_1: copy table
> "torque".impressions_archive"
> 916705:2016-09-11 23:14:25 PDT CONFIG remoteWorkerThread_1: Begin COPY of
> table "torque"."impressions_archive"
> 916811:2016-09-11 23:14:25 PDT ERROR  remoteWorkerThread_1: "select
> "_cls".copyFields(237);"
> 916907:2016-09-11 23:14:25 PDT WARN   remoteWorkerThread_1: data copy for
> set 2 failed 1 times - sleep 15 seconds
> 917014:2016-09-11 23:14:25 PDT INFO   cleanupThread: 7606.655 seconds for
> cleanupEvent()
>
> This run,  I added keep-alives by the following method. (and the timing
> and results are the same without them, set 2 fails with error 237).
>
> Adding the following to both slon commands on the origin and the new node
>
> tcp_keepalive_idle 300 tcp_keepalive_count 5 tcp_keepalive_interval 300
>
> Now not entirely sure how this is suppose to work and did I not tune this
> right. It obviously fails at the 30 minute mark, this is 25 minutes,
> however the servers never loses connection (I have a ping (not quite the
> same), but it has zero packet loss over the 2+ hours that these attempts to
> get things replicated take)). So maybe someone smarter then me can advice
> how I should tune the keep alives if that's what is happening.
>
> I thought it would only use the keep-alives if it felt the partner was no
> longer there, but since i know pings show there is no connectivity issues,
> I'm at a loss. AGAIN :)
>
> Thanks for the assist
>
> Tory
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160911/3498240d/attachment.htm 

From ttignor at akamai.com  Mon Sep 12 07:52:16 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 12 Sep 2016 14:52:16 +0000
Subject: [Slony1-general] sync performance
Message-ID: <6308B019-C209-4F56-9D1C-D46B51ECA708@akamai.com>


                Hello slony1 community,
                We?ve recently been testing communication reliability between our cluster nodes. Our config is a simple setup with one provider producing a modest volume of changes (measured in KB/s) consumed by 5 direct subscribers, though these are geographically distributed. The test is just a sync event followed by a wait on the sync originator. Example:

cluster name = ams_cluster;
node 5 admin
      conninfo='dbname=ams
      host=23.79.242.182
      user=ams_slony
      sslmode=verify-ca
      sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
      sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
      sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem';
node 2 admin conninfo = 'dbname=ams user=ams_slony';

sync(id=2);
wait for event (origin=2, confirmed=5, wait on=2, timeout=30);

                Tests show the script takes 10-20 secs to run on different nodes.
                Can anyone explain what?s happening internally during this time, and why it takes so long? On a healthy, lightly loaded system, we might have hoped for a sync response in just a couple seconds. Our slon daemons are running with mostly default startup options.
                Thanks in advance,

                Tom    ?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160912/f5b26e8e/attachment-0001.htm 

From ttignor at akamai.com  Mon Sep 12 08:39:47 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 12 Sep 2016 15:39:47 +0000
Subject: [Slony1-general] sync performance
In-Reply-To: <6308B019-C209-4F56-9D1C-D46B51ECA708@akamai.com>
References: <6308B019-C209-4F56-9D1C-D46B51ECA708@akamai.com>
Message-ID: <B79399BA-3321-4DB2-897C-E8B9482FBACF@akamai.com>


                Seems I have an additional data point: the sync test always takes longer (> 20 secs) if I include conninfo for all cluster nodes instead of just the local node. I had previously thought conninfo data was only used when needed. Is this not the case?

                Tom    ?


From: Tom Tignor <ttignor at akamai.com>
Date: Monday, September 12, 2016 at 10:52 AM
To: "slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
Subject: sync performance


                Hello slony1 community,
                We?ve recently been testing communication reliability between our cluster nodes. Our config is a simple setup with one provider producing a modest volume of changes (measured in KB/s) consumed by 5 direct subscribers, though these are geographically distributed. The test is just a sync event followed by a wait on the sync originator. Example:

cluster name = ams_cluster;
node 5 admin
      conninfo='dbname=ams
      host=23.79.242.182
      user=ams_slony
      sslmode=verify-ca
      sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
      sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
      sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem';
node 2 admin conninfo = 'dbname=ams user=ams_slony';

sync(id=2);
wait for event (origin=2, confirmed=5, wait on=2, timeout=30);

                Tests show the script takes 10-20 secs to run on different nodes.
                Can anyone explain what?s happening internally during this time, and why it takes so long? On a healthy, lightly loaded system, we might have hoped for a sync response in just a couple seconds. Our slon daemons are running with mostly default startup options.
                Thanks in advance,

                Tom    ?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160912/07687b70/attachment.htm 

From ttignor at akamai.com  Mon Sep 12 11:29:27 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Mon, 12 Sep 2016 18:29:27 +0000
Subject: [Slony1-general] Controlled Switchover
Message-ID: <FB5E7807-C778-46F5-A681-C6E244EAE6B5@akamai.com>


                Hello slony1 users,
                I?m looking at ?controlled switchover.? While there are some varying accounts online, I see the 2.2.4 and 2.2.5 doc both describe these ops as the method to reliably pass the origin role from one node to another. We have recently seen some deadlock errors doing ops slightly different than those here. Can someone confirm this is the expected method? Does it matter where the slonik script is run?

---- doc ----
A small slonik script executes the following commands:

lock set (id = 1, origin = 1);
wait for event (origin = 1, confirmed = 2, wait on=1);
move set (id = 1, old origin = 1, new origin = 2);
wait for event (origin = 1, confirmed = 2, wait on=1);
After these commands, the origin (master role) of data set 1 has been transferred to node2. And it is not simply transferred; it is done in a fashion such that node1 becomes a fully synchronized subscriber, actively replicating the set. So the two nodes have switched roles completely.
---- doc ----

                Thanks,

                Tom    ?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160912/15af2e53/attachment.htm 

From tmblue at gmail.com  Mon Sep 12 11:33:38 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 12 Sep 2016 11:33:38 -0700
Subject: [Slony1-general] Need some help again, keep alives,
 wide area replication , set failure STILL
Message-ID: <CAEaSS0bCtjPdNA6Yy=XEBmUtgQETqagG4iNc2ayA7jjEvBkFCA@mail.gmail.com>

> On Sun, Sep 11, 2016 at 11:41 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
>> Jan has helped me before, giving me ideas to help with wide area
>> replication where it seems that the connection drops between a large copy
>> set and/or an index creation,  when there is no bits crossing the wire and
>> the connections are dropped by the FW or other so Slony finishes up a
>> table, index creation and attempts to grab the next table, but the
>> connection is no longer there, so Slony says failed and attempts again.
>>
>> I think I'm running into this between my Colo and Amazon, using their VPN
>> gateway.
>>
>> Here is the snippet of logs, there is no index here, we dropped it on the
>> new node, so that it would not fail, but what's odd here is that it copies
>> all the data and 35 minutes later it reports the time, which tells me it's
>> doing something, but I'm not sure what, if there is no index on that table.
>> (there is a primary key with maintains integrity, and we didn't think we
>> should drop that). but there are no other indexes, so the 35 minutes or
>> whatever is a mystery..
>>
>>
>> 2016-09-11 21:32:24 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
>> "torque"."adimpressions"
>> 2016-09-11 *22:39:39 *PDT CONFIG remoteWorkerThread_1: 76955497834 bytes
>> copied for table "torque"."adimpressions"
>> 916499:2016-09-11 *23:14:25 *PDT CONFIG remoteWorkerThread_1: 6121.393
>> seconds to copy table "torque"."impressions"
>> 916608:2016-09-11 23:14:25 PDT CONFIG remoteWorkerThread_1: copy table
>> "torque".impressions_archive"
>> 916705:2016-09-11 23:14:25 PDT CONFIG remoteWorkerThread_1: Begin COPY of
>> table "torque"."impressions_archive"
>> 916811:2016-09-11 23:14:25 PDT ERROR  remoteWorkerThread_1: "select
>> "_cls".copyFields(237);"
>> 916907:2016-09-11 23:14:25 PDT WARN   remoteWorkerThread_1: data copy for
>> set 2 failed 1 times - sleep 15 seconds
>> 917014:2016-09-11 23:14:25 PDT INFO   cleanupThread: 7606.655 seconds for
>> cleanupEvent()
>>
>> This run,  I added keep-alives by the following method. (and the timing
>> and results are the same without them, set 2 fails with error 237).
>>
>> Adding the following to both slon commands on the origin and the new node
>>
>> tcp_keepalive_idle 300 tcp_keepalive_count 5 tcp_keepalive_interval 300
>>
>> Now not entirely sure how this is suppose to work and did I not tune this
>> right. It obviously fails at the 30 minute mark, this is 25 minutes,
>> however the servers never loses connection (I have a ping (not quite the
>> same), but it has zero packet loss over the 2+ hours that these attempts to
>> get things replicated take)). So maybe someone smarter then me can advice
>> how I should tune the keep alives if that's what is happening.
>>
>> I thought it would only use the keep-alives if it felt the partner was no
>> longer there, but since i know pings show there is no connectivity issues,
>> I'm at a loss. AGAIN :)
>>
>> Thanks for the assist
>>
>> Tory
>>
>
> Okay keepalives didn't work, but maybe I configured the slon.conf wrong,
there does not appear to be any real examples

I used:

tcp_keepalive_time = 5

tcp_keepalive_probes = 24

tcp_keepalive_intvl = 5

While my kernel is set at, maybe I need to adjust the kernel as well?

net.ipv4.tcp_keepalive_time = 7200

net.ipv4.tcp_keepalive_probes = 9

net.ipv4.tcp_keepalive_intvl = 75

2016-09-12 09:27:48 PDT CONFIG remoteWorkerThread_1: 1869.486 seconds to
copy table "torque"."impressions_daily"

2016-09-12 09:27:48 PDT CONFIG remoteWorkerThread_1: copy table
"torque"."impressions"

2016-09-12 09:27:48 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"torque"."impressions"

NOTICE:  truncate of "torque"."impressions" succeeded

2016-09-12 10:31:09 PDT CONFIG remoteWorkerThread_1: 77048102322 bytes
copied for table "torque"."adimpressions"

2016-09-12 11:02:56 PDT CONFIG remoteWorkerThread_1: 5708.515 seconds to
copy table "torque"."impressions"

2016-09-12 11:02:56 PDT CONFIG remoteWorkerThread_1: copy table
"torque"."impressions_archive"

2016-09-12 11:02:56 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
"torque"."impressions_archive"

2016-09-12 11:02:56 PDT ERROR  remoteWorkerThread_1: "select
"_cls".copyFields(237);"
2016-09-12 11:02:56 PDT WARN   remoteWorkerThread_1: data copy for set 2
failed 1 times - sleep 15 seconds

There are no indexes, so I don't know what Slon is doing for the 31 minutes
between when the data is finished copied and it attempts to start the next
table.

More suggestions? I know I'm being needy but I'm spinning my wheels it seems

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160912/d3d6a1b1/attachment-0001.htm 

From steve at ssinger.info  Mon Sep 12 13:36:31 2016
From: steve at ssinger.info (Steve Singer)
Date: Mon, 12 Sep 2016 16:36:31 -0400
Subject: [Slony1-general] Need some help again, keep alives,
 wide area replication , set failure STILL
In-Reply-To: <CAEaSS0bCtjPdNA6Yy=XEBmUtgQETqagG4iNc2ayA7jjEvBkFCA@mail.gmail.com>
References: <CAEaSS0bCtjPdNA6Yy=XEBmUtgQETqagG4iNc2ayA7jjEvBkFCA@mail.gmail.com>
Message-ID: <57D711CF.8070708@ssinger.info>

On 09/12/2016 02:33 PM, Tory M Blue wrote:
>

>
> Okay keepalives didn't work, but maybe I configured the slon.conf wrong,
> there does not appear to be any real examples
>
> I used:
>
> tcp_keepalive_time = 5
>
> tcp_keepalive_probes = 24
>
> tcp_keepalive_intvl = 5

tcp_keepalive_idle=5
tcp_keepalive_count=24
tcp_keepalive_interval=5


http://www.slony.info/documentation/2.2/slon-config-connection.html


>
> While my kernel is set at, maybe I need to adjust the kernel as well?
>
> net.ipv4.tcp_keepalive_time = 7200
>
> net.ipv4.tcp_keepalive_probes = 9
>
> net.ipv4.tcp_keepalive_intvl = 75
>
>
> 2016-09-12 09:27:48 PDT CONFIG remoteWorkerThread_1: 1869.486 seconds to
> copy table "torque"."impressions_daily"
>
> 2016-09-12 09:27:48 PDT CONFIG remoteWorkerThread_1: copy table
> "torque"."impressions"
>
> 2016-09-12 09:27:48 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
> "torque"."impressions"
>
> NOTICE:  truncate of "torque"."impressions" succeeded
>
> 2016-09-12 10:31:09 PDT CONFIG remoteWorkerThread_1: 77048102322 bytes
> copied for table "torque"."adimpressions"
>
> 2016-09-12 11:02:56 PDT CONFIG remoteWorkerThread_1: 5708.515 seconds to
> copy table "torque"."impressions"
>
> 2016-09-12 11:02:56 PDT CONFIG remoteWorkerThread_1: copy table
> "torque"."impressions_archive"
>
> 2016-09-12 11:02:56 PDT CONFIG remoteWorkerThread_1: Begin COPY of table
> "torque"."impressions_archive"
>
> 2016-09-12 11:02:56 PDT ERROR  remoteWorkerThread_1: "select
> "_cls".copyFields(237);"
>
> 2016-09-12 11:02:56 PDT WARN   remoteWorkerThread_1: data copy for set 2
> failed 1 times - sleep 15 seconds
>
> There are no indexes, so I don't know what Slon is doing for the 31
> minutes between when the data is finished copied and it attempts to
> start the next table.
>
> More suggestions? I know I'm being needy but I'm spinning my wheels it seems
>
> Thanks
> Tory
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From steve at ssinger.info  Mon Sep 12 13:38:31 2016
From: steve at ssinger.info (Steve Singer)
Date: Mon, 12 Sep 2016 16:38:31 -0400
Subject: [Slony1-general] sync performance
In-Reply-To: <B79399BA-3321-4DB2-897C-E8B9482FBACF@akamai.com>
References: <6308B019-C209-4F56-9D1C-D46B51ECA708@akamai.com>
	<B79399BA-3321-4DB2-897C-E8B9482FBACF@akamai.com>
Message-ID: <57D71247.2080107@ssinger.info>

On 09/12/2016 11:39 AM, Tignor, Tom wrote:
>                  Seems I have an additional data point: the sync test
> always takes longer (> 20 secs) if I include conninfo for all cluster
> nodes instead of just the local node. I had previously thought conninfo
> data was only used when needed. Is this not the case?

What if you do

  sync(id=2);

  wait for event (origin=2, confirmed=5, wait on=2, timeout=30);

  sync(id=2);

  wait for event (origin=2, confirmed=5, wait on=2, timeout=30);
  sync(id=2);

  wait for event (origin=2, confirmed=5, wait on=2, timeout=30);


3 times (or more) in a row, does it still take about the same amount of 
time as 1 sync ?

When slonik starts up it contacts all the nodes it has admin conninfo 
for to get the current state/last event from each node.  Maybe your time 
is spent establishing all those connections over SSL





>
>                  Tom J
>
> *From: *Tom Tignor <ttignor at akamai.com>
> *Date: *Monday, September 12, 2016 at 10:52 AM
> *To: *"slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
> *Subject: *sync performance
>
>                  Hello slony1 community,
>
>                  We?ve recently been testing communication reliability
> between our cluster nodes. Our config is a simple setup with one
> provider producing a modest volume of changes (measured in KB/s)
> consumed by 5 direct subscribers, though these are geographically
> distributed. The test is just a sync event followed by a wait on the
> sync originator. Example:
>
> cluster name = ams_cluster;
>
> node 5 admin
>
>        conninfo='dbname=ams
>
>        host=23.79.242.182
>
>        user=ams_slony
>
>        sslmode=verify-ca
>
>        sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
>
>        sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
>
>        sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem';
>
> node 2 admin conninfo = 'dbname=ams user=ams_slony';
>
> sync(id=2);
>
> wait for event (origin=2, confirmed=5, wait on=2, timeout=30);
>
>                  Tests show the script takes 10-20 secs to run on
> different nodes.
>
>                  Can anyone explain what?s happening internally during
> this time, and why it takes so long? On a healthy, lightly loaded
> system, we might have hoped for a sync response in just a couple
> seconds. Our slon daemons are running with mostly default startup options.
>
>                  Thanks in advance,
>
>                  Tom J
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From tmblue at gmail.com  Mon Sep 12 17:57:31 2016
From: tmblue at gmail.com (Tory M Blue)
Date: Mon, 12 Sep 2016 17:57:31 -0700
Subject: [Slony1-general] Need some help again, keep alives,
 wide area replication , set failure STILL
In-Reply-To: <57D711CF.8070708@ssinger.info>
References: <CAEaSS0bCtjPdNA6Yy=XEBmUtgQETqagG4iNc2ayA7jjEvBkFCA@mail.gmail.com>
	<57D711CF.8070708@ssinger.info>
Message-ID: <CAEaSS0aGxUO+OM_CZz8GTFJKHqdnpAjtG34mm8dNQB2YGwaS8Q@mail.gmail.com>

On Mon, Sep 12, 2016 at 1:36 PM, Steve Singer <steve at ssinger.info> wrote:

> On 09/12/2016 02:33 PM, Tory M Blue wrote:
>
>>
>>
>
>> Okay keepalives didn't work, but maybe I configured the slon.conf wrong,
>> there does not appear to be any real examples
>>
>> I used:
>>
>> tcp_keepalive_time = 5
>>
>> tcp_keepalive_probes = 24
>>
>> tcp_keepalive_intvl = 5
>>
>
> tcp_keepalive_idle=5
> tcp_keepalive_count=24
> tcp_keepalive_interval=5
>
>
> http://www.slony.info/documentation/2.2/slon-config-connection.html


Yes thanks Steve, I didn't reply back but i went looking to make sure I
didn't muck up the config (which I did, but slon was more than happy to run
that way (some checking would be nice :))

And that solved my issues, using Jans aggressive settings and I'm good to
go 5 hours of replication followed by what should be about 8 hours of index
building.!!

Thanks all, again you managed to hold my hand :)

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160912/12642700/attachment.htm 

From ttignor at akamai.com  Tue Sep 13 06:59:46 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Tue, 13 Sep 2016 13:59:46 +0000
Subject: [Slony1-general] sync performance
In-Reply-To: <57D71247.2080107@ssinger.info>
References: <6308B019-C209-4F56-9D1C-D46B51ECA708@akamai.com>
	<B79399BA-3321-4DB2-897C-E8B9482FBACF@akamai.com>
	<57D71247.2080107@ssinger.info>
Message-ID: <62980969-0FE1-4A21-B09B-2A203D48C0B4@akamai.com>


	Interesting test results. By adding date commands inside and outside the script, it?s clear there?s 11-12 secs of startup contact before any commands get going. After that, I see syncs can take anywhere from 6-15 secs to execute. Once in a while, I?l also get a postgres timeout error, and I know the DB hasn?t gone down.
	Early on I adopted a habit of providing all conninfo for every node at the start of each script. It seems now I should be aiming for either minimal conn info or fewer scripts, or both.

root at prodrpl-Amst:~# date && akaslonik /tmp/commcheck-2.slk 
Tue Sep 13 13:50:14 UTC 2016
/tmp/commcheck-2.slk:44: 2016-09-13 13:50:26
/tmp/commcheck-2.slk:47: 2016-09-13 13:50:34
/tmp/commcheck-2.slk:50: waiting for event (2,5001432258) to be confirmed on node 5
/tmp/commcheck-2.slk:51: 2016-09-13 13:50:47
/tmp/commcheck-2.slk:55: 2016-09-13 13:50:53
root at prodrpl-Amst:~# 
root at prodrpl-Amst:~# 
root at prodrpl-Amst:~# date && akaslonik /tmp/commcheck-2.slk 
Tue Sep 13 13:51:01 UTC 2016
/tmp/commcheck-2.slk:44: 2016-09-13 13:51:12
/tmp/commcheck-2.slk:46: waiting for event (2,5001432264) to be confirmed on node 5
/tmp/commcheck-2.slk:47: 2016-09-13 13:51:22
/tmp/commcheck-2.slk:51: 2016-09-13 13:51:31
/tmp/commcheck-2.slk:55: 2016-09-13 13:51:40
root at prodrpl-Amst:~# 
root at prodrpl-Amst:~# date && akaslonik /tmp/commcheck-2.slk 
Tue Sep 13 13:51:47 UTC 2016
/tmp/commcheck-2.slk:44: 2016-09-13 13:51:58
/tmp/commcheck-2.slk:46: waiting for event (2,5001432272) to be confirmed on node 5
/tmp/commcheck-2.slk:47: 2016-09-13 13:52:12
/tmp/commcheck-2.slk:50: waiting for event (2,5001432274) to be confirmed on node 5
/tmp/commcheck-2.slk:51: 2016-09-13 13:52:23
/tmp/commcheck-2.slk:54: waiting for event (2,5001432276) to be confirmed on node 5
/tmp/commcheck-2.slk:55: 2016-09-13 13:52:38
root at prodrpl-Amst:~# 


	Tom    ?


On 9/12/16, 4:38 PM, "Steve Singer" <steve at ssinger.info> wrote:

    On 09/12/2016 11:39 AM, Tignor, Tom wrote:
    >                  Seems I have an additional data point: the sync test
    > always takes longer (> 20 secs) if I include conninfo for all cluster
    > nodes instead of just the local node. I had previously thought conninfo
    > data was only used when needed. Is this not the case?
    
    What if you do
    
      sync(id=2);
    
      wait for event (origin=2, confirmed=5, wait on=2, timeout=30);
    
      sync(id=2);
    
      wait for event (origin=2, confirmed=5, wait on=2, timeout=30);
      sync(id=2);
    
      wait for event (origin=2, confirmed=5, wait on=2, timeout=30);
    
    
    3 times (or more) in a row, does it still take about the same amount of 
    time as 1 sync ?
    
    When slonik starts up it contacts all the nodes it has admin conninfo 
    for to get the current state/last event from each node.  Maybe your time 
    is spent establishing all those connections over SSL
    
    
    
    
    
    >
    >                  Tom J
    >
    > *From: *Tom Tignor <ttignor at akamai.com>
    > *Date: *Monday, September 12, 2016 at 10:52 AM
    > *To: *"slony1-general at lists.slony.info" <slony1-general at lists.slony.info>
    > *Subject: *sync performance
    >
    >                  Hello slony1 community,
    >
    >                  We?ve recently been testing communication reliability
    > between our cluster nodes. Our config is a simple setup with one
    > provider producing a modest volume of changes (measured in KB/s)
    > consumed by 5 direct subscribers, though these are geographically
    > distributed. The test is just a sync event followed by a wait on the
    > sync originator. Example:
    >
    > cluster name = ams_cluster;
    >
    > node 5 admin
    >
    >        conninfo='dbname=ams
    >
    >        host=23.79.242.182
    >
    >        user=ams_slony
    >
    >        sslmode=verify-ca
    >
    >        sslcert=/usr/local/akamai/.ams_certs/complete-ams_slony.crt
    >
    >        sslkey=/usr/local/akamai/.ams_certs/ams_slony.private_key
    >
    >        sslrootcert=/usr/local/akamai/etc/ssl_ca/canonical_ca_roots.pem';
    >
    > node 2 admin conninfo = 'dbname=ams user=ams_slony';
    >
    > sync(id=2);
    >
    > wait for event (origin=2, confirmed=5, wait on=2, timeout=30);
    >
    >                  Tests show the script takes 10-20 secs to run on
    > different nodes.
    >
    >                  Can anyone explain what?s happening internally during
    > this time, and why it takes so long? On a healthy, lightly loaded
    > system, we might have hoped for a sync response in just a couple
    > seconds. Our slon daemons are running with mostly default startup options.
    >
    >                  Thanks in advance,
    >
    >                  Tom J
    >
    >
    >
    > _______________________________________________
    > Slony1-general mailing list
    > Slony1-general at lists.slony.info
    > http://lists.slony.info/mailman/listinfo/slony1-general
    >
    
    


From scott.marlowe at gmail.com  Fri Sep 16 07:41:56 2016
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri, 16 Sep 2016 08:41:56 -0600
Subject: [Slony1-general] SET ADD TABLE argument TABLES basically useless on
	even slightly busy systems.
Message-ID: <CAOR=d=0R9ZVPyJGdhTqdWCFYGStOvaC9zyXN0V8uRAbezmwK6g@mail.gmail.com>

So I tried to subscribe a set with SET ADD TABLES using the regular
expression with TABLES and every single time it would error out with a
deadlock detected.

It's a cool idea, but the way it takes out locks seems to make it not
really useful.

Are there any plans to have it operate similar to creating a long list
of tables and running a single table add for each one, so it doesn't
get deadlocks?

Just wondering, it's not a world-stopper, just a slight annoyance.

-- 
To understand recursion, one must first understand recursion.

From steve at ssinger.info  Sat Sep 17 18:18:31 2016
From: steve at ssinger.info (Steve Singer)
Date: Sat, 17 Sep 2016 21:18:31 -0400 (EDT)
Subject: [Slony1-general] SET ADD TABLE argument TABLES basically
 useless on even slightly busy systems.
In-Reply-To: <CAOR=d=0R9ZVPyJGdhTqdWCFYGStOvaC9zyXN0V8uRAbezmwK6g@mail.gmail.com>
References: <CAOR=d=0R9ZVPyJGdhTqdWCFYGStOvaC9zyXN0V8uRAbezmwK6g@mail.gmail.com>
Message-ID: <alpine.DEB.2.11.1609172115410.22223@opti.atlantida>

On Fri, 16 Sep 2016, Scott Marlowe wrote:

> So I tried to subscribe a set with SET ADD TABLES using the regular
> expression with TABLES and every single time it would error out with a
> deadlock detected.
>
> It's a cool idea, but the way it takes out locks seems to make it not
> really useful.
>
> Are there any plans to have it operate similar to creating a long list
> of tables and running a single table add for each one, so it doesn't
> get deadlocks?
>
> Just wondering, it's not a world-stopper, just a slight annoyance.

I don't think anyone has raised this before.

The issue with having 'set add table' use an individual transaction for each 
table would be that if the command failed in the middle some tables would be 
added and some woudln't be.

I guess we could have an option to to give this behaviour if people wanted 
but I wouldn't make it the default.

Steve


>
> -- 
> To understand recursion, one must first understand recursion.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From ttignor at akamai.com  Fri Sep 23 05:45:03 2016
From: ttignor at akamai.com (Tignor, Tom)
Date: Fri, 23 Sep 2016 12:45:03 +0000
Subject: [Slony1-general] switchover
Message-ID: <2530D978-9A09-4A96-9B23-186F4DA6512F@akamai.com>


                Hello slony folks,
                I think this should be familiar territory for some: the doc has this example for performing controlled switchover.

A small slonik script executes the following commands:

lock set (id = 1, origin = 1);
wait for event (origin = 1, confirmed = 2);
move set (id = 1, old origin = 1, new origin = 2);
wait for event (origin = 1, confirmed = 2, wait on=1);

                It?s been noted that the first wait op needs a ?wait on=1? arg.
                Questions: does it matter which node this script is run on?
                Is it also helpful to add waits with ?wait on=2?? Or is it guaranteed (100% no timing issues) that waiting on the old origin node 1 insures that node 2 is ready to go as soon as the above-listed waits finish?
                Thanks in advance,

                Tom    ?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20160923/136e360a/attachment.htm 

