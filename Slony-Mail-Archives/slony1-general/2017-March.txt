From tmblue at gmail.com  Thu Mar  2 20:47:25 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 2 Mar 2017 20:47:25 -0800
Subject: [Slony1-general] CentOs / redhat Spec file
Message-ID: <CAEaSS0aG8gLAqEcFas4_4ZUy2+y2Yi5ZiaRg_vwkcm-4CxptYg@mail.gmail.com>

Wondering if someone can help me out.

I'm trying to migrate to the standard Postgres build and slony build.
However right now the repo has 9.5 postgresql built with 2.2.4 or 2.2.5
slony.

I am still running a custom postgresql 9.4.5 with slony 2.2.3 (custom
package, custom rpm).. While I can run different postgresql versions, I
can't run different slony versions and since I can't take my cluster down,
I need to upgrade single nodes.

This means taking a server and upgrading to Postgresql 9.5 for example but
then installing and continue running slony 2.2.3, until all 4-5 nodes are
upgraded. i can then install the latest slony and do a quick hit to do the
slony upgrade. Hopefully this will be the last time I have to do something
custom

So for now, I need to install 9.5  with slony 2.2.3 and looking for a slony
spec file. The spec file in the source code is like 12 years out of date
(2005). so was wondering if someone could save me some headaches and share
a more recent spec file for 2.x that I can tweak , at least to get the
paths correct for

I guess I can also just build against 9.5 and package up the various
bin/lib/share directories and just copy them in the right place but cleanup
won't be all that fun..

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170302/d33a4f97/attachment.htm 

From tmblue at gmail.com  Fri Mar  3 09:46:53 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Mar 2017 09:46:53 -0800
Subject: [Slony1-general] build error , can 2.2.3 be built with 9.5?
Message-ID: <CAEaSS0b-7mqnckfje-J4iXQJMToov4W7KALd+H5BTZo7P+f17Q@mail.gmail.com>

I guess the first question is can I even build 2.2.3 against 9.5? Before I
continue to beet my head against the desk.

If the answer is yes, anyone want to take a stab at what is happening with
this error in the make process

gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o runtime_config.o
runtime_config.c

*runtime_config.c:* In function ?*rtcfg_setNodeLastEvent*?:

*runtime_config.c:193:45:* *error: *expected ?*)*? before ?*INT64_FORMAT*?

     "setNodeLastEvent: no_id=%d event_seq=" INT64_FORMAT "\n",


CentOS7.2


Thanks

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170303/b9dff0e4/attachment.htm 

From tmblue at gmail.com  Fri Mar  3 09:59:55 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Mar 2017 09:59:55 -0800
Subject: [Slony1-general] build error , can 2.2.3 be built with 9.5?
In-Reply-To: <CAEaSS0b-7mqnckfje-J4iXQJMToov4W7KALd+H5BTZo7P+f17Q@mail.gmail.com>
References: <CAEaSS0b-7mqnckfje-J4iXQJMToov4W7KALd+H5BTZo7P+f17Q@mail.gmail.com>
Message-ID: <CAEaSS0bq+8Bg3DrS=JLLiDG9uQOqLDPOThdY7PzcaGD+zmiBig@mail.gmail.com>

On Fri, Mar 3, 2017 at 9:46 AM, Tory M Blue <tmblue at gmail.com> wrote:

> I guess the first question is can I even build 2.2.3 against 9.5? Before I
> continue to beet my head against the desk.
>
> If the answer is yes, anyone want to take a stab at what is happening with
> this error in the make process
>
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
> -I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o runtime_config.o
> runtime_config.c
>
> *runtime_config.c:* In function ?*rtcfg_setNodeLastEvent*?:
>
> *runtime_config.c:193:45:* *error: *expected ?*)*? before ?*INT64_FORMAT*?
>
>      "setNodeLastEvent: no_id=%d event_seq=" INT64_FORMAT "\n",
>
>
> CentOS7.2
>
>
> Thanks
>


That was very odd, I brought down the git source and configured from that
and it was able to make without an issue, so not sure what was missing or
wrong with my 2.2.3 src file. Very weird.

But I'm good!! GIT was clean and worked.

Thanks
tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170303/7a5f34b3/attachment.htm 

From tmblue at gmail.com  Fri Mar  3 13:25:13 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Mar 2017 13:25:13 -0800
Subject: [Slony1-general] build error , can 2.2.3 be built with 9.5?
In-Reply-To: <CAEaSS0bq+8Bg3DrS=JLLiDG9uQOqLDPOThdY7PzcaGD+zmiBig@mail.gmail.com>
References: <CAEaSS0b-7mqnckfje-J4iXQJMToov4W7KALd+H5BTZo7P+f17Q@mail.gmail.com>
	<CAEaSS0bq+8Bg3DrS=JLLiDG9uQOqLDPOThdY7PzcaGD+zmiBig@mail.gmail.com>
Message-ID: <CAEaSS0au8=mG1Xtiazah0yg3vuKO4Ego=F3eTootAfR2Yg2c2w@mail.gmail.com>

On Fri, Mar 3, 2017 at 9:59 AM, Tory M Blue <tmblue at gmail.com> wrote:

>
>
> On Fri, Mar 3, 2017 at 9:46 AM, Tory M Blue <tmblue at gmail.com> wrote:
>
>> I guess the first question is can I even build 2.2.3 against 9.5? Before
>> I continue to beet my head against the desk.
>>
>> If the answer is yes, anyone want to take a stab at what is happening
>> with this error in the make process
>>
>> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
>> -I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o runtime_config.o
>> runtime_config.c
>>
>> *runtime_config.c:* In function ?*rtcfg_setNodeLastEvent*?:
>>
>> *runtime_config.c:193:45:* *error: *expected ?*)*? before ?*INT64_FORMAT*
>> ?
>>
>>      "setNodeLastEvent: no_id=%d event_seq=" INT64_FORMAT "\n",
>>
>>
>> CentOS7.2
>>
>>
>> Thanks
>>
>
>
> That was very odd, I brought down the git source and configured from that
> and it was able to make without an issue, so not sure what was missing or
> wrong with my 2.2.3 src file. Very weird.
>
> But I'm good!! GIT was clean and worked.
>
> Thanks
> tory
>

Just closing this out, since I made an error. I downloaded 2.3.0 which
works with 9.5, 2.2.3 does not seem to be compatable with postgres9.5 or at
least that is what is causing the error. Grabbed 2.2.3 source from git and
receive the same error. 2.3.0 with Postgresl95 works fine, but 2.2.3
encounters this error during the MAKE

gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o slon.o slon.c

gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o runtime_config.o
runtime_config.c

*runtime_config.c:* In function ?*rtcfg_setNodeLastEvent*?:

*runtime_config.c:193:45:* *error: *expected ?*)*? before ?*INT64_FORMAT*?

     "setNodeLastEvent: no_id=%d event_seq=" INT64_FORMAT "\n",

*                                             ^*

make[2]: *** [runtime_config.o] Error 1

make[2]: Leaving directory `/home/tblue/GIT/slony1-engine-3204f06/src/slon'

make[1]: *** [all] Error 2

make[1]: Leaving directory `/home/tblue/GIT/slony1-engine-3204f06/src'

make: *** [all] Error 2


So bah!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170303/fc9b31cf/attachment.htm 

From tmblue at gmail.com  Fri Mar  3 13:46:12 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Mar 2017 13:46:12 -0800
Subject: [Slony1-general] build error , can 2.2.3 be built with 9.5?
In-Reply-To: <CAEaSS0au8=mG1Xtiazah0yg3vuKO4Ego=F3eTootAfR2Yg2c2w@mail.gmail.com>
References: <CAEaSS0b-7mqnckfje-J4iXQJMToov4W7KALd+H5BTZo7P+f17Q@mail.gmail.com>
	<CAEaSS0bq+8Bg3DrS=JLLiDG9uQOqLDPOThdY7PzcaGD+zmiBig@mail.gmail.com>
	<CAEaSS0au8=mG1Xtiazah0yg3vuKO4Ego=F3eTootAfR2Yg2c2w@mail.gmail.com>
Message-ID: <CAEaSS0aorhn1DLcCZ1OAnfj6jpT8s++4JM9KEV8uJL1vuf9tEw@mail.gmail.com>

On Fri, Mar 3, 2017 at 1:25 PM, Tory M Blue <tmblue at gmail.com> wrote:

>
>
> On Fri, Mar 3, 2017 at 9:59 AM, Tory M Blue <tmblue at gmail.com> wrote:
>
>>
>>
>> On Fri, Mar 3, 2017 at 9:46 AM, Tory M Blue <tmblue at gmail.com> wrote:
>>
>>> I guess the first question is can I even build 2.2.3 against 9.5? Before
>>> I continue to beet my head against the desk.
>>>
>>> If the answer is yes, anyone want to take a stab at what is happening
>>> with this error in the make process
>>>
>>> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
>>> -I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o runtime_config.o
>>> runtime_config.c
>>>
>>> *runtime_config.c:* In function ?*rtcfg_setNodeLastEvent*?:
>>>
>>> *runtime_config.c:193:45:* *error: *expected ?*)*? before ?
>>> *INT64_FORMAT*?
>>>
>>>      "setNodeLastEvent: no_id=%d event_seq=" INT64_FORMAT "\n",
>>>
>>>
>>> CentOS7.2
>>>
>>>
>>> Thanks
>>>
>>
>>
>> That was very odd, I brought down the git source and configured from that
>> and it was able to make without an issue, so not sure what was missing or
>> wrong with my 2.2.3 src file. Very weird.
>>
>> But I'm good!! GIT was clean and worked.
>>
>> Thanks
>> tory
>>
>
> Just closing this out, since I made an error. I downloaded 2.3.0 which
> works with 9.5, 2.2.3 does not seem to be compatable with postgres9.5 or at
> least that is what is causing the error. Grabbed 2.2.3 source from git and
> receive the same error. 2.3.0 with Postgresl95 works fine, but 2.2.3
> encounters this error during the MAKE
>
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
> -I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o slon.o slon.c
>
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
> -I../.. -I../../src/slon -I/usr/pgsql-9.5/include/ -c -o runtime_config.o
> runtime_config.c
>
> *runtime_config.c:* In function ?*rtcfg_setNodeLastEvent*?:
>
> *runtime_config.c:193:45:* *error: *expected ?*)*? before ?*INT64_FORMAT*?
>
>      "setNodeLastEvent: no_id=%d event_seq=" INT64_FORMAT "\n",
>
> *                                             ^*
>
> make[2]: *** [runtime_config.o] Error 1
>
> make[2]: Leaving directory `/home/tblue/GIT/slony1-
> engine-3204f06/src/slon'
>
> make[1]: *** [all] Error 2
>
> make[1]: Leaving directory `/home/tblue/GIT/slony1-engine-3204f06/src'
>
> make: *** [all] Error 2
>
>
> So bah!
>
Sorry for polluting the list, as 2.2.3 is a bit old now.

 slony1-engine-3204f06  can't build with postgres9.5

 slony1-engine-29a1cd9  can build with postgresql9.5 without errors (yet to
test post build but the build works).

Wondering if there is a 2.2.3 build that is between 29a and 320 that  has
all the bug fixes?

Thanks

Tory


 slony1-engine-955aa71 seems to be the first 2.2.4 build so this one is out.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170303/5d0ddcab/attachment.htm 

From nawazid at yahoo.com  Sun Mar 12 19:23:19 2017
From: nawazid at yahoo.com (Nawaz Ahmed)
Date: Mon, 13 Mar 2017 02:23:19 +0000 (UTC)
Subject: [Slony1-general] postgresql version upgrade
References: <773734485.4235053.1489371799283.ref@mail.yahoo.com>
Message-ID: <773734485.4235053.1489371799283@mail.yahoo.com>


Hi Folks,

I have a couple of queries regarding upgrade of a PostgreSQL database instance using Slony-I. I came across the below link which lists a step-by-step process of version upgrade. 

http://slony.info/documentation/1.2/versionupgrade.html


The queries are below:

1) Although it comes under the documentation of Slony version 1.2.23, does it still hold good for the Slony version 2.2.5 to upgrade a postgres from 9.0 to 9.5 or 9.6.

2) Will the process be straight forward just like the one stated under version 1.2.23 link above, or are there any challenges/heads ups to be cautious about ?

3) Has anyone else done it in here, who is willing to share their experience of whether they had to do something additional or they simply followed the process in the above link.


Thanks & Regards,?Nawaz Ahmed?+61 414 513 643
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170313/56cdb6c5/attachment.htm 

From vivek at khera.org  Mon Mar 13 06:39:18 2017
From: vivek at khera.org (Vick Khera)
Date: Mon, 13 Mar 2017 09:39:18 -0400
Subject: [Slony1-general] postgresql version upgrade
In-Reply-To: <773734485.4235053.1489371799283@mail.yahoo.com>
References: <773734485.4235053.1489371799283.ref@mail.yahoo.com>
	<773734485.4235053.1489371799283@mail.yahoo.com>
Message-ID: <CALd+dcf7jB7jgL9rVMNf=uhk19yqoDA=T1FmfgbXgH5v3sc79A@mail.gmail.com>

On Sun, Mar 12, 2017 at 10:23 PM, Nawaz Ahmed <nawazid at yahoo.com> wrote:

> 1) Although it comes under the documentation of Slony version 1.2.23, does
> it still hold good for the Slony version 2.2.5 to upgrade a postgres from
> 9.0 to 9.5 or 9.6.
>

The only limit is that the version of slony you run has to be identical and
support both versions of postgresql you have.

You are also going to need to validate your application works with your
newer version of postgres. The release notes for each intermediate version
will help you identify things you need to change and test thoroughly.


>
> 2) Will the process be straight forward just like the one stated under
> version 1.2.23 link above, or are there any challenges/heads ups to be
> cautious about ?
>

Yes, it is amazingly simple. You replicate from your current to the newer
version of postgres, then do that switchover step outlined.


>
> 3) Has anyone else done it in here, who is willing to share their
> experience of whether they had to do something additional or they simply
> followed the process in the above link.
>

No, the steps are very straight forward. How you stop your application from
updating the DB for the time it needs to switch over is however, for you to
figure out.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170313/e4dfd2a1/attachment.htm 

From scott.marlowe at gmail.com  Mon Mar 13 07:22:08 2017
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Mon, 13 Mar 2017 08:22:08 -0600
Subject: [Slony1-general] postgresql version upgrade
In-Reply-To: <CALd+dcf7jB7jgL9rVMNf=uhk19yqoDA=T1FmfgbXgH5v3sc79A@mail.gmail.com>
References: <773734485.4235053.1489371799283.ref@mail.yahoo.com>
	<773734485.4235053.1489371799283@mail.yahoo.com>
	<CALd+dcf7jB7jgL9rVMNf=uhk19yqoDA=T1FmfgbXgH5v3sc79A@mail.gmail.com>
Message-ID: <CAOR=d=2e5Y0305LHSRNdUd8_rNie+q+5yPLgWZX1W5OSazvCrQ@mail.gmail.com>

On Mon, Mar 13, 2017 at 7:39 AM, Vick Khera <vivek at khera.org> wrote:
>
> On Sun, Mar 12, 2017 at 10:23 PM, Nawaz Ahmed <nawazid at yahoo.com> wrote:
>> 3) Has anyone else done it in here, who is willing to share their
>> experience of whether they had to do something additional or they simply
>> followed the process in the above link.
>
>
> No, the steps are very straight forward. How you stop your application from
> updating the DB for the time it needs to switch over is however, for you to
> figure out.

I've used slony this way multiple times and the only "issue" I've run
into is that I've often had to compile the slony version I needed on
one end or the other because it wasn't available for a given
distribution / version I was using. All you need extra are the
postgresql-devel packages that slony needs to hit.

pgbouncer is what I used to direct my application on which / how to
talk to my servers. Downtime was measured in minutes (or fractions
thereof) for databases as big as 5TB.

-- 
To understand recursion, one must first understand recursion.

From Olivier.Bernhard at digora.com  Mon Mar 27 06:12:13 2017
From: Olivier.Bernhard at digora.com (Olivier Bernhard)
Date: Mon, 27 Mar 2017 13:12:13 +0000
Subject: [Slony1-general] What's the best practice when configuration
 mutiple sealed 1 to 1 database replications ?
Message-ID: <3984BA89EE577C41A14CBB8F9B93481E647E1C@JEKYLL.Digorix2.digora.com>

Hi,
I'm currently trying to solve an issue i have on a slony based replication platform (slony 1.94):

There's a master postgresql database server (on which master nodes are running) and a slave postgresql database server ( on which slave nodes are running).

On the master server I have the following databases :

DB1, DB3, DB5, DB7 ....

On the slave server I have the following database :

DB2, DB4,DB6, DB8 ....

Basically, the following replication is expected :

DB1 -> DB2
DB3 -> DB4
DB5 -> DB6
Etc ....

However I have noticed I have many processes which are not expected to be here. One of these databases is called "ferrandi" and has the same name on the slave database server.

select distinct
datname,
application_name,
client_addr,
substring(query from 1 for 30) query_text,
count(1) over (partition by datname) cnt_total,
count(1) over (partition by datname,application_name,client_addr,substring(query from 1 for 30) ) cnt
from
pg_stat_activity
order by cnt_total, datname, application_name;

   datname   |      application_name      |  client_addr  |           query_text           | cnt_total | cnt
-------------+----------------------------+---------------+--------------------------------+-----------+-----
ferrandi    | slon.local_cleanup         | 172.16.173.35 | select nspname, relname from " |       200 |   1
ferrandi    | slon.local_listen          | 172.16.173.35 | rollback transaction;          |       200 |   1
ferrandi    | slon.local_monitor         | 172.16.173.35 | commit;                        |       200 |   1
ferrandi    | slon.local_sync            | 172.16.173.35 | rollback transaction;          |       200 |   1
ferrandi    | slon.node_9_listen         | 172.16.173.35 | select con_origin, con_receive |       200 |  13
ferrandi    | slon.origin_10_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_11_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_12_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_13_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_14_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_1_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_2_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_3_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_4_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_5_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_6_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_7_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_8_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_9_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_9_provider_9   | 172.16.173.35 | select SL.seql_seqid, max(SL.s |       200 |   1
ferrandi    | slon.remoteWorkerThread_1  | 172.16.173.35 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_10 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_11 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_12 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_13 | 172.16.173.35 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_14 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_15 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_16 | 172.16.173.35 | select "_replication_ccir".log |       200 |   1
ferrandi    | slon.remoteWorkerThread_17 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_18 | 172.16.173.35 | select "_replication_ccir".log |       200 |   1
ferrandi    | slon.remoteWorkerThread_19 | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_2  | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_20 | 172.16.173.35 | select "_replication_ccir".log |       200 |   1
ferrandi    | slon.remoteWorkerThread_3  | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_4  | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_5  | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_6  | 172.16.173.35 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_7  | 172.16.173.35 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_8  | 172.16.173.35 | insert into "_replication_ccir |       200 |   1

The problem I have is with these multiple processes (basically 168 idel processes):

ferrandi    | slon.node_9_listen         | 172.16.173.35 | select con_origin, con_receive |       200 |  13
ferrandi    | slon.origin_10_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_11_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_12_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_13_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_14_provider_9  | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_1_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_2_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_3_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_4_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_5_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_6_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_7_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_8_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_9_provider_9   | 172.16.173.35 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_9_provider_9   | 172.16.173.35 | select SL.seql_seqid, max(SL.s |       200 |   1

ferrandi database is node 9 while the slave is node 10. So there should be no relation between node 9 and nodes 1,2,3,4,5,6,7,8,11,12...20 which are other databases. Only node 10 should be working with node 9.

Checking the configuration in the slony schema, I can see that whatever the master database is, the configuration table contain all the nodes, while I guess only nodes 9 and 10 should be referenced in this specific replication which only involves node 9 and node 10.

Could someone confirm that is node 9 and 10 are only supposed to work together (with no other database involved) then I should not have all the nodes declared in the configuration ? If so then I guess people who have done this have applied a single same configuration file to all the master databases, and this may explain why I have all these processes.

Thanks and Best Regards,
Olivier


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20170327/62f6ea1b/attachment.htm 

From stephane.schildknecht at postgres.fr  Mon Mar 27 06:35:45 2017
From: stephane.schildknecht at postgres.fr (=?UTF-8?Q?St=c3=a9phane_Schildknecht?=)
Date: Mon, 27 Mar 2017 15:35:45 +0200
Subject: [Slony1-general] What's the best practice when configuration
 mutiple sealed 1 to 1 database replications ?
In-Reply-To: <3984BA89EE577C41A14CBB8F9B93481E647E1C@JEKYLL.Digorix2.digora.com>
References: <3984BA89EE577C41A14CBB8F9B93481E647E1C@JEKYLL.Digorix2.digora.com>
Message-ID: <dbdff7fb-92e8-6f2a-fadb-dd1a7f6f925c@postgres.fr>

Le 27/03/2017 ? 15:12, Olivier Bernhard a ?crit :
> Hi,
> 
> I?m currently trying to solve an issue i have on a slony based replication
> platform (slony 1.94):

I'm not sure this is a version number related to available Slony branches.

> 
> There?s a master postgresql database server (on which master nodes are running)
> and a slave postgresql database server ( on which slave nodes are running).
> On the master server I have the following databases :
> DB1, DB3, DB5, DB7 ?.
> On the slave server I have the following database :
> DB2, DB4,DB6, DB8 ?.

> Basically, the following replication is expected :
> DB1 -> DB2
> DB3 -> DB4
> DB5 -> DB6
> Etc ?.

> 
> However I have noticed I have many processes which are not expected to be here.
> One of these databases is called ?ferrandi? and has the same name on the slave
> database server.

You told us just 2 lines upper that DB don't have same name on provider and
subscriber.

So, if I understand correctly, you have two servers and a bunch of databases.
How many sets of replication did you set ?

Could you paste your configuration ?
Cluster, nodes, set...

Maybe by having a look at slony schema in the databases (sl_nodes, sl_set,
sl_tables...)

(...)
>  
> 
> ferrandi database is node 9 while the slave is node 10. So there should be no
> relation between node 9 and nodes 1,2,3,4,5,6,7,8,11,12?20 which are other
> databases. Only node 10 should be working with node 9.
> 
>  
> 
> Checking the configuration in the slony schema, I can see that whatever the
> master database is, the configuration table contain all the nodes, while I
> guess only nodes 9 and 10 should be referenced in this specific replication
> which only involves node 9 and node 10.

I guess you have a single slony cluster, and everything goes into it.

Slony relies on the definition of a cluster, and a network of nodes all able to
communicate with each other.

> 
>  
> 
> Could someone confirm that is node 9 and 10 are only supposed to work together
> (with no other database involved) then I should not have all the nodes declared
> in the configuration ? If so then I guess people who have done this have
> applied a single same configuration file to all the master databases, and this
> may explain why I have all these processes.
> 

Let us see you configuration, but, it may be that everything is configured
within the same cluster, and without any restriction on sl_path.

-- 
Dr. St?phane Schildknecht

Contact r?gional PostgreSQL pour l'Europe francophone
G?rant de Loxodata, soci?t? de conseil, support et formation

01.79.72.57.75

From Olivier.Bernhard at digora.com  Mon Mar 27 08:05:36 2017
From: Olivier.Bernhard at digora.com (Olivier Bernhard)
Date: Mon, 27 Mar 2017 15:05:36 +0000
Subject: [Slony1-general] What's the best practice when configuration
 mutiple sealed 1 to 1 database replications ?
In-Reply-To: <dbdff7fb-92e8-6f2a-fadb-dd1a7f6f925c@postgres.fr>
References: <3984BA89EE577C41A14CBB8F9B93481E647E1C@JEKYLL.Digorix2.digora.com>
	<dbdff7fb-92e8-6f2a-fadb-dd1a7f6f925c@postgres.fr>
Message-ID: <3984BA89EE577C41A14CBB8F9B93481E648404@JEKYLL.Digorix2.digora.com>

Hi, I apologize for not having been clear enough.

1) Yes we have 2 postgresql servers and a bunch of databases running on these 2 servers.

Let's call the 2 servers SERV1(172.16.173.35) and SERV2(172.16.173.36)

On SERV1, I have the following databases running :

===================================
postgres=# \l
                                   List of databases
    Name     |   Owner   | Encoding |   Collate   |    Ctype    |   Access privileges
-------------+-----------+----------+-------------+-------------+-----------------------
base_pilote | back_user | UTF8     | fr_FR.UTF-8 | fr_FR.UTF-8 |
essym       | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
ferrandi    | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
ferrandi_en | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
gescia      | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
isipca      | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
isipcaen    | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
itescia     | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =Tc/postgres         +
             |           |          |             |             | postgres=CTc/postgres
lafabrique  | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
postgres    | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
supdev      | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
supdevente  | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
template0   | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
             |           |          |             |             | postgres=CTc/postgres
template1   | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
             |           |          |             |             | postgres=CTc/postgres
upmc        | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
===================================
    
On SERV2, I have the exact same databases running (except they are replicated from SERV1 with slony) :

===================================
postgres=# \l
                                   List of databases
    Name     |   Owner   | Encoding |   Collate   |    Ctype    |   Access privileges
-------------+-----------+----------+-------------+-------------+-----------------------
base_pilote | back_user | UTF8     | fr_FR.UTF-8 | fr_FR.UTF-8 |
essym       | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
ferrandi    | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
ferrandi_en | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
gescia      | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
isipca      | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
isipcaen    | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
itescia     | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =Tc/postgres         +
             |           |          |             |             | postgres=CTc/postgres
lafabrique  | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
postgres    | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
supdev      | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
supdevente  | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
template0   | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
             |           |          |             |             | postgres=CTc/postgres
template1   | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
             |           |          |             |             | postgres=CTc/postgres
upmc        | postgres  | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
===================================

Master databases are on SERV1
Slave databases are on SERV2

Whatever the master database I connect to , the same configuration is in place :

ferrandi=# select * from sl_node ;
no_id | no_active |             no_comment              | no_failed
-------+-----------+-------------------------------------+-----------
     1 | t         | Node 1 - itescia at 172.16.173.35      | f
     2 | t         | Node 2 - itescia at 172.16.173.36      | f
     3 | t         | Node 3 - isipca at 172.16.173.35       | f
     4 | t         | Node 4 - isipca at 172.16.173.36       | f
     5 | t         | Node 5 - lafabrique at 172.16.173.35   | f
     6 | t         | Node 6 - lafabrique at 172.16.173.36   | f
     7 | t         | Node 7 - supdev at 172.16.173.35       | f
     8 | t         | Node 8 - supdev at 172.16.173.36       | f
     9 | t         | Node 9 - ferrandi at 172.16.173.35     | f
    10 | t         | Node 10 - ferrandi at 172.16.173.36    | f
    11 | t         | Node 11 - upmc at 172.16.173.35        | f
    12 | t         | Node 12 - upmc at 172.16.173.36        | f
    13 | t         | Node 13 - essym at 172.16.173.35       | f
    14 | t         | Node 14 - essym at 172.16.173.36       | f
    15 | t         | Node 15 - gescia at 172.16.173.35      | f
    16 | t         | Node 16 - gescia at 172.16.173.36      | f
    17 | t         | Node 17 - isipcaen at 172.16.173.35    | f
    18 | t         | Node 18 - isipcaen at 172.16.173.36    | f
    19 | t         | Node 19 - ferrandi_en at 172.16.173.35 | f
    20 | t         | Node 20 - ferrandi_en at 172.16.173.36 | f

So basically,
Node 1 is replicated on Node 2
Node 3 is replicated on Node 4
.....
Node 19 is replicated on Node 20.


For each Master/slave I have one replication set (a large set of tables I can't copy/paste here because there are more than 100 tables in the set).

The databases have different names, but they have the exact same set of tables which is replicated on their corresponding slave database.

So there's a unique configuration file which declares Master/slave nodes as well as the replications sets (which are repeated, once for each master, even if the databases contain the exact same tables).

This configuration file has been run against each master database (node) so each master database slony schema contains the definition of the 20 nodes. 
However, each master database  (node) is only replicated in a single slave database (node).

So 
1) I don't understand why the configuration of each Master/slave nodes should contain the definition of the 20 nodes 
2) It leads to having for each master / slave nodes a bunch of 168 processes which does not seem to be used, and consume lots of connexions

select distinct
datname,
application_name,
client_addr,
substring(query from 1 for 30) query_text,
count(1) over (partition by datname) cnt_total,
count(1) over (partition by datname,application_name,client_addr,substring(query from 1 for 30) ) cnt
from
pg_stat_activity
order by cnt_total, datname, application_name;

   datname   |      application_name      |  client_addr  | client_hostname |           query_text           | cnt_total | cnt
-------------+----------------------------+---------------+-----------------+--------------------------------+-----------+-----
ferrandi_en | slon.node_19_listen        | 172.16.173.35 |                 | select con_origin, con_receive |        14 |  14 => Replication has been disabled for this database
gescia      | slon.node_15_listen        | 172.16.173.35 |                 | select con_origin, con_receive |        14 |  14 => Replication has been disabled for this database
isipcaen    | slon.node_17_listen        | 172.16.173.35 |                 | select con_origin, con_receive |        14 |  14 => Replication has been disabled for this database

ferrandi    | psql                       |               |                 | select * from sl_listen order  |       200 |   1
ferrandi    | slon.local_cleanup         | 172.16.173.35 |                 | begin;lock table "_replication |       200 |   1
ferrandi    | slon.local_listen          | 172.16.173.35 |                 | rollback transaction;          |       200 |   1
ferrandi    | slon.local_monitor         | 172.16.173.35 |                 | commit;                        |       200 |   1
ferrandi    | slon.local_sync            | 172.16.173.35 |                 | rollback transaction;          |       200 |   1
ferrandi    | slon.node_9_listen         | 172.16.173.35 |                 | select con_origin, con_receive |       200 |  13
ferrandi    | slon.origin_10_provider_9  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_11_provider_9  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_12_provider_9  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_13_provider_9  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_14_provider_9  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_1_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_2_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_3_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_4_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_5_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_6_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_7_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
ferrandi    | slon.origin_8_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_9_provider_9   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
ferrandi    | slon.origin_9_provider_9   | 172.16.173.35 |                 | select SL.seql_seqid, max(SL.s |       200 |   1
ferrandi    | slon.remoteWorkerThread_1  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_10 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_11 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_12 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_13 | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_14 | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_15 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_16 | 172.16.173.35 |                 | select "_replication_ccir".log |       200 |   1
ferrandi    | slon.remoteWorkerThread_17 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_18 | 172.16.173.35 |                 | select "_replication_ccir".log |       200 |   1
ferrandi    | slon.remoteWorkerThread_19 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_2  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_20 | 172.16.173.35 |                 | select "_replication_ccir".log |       200 |   1
ferrandi    | slon.remoteWorkerThread_3  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_4  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_5  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_6  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
ferrandi    | slon.remoteWorkerThread_7  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
ferrandi    | slon.remoteWorkerThread_8  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1

isipca      | slon.local_cleanup         | 172.16.173.35 |                 | begin;lock table "_replication |       200 |   1
isipca      | slon.local_listen          | 172.16.173.35 |                 | rollback transaction;          |       200 |   1
isipca      | slon.local_monitor         | 172.16.173.35 |                 | commit;                        |       200 |   1
isipca      | slon.local_sync            | 172.16.173.35 |                 | commit transaction;            |       200 |   1
isipca      | slon.node_3_listen         | 172.16.173.35 |                 | select con_origin, con_receive |       200 |  13
isipca      | slon.origin_10_provider_3  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_11_provider_3  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
isipca      | slon.origin_12_provider_3  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_13_provider_3  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
isipca      | slon.origin_14_provider_3  | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_1_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_2_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_3_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_3_provider_3   | 172.16.173.35 |                 | select SL.seql_seqid, max(SL.s |       200 |   1
isipca      | slon.origin_4_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_5_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
isipca      | slon.origin_6_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_7_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
isipca      | slon.origin_8_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  12
isipca      | slon.origin_9_provider_3   | 172.16.173.35 |                 | rollback transaction; set enab |       200 |  11
isipca      | slon.remoteWorkerThread_1  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_10 | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_11 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_12 | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_13 | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_14 | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_15 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_16 | 172.16.173.35 |                 | select "_replication_ccir".log |       200 |   1
isipca      | slon.remoteWorkerThread_17 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_18 | 172.16.173.35 |                 | select "_replication_ccir".log |       200 |   1
isipca      | slon.remoteWorkerThread_19 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_2  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_20 | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_4  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_5  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_6  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_7  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1
isipca      | slon.remoteWorkerThread_8  | 172.16.173.35 |                 | insert into "_replication_ccir |       200 |   1
isipca      | slon.remoteWorkerThread_9  | 172.16.173.35 |                 | select "_replication_ccir".for |       200 |   1

?
?
?
=========================

The problem to me is all these "slon.origin_XX_provider_Y" processes which exist for each database, while a single database is supposed to be replicated in only one slave database.
I don't see the reason why all these processes referencing other nodes are established on a database node which is not supposed to work with these other nodes.

And whatever the master database is, when I connect to it and query sl_node or sl_path, it looks like that all possible combinations are existing.

=========================
ferrandi=# select * from sl_path order by pa_server, pa_client ;
pa_server | pa_client |                                     pa_conninfo                                      | pa_connretry
-----------+-----------+--------------------------------------------------------------------------------------+--------------
         1 |         2 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         1 |         3 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         1 |         4 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
...
...
...
         1 |        17 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         1 |        18 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         1 |        19 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         1 |        20 | host=172.16.173.35 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         2 |         1 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         2 |         3 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         2 |         4 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         2 |         5 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
...
...
...
         2 |        18 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         2 |        19 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         2 |        20 | host=172.16.173.36 dbname=itescia user=slonyuser port=5432 password=2zG8zKUz774H     |           10
         3 |         1 | host=172.16.173.35 dbname=isipca user=slonyuser port=5432 password=2zG8zKUz774H      |           10
         3 |         2 | host=172.16.173.35 dbname=isipca user=slonyuser port=5432 password=2zG8zKUz774H      |           10
         3 |         4 | host=172.16.173.35 dbname=isipca user=slonyuser port=5432 password=2zG8zKUz774H      |           10
         3 |         5 | host=172.16.173.35 dbname=isipca user=slonyuser port=5432 password=2zG8zKUz774H      |           10
...
...
...
...
...
...
        20 |        18 | host=172.16.173.36 dbname=ferrandi_en user=slonyuser port=5432 password=2zG8zKUz774H |           10
        20 |        19 | host=172.16.173.36 dbname=ferrandi_en user=slonyuser port=5432 password=2zG8zKUz774H |           10
=========================

It basically means that each time a new database is added with it's corresponding slave, I have 200 more connexions, while it seems to me I should only have connexions related to the replication of the master and the slave only.

That's why I wonder if it's normal to have the same global slony replication configuration in each Master database slony schema, while a Master database is only supposed to be replicated in a specific slave database.

Thanks and Best Regards,
Olivier

