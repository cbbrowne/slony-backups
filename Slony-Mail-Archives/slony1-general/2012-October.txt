From vivek at khera.org  Mon Oct  1 07:26:32 2012
From: vivek at khera.org (Vick Khera)
Date: Mon, 1 Oct 2012 10:26:32 -0400
Subject: [Slony1-general] Slony replication between two database (server
 and workshop) sometimes not fired
In-Reply-To: <34482368.post@talk.nabble.com>
References: <34482368.post@talk.nabble.com>
Message-ID: <CALd+dcfWvzj6riNjEXiMaoEY0LsznJa0EE6MPDonFUrQwNXHWA@mail.gmail.com>

Slony disables triggers on replicated tables on all but the origin server.
 Thus, table A on your workshop will not fire any triggers when it is
updated. Also, it should be denying you to alter any rows in the table A on
workshop.  How are you doing that?

On Wed, Sep 26, 2012 at 9:53 AM, nicogineau <nicolas.gineau at productys.com>wrote:

> - Fire of a trigger on modification, which updates a field of the B table
> on
> the Workshop --> OK,
> - Here is my problem: the modification of the field of the B table on the
> Workshop is not replicated on the Server.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121001/faadacf0/attachment.html 

From ssinger at ca.afilias.info  Mon Oct  1 10:42:28 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 01 Oct 2012 13:42:28 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5065EF02.2010007@consistentstate.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
	<5064C487.6040709@Yahoo.com> <5065A2EA.3030606@ca.afilias.info>
	<5065C0DF.60609@Yahoo.com> <5065C270.9090109@ca.afilias.info>
	<5065EF02.2010007@consistentstate.com>
Message-ID: <5069D604.5090100@ca.afilias.info>

On 12-09-28 02:40 PM, Brian Fehrle wrote:

> Is there anything I can provide that could help us dig into this more?
> Are we counting this as a bug that may have been fixed in 2.2(haven't
> looked at what you mentioned yet Steve)?

My best guess is that this like the situation that Jan described where 
one thread was processing/adding a SYNC from the dropped node at the 
same time a different thread in the same slon was processing the DROP 
NODE.

My understanding is that the automatic WAIT FOR in 2.1 only waits for 
non-sync events to propogate everywhere before starting with the DROP 
NODE.  I *think* this has been fixed in 2.2 but 2.2 does need more testing.



>
> Thanks,
> - Brian F
>
>>
>>>
>>> Jan
>>>
>>
>


From JanWieck at Yahoo.com  Mon Oct  1 12:36:30 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon, 01 Oct 2012 15:36:30 -0400
Subject: [Slony1-general] Issue when adding node to replication
In-Reply-To: <5065EF02.2010007@consistentstate.com>
References: <50649C28.2010002@consistentstate.com> <5064A859.9030606@Yahoo.com>
	<5064A8F0.9080801@consistentstate.com>
	<5064AFC8.6080105@consistentstate.com>
	<5064C487.6040709@Yahoo.com> <5065A2EA.3030606@ca.afilias.info>
	<5065C0DF.60609@Yahoo.com> <5065C270.9090109@ca.afilias.info>
	<5065EF02.2010007@consistentstate.com>
Message-ID: <5069F0BE.4060807@Yahoo.com>

On 9/28/2012 2:40 PM, Brian Fehrle wrote:
> I'm going to go ahead and delete the offending row in sl_event for the
> node that doesn't exist. This is a production environment so I need to
> get up and running again.

Deleting a SYNC event of a non-origin node can never have any negative 
side effects. They are noise that keeps sl_status looking good, but 
that's really all they do.

SYNC events from a set origin are a little different. A SYNC immediately 
before an EXECUTE_SCRIPT event prior to 2.2 may be important. Other than 
that, they don't matter that much either.

When a node has fallen behind, it deliberately skips a number of SYNC 
events, a feature we call sync-grouping, to catch up in larger leaps. 
There is no difference at all between slon grouping SYNC events and some 
SYNC event rows missing in sl_event. Absolutely zero difference.

The difference for the SYNC event from a set origin in front of an 
EXECUTE_SCRIPT is because that DDL may add a column to a table. That 
SYNC event was supposed to make sure that the column exists on a 
subscriber when the first data containing that column appeared in 
sl_log_N. That all changed in 2.2 as well, where DDL from script 
execution now travels within sl_log_N too, so as of 2.2 you can simply 
delete all SYNC events from sl_event if you feel like. Slon will 
generate another SYNC soon and the processing of that will just leap the 
receivers to that state.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From david at mudah.my  Wed Oct  3 01:58:02 2012
From: david at mudah.my (David Chin)
Date: Wed, 3 Oct 2012 08:58:02 +0000 (UTC)
Subject: [Slony1-general]
	=?utf-8?q?problem_with_slonik=5Fmove=5Fset_only_?=
	=?utf-8?q?changing_the_subscription_on_one_node?=
References: <4845263F.5030401@albourne.com>
Message-ID: <loom.20121003T105626-572@post.gmane.org>

Hi Martin,

It seems like we share the sentiment. It seems like it has become a cascaded
replication.

Did you manage to resolve this? Can share?

-David


From nyamada at millburncorp.com  Thu Oct  4 11:47:12 2012
From: nyamada at millburncorp.com (Norman Yamada)
Date: Thu, 4 Oct 2012 14:47:12 -0400
Subject: [Slony1-general] pg_reorg and reorganizing tables in slony cluster
Message-ID: <434FA33C-7A0B-4B85-92D6-44533C19B163@millburncorp.com>

Dear all -- I've discovered a problem using pg_reorg 1.1.7 with Slony versions 2.0 and/or 2.1. 

If you run a reorg on a table that is part of a slony slave set while there is write activity on the master node, pg_reorg will silently drop changes committed by slony to the slave node during the reorganization.

The problem is that the z_reorg_trigger doesn't fire on the changes that slony commits to the slave table, because slony has a session_replication_role on the slave of REPLICA. 

To get around this, after the z_reorg_trigger is created on a table, you have to run the command

ALTER TABLE [table_name] ENABLE ALWAYS TRIGGER z_reorg_trigger. Then the log table will capture changes pushed by Slony (or other trigger-based replication programs) during the time of reorganizing the table.

This is a subtle problem, because pg_reorg works fine with reorganizing tables on the _master_ node of a slony set, but it will drop changes on the slave nodes silently.

If I have time, I'll try to write at least a rough patch for pg_reorg to fix this, but I'm hoping that both the pg_reorg and slony community can be aware of this problem. Until it's fixed, best practice would be to stop slony while reorganizing tables on a slave node.

Best,

Norman Yamada

(Cross-posted to reorg_general at pgfoundry.org)

From mjames at profitpoint.com  Thu Oct  4 11:52:28 2012
From: mjames at profitpoint.com (Mike James)
Date: Thu, 4 Oct 2012 18:52:28 +0000
Subject: [Slony1-general] offline log shipping errors
Message-ID: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>

Hi, our DBA stopped log shipping to create a new replication set. When he resumed log shipping, the next sequence number on the offline node is not consistent with the new log sequence numbers coming from the cluster. This is my 1st experience using Postgres/Slony and I don't know how to resolve this. I have not found a straightforward resolution of this in Google searches - probably I'm using the wrong keywords.

Please point me in the right direction. Thanks, Mike
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121004/7bbe681b/attachment.htm 

From ssinger at ca.afilias.info  Thu Oct  4 12:02:09 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 04 Oct 2012 15:02:09 -0400
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <506DDD31.20507@ca.afilias.info>

On 12-10-04 02:52 PM, Mike James wrote:
> Hi, our DBA stopped log shipping to create a new replication set. When
> he resumed log shipping, the next sequence number on the offline node is
> not consistent with the new log sequence numbers coming from the
> cluster. This is my 1^st experience using Postgres/Slony and I don?t
> know how to resolve this. I have not found a straightforward resolution
> of this in Google searches ? probably I?m using the wrong keywords.
>

What exactly do you mean by 'stopped log shipping' ?

Do you mean that he shutdown the slon then restarted it without the -a 
option?

If so I think you will need to rebuild the offline node from scratch 
with slony1_dump.sh because there is no longer a record of the changes 
that the slon had processed when it was running without the -a option.




> Please point me in the right direction. Thanks, Mike
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From mjames at profitpoint.com  Thu Oct  4 12:13:08 2012
From: mjames at profitpoint.com (Mike James)
Date: Thu, 4 Oct 2012 19:13:08 +0000
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <506DDD31.20507@ca.afilias.info>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
Message-ID: <F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>

Well, if I had to guess, that's what I would think. The previous sysadmin created some scripts to manage slony, but I'm not sure that they're working effectively.

Mike

-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info] 
Sent: Thursday, October 04, 2012 3:02 PM
To: Mike James
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] offline log shipping errors

On 12-10-04 02:52 PM, Mike James wrote:
> Hi, our DBA stopped log shipping to create a new replication set. When 
> he resumed log shipping, the next sequence number on the offline node 
> is not consistent with the new log sequence numbers coming from the 
> cluster. This is my 1^st experience using Postgres/Slony and I don't 
> know how to resolve this. I have not found a straightforward 
> resolution of this in Google searches - probably I'm using the wrong keywords.
>

What exactly do you mean by 'stopped log shipping' ?

Do you mean that he shutdown the slon then restarted it without the -a option?

If so I think you will need to rebuild the offline node from scratch with slony1_dump.sh because there is no longer a record of the changes that the slon had processed when it was running without the -a option.




> Please point me in the right direction. Thanks, Mike
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>




From mark.steben at drivedominion.com  Thu Oct  4 12:38:37 2012
From: mark.steben at drivedominion.com (Mark Steben)
Date: Thu, 4 Oct 2012 15:38:37 -0400
Subject: [Slony1-general] slony init_cluster problem
Message-ID: <CADyzmyzGFWNY2uFtB9RRgA7VA+enRP3XPhOruDsFrN0Bih=eOg@mail.gmail.com>

*Hi, trying to initialize a new cluster using the alt-perl scripts provided
 in the install
Here are the specs

MASTER:
linux installed on a VM setup.
(The VM is installed on a Windows 7 OS running on a HP Elitebook laptop)
   8.0.4 build-744019
 the uname -a command:
Linux localhost.localdomain 2.6.18-308.13.1.el5 #1 SMP Tue Aug 21 17:10:18
EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
**total shared memory available: 2 GB
postgres installed: 8.3.21 (with enable_thread_safety)
slony installed: 2.1.1

***
*SLAVE:
linux installed on a dell desktop
(The primary OS on this machine is linux)
 the uname -a command:
Linux msteben-centos.autorevenue.com 2.6.1**8-194.11.4.el5 #1 SMP Tue Sep
21 05:04:09 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux
**total shared memory available: 2 GB
**postgres installed: 8.3.21 (with enable_thread_safety)
slony installed: 2.1.1

PROBLEM:
 I attempt to run the altperl script ./slonik_init_cluster from the slave
  the connection gets through to the master, but all it does on the master
is
  print the replication schema (all the sl tables) over and over and over
again
  in the postgres logs.  It never gets to creating the _reptest schema, then
  defining the sl_tables.  When I do a query on pg_stat_activity on the
master
  the one query actually running is the CREATE TABLE sl_nodes statement.
It never
  finishes.  I finally have to kill the connection and nothing gets done.

  I've attached the postgresql.conf from both servers as well as the
slon_tools.conf
  from the slave.  The admin user referenced in the slon_tools is slony and
  is defined as a superuser.

  Any help appreciated.  Any other info you need, please let me know.

Mark Steben
 DBA at AutoRevenue    *
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121004/7d7a3e59/attachment.htm 
-------------- next part --------------
# 
# Author: Christopher Browne
# Copyright 2004-2009 Afilias Canada
# Revised extensively by Steve Simms

# Keeping the following three lines for backwards compatibility in
# case this gets incorporated into a 1.0.6 release.
#
# TODO: The scripts should check for an environment variable
# containing the location of a configuration file.  That would
# simplify this configuration file and allow Slony-I tools to still work
# in situations where it doesn't exist.
#
if ($ENV{"SLONYNODES"}) {
    require $ENV{"SLONYNODES"};
} else {

    # The name of the replication cluster.  This will be used to
    # create a schema named _$CLUSTER_NAME in the database which will
    # contain Slony-related data.
    $CLUSTER_NAME = 'reptest';

    # The directory where Slony should record log messages.  This
    # directory will need to be writable by the user that invokes
    # Slony.
    # $LOGDIR = '/var/log/slony1';
     $LOGDIR = '/home/postgres/slony1';


    # (Optional) If you would like to use Apache's rotatelogs tool to
    # manage log output, uncomment the following line and ensure that
    # it points to the executable.
    #
    # $APACHE_ROTATOR = '/usr/local/apache/bin/rotatelogs';

    # Log line suffix for Slony-I log. For options, look at date(1) 
    # man page.
    #
    # $LOG_NAME_SUFFIX = '%a';

    # SYNC check interval (slon -s option)
    # $SYNC_CHECK_INTERVAL = 1000;

    # Which node is the default master for all sets?
    $MASTERNODE = 1;

    # Which debugging level to use?  [0-4]
    $DEBUGLEVEL = 0;

    # Include add_node lines for each node in the cluster.  Be sure to
    # use host names that will resolve properly on all nodes
    # (i.e. only use 'localhost' if all nodes are on the same host).
    # Also, note that the user must be a superuser account.

    add_node(node     => 1,
             host     => '10.10.6.136',
             dbname   => 'slonytst',
             port     => 5432,
             user     => 'slony',
             password => 'elephant1234');

    add_node(node     => 2,
             host     => '10.10.4.34',
             dbname   => 'slonytst',
             port     => 5432,
             user     => 'slony',
             password => 'elephant1234');

#    add_node(node     => 3,
#            host     => 'server3',
#            dbname   => 'database',
#            port     => 5432,
#            user     => 'postgres',
#            password => '');

    # If the node should only receive event notifications from a
    # single node (e.g. if it can't access the other nodes), you can
    # specify a single parent.  The downside to this approach is that
    # if the parent goes down, your node becomes stranded.

#    add_node(node     => 4,
#            parent   => 3,
#            host     => 'server4',
#            dbname   => 'database',
#            port     => 5432,
#            user     => 'postgres',
#             password => '');

}

# The $SLONY_SETS variable contains information about all of the sets
# in your cluster.

$SLONY_SETS = {
    "seta" => {
        "set_id" => 1,
        "table_id"    => 1,
        "sequence_id" => 1,
        "pkeyedtables" => [
"public.clients",
"public.queues",
"public.email_history",
"public.mailer_queue",
"public.queuenodes",
"public.states",
"public.emailrcpt_mileage_history",
],

"sequences" =>
    [
 "mmseq",
 "mailpollseq",
 "mailer_queue_mailer_queue_pk_seq",
 "email_history_id_seq",
 "emailrcpt_mileage_history_pk_id_seq",
   ]  
},
   
#    "set2" => {
#       "set_id"       => 2,
#       "table_id"     => 389,
#       "sequence_id"  => 204,
#       "pkeyedtables" => ["public.queuenodes"],
#       "keyedtables"  => {},
#       "sequences"    => [],
#    },

};

# Keeping the following three lines for backwards compatibility in
# case this gets incorporated into a 1.0.6 release.
#
# TODO: The scripts should check for an environment variable
# containing the location of a configuration file.  That would
# simplify this configuration file and allow Slony tools to still work
# in situations where it doesn't exist.
#
if ($ENV{"SLONYSET"}) {
    require $ENV{"SLONYSET"};
}

# Please do not add or change anything below this point.
1;
-------------- next part --------------
A non-text attachment was scrubbed...
Name: postgresql.conf.slave
Type: application/octet-stream
Size: 16851 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20121004/7d7a3e59/attachment-0002.obj 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: postgresql.conf.master
Type: application/octet-stream
Size: 16740 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20121004/7d7a3e59/attachment-0003.obj 

From mjames at profitpoint.com  Thu Oct  4 13:11:44 2012
From: mjames at profitpoint.com (Mike James)
Date: Thu, 4 Oct 2012 20:11:44 +0000
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>

OK, I was mistaken. The slon on the subscriber that ships the logs was restarted with the -a option. The slon on the Master was not started with the -a option.

Mike

-----Original Message-----
From: slony1-general-bounces at lists.slony.info [mailto:slony1-general-bounces at lists.slony.info] On Behalf Of Mike James
Sent: Thursday, October 04, 2012 3:13 PM
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] offline log shipping errors

Well, if I had to guess, that's what I would think. The previous sysadmin created some scripts to manage slony, but I'm not sure that they're working effectively.

Mike

-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info]
Sent: Thursday, October 04, 2012 3:02 PM
To: Mike James
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] offline log shipping errors

On 12-10-04 02:52 PM, Mike James wrote:
> Hi, our DBA stopped log shipping to create a new replication set. When 
> he resumed log shipping, the next sequence number on the offline node 
> is not consistent with the new log sequence numbers coming from the 
> cluster. This is my 1^st experience using Postgres/Slony and I don't 
> know how to resolve this. I have not found a straightforward 
> resolution of this in Google searches - probably I'm using the wrong keywords.
>

What exactly do you mean by 'stopped log shipping' ?

Do you mean that he shutdown the slon then restarted it without the -a option?

If so I think you will need to rebuild the offline node from scratch with slony1_dump.sh because there is no longer a record of the changes that the slon had processed when it was running without the -a option.




> Please point me in the right direction. Thanks, Mike
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>



_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



From ssinger at ca.afilias.info  Thu Oct  4 14:39:17 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 04 Oct 2012 17:39:17 -0400
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <506E0205.5020905@ca.afilias.info>

On 12-10-04 04:11 PM, Mike James wrote:
> OK, I was mistaken. The slon on the subscriber that ships the logs was restarted with the -a option. The slon on the Master was not started with the -a option.

Then I'm a bit confused about steps were done that resulted in log 
shipping getting out of sync.

1. If setup log shipping and started slon with -a on the slave
2. Added some tables to replication

I would expect log shipping to continue to work.




>
> Mike
>
> -----Original Message-----
> From: slony1-general-bounces at lists.slony.info [mailto:slony1-general-bounces at lists.slony.info] On Behalf Of Mike James
> Sent: Thursday, October 04, 2012 3:13 PM
> Cc: slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] offline log shipping errors
>
> Well, if I had to guess, that's what I would think. The previous sysadmin created some scripts to manage slony, but I'm not sure that they're working effectively.
>
> Mike
>
> -----Original Message-----
> From: Steve Singer [mailto:ssinger at ca.afilias.info]
> Sent: Thursday, October 04, 2012 3:02 PM
> To: Mike James
> Cc: slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] offline log shipping errors
>
> On 12-10-04 02:52 PM, Mike James wrote:
>> Hi, our DBA stopped log shipping to create a new replication set. When
>> he resumed log shipping, the next sequence number on the offline node
>> is not consistent with the new log sequence numbers coming from the
>> cluster. This is my 1^st experience using Postgres/Slony and I don't
>> know how to resolve this. I have not found a straightforward
>> resolution of this in Google searches - probably I'm using the wrong keywords.
>>
>
> What exactly do you mean by 'stopped log shipping' ?
>
> Do you mean that he shutdown the slon then restarted it without the -a option?
>
> If so I think you will need to rebuild the offline node from scratch with slony1_dump.sh because there is no longer a record of the changes that the slon had processed when it was running without the -a option.
>
>
>
>
>> Please point me in the right direction. Thanks, Mike
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From ragavendra.dba at gmail.com  Thu Oct  4 21:54:29 2012
From: ragavendra.dba at gmail.com (Raghav)
Date: Fri, 5 Oct 2012 10:24:29 +0530
Subject: [Slony1-general] slony init_cluster problem
In-Reply-To: <CADyzmyzGFWNY2uFtB9RRgA7VA+enRP3XPhOruDsFrN0Bih=eOg@mail.gmail.com>
References: <CADyzmyzGFWNY2uFtB9RRgA7VA+enRP3XPhOruDsFrN0Bih=eOg@mail.gmail.com>
Message-ID: <CANwAqWghun3J9weE2a5bAPQDAiiakhTr0+Ea+SQb06je6szABQ@mail.gmail.com>

On Fri, Oct 5, 2012 at 1:08 AM, Mark Steben
<mark.steben at drivedominion.com>wrote:

> *Hi, trying to initialize a new cluster using the alt-perl scripts
> provided
>  in the install
> Here are the specs
>
> MASTER:
> linux installed on a VM setup.
> (The VM is installed on a Windows 7 OS running on a HP Elitebook laptop)
>    8.0.4 build-744019
>  the uname -a command:
> Linux localhost.localdomain 2.6.18-308.13.1.el5 #1 SMP Tue Aug 21 17:10:18
> EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
> **total shared memory available: 2 GB
> postgres installed: 8.3.21 (with enable_thread_safety)
> slony installed: 2.1.1
>
> ***
> *SLAVE:
> linux installed on a dell desktop
> (The primary OS on this machine is linux)
>  the uname -a command:
> Linux msteben-centos.autorevenue.com 2.6.1**8-194.11.4.el5 #1 SMP Tue Sep
> 21 05:04:09 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux
> **total shared memory available: 2 GB
> **postgres installed: 8.3.21 (with enable_thread_safety)
> slony installed: 2.1.1
>
> PROBLEM:
>  I attempt to run the altperl script ./slonik_init_cluster from the slave
>   the connection gets through to the master, but all it does on the master
> is
>   print the replication schema (all the sl tables) over and over and over
> again
>   in the postgres logs.  It never gets to creating the _reptest schema,
> then
>   defining the sl_tables.  When I do a query on pg_stat_activity on the
> master
>   the one query actually running is the CREATE TABLE sl_nodes statement.
> It never
>   finishes.  I finally have to kill the connection and nothing gets done.
>
>   I've attached the postgresql.conf from both servers as well as the
> slon_tools.conf
>   from the slave.  The admin user referenced in the slon_tools is slony and
>   is defined as a superuser.
>
>   Any help appreciated.  Any other info you need, please let me know.
>
> Mark Steben
>  DBA at AutoRevenue    *
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
Before doing any guesses, slony logs(also pg_logs) would be helpful to know
why exactly CREATE TABLE SL_NODE is in waiting state.

-- 
Regards
Raghav
Blog: htt://raghavt.blogspot.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121005/ea0cf253/attachment.htm 

From mjames at profitpoint.com  Fri Oct  5 05:37:45 2012
From: mjames at profitpoint.com (Mike James)
Date: Fri, 5 Oct 2012 12:37:45 +0000
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <506E0205.5020905@ca.afilias.info>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506E0205.5020905@ca.afilias.info>
Message-ID: <F024DCE3402750409CD407C414C07E1A8CAFF8@BY2PRD0811MB404.namprd08.prod.outlook.com>

Here's what the DBA did as I understand it. This is his procedure for changing the replication set. Slony version is slony1-2.0.3-rc. Postgres 8.3.9 on the Master. I might be mis-using some of the terminology, as I'm not a DBA and this is my 1st foray with slony. 
1. Kill the slon running on the master and slave. Leave the slon on the remote host running.
2. he uses PGAdmin to remove the slony schema (slony replication cluster) from Master and Slave.
3. regenerate the set records using a shell script on the Master.
4. use slonik to initialize the cluster with the set records from above on Master and Slave.
5. start the slon on the Master.
6. start the slon on the Slave with -a option.
7. use slonik to subscribe the Slave to the Master.

On the remote postgres server, the value of at_counter in the _cluster1.sl_archive_tracking table is 338005. But it looks like the new logs being shipped started over at sequence number 1.

Does that description make sense? This can't be the optimal way to add a table to the replication set!

Mike

-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info] 
Sent: Thursday, October 04, 2012 5:39 PM
To: Mike James
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] offline log shipping errors

On 12-10-04 04:11 PM, Mike James wrote:
> OK, I was mistaken. The slon on the subscriber that ships the logs was restarted with the -a option. The slon on the Master was not started with the -a option.

Then I'm a bit confused about steps were done that resulted in log shipping getting out of sync.

1. If setup log shipping and started slon with -a on the slave 2. Added some tables to replication

I would expect log shipping to continue to work.




From mark.steben at drivedominion.com  Fri Oct  5 06:25:00 2012
From: mark.steben at drivedominion.com (Mark Steben)
Date: Fri, 5 Oct 2012 09:25:00 -0400
Subject: [Slony1-general] Fwd:  slony init_cluster problem
In-Reply-To: <CADyzmyzoJ38mGRaLPDJQtHzMLHA442YrQak4i3GeHWaXpbjJzw@mail.gmail.com>
References: <CADyzmyzGFWNY2uFtB9RRgA7VA+enRP3XPhOruDsFrN0Bih=eOg@mail.gmail.com>
	<CANwAqWghun3J9weE2a5bAPQDAiiakhTr0+Ea+SQb06je6szABQ@mail.gmail.com>
	<CADyzmyzoJ38mGRaLPDJQtHzMLHA442YrQak4i3GeHWaXpbjJzw@mail.gmail.com>
Message-ID: <CADyzmyxD3+GQfkFXd11JUOBjk4OCz45o3fW1LBqJC7_nXX_9hA@mail.gmail.com>

*Sorry for redundancy: forgot to include the slony1-general list in my
first response
*
---------- Forwarded message ----------
From: Mark Steben <mark.steben at drivedominion.com>
Date: Fri, Oct 5, 2012 at 9:22 AM
Subject: Re: [Slony1-general] slony init_cluster problem
To: Raghav <ragavendra.dba at gmail.com>


*First of all, I apologize for the excessive length of my first query.
I'll try to keep everything short and sweet.

In response to Raghav's query: no slony logs have been created yet because
I haven't issued
a start_slon statement. Can't do that till I initialize a cluster.
**Also, all the pg_logs show are the printouts of the script to create the
schema.
Finally, check the pg_stat_activity listing I attached.
The CREATE table for sl_node is not in
 a wait state.
*

On Fri, Oct 5, 2012 at 12:54 AM, Raghav <ragavendra.dba at gmail.com> wrote:

>
> On Fri, Oct 5, 2012 at 1:08 AM, Mark Steben <mark.steben at drivedominion.com
> > wrote:
>
>> *Hi, trying to initialize a new cluster using the alt-perl scripts
>> provided
>>  in the install
>> Here are the specs
>>
>> MASTER:
>> linux installed on a VM setup.
>> (The VM is installed on a Windows 7 OS running on a HP Elitebook laptop)
>>    8.0.4 build-744019
>>  the uname -a command:
>> Linux localhost.localdomain 2.6.18-308.13.1.el5 #1 SMP Tue Aug 21
>> 17:10:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
>> **total shared memory available: 2 GB
>> postgres installed: 8.3.21 (with enable_thread_safety)
>> slony installed: 2.1.1
>>
>> ***
>> *SLAVE:
>> linux installed on a dell desktop
>> (The primary OS on this machine is linux)
>>  the uname -a command:
>> Linux msteben-centos.autorevenue.com 2.6.1**8-194.11.4.el5 #1 SMP Tue
>> Sep 21 05:04:09 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux
>> **total shared memory available: 2 GB
>> **postgres installed: 8.3.21 (with enable_thread_safety)
>> slony installed: 2.1.1
>>
>> PROBLEM:
>>  I attempt to run the altperl script ./slonik_init_cluster from the slave
>>   the connection gets through to the master, but all it does on the
>> master is
>>   print the replication schema (all the sl tables) over and over and over
>> again
>>   in the postgres logs.  It never gets to creating the _reptest schema,
>> then
>>   defining the sl_tables.  When I do a query on pg_stat_activity on the
>> master
>>   the one query actually running is the CREATE TABLE sl_nodes statement.
>> It never
>>   finishes.  I finally have to kill the connection and nothing gets done.
>>
>>   I've attached the postgresql.conf from both servers as well as the
>> slon_tools.conf
>>   from the slave.  The admin user referenced in the slon_tools is slony
>> and
>>   is defined as a superuser.
>>
>>   Any help appreciated.  Any other info you need, please let me know.
>>
>> Mark Steben
>>  DBA at AutoRevenue    *
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
> Before doing any guesses, slony logs(also pg_logs) would be helpful to
> know why exactly CREATE TABLE SL_NODE is in waiting state.
>
> --
> Regards
> Raghav
> Blog: htt://raghavt.blogspot.com/
>
>


-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase<http://www.autobase.net/>

  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>







-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase<http://www.autobase.net/>

  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121005/ff777af3/attachment-0001.htm 
-------------- next part --------------
-[ RECORD 2 ]-+--------------------------------------------------------------------------------------------------
datid         | 469329490
datname       | slonytst
procpid       | 6042
usesysid      | 16385
usename       | slony
current_query | -- ----------------------------------------------------------------------
              : -- slony1_base.sql
              : --
              : --    Declaration of the basic replication schema.
              : --
              : --      Copyright (c) 2003-2009, PostgreSQL Global Development Group
              : --      Author: Jan Wieck, Afilias USA INC.
              : --
              : -- 
              : -- ----------------------------------------------------------------------
              : 
              : 
              : -- **********************************************************************
              : -- * Tables
              : -- **********************************************************************
              : 
              : 
              : -- ----------------------------------------------------------------------
              : -- TABLE sl_node
              : -- ----------------------------------------------------------------------
              : create table "_reptest".sl_node (
              :         no_id                           int4,
              :         no_active                       bool,
              :         no_comment                      text,
              :         CONSTRAINT "sl_node-pkey"
              :                 PRIMARY KEY (no_id)
              : ) WITHOUT OIDS;
              : comment on table "_reptest".sl_node is 'Holds the list of nodes associated with this namespace.';
              : comment on column "_reptest".sl_node.no_id is 'The unique ID number for the node';  
              : comment on column "_rep
waiting       | f
xact_start    | 2012-10-05 09:05:36.20057-04
query_start   | 2012-10-05 09:05:36.446774-04
backend_start | 2012-10-05 09:05:35.990462-04
client_addr   | 10.10.4.34
client_port   | 35521


From ssinger at ca.afilias.info  Fri Oct  5 06:38:15 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 05 Oct 2012 09:38:15 -0400
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <F024DCE3402750409CD407C414C07E1A8CAFF8@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506E0205.5020905@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8CAFF8@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <506EE2C7.9030204@ca.afilias.info>

On 12-10-05 08:37 AM, Mike James wrote:
> Here's what the DBA did as I understand it. This is his procedure for changing the replication set. Slony version is slony1-2.0.3-rc. Postgres 8.3.9 on the Master. I might be mis-using some of the terminology, as I'm not a DBA and this is my 1st foray with slony.
> 1. Kill the slon running on the master and slave. Leave the slon on the remote host running.
> 2. he uses PGAdmin to remove the slony schema (slony replication cluster) from Master and Slave.
> 3. regenerate the set records using a shell script on the Master.
> 4. use slonik to initialize the cluster with the set records from above on Master and Slave.
> 5. start the slon on the Master.
> 6. start the slon on the Slave with -a option.
> 7. use slonik to subscribe the Slave to the Master.
>
> On the remote postgres server, the value of at_counter in the _cluster1.sl_archive_tracking table is 338005. But it looks like the new logs being shipped started over at sequence number 1.
>
> Does that description make sense? This can't be the optimal way to add a table to the replication set!
>
What you describe makes perfect sense.  Step 2, removing the slony 
schema uninstalls slony from both the master and the slave.  When you 
reinstall slony with step 4 you are creating a new slony cluster.  The 
offline node isn't in sync with the new cluster it was sync'd with the 
old one.  You will need to rebuild your offline node with slony1_dump.sh

The proper way to add tables to a replication set is described here
http://www.slony.info/documentation/2.0/administration.html#ADDTHINGS

If your using the altperl tools you can look at slonik_create_set, 
slonik_subscribe_set and slonik_merge_sets



> Mike
>
> -----Original Message-----
> From: Steve Singer [mailto:ssinger at ca.afilias.info]
> Sent: Thursday, October 04, 2012 5:39 PM
> To: Mike James
> Cc: slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] offline log shipping errors
>
> On 12-10-04 04:11 PM, Mike James wrote:
>> OK, I was mistaken. The slon on the subscriber that ships the logs was restarted with the -a option. The slon on the Master was not started with the -a option.
>
> Then I'm a bit confused about steps were done that resulted in log shipping getting out of sync.
>
> 1. If setup log shipping and started slon with -a on the slave 2. Added some tables to replication
>
> I would expect log shipping to continue to work.
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From mjames at profitpoint.com  Fri Oct  5 11:46:05 2012
From: mjames at profitpoint.com (Mike James)
Date: Fri, 5 Oct 2012 18:46:05 +0000
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <506EE2C7.9030204@ca.afilias.info>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506E0205.5020905@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8CAFF8@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506EE2C7.9030204@ca.afilias.info>
Message-ID: <F024DCE3402750409CD407C414C07E1A8CE176@BY2PRD0811MB404.namprd08.prod.outlook.com>

Newbie alert...  Ok, I'm following the online docs (http://slony.info/documentation/2.0/logshipping.html#AEN1572). I started the subscriber slon with the -a option. I ran the slony1_dump.sh script and captured the output to a file. Copied the file to the remote server. 

So, my next steps are:
1. stop slon on the remote server, which is out-of-sync anyway.
2. do I need to drop the out-of-sync database from the remote server? 
3. what psql commandline do I need to run on the remote server? I realize this is pretty basic, but as noted, I'm not the DBA. He's on vacation this week. :(
4. If I then restart slon on the remote server, will it catch up? 

Thanks for your patience! 
Mike


-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info] 
Sent: Friday, October 05, 2012 9:38 AM
To: Mike James
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] offline log shipping errors

On 12-10-05 08:37 AM, Mike James wrote:
> Here's what the DBA did as I understand it. This is his procedure for changing the replication set. Slony version is slony1-2.0.3-rc. Postgres 8.3.9 on the Master. I might be mis-using some of the terminology, as I'm not a DBA and this is my 1st foray with slony.
> 1. Kill the slon running on the master and slave. Leave the slon on the remote host running.
> 2. he uses PGAdmin to remove the slony schema (slony replication cluster) from Master and Slave.
> 3. regenerate the set records using a shell script on the Master.
> 4. use slonik to initialize the cluster with the set records from above on Master and Slave.
> 5. start the slon on the Master.
> 6. start the slon on the Slave with -a option.
> 7. use slonik to subscribe the Slave to the Master.
>
> On the remote postgres server, the value of at_counter in the _cluster1.sl_archive_tracking table is 338005. But it looks like the new logs being shipped started over at sequence number 1.
>
> Does that description make sense? This can't be the optimal way to add a table to the replication set!
>
What you describe makes perfect sense.  Step 2, removing the slony schema uninstalls slony from both the master and the slave.  When you reinstall slony with step 4 you are creating a new slony cluster.  The offline node isn't in sync with the new cluster it was sync'd with the old one.  You will need to rebuild your offline node with slony1_dump.sh

The proper way to add tables to a replication set is described here http://www.slony.info/documentation/2.0/administration.html#ADDTHINGS

If your using the altperl tools you can look at slonik_create_set, slonik_subscribe_set and slonik_merge_sets



From ssinger at ca.afilias.info  Sat Oct  6 07:09:42 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Sat, 06 Oct 2012 10:09:42 -0400
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <F024DCE3402750409CD407C414C07E1A8CE176@BY2PRD0811MB404.namprd08.prod.outlook.com>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506E0205.5020905@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8CAFF8@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506EE2C7.9030204@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8CE176@BY2PRD0811MB404.namprd08.prod.outlook.com>
Message-ID: <50703BA6.2050304@ca.afilias.info>

On 12-10-05 02:46 PM, Mike James wrote:
> Newbie alert...  Ok, I'm following the online docs (http://slony.info/documentation/2.0/logshipping.html#AEN1572). I started the subscriber slon with the -a option. I ran the slony1_dump.sh script and captured the output to a file. Copied the file to the remote server.
>
> So, my next steps are:
> 1. stop slon on the remote server, which is out-of-sync anyway.

I'm a bit confused again.   Which server is your 'remote' server, you 
previously were talking about a master server, a slave server and an 
offline node.

The slave server that is running the slon instance with '-a' is part of 
your replication set and should be up-to-date.

> 2. do I need to drop the out-of-sync database from the remote server?


I don't think you need to drop the database from the offline/log 
shipping target node.  If you run the slony1_dump.sh script the output 
it generates should truncate all the tables in the offine databsae and 
re-seed them.



> 3. what psql commandline do I need to run on the remote server? I realize this is pretty basic, but as noted, I'm not the DBA. He's on vacation this week. :(
> 4. If I then restart slon on the remote server, will it catch up?
>

If you restart the slon node on the slave server it should catch up, you 
shouldn't need to run any extra psql commands.


You then what to run slony1_dump.sh against the slave database and 
capture redirect the output of that to psql.

I *think* something like
slony1_dump.sh mydbname myclustername | psql -h offline_host_name mydbname

is what you want.  You would then resume the logshipping apply daemon.



> Thanks for your patience!
> Mike
>
>
> -----Original Message-----
> From: Steve Singer [mailto:ssinger at ca.afilias.info]
> Sent: Friday, October 05, 2012 9:38 AM
> To: Mike James
> Cc: slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] offline log shipping errors
>
> On 12-10-05 08:37 AM, Mike James wrote:
>> Here's what the DBA did as I understand it. This is his procedure for changing the replication set. Slony version is slony1-2.0.3-rc. Postgres 8.3.9 on the Master. I might be mis-using some of the terminology, as I'm not a DBA and this is my 1st foray with slony.
>> 1. Kill the slon running on the master and slave. Leave the slon on the remote host running.
>> 2. he uses PGAdmin to remove the slony schema (slony replication cluster) from Master and Slave.
>> 3. regenerate the set records using a shell script on the Master.
>> 4. use slonik to initialize the cluster with the set records from above on Master and Slave.
>> 5. start the slon on the Master.
>> 6. start the slon on the Slave with -a option.
>> 7. use slonik to subscribe the Slave to the Master.
>>
>> On the remote postgres server, the value of at_counter in the _cluster1.sl_archive_tracking table is 338005. But it looks like the new logs being shipped started over at sequence number 1.
>>
>> Does that description make sense? This can't be the optimal way to add a table to the replication set!
>>
> What you describe makes perfect sense.  Step 2, removing the slony schema uninstalls slony from both the master and the slave.  When you reinstall slony with step 4 you are creating a new slony cluster.  The offline node isn't in sync with the new cluster it was sync'd with the old one.  You will need to rebuild your offline node with slony1_dump.sh
>
> The proper way to add tables to a replication set is described here http://www.slony.info/documentation/2.0/administration.html#ADDTHINGS
>
> If your using the altperl tools you can look at slonik_create_set, slonik_subscribe_set and slonik_merge_sets
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From mjames at profitpoint.com  Mon Oct  8 11:51:20 2012
From: mjames at profitpoint.com (Mike James)
Date: Mon, 8 Oct 2012 18:51:20 +0000
Subject: [Slony1-general] offline log shipping errors
In-Reply-To: <50703BA6.2050304@ca.afilias.info>
References: <F024DCE3402750409CD407C414C07E1A8C3D6E@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506DDD31.20507@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8C3FEB@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<F024DCE3402750409CD407C414C07E1A8C40CC@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506E0205.5020905@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8CAFF8@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<506EE2C7.9030204@ca.afilias.info>
	<F024DCE3402750409CD407C414C07E1A8CE176@BY2PRD0811MB404.namprd08.prod.outlook.com>
	<50703BA6.2050304@ca.afilias.info>
Message-ID: <F024DCE3402750409CD407C414C07E1A8D44C4@BY2PRD0811MB404.namprd08.prod.outlook.com>

Thanks, you've been incredibly patient.

-----Original Message-----
From: Steve Singer [mailto:ssinger at ca.afilias.info] 
Sent: Saturday, October 06, 2012 10:10 AM
To: Mike James
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] offline log shipping errors

On 12-10-05 02:46 PM, Mike James wrote:
> Newbie alert...  Ok, I'm following the online docs (http://slony.info/documentation/2.0/logshipping.html#AEN1572). I started the subscriber slon with the -a option. I ran the slony1_dump.sh script and captured the output to a file. Copied the file to the remote server.
>
> So, my next steps are:
> 1. stop slon on the remote server, which is out-of-sync anyway.

I'm a bit confused again.   Which server is your 'remote' server, you 
previously were talking about a master server, a slave server and an offline node.

The slave server that is running the slon instance with '-a' is part of your replication set and should be up-to-date.

> 2. do I need to drop the out-of-sync database from the remote server?


I don't think you need to drop the database from the offline/log shipping target node.  If you run the slony1_dump.sh script the output it generates should truncate all the tables in the offine databsae and re-seed them.



> 3. what psql commandline do I need to run on the remote server? I 
> realize this is pretty basic, but as noted, I'm not the DBA. He's on vacation this week. :( 4. If I then restart slon on the remote server, will it catch up?
>

If you restart the slon node on the slave server it should catch up, you shouldn't need to run any extra psql commands.


You then what to run slony1_dump.sh against the slave database and 
capture redirect the output of that to psql.

I *think* something like
slony1_dump.sh mydbname myclustername | psql -h offline_host_name mydbname

is what you want.  You would then resume the logshipping apply daemon.



> Thanks for your patience!
> Mike
>
>
> -----Original Message-----
> From: Steve Singer [mailto:ssinger at ca.afilias.info]
> Sent: Friday, October 05, 2012 9:38 AM
> To: Mike James
> Cc: slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] offline log shipping errors
>
> On 12-10-05 08:37 AM, Mike James wrote:
>> Here's what the DBA did as I understand it. This is his procedure for changing the replication set. Slony version is slony1-2.0.3-rc. Postgres 8.3.9 on the Master. I might be mis-using some of the terminology, as I'm not a DBA and this is my 1st foray with slony.
>> 1. Kill the slon running on the master and slave. Leave the slon on the remote host running.
>> 2. he uses PGAdmin to remove the slony schema (slony replication cluster) from Master and Slave.
>> 3. regenerate the set records using a shell script on the Master.
>> 4. use slonik to initialize the cluster with the set records from above on Master and Slave.
>> 5. start the slon on the Master.
>> 6. start the slon on the Slave with -a option.
>> 7. use slonik to subscribe the Slave to the Master.
>>
>> On the remote postgres server, the value of at_counter in the _cluster1.sl_archive_tracking table is 338005. But it looks like the new logs being shipped started over at sequence number 1.
>>
>> Does that description make sense? This can't be the optimal way to add a table to the replication set!
>>
> What you describe makes perfect sense.  Step 2, removing the slony schema uninstalls slony from both the master and the slave.  When you reinstall slony with step 4 you are creating a new slony cluster.  The offline node isn't in sync with the new cluster it was sync'd with the old one.  You will need to rebuild your offline node with slony1_dump.sh
>
> The proper way to add tables to a replication set is described here http://www.slony.info/documentation/2.0/administration.html#ADDTHINGS
>
> If your using the altperl tools you can look at slonik_create_set, slonik_subscribe_set and slonik_merge_sets
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>




From nyamada at millburncorp.com  Thu Oct  4 11:47:12 2012
From: nyamada at millburncorp.com (Norman Yamada)
Date: Thu, 4 Oct 2012 14:47:12 -0400
Subject: [Slony1-general] pg_reorg and reorganizing tables in slony cluster
Message-ID: <434FA33C-7A0B-4B85-92D6-44533C19B163@millburncorp.com>

Dear all -- I've discovered a problem using pg_reorg 1.1.7 with Slony versions 2.0 and/or 2.1. 

If you run a reorg on a table that is part of a slony slave set while there is write activity on the master node, pg_reorg will silently drop changes committed by slony to the slave node during the reorganization.

The problem is that the z_reorg_trigger doesn't fire on the changes that slony commits to the slave table, because slony has a session_replication_role on the slave of REPLICA. 

To get around this, after the z_reorg_trigger is created on a table, you have to run the command

ALTER TABLE [table_name] ENABLE ALWAYS TRIGGER z_reorg_trigger. Then the log table will capture changes pushed by Slony (or other trigger-based replication programs) during the time of reorganizing the table.

This is a subtle problem, because pg_reorg works fine with reorganizing tables on the _master_ node of a slony set, but it will drop changes on the slave nodes silently.

If I have time, I'll try to write at least a rough patch for pg_reorg to fix this, but I'm hoping that both the pg_reorg and slony community can be aware of this problem. Until it's fixed, best practice would be to stop slony while reorganizing tables on a slave node.

Best,

Norman Yamada

(Cross-posted to reorg_general at pgfoundry.org)

From rroblak at gmail.com  Tue Oct  9 16:57:49 2012
From: rroblak at gmail.com (Ryan Oblak)
Date: Tue, 9 Oct 2012 16:57:49 -0700
Subject: [Slony1-general] Staging/reporting replication
Message-ID: <CA+W5Wuccg5_+sEU0hd7UzLSEukUTo5ZczuoiSFt-Lq4aiWpThA@mail.gmail.com>

Hi,

I'd like to replicate my production database to 2 separate Postgres
instances --- one for reporting and one for staging. The main requirement
is that the data needs be current up to the state of the production db the
day before.


From glynastill at yahoo.co.uk  Wed Oct 10 02:02:52 2012
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed, 10 Oct 2012 10:02:52 +0100 (BST)
Subject: [Slony1-general] Staging/reporting replication
In-Reply-To: <CA+W5Wuccg5_+sEU0hd7UzLSEukUTo5ZczuoiSFt-Lq4aiWpThA@mail.gmail.com>
References: <CA+W5Wuccg5_+sEU0hd7UzLSEukUTo5ZczuoiSFt-Lq4aiWpThA@mail.gmail.com>
Message-ID: <1349859772.27374.YahooMailNeo@web133204.mail.ir2.yahoo.com>

> From: Ryan Oblak <rroblak at gmail.com>
>Subject: [Slony1-general] Staging/reporting replication
> 
>Hi,
>
>
>I'd
 like to replicate my production database to 2 separate Postgres 
instances --- one for reporting and one for staging. The main 
requirement is that the data needs be current up to the state of the 
production db the day before.?
>
>
>From what I've 
read it seems like using an asynchronous replication solution (i.e. 
slony) or just a simple pg_dump/pg_restore script would work. Ideally 
I'd like to have as little additional load on the production server as 
possible. Also, the production db is currently at 12GB (and growing 
steadily), so I'm a bit concerned that doing a full pg_restore every day
 might become infeasible at some point.
>
>
>Any thoughts?
>
>
>Thanks,
>Ryan


If
 you want to have a slave lag behind you can pass the "lag_interval" 
parameter to the slon, or set it in the slon.conf you're passing.? If 
you needed the data to be a point in time you could probably do that 
with slonys log shipping.? This would of course be a read only replica.

I'd
 guess a slony slave would be best for your reporting, but for a staging
 environment you might want to just go with a dump and restore.

Here
 I've a couple of servers (old dell 1850s with md raid, hopefully to be 
upgraded soon) that restore our latest backups daily in the early hours,
 the database is 160Gb at present and it takes 4 hours including checks 
to ensure the backups are good and a database wide vacuum analyse.? This
 serves two purposes; to check the backups are viable and once done 
provide a staging environment for our devs.

Glyn

From rroblak at gmail.com  Wed Oct 10 11:10:29 2012
From: rroblak at gmail.com (Ryan Oblak)
Date: Wed, 10 Oct 2012 11:10:29 -0700
Subject: [Slony1-general] Staging/reporting replication
In-Reply-To: <1349859772.27374.YahooMailNeo@web133204.mail.ir2.yahoo.com>
References: <CA+W5Wuccg5_+sEU0hd7UzLSEukUTo5ZczuoiSFt-Lq4aiWpThA@mail.gmail.com>
	<1349859772.27374.YahooMailNeo@web133204.mail.ir2.yahoo.com>
Message-ID: <CA+W5Wucdx+n-_q5Ee57LK0KLEc7ph-O7n6vt9VHUCF3Cn3H-3g@mail.gmail.com>

Thanks for your thoughts, Glyn.

I like the idea of using the staging database to check backups in addition
to staging. Restoring sounds like it could be a viable option for both
reporting and staging if 160Gb only takes 4 hours. Though, of course, my
mileage may vary as I'm using virtualized servers.


On Wed, Oct 10, 2012 at 2:02 AM, Glyn Astill <glynastill at yahoo.co.uk> wrote:

> > From: Ryan Oblak <rroblak at gmail.com>
> >Subject: [Slony1-general] Staging/reporting replication
> >
> >Hi,
> >
> >
> >I'd
>  like to replicate my production database to 2 separate Postgres
> instances --- one for reporting and one for staging. The main
> requirement is that the data needs be current up to the state of the
> production db the day before.
> >
> >
> >From what I've
> read it seems like using an asynchronous replication solution (i.e.
> slony) or just a simple pg_dump/pg_restore script would work. Ideally
> I'd like to have as little additional load on the production server as
> possible. Also, the production db is currently at 12GB (and growing
> steadily), so I'm a bit concerned that doing a full pg_restore every day
>  might become infeasible at some point.
> >
> >
> >Any thoughts?
> >
> >
> >Thanks,
> >Ryan
>
>
> If
>  you want to have a slave lag behind you can pass the "lag_interval"
> parameter to the slon, or set it in the slon.conf you're passing.  If
> you needed the data to be a point in time you could probably do that
> with slonys log shipping.  This would of course be a read only replica.
>
> I'd
>  guess a slony slave would be best for your reporting, but for a staging
>  environment you might want to just go with a dump and restore.
>
> Here
>  I've a couple of servers (old dell 1850s with md raid, hopefully to be
> upgraded soon) that restore our latest backups daily in the early hours,
>  the database is 160Gb at present and it takes 4 hours including checks
> to ensure the backups are good and a database wide vacuum analyse.  This
>  serves two purposes; to check the backups are viable and once done
> provide a staging environment for our devs.
>
> Glyn
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121010/240e1441/attachment.htm 

From luongnvn at gmail.com  Sun Oct 21 04:12:39 2012
From: luongnvn at gmail.com (luongnvn vv)
Date: Sun, 21 Oct 2012 18:12:39 +0700
Subject: [Slony1-general] manual replication
Message-ID: <CAE+OF80mWpBLW+dS4dopMkZZAhawsGy78SmfQvi-7bvwbXnHqg@mail.gmail.com>

Hi all! I'm newbie on slony-i, i'm usingdrug version 2.1.2, i have 2
database on 2 server name: S_1 and S_2, im want set slony cluster in some
table: P_1, P_detail_1, P_result_1 on S_1; and  P_2, P_detail_2, P_result_2
on S_2.
I'm has set 2 replication set:
C_1: master: P_1, P_detail_1 ; slave: P_2, P_detail_2
C_2: master: P_reault_2; slave P_result_1
the cluster install is ok, replacation on 2 cluster is ok.
Now i'm develop my solution: on C_1, when P_detail_2 is update, i set a
trigger to insert if new value or update if old value to P_result_2, and
new insert or update value will be replication to P_result_1 by C_2, and
when P_result_1 is update, i set a trigger to update to P_1 or P_detail_1.
This solution will make a cycle in 2 datebase.
But when i'm set up trigger it make a big proplem, when P_detail_2 update
by C_1, my trigger is fire, it make insert or update ok, but it not
replication to P_result_1 (C_2 im tested ok). im view in catalog of
replication C_2 (S_2 to S_1) in table sl_log_1 store log when my trigger
run.
Someone have idea for my solution, please help, im have thank you a lot :).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20121021/50a2c4f1/attachment.htm 

From mail at joeconway.com  Wed Oct 24 13:23:28 2012
From: mail at joeconway.com (Joe Conway)
Date: Wed, 24 Oct 2012 15:23:28 -0500
Subject: [Slony1-general] pg_dump, slony databases, and locking
Message-ID: <50884E40.8050907@joeconway.com>

In the fine manual it says:

    "If you pg_dump your database avoid dumping your Slony schemas or
     else pg_dump's locking will compete with Slony's own locking which
     could stop Slony replication for the duration of the pg_dump.
     Exclude the Slony schemas from pg_dump with
     --exclude-schema=schemaname to specifically exclude your Slony
     schema."

Question: if I exclude dumping the slony schema, how can I successfully
restore my tables? My tables all have dependencies on the slony
triggers, which will then be missing when I go to restore the tables.

Thanks,

Joe


-- 
Joe Conway
credativ LLC: http://www.credativ.us
Linux, PostgreSQL, and general Open Source
Training, Service, Consulting, & 24x7 Support

From scott.marlowe at gmail.com  Wed Oct 24 13:43:21 2012
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 24 Oct 2012 14:43:21 -0600
Subject: [Slony1-general] Slony 1.2.xx postgres compatibility.
Message-ID: <CAOR=d=16mi1BDqm2K0SWc-TwnL+xHWXm6=xONBfCf-YQ_FPdrg@mail.gmail.com>

What is the latest version of PostgreSQL that slony 1.2.xx supports
and works properly with?  We're running pg 8.4 at work and want to
upgrade to pg 9.2.  But I've seen some posts about not being able to
compile slony 1.2.xx against pg 9.2.

The release notes mention fixes to make it pg 9.0 compatible but
there's no mention of 9.1 or 9.2...

-- 
To understand recursion, one must first understand recursion.

From ssinger at ca.afilias.info  Wed Oct 24 14:46:37 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 24 Oct 2012 17:46:37 -0400
Subject: [Slony1-general] Slony 1.2.xx postgres compatibility.
In-Reply-To: <CAOR=d=16mi1BDqm2K0SWc-TwnL+xHWXm6=xONBfCf-YQ_FPdrg@mail.gmail.com>
References: <CAOR=d=16mi1BDqm2K0SWc-TwnL+xHWXm6=xONBfCf-YQ_FPdrg@mail.gmail.com>
Message-ID: <508861BD.50003@ca.afilias.info>

On 12-10-24 04:43 PM, Scott Marlowe wrote:
> What is the latest version of PostgreSQL that slony 1.2.xx supports
> and works properly with?  We're running pg 8.4 at work and want to
> upgrade to pg 9.2.  But I've seen some posts about not being able to
> compile slony 1.2.xx against pg 9.2.
>
> The release notes mention fixes to make it pg 9.0 compatible but
> there's no mention of 9.1 or 9.2...
>

That is correct.  1.2 should work on 9.0

It should compile against 9.1 but you will encounter issues due to 
locking or deadlocks.  It MIGHT work well enough for you to get your 
data off.

I don't think 1.2 compiles against 9.2


Slony 2.1.x should work with Postgresql 8.4 and 9.2, I would recommend 
upgrading slony first.   If you had to upgrade from a version of 
postgresql prior to 8.3 (ie 8.2 or 8.1) then you can do the upgrade in 
stages.



From ssinger at ca.afilias.info  Wed Oct 24 14:52:03 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 24 Oct 2012 17:52:03 -0400
Subject: [Slony1-general] pg_dump, slony databases, and locking
In-Reply-To: <50884E40.8050907@joeconway.com>
References: <50884E40.8050907@joeconway.com>
Message-ID: <50886303.4080606@ca.afilias.info>

On 12-10-24 04:23 PM, Joe Conway wrote:
> In the fine manual it says:
>
>      "If you pg_dump your database avoid dumping your Slony schemas or
>       else pg_dump's locking will compete with Slony's own locking which
>       could stop Slony replication for the duration of the pg_dump.
>       Exclude the Slony schemas from pg_dump with
>       --exclude-schema=schemaname to specifically exclude your Slony
>       schema."
>
> Question: if I exclude dumping the slony schema, how can I successfully
> restore my tables? My tables all have dependencies on the slony
> triggers, which will then be missing when I go to restore the tables.
>

Yes the CREATE TRIGGER commands in the restore will fail with errors, 
but if your restore isn't done as part of a transaction then it leaves 
you with a working database, with data minus slony triggers.

You could also filter your schema dump through grep to exclude the triggers.



> Thanks,
>
> Joe
>
>


From cbbrowne at afilias.info  Wed Oct 24 14:58:41 2012
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 24 Oct 2012 17:58:41 -0400
Subject: [Slony1-general] pg_dump, slony databases, and locking
In-Reply-To: <50884E40.8050907@joeconway.com>
References: <50884E40.8050907@joeconway.com>
Message-ID: <CANfbgbYn8TVMKn2pi25wbGxXQ-8991kvKu83+AkTqCo_6TxDgA@mail.gmail.com>

On Wed, Oct 24, 2012 at 4:23 PM, Joe Conway <mail at joeconway.com> wrote:
> In the fine manual it says:
>
>     "If you pg_dump your database avoid dumping your Slony schemas or
>      else pg_dump's locking will compete with Slony's own locking which
>      could stop Slony replication for the duration of the pg_dump.
>      Exclude the Slony schemas from pg_dump with
>      --exclude-schema=schemaname to specifically exclude your Slony
>      schema."
>
> Question: if I exclude dumping the slony schema, how can I successfully
> restore my tables? My tables all have dependencies on the slony
> triggers, which will then be missing when I go to restore the tables.

It all depends on whether you treat that failure as an error or not.

Arguably, the failure to restore the triggers isn't a "real" error, if
you squint at things in a particular way.

Alternatively, I suppose you could do a separate dump of *just* the
Slony schema.  That has *some* risk of locking conflict, but it should
be an entirely smaller risk, as it's not encompassing the likely-large
backup of your application's schema and data.  Dump the Slony schema,
and load it first, and that would allow the
schema-excluding-Slony-bits to work.

But my preference would be to squint at things sideways and say, "It's
OK that those triggers didn't load in."

From scott.marlowe at gmail.com  Wed Oct 24 15:20:01 2012
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 24 Oct 2012 16:20:01 -0600
Subject: [Slony1-general] Slony 1.2.xx postgres compatibility.
In-Reply-To: <508861BD.50003@ca.afilias.info>
References: <CAOR=d=16mi1BDqm2K0SWc-TwnL+xHWXm6=xONBfCf-YQ_FPdrg@mail.gmail.com>
	<508861BD.50003@ca.afilias.info>
Message-ID: <CAOR=d=1fDRNz_bygR4cHRG0ThqKmdwEPmVDNUzWr5Q0H+sBuHg@mail.gmail.com>

On Wed, Oct 24, 2012 at 3:46 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> On 12-10-24 04:43 PM, Scott Marlowe wrote:
>>
>> What is the latest version of PostgreSQL that slony 1.2.xx supports
>> and works properly with?  We're running pg 8.4 at work and want to
>> upgrade to pg 9.2.  But I've seen some posts about not being able to
>> compile slony 1.2.xx against pg 9.2.
>>
>> The release notes mention fixes to make it pg 9.0 compatible but
>> there's no mention of 9.1 or 9.2...
>>
>
> That is correct.  1.2 should work on 9.0
>
> It should compile against 9.1 but you will encounter issues due to locking
> or deadlocks.  It MIGHT work well enough for you to get your data off.
>
> I don't think 1.2 compiles against 9.2
>
>
> Slony 2.1.x should work with Postgresql 8.4 and 9.2, I would recommend
> upgrading slony first.   If you had to upgrade from a version of postgresql
> prior to 8.3 (ie 8.2 or 8.1) then you can do the upgrade in stages.

Thanks I was pretty sure this was the case.  We are looking at
upgrading both and since 2.1 works with 8.4 that seems the way to go.
We might have to compile from slony source on the older servers tho.

From mail at joeconway.com  Wed Oct 24 17:15:58 2012
From: mail at joeconway.com (Joe Conway)
Date: Wed, 24 Oct 2012 19:15:58 -0500
Subject: [Slony1-general] pg_dump, slony databases, and locking
In-Reply-To: <CANfbgbYn8TVMKn2pi25wbGxXQ-8991kvKu83+AkTqCo_6TxDgA@mail.gmail.com>
References: <50884E40.8050907@joeconway.com>
	<CANfbgbYn8TVMKn2pi25wbGxXQ-8991kvKu83+AkTqCo_6TxDgA@mail.gmail.com>
Message-ID: <508884BE.2020302@joeconway.com>

On 10/24/2012 04:58 PM, Christopher Browne wrote:
> Arguably, the failure to restore the triggers isn't a "real" error, if
> you squint at things in a particular way.
> 
> Alternatively, I suppose you could do a separate dump of *just* the
> Slony schema.  That has *some* risk of locking conflict, but it should
> be an entirely smaller risk, as it's not encompassing the likely-large
> backup of your application's schema and data.  Dump the Slony schema,
> and load it first, and that would allow the
> schema-excluding-Slony-bits to work.
> 
> But my preference would be to squint at things sideways and say, "It's
> OK that those triggers didn't load in."

Thanks for the response (and Steve's as well). We are scripting the
restore and therefore prefer to treat all errors as a failure.
Unfortunately if we "allow" the restore trigger errors it will make it
difficult to disallow other errors. But I guess it is something to consider.

And in fact we are already dumping and restoring the slony schema
separately first, and encountered the described deadlock which caused
pg_dump to fail, which prompted the question in the first place.

Joe

-- 
Joe Conway
credativ LLC: http://www.credativ.us
Linux, PostgreSQL, and general Open Source
Training, Service, Consulting, & 24x7 Support

From JanWieck at Yahoo.com  Wed Oct 24 17:50:38 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 24 Oct 2012 20:50:38 -0400
Subject: [Slony1-general] pg_dump, slony databases, and locking
In-Reply-To: <508884BE.2020302@joeconway.com>
References: <50884E40.8050907@joeconway.com>
	<CANfbgbYn8TVMKn2pi25wbGxXQ-8991kvKu83+AkTqCo_6TxDgA@mail.gmail.com>
	<508884BE.2020302@joeconway.com>
Message-ID: <50888CDE.7070606@Yahoo.com>

On 10/24/2012 8:15 PM, Joe Conway wrote:
> On 10/24/2012 04:58 PM, Christopher Browne wrote:
>> Arguably, the failure to restore the triggers isn't a "real" error, if
>> you squint at things in a particular way.
>>
>> Alternatively, I suppose you could do a separate dump of *just* the
>> Slony schema.  That has *some* risk of locking conflict, but it should
>> be an entirely smaller risk, as it's not encompassing the likely-large
>> backup of your application's schema and data.  Dump the Slony schema,
>> and load it first, and that would allow the
>> schema-excluding-Slony-bits to work.
>>
>> But my preference would be to squint at things sideways and say, "It's
>> OK that those triggers didn't load in."
>
> Thanks for the response (and Steve's as well). We are scripting the
> restore and therefore prefer to treat all errors as a failure.
> Unfortunately if we "allow" the restore trigger errors it will make it
> difficult to disallow other errors. But I guess it is something to consider.

Not that this will be of any help for you, but I consider it a missing 
feature of pg_dump that it allows to omit a schema, but not to omit any 
references to that schema like triggers, foreign keys and so on.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Sun Oct 28 20:43:32 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sun, 28 Oct 2012 23:43:32 -0400
Subject: [Slony1-general] slony.info entered the hall of fame at
	nerdydata.com
Message-ID: <508DFB64.3060605@Yahoo.com>

http://nerdydata.com/slony.info#GoldMember

We must have done something right.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

