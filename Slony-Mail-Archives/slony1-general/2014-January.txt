From matthias at aic.at  Thu Jan  2 08:36:32 2014
From: matthias at aic.at (Matthias Leopold)
Date: Thu, 02 Jan 2014 17:36:32 +0100
Subject: [Slony1-general] testing slony state in slony 2.0.4
Message-ID: <52C59590.6010905@aic.at>

hi,

i successfully used test_slony_state-dbi.pl with slony 1.2.

after our cluster got rebuilt with slony 2.0 (which wasn't done by 
me...) replication seems to work but the test script keeps complaining 
about events and confirmations not propagating for all of the nodes. as 
i said replication seems to work, logs are fine, dropping and readding a 
node doesn't make a difference, sl_log_1/sl_log_2 look unsuspicious. 
slony 2.0.4 is from debian 6.

can you tell me where to look next?

thx
matthias


From scott.marlowe at gmail.com  Thu Jan  2 08:49:43 2014
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 2 Jan 2014 09:49:43 -0700
Subject: [Slony1-general] testing slony state in slony 2.0.4
In-Reply-To: <52C59590.6010905@aic.at>
References: <52C59590.6010905@aic.at>
Message-ID: <CAOR=d=1fO3=9GPZ-PEvRX4KtwbfqPmBiwzqX-HoqLcyq2sBD5Q@mail.gmail.com>

On Thu, Jan 2, 2014 at 9:36 AM, Matthias Leopold <matthias at aic.at> wrote:
> hi,
>
> i successfully used test_slony_state-dbi.pl with slony 1.2.
>
> after our cluster got rebuilt with slony 2.0 (which wasn't done by
> me...) replication seems to work but the test script keeps complaining
> about events and confirmations not propagating for all of the nodes. as
> i said replication seems to work, logs are fine, dropping and readding a
> node doesn't make a difference, sl_log_1/sl_log_2 look unsuspicious.
> slony 2.0.4 is from debian 6.
>
> can you tell me where to look next?

Into updating? 2.0.4 had a fair few bugs, and 2.0.7 is the latest 2.0
release. I tried 2.0.4 a ways back and had some issues with it much
like what you're experiencing that 2.0.5 fixed.

From tmblue at gmail.com  Thu Jan  2 11:35:15 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 2 Jan 2014 11:35:15 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
	12 hours to replicate DB
Message-ID: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>

Wondering what settings I need to speed this up. To do a rebuild of a db it
takes a long time, 6 hours for a singe  table. No I/O issues, no load, just
slon postgres taking their sweet old time. I would like to use the
resources available to speed this up.

The table is

2013-12-21 19:17:58 PST CONFIG remoteWorkerThread_1: Begin COPY of table
"impressions"
2013-12-21 19:37:03 PST CONFIG remoteWorkerThread_1: 12657163552 bytes
copied for table ?impressions?
2013-12-22 01:40:22 PST CONFIG remoteWorkerThread_1: 22944.144 seconds to
copy table ?impressions? <? 6 hours

Postgres 9.2.4 slony 2.1.3

This is a larger table, but because of bloat etc, we need to do ground ups
to clean it out every so often (Vacuums don't do it).


Slony config , pretty much at default  other than sync interval.

# Check for updates at least this often in milliseconds.
# Range: [10-60000], default 2000
sync_interval=1000
#sync_interval_timeout=10000
# apply every single SYNC by itself.
# Range:  [0,100], default: 6
#sync_group_maxsize=6
#sync_max_rowsize=8192
#sync_max_largemem=5242880

I either need some advanced settings for when we are doing a rebuild, to
speed up the process, or I need to do some configurations that stay during
normal workloads as well. But normal workloads things are replicated and
keep in sync, it's just the rebuild portion. I would like to see it
actually stressing my boxen :)

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140102/4b4c7697/attachment.htm 

From cbbrowne at afilias.info  Thu Jan  2 12:33:41 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 2 Jan 2014 15:33:41 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
Message-ID: <CANfbgbYp7VGL9=jYMxdmfC3PRwx=q3dnU8e0sdgHURcdhy+RAA@mail.gmail.com>

On Thu, Jan 2, 2014 at 2:35 PM, Tory M Blue <tmblue at gmail.com> wrote:

>
> Wondering what settings I need to speed this up. To do a rebuild of a db
> it takes a long time, 6 hours for a singe  table. No I/O issues, no load,
> just slon postgres taking their sweet old time. I would like to use the
> resources available to speed this up.
>
> The table is
>
> 2013-12-21 19:17:58 PST CONFIG remoteWorkerThread_1: Begin COPY of table
> "impressions"
> 2013-12-21 19:37:03 PST CONFIG remoteWorkerThread_1: 12657163552 bytes
> copied for table ?impressions?
> 2013-12-22 01:40:22 PST CONFIG remoteWorkerThread_1: 22944.144 seconds to
> copy table ?impressions? <? 6 hours
>
> Postgres 9.2.4 slony 2.1.3
>
> This is a larger table, but because of bloat etc, we need to do ground ups
> to clean it out every so often (Vacuums don't do it).
>
>
> Slony config , pretty much at default  other than sync interval.
>
> # Check for updates at least this often in milliseconds.
> # Range: [10-60000], default 2000
> sync_interval=1000
> #sync_interval_timeout=10000
> # apply every single SYNC by itself.
> # Range:  [0,100], default: 6
> #sync_group_maxsize=6
> #sync_max_rowsize=8192
> #sync_max_largemem=5242880
>
> I either need some advanced settings for when we are doing a rebuild, to
> speed up the process, or I need to do some configurations that stay during
> normal workloads as well. But normal workloads things are replicated and
> keep in sync, it's just the rebuild portion. I would like to see it
> actually stressing my boxen :)
>
>
The one place where it might be worth modifying configuration is to change
the amount of memory being used for sorts, as the step *after* the COPY of
table "impressions" (which likely takes most of the remaining 6h of the
subscription process) is to reindex that table.  If the reindex takes 4h,
changing the GUC might reduce that 4h significantly.

http://www.postgresql.org/docs/9.2/static/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM


Aside from that, there's not too much else to improve in practice, and not
too terribly much even in theory.

We've had discussions (not too lately; I don't think anything relevant has
changed too terribly much) about the idea of doing some multi-threading of
the SUBSCRIBE/COPY process.  The notable idea was to split off the REINDEX
process to a series of extra threads so that it could be done in parallel.
 In that you have one Very Very Large Table, only a limited amount of
benefit could be gotten from that; if the biggest table involves 6h of COPY
and 4h of REINDEX, then the fastest, in theory, that the subscription could
get done is 10h, which probably isn't enough improvement to be worth the
added code + fragility that would result.

The last time the concept was discussed, the conclusion was made to not
implement parallel processing of REINDEX, on the basis that:
a) It would be a fair bit of code, adding potential for new bugs;
b) It would add more potential for lock and deadlock conditions;
c) In the common case where one table's data dominates the database (which
sure seems to be the case for you), there's little benefit as the
theoretical minimum subscription time is *at minimum*
    time-to-COPY biggest table + time to reindex biggest table
which makes the optimization pretty futile.

Your case doesn't seem to invalidate the reasoning.

You could get *some* benefit from splitting things into two sets, one
consisting of the table "impressions" and the other consisting of the rest.
  I'd hazard the guess that there's not too much need to clean up the
tables other than "impressions"; you could drop the 'outage' to the
theoretical minimum if you don't bother re-subscribing the other tables.

I wonder if there's merit to trying to split "impressions" into several
tables, one which would contain the "hot, heavily updated" data, and others
containing "cooked/complete" data that doesn't need to be reorganized.  It
may not be easy to distinguish between data that is "complete" and data
that is still changing a lot.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140102/7266606e/attachment.htm 

From tmblue at gmail.com  Thu Jan  2 12:59:33 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 2 Jan 2014 12:59:33 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CANfbgbYp7VGL9=jYMxdmfC3PRwx=q3dnU8e0sdgHURcdhy+RAA@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CANfbgbYp7VGL9=jYMxdmfC3PRwx=q3dnU8e0sdgHURcdhy+RAA@mail.gmail.com>
Message-ID: <CAEaSS0ZW8wwr558T7__7Dbeh19KKd-SGGxTcqu0xekyBvTB6yw@mail.gmail.com>

On Thu, Jan 2, 2014 at 12:33 PM, Christopher Browne
<cbbrowne at afilias.info>wrote:

> On Thu, Jan 2, 2014 at 2:35 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
>>
>> Wondering what settings I need to speed this up. To do a rebuild of a db
>> it takes a long time, 6 hours for a singe  table. No I/O issues, no load,
>> just slon postgres taking their sweet old time. I would like to use the
>> resources available to speed this up.
>>
>> The table is
>>
>> 2013-12-21 19:17:58 PST CONFIG remoteWorkerThread_1: Begin COPY of table
>> "impressions"
>> 2013-12-21 19:37:03 PST CONFIG remoteWorkerThread_1: 12657163552 bytes
>> copied for table ?impressions?
>> 2013-12-22 01:40:22 PST CONFIG remoteWorkerThread_1: 22944.144 seconds to
>> copy table ?impressions? <? 6 hours
>>
>> Postgres 9.2.4 slony 2.1.3
>>
>> This is a larger table, but because of bloat etc, we need to do ground
>> ups to clean it out every so often (Vacuums don't do it).
>>
>>
>> Slony config , pretty much at default  other than sync interval.
>>
>> # Check for updates at least this often in milliseconds.
>> # Range: [10-60000], default 2000
>> sync_interval=1000
>> #sync_interval_timeout=10000
>> # apply every single SYNC by itself.
>> # Range:  [0,100], default: 6
>> #sync_group_maxsize=6
>> #sync_max_rowsize=8192
>> #sync_max_largemem=5242880
>>
>> I either need some advanced settings for when we are doing a rebuild, to
>> speed up the process, or I need to do some configurations that stay during
>> normal workloads as well. But normal workloads things are replicated and
>> keep in sync, it's just the rebuild portion. I would like to see it
>> actually stressing my boxen :)
>>
>>
> The one place where it might be worth modifying configuration is to change
> the amount of memory being used for sorts, as the step *after* the COPY of
> table "impressions" (which likely takes most of the remaining 6h of the
> subscription process) is to reindex that table.  If the reindex takes 4h,
> changing the GUC might reduce that 4h significantly.
>
>
> http://www.postgresql.org/docs/9.2/static/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM
>
>
> Aside from that, there's not too much else to improve in practice, and not
> too terribly much even in theory.
>
> We've had discussions (not too lately; I don't think anything relevant has
> changed too terribly much) about the idea of doing some multi-threading of
> the SUBSCRIBE/COPY process.  The notable idea was to split off the REINDEX
> process to a series of extra threads so that it could be done in parallel.
>  In that you have one Very Very Large Table, only a limited amount of
> benefit could be gotten from that; if the biggest table involves 6h of COPY
> and 4h of REINDEX, then the fastest, in theory, that the subscription could
> get done is 10h, which probably isn't enough improvement to be worth the
> added code + fragility that would result.
>
> The last time the concept was discussed, the conclusion was made to not
> implement parallel processing of REINDEX, on the basis that:
> a) It would be a fair bit of code, adding potential for new bugs;
> b) It would add more potential for lock and deadlock conditions;
> c) In the common case where one table's data dominates the database (which
> sure seems to be the case for you), there's little benefit as the
> theoretical minimum subscription time is *at minimum*
>     time-to-COPY biggest table + time to reindex biggest table
> which makes the optimization pretty futile.
>
> Your case doesn't seem to invalidate the reasoning.
>
> You could get *some* benefit from splitting things into two sets, one
> consisting of the table "impressions" and the other consisting of the rest.
>   I'd hazard the guess that there's not too much need to clean up the
> tables other than "impressions"; you could drop the 'outage' to the
> theoretical minimum if you don't bother re-subscribing the other tables.
>
> I wonder if there's merit to trying to split "impressions" into several
> tables, one which would contain the "hot, heavily updated" data, and others
> containing "cooked/complete" data that doesn't need to be reorganized.  It
> may not be easy to distinguish between data that is "complete" and data
> that is still changing a lot.
>

Thanks Chris

There are actually 3 tables that are significant but this one shows to take
the longest, the others are a couple to 4 hours each. but if it's the
reindex portion, i will look to see what my memory settings are currently
as that could be a win. We note it on most of our DB's the index creation
always takes significant time with no apparent flexing of the iron's
muscle, so if it's just memory, we will do some tuning.

I've asked engineering about splitting the table, but the answers is
usually the same, no :) hahaha

Thanks again!

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140102/88c620a6/attachment.htm 

From shurale9811 at gmail.com  Thu Jan  2 13:08:40 2014
From: shurale9811 at gmail.com (Shamshinur)
Date: Thu, 2 Jan 2014 16:08:40 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
	12 hours to replicate DB
In-Reply-To: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
Message-ID: <00ea01cf07fe$d0272e30$70758a90$@com>

Very strange, shouldn't take that long, you may try increase  the value of
maintenance_work_mem  in postgres.conf, to speed up COPY

For instance:

 maintenance_work_mem = 1024MB

 

 

From: slony1-general-bounces at lists.slony.info
[mailto:slony1-general-bounces at lists.slony.info] On Behalf Of Tory M Blue
Sent: Thursday, January 02, 2014 2:35 PM
To: slony1-general
Subject: [Slony1-general] 6 hours to replicate single table, 12 hours to
replicate DB

 

 

Wondering what settings I need to speed this up. To do a rebuild of a db it
takes a long time, 6 hours for a singe  table. No I/O issues, no load, just
slon postgres taking their sweet old time. I would like to use the resources
available to speed this up.

 

The table is

 

2013-12-21 19:17:58 PST CONFIG remoteWorkerThread_1: Begin COPY of table
"impressions"

2013-12-21 19:37:03 PST CONFIG remoteWorkerThread_1: 12657163552 bytes
copied for table "impressions"  

2013-12-22 01:40:22 PST CONFIG remoteWorkerThread_1: 22944.144 seconds to
copy table "impressions" <- 6 hours

 

Postgres 9.2.4 slony 2.1.3

 

This is a larger table, but because of bloat etc, we need to do ground ups
to clean it out every so often (Vacuums don't do it).

 

 

Slony config , pretty much at default  other than sync interval.

 

# Check for updates at least this often in milliseconds.

# Range: [10-60000], default 2000

sync_interval=1000

#sync_interval_timeout=10000

# apply every single SYNC by itself.

# Range:  [0,100], default: 6

#sync_group_maxsize=6

#sync_max_rowsize=8192

#sync_max_largemem=5242880

 

I either need some advanced settings for when we are doing a rebuild, to
speed up the process, or I need to do some configurations that stay during
normal workloads as well. But normal workloads things are replicated and
keep in sync, it's just the rebuild portion. I would like to see it actually
stressing my boxen :)

 

Thanks

Tory

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140102/84b763b5/attachment-0001.htm 

From tmblue at gmail.com  Thu Jan  2 13:12:13 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 2 Jan 2014 13:12:13 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <00ea01cf07fe$d0272e30$70758a90$@com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<00ea01cf07fe$d0272e30$70758a90$@com>
Message-ID: <CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>

On Thu, Jan 2, 2014 at 1:08 PM, Shamshinur <shurale9811 at gmail.com> wrote:

> Very strange, shouldn?t take that long, you may try increase  the value of
>  maintenance_work_mem  in postgres.conf, to speed up COPY
>
> For instance:
>
>  maintenance_work_mem = 1024MB
>
>
>
>
>
> *From:* slony1-general-bounces at lists.slony.info [mailto:
> slony1-general-bounces at lists.slony.info] *On Behalf Of *Tory M Blue
> *Sent:* Thursday, January 02, 2014 2:35 PM
> *To:* slony1-general
> *Subject:* [Slony1-general] 6 hours to replicate single table, 12 hours
> to replicate DB
>
>
>
>
>
> Wondering what settings I need to speed this up. To do a rebuild of a db
> it takes a long time, 6 hours for a singe  table. No I/O issues, no load,
> just slon postgres taking their sweet old time. I would like to use the
> resources available to speed this up.
>
>
>
> The table is
>
>
>
> 2013-12-21 19:17:58 PST CONFIG remoteWorkerThread_1: Begin COPY of table
> "impressions"
>
> 2013-12-21 19:37:03 PST CONFIG remoteWorkerThread_1: 12657163552 bytes
> copied for table ?impressions?
>
> 2013-12-22 01:40:22 PST CONFIG remoteWorkerThread_1: 22944.144 seconds to
> copy table ?impressions? <? 6 hours
>
>
>
> Postgres 9.2.4 slony 2.1.3
>
>
>
> This is a larger table, but because of bloat etc, we need to do ground ups
> to clean it out every so often (Vacuums don't do it).
>
>
>
>
>
> Slony config , pretty much at default  other than sync interval.
>
>
>
> # Check for updates at least this often in milliseconds.
>
> # Range: [10-60000], default 2000
>
> sync_interval=1000
>
> #sync_interval_timeout=10000
>
> # apply every single SYNC by itself.
>
> # Range:  [0,100], default: 6
>
> #sync_group_maxsize=6
>
> #sync_max_rowsize=8192
>
> #sync_max_largemem=5242880
>
>
>
> I either need some advanced settings for when we are doing a rebuild, to
> speed up the process, or I need to do some configurations that stay during
> normal workloads as well. But normal workloads things are replicated and
> keep in sync, it's just the rebuild portion. I would like to see it
> actually stressing my boxen :)
>
>
>
> Thanks
>
> Tory
>


We are getting outside of slon and into postgres and i'll try to keep this
light, here are my postgresql custom config aspects

shared_buffers = 2000MB
max_prepared_transactions = 0
work_mem = 100MB
maintenance_work_mem = 128MB


checkpoint_segments = 75                # in logfile segments, min 1, 16MB
each
checkpoint_timeout = 10min              # range 30s-1h
#checkpoint_completion_target = 0.5     # checkpoint target duration, 0.0 -
1.0
checkpoint_warning = 3600s

The hardware is being refreshed but right now it's an 8 core, 32GB CentOS
system, being replaced with some pretty big hardware 256GB/32cores many
SSD's. But tuning this now and knowing where I need to tweak when I add the
added capacity would be great.

thanks again
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140102/f34ae09a/attachment.htm 

From matthias at aic.at  Fri Jan  3 10:44:22 2014
From: matthias at aic.at (Matthias Leopold)
Date: Fri, 03 Jan 2014 19:44:22 +0100
Subject: [Slony1-general] testing slony state in slony 2.0.4
In-Reply-To: <CAOR=d=1fO3=9GPZ-PEvRX4KtwbfqPmBiwzqX-HoqLcyq2sBD5Q@mail.gmail.com>
References: <52C59590.6010905@aic.at>
	<CAOR=d=1fO3=9GPZ-PEvRX4KtwbfqPmBiwzqX-HoqLcyq2sBD5Q@mail.gmail.com>
Message-ID: <52C70506.9080507@aic.at>

Am 2014-01-02 17:49, schrieb Scott Marlowe:
> On Thu, Jan 2, 2014 at 9:36 AM, Matthias Leopold <matthias at aic.at> wrote:
>> hi,
>>
>> i successfully used test_slony_state-dbi.pl with slony 1.2.
>>
>> after our cluster got rebuilt with slony 2.0 (which wasn't done by
>> me...) replication seems to work but the test script keeps complaining
>> about events and confirmations not propagating for all of the nodes. as
>> i said replication seems to work, logs are fine, dropping and readding a
>> node doesn't make a difference, sl_log_1/sl_log_2 look unsuspicious.
>> slony 2.0.4 is from debian 6.
>>
>> can you tell me where to look next?
>
> Into updating? 2.0.4 had a fair few bugs, and 2.0.7 is the latest 2.0
> release. I tried 2.0.4 a ways back and had some issues with it much
> like what you're experiencing that 2.0.5 fixed.
>

thx, updating (rolling my own 2.0.7 debian package) indeed solved the 
issue. i'm still putting too much trust in distribution packages after 
all those years...

matthias

From JanWieck at Yahoo.com  Fri Jan  3 11:33:17 2014
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 03 Jan 2014 14:33:17 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>	<00ea01cf07fe$d0272e30$70758a90$@com>
	<CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>
Message-ID: <52C7107D.70709@Yahoo.com>

On 01/02/14 16:12, Tory M Blue wrote:
> We are getting outside of slon and into postgres and i'll try to keep
> this light, here are my postgresql custom config aspects

Postgres and its architecture very much affect the behavior and
performance of Slony. I'm afraid that they cannot be viewed completely
isolate from each other.

> The hardware is being refreshed but right now it's an 8 core, 32GB
> CentOS system, being replaced with some pretty big hardware
> 256GB/32cores many SSD's. But tuning this now and knowing where I need
> to tweak when I add the added capacity would be great.

The slon process uses one single database connection to the data
provider for the copy operation and another single database connection
to feed the data into the new replica. A single Postgres session only
utilizes one single core. It therefore makes zero difference if you are
upgrading from a dual core to 64 cores or more.

The suggestions made about work mem and maintenance work mem are valid.
If you are using a dedicated DB user for all your slon database
connections you can tweak these per server via ALTER USER.


Regards,
Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From tmblue at gmail.com  Fri Jan  3 11:42:09 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Jan 2014 11:42:09 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <52C7107D.70709@Yahoo.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<00ea01cf07fe$d0272e30$70758a90$@com>
	<CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>
	<52C7107D.70709@Yahoo.com>
Message-ID: <CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>

On Fri, Jan 3, 2014 at 11:33 AM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 01/02/14 16:12, Tory M Blue wrote:
> > We are getting outside of slon and into postgres and i'll try to keep
> > this light, here are my postgresql custom config aspects
>
> Postgres and its architecture very much affect the behavior and
> performance of Slony. I'm afraid that they cannot be viewed completely
> isolate from each other.
>
> > The hardware is being refreshed but right now it's an 8 core, 32GB
> > CentOS system, being replaced with some pretty big hardware
> > 256GB/32cores many SSD's. But tuning this now and knowing where I need
> > to tweak when I add the added capacity would be great.
>
> The slon process uses one single database connection to the data
> provider for the copy operation and another single database connection
> to feed the data into the new replica. A single Postgres session only
> utilizes one single core. It therefore makes zero difference if you are
> upgrading from a dual core to 64 cores or more.
>
> The suggestions made about work mem and maintenance work mem are valid.
> If you are using a dedicated DB user for all your slon database
> connections you can tweak these per server via ALTER USER.
>
>
> Regards,
> Jan



Okay that is interesting.. "If you are using a dedicated DB user for all
your slon database connections, you can tweak these per server via Alter
User", what will that do for me? We do use the same user across the board
and i figured I would just modify my postgresql.conf. So i'm a tad confused
:)

And since it appears that my index creation is the big time hole, I'm going
to be testing work mem, by going from 100M to 2GB+ and i'll reduce my
vacuum processes from 7 to maybe 2 or 3, so I don't blow anything out . But
you have intrigued me :)

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140103/3b76fdd7/attachment.htm 

From JanWieck at Yahoo.com  Fri Jan  3 12:41:04 2014
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 03 Jan 2014 15:41:04 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>	<00ea01cf07fe$d0272e30$70758a90$@com>	<CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>	<52C7107D.70709@Yahoo.com>
	<CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>
Message-ID: <52C72060.6060103@Yahoo.com>

On 01/03/14 14:42, Tory M Blue wrote:
> 
> 
> 
> On Fri, Jan 3, 2014 at 11:33 AM, Jan Wieck <JanWieck at yahoo.com
> <mailto:JanWieck at yahoo.com>> wrote:
> 
>     On 01/02/14 16:12, Tory M Blue wrote:
>     > We are getting outside of slon and into postgres and i'll try to keep
>     > this light, here are my postgresql custom config aspects
> 
>     Postgres and its architecture very much affect the behavior and
>     performance of Slony. I'm afraid that they cannot be viewed completely
>     isolate from each other.
> 
>     > The hardware is being refreshed but right now it's an 8 core, 32GB
>     > CentOS system, being replaced with some pretty big hardware
>     > 256GB/32cores many SSD's. But tuning this now and knowing where I need
>     > to tweak when I add the added capacity would be great.
> 
>     The slon process uses one single database connection to the data
>     provider for the copy operation and another single database connection
>     to feed the data into the new replica. A single Postgres session only
>     utilizes one single core. It therefore makes zero difference if you are
>     upgrading from a dual core to 64 cores or more.
> 
>     The suggestions made about work mem and maintenance work mem are valid.
>     If you are using a dedicated DB user for all your slon database
>     connections you can tweak these per server via ALTER USER.
> 
> 
>     Regards,
>     Jan
> 
> 
> 
> Okay that is interesting.. "If you are using a dedicated DB user for all
> your slon database connections, you can tweak these per server via Alter
> User", what will that do for me? We do use the same user across the
> board and i figured I would just modify my postgresql.conf. So i'm a tad
> confused :)

Assuming you have a user "slon" and all your slon processes use that DB
user to do their work. You connect with psql to a Postgres server as a
superuser ("postgres" for example) and issue the command

    ALTER USER slon SET work_mem TO "2048MB";


From scott.marlowe at gmail.com  Fri Jan  3 17:03:47 2014
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri, 3 Jan 2014 18:03:47 -0700
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
Message-ID: <CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>

On Thu, Jan 2, 2014 at 12:35 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
> Wondering what settings I need to speed this up. To do a rebuild of a db it
> takes a long time, 6 hours for a singe  table. No I/O issues, no load, just
> slon postgres taking their sweet old time. I would like to use the resources
> available to speed this up.

1: How fast can you scp a file between these two boxes?
2: How far apart are they / is there a lot of latency between them?
3: How fast can you create a good sized file on the slave? (dd
if=/etc/zero of=bigfile bs=1000000 count=1000)

If they are far apart then make sure your slon daemon is running on
the slave not the master. This can make a huge difference.

> #sync_max_rowsize=8192

Try turning this up to 32768 as well.
-- 
To understand recursion, one must first understand recursion.

From tmblue at gmail.com  Fri Jan  3 17:25:47 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Jan 2014 17:25:47 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
Message-ID: <CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>

On Fri, Jan 3, 2014 at 5:03 PM, Scott Marlowe <scott.marlowe at gmail.com>wrote:

> On Thu, Jan 2, 2014 at 12:35 PM, Tory M Blue <tmblue at gmail.com> wrote:
> >
> > Wondering what settings I need to speed this up. To do a rebuild of a db
> it
> > takes a long time, 6 hours for a singe  table. No I/O issues, no load,
> just
> > slon postgres taking their sweet old time. I would like to use the
> resources
> > available to speed this up.
>
> 1: How fast can you scp a file between these two boxes?
> 2: How far apart are they / is there a lot of latency between them?
> 3: How fast can you create a good sized file on the slave? (dd
> if=/etc/zero of=bigfile bs=1000000 count=1000)
>
> If they are far apart then make sure your slon daemon is running on
> the slave not the master. This can make a huge difference.
>
> > #sync_max_rowsize=8192
>
> Try turning this up to 32768 as well.
>

1) Very quick , Accessing an iscsi volume I can scp  the bigfile across at
roughly 45MB/s, so plenty.
2) Again same segment, nothing in the way
3) ISCSI volume (ISCSI device is running at roughly 6GB network speed)
 [user at idb02 tmp]# time dd if=/dev/zero of=bigfile bs=1000000 count=1000
1000+0 records in
1000+0 records out
1000000000 bytes (1.0 GB) copied, 1.52658 s, 655 MB/s

real 0m1.529s
user 0m0.002s
sys 0m1.526s

"Slon daemon is running on the slave not the master", ummm I'm running the
slon daemon on all of my DB's (master/Slave, QuerySlave1, QuerySlave2)

 slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &

I'll give the sync_max_rowsize a check as well

Thanks again

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140103/dc0ffbd0/attachment.htm 

From scott.marlowe at gmail.com  Fri Jan  3 17:45:06 2014
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri, 3 Jan 2014 18:45:06 -0700
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
	<CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
Message-ID: <CAOR=d=2W=BM8B1urp0W6X04GkkKeY1oL5XHewb_S4JptD1dgYQ@mail.gmail.com>

On Fri, Jan 3, 2014 at 6:25 PM, Tory M Blue <tmblue at gmail.com> wrote:
> "Slon daemon is running on the slave not the master", ummm I'm running the
> slon daemon on all of my DB's (master/Slave, QuerySlave1, QuerySlave2)
>
>  slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
> user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &

I mean that the actual daemon needs to be running ON the slave node,
not on the master node. Not sure if I wasn't clear, or if you weren't
clear. I.e. you start the daemon on each slave, not on the master. IF
that's what you're doing then just ignore this.

From tmblue at gmail.com  Fri Jan  3 18:04:19 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Jan 2014 18:04:19 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAOR=d=2W=BM8B1urp0W6X04GkkKeY1oL5XHewb_S4JptD1dgYQ@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
	<CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
	<CAOR=d=2W=BM8B1urp0W6X04GkkKeY1oL5XHewb_S4JptD1dgYQ@mail.gmail.com>
Message-ID: <CAEaSS0akcn5nxMGNS=jX-YQ8YzAsD-PbVA2ci9nPZ_Le=V0fJw@mail.gmail.com>

On Fri, Jan 3, 2014 at 5:45 PM, Scott Marlowe <scott.marlowe at gmail.com>wrote:

> On Fri, Jan 3, 2014 at 6:25 PM, Tory M Blue <tmblue at gmail.com> wrote:
> > "Slon daemon is running on the slave not the master", ummm I'm running
> the
> > slon daemon on all of my DB's (master/Slave, QuerySlave1, QuerySlave2)
> >
> >  slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
> > user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &
>
> I mean that the actual daemon needs to be running ON the slave node,
> not on the master node. Not sure if I wasn't clear, or if you weren't
> clear. I.e. you start the daemon on each slave, not on the master. IF
> that's what you're doing then just ignore this.
>


No, I run the daemon on all nodes.  I feel I'm totally not following or
have confused myself :)

But I run the upper command with a $MASTERHOST substitution.

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140103/4162b6a2/attachment.htm 

From scott.marlowe at gmail.com  Fri Jan  3 18:42:10 2014
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri, 3 Jan 2014 19:42:10 -0700
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0akcn5nxMGNS=jX-YQ8YzAsD-PbVA2ci9nPZ_Le=V0fJw@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
	<CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
	<CAOR=d=2W=BM8B1urp0W6X04GkkKeY1oL5XHewb_S4JptD1dgYQ@mail.gmail.com>
	<CAEaSS0akcn5nxMGNS=jX-YQ8YzAsD-PbVA2ci9nPZ_Le=V0fJw@mail.gmail.com>
Message-ID: <CAOR=d=0V6271Nj9H56WOBRsU7ZX2J+_t0dFcaTsdi0ew6kV1cA@mail.gmail.com>

On Fri, Jan 3, 2014 at 7:04 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
>
>
> On Fri, Jan 3, 2014 at 5:45 PM, Scott Marlowe <scott.marlowe at gmail.com>
> wrote:
>>
>> On Fri, Jan 3, 2014 at 6:25 PM, Tory M Blue <tmblue at gmail.com> wrote:
>> > "Slon daemon is running on the slave not the master", ummm I'm running
>> > the
>> > slon daemon on all of my DB's (master/Slave, QuerySlave1, QuerySlave2)
>> >
>> >  slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
>> > user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &
>>
>> I mean that the actual daemon needs to be running ON the slave node,
>> not on the master node. Not sure if I wasn't clear, or if you weren't
>> clear. I.e. you start the daemon on each slave, not on the master. IF
>> that's what you're doing then just ignore this.
>
>
>
> No, I run the daemon on all nodes.  I feel I'm totally not following or have
> confused myself :)
>
> But I run the upper command with a $MASTERHOST substitution.

So on which MACHINE HOST do you run the above commands? I.e are you
running that command x times on the master host to connect to all
slaves, or are yuo running it once on each host for that one node.

-- 
To understand recursion, one must first understand recursion.

From tmblue at gmail.com  Fri Jan  3 19:46:32 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 3 Jan 2014 19:46:32 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAOR=d=0V6271Nj9H56WOBRsU7ZX2J+_t0dFcaTsdi0ew6kV1cA@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
	<CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
	<CAOR=d=2W=BM8B1urp0W6X04GkkKeY1oL5XHewb_S4JptD1dgYQ@mail.gmail.com>
	<CAEaSS0akcn5nxMGNS=jX-YQ8YzAsD-PbVA2ci9nPZ_Le=V0fJw@mail.gmail.com>
	<CAOR=d=0V6271Nj9H56WOBRsU7ZX2J+_t0dFcaTsdi0ew6kV1cA@mail.gmail.com>
Message-ID: <CAEaSS0azqvv5bzD3ytiedTntM3v7K8uB3C5jgLo2OQrb4RML4g@mail.gmail.com>

On Fri, Jan 3, 2014 at 6:42 PM, Scott Marlowe <scott.marlowe at gmail.com>wrote:

> On Fri, Jan 3, 2014 at 7:04 PM, Tory M Blue <tmblue at gmail.com> wrote:
> >
> >
> >
> > On Fri, Jan 3, 2014 at 5:45 PM, Scott Marlowe <scott.marlowe at gmail.com>
> > wrote:
> >>
> >> On Fri, Jan 3, 2014 at 6:25 PM, Tory M Blue <tmblue at gmail.com> wrote:
> >> > "Slon daemon is running on the slave not the master", ummm I'm running
> >> > the
> >> > slon daemon on all of my DB's (master/Slave, QuerySlave1, QuerySlave2)
> >> >
> >> >  slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
> >> > user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &
> >>
> >> I mean that the actual daemon needs to be running ON the slave node,
> >> not on the master node. Not sure if I wasn't clear, or if you weren't
> >> clear. I.e. you start the daemon on each slave, not on the master. IF
> >> that's what you're doing then just ignore this.
> >
> >
> >
> > No, I run the daemon on all nodes.  I feel I'm totally not following or
> have
> > confused myself :)
> >
> > But I run the upper command with a $MASTERHOST substitution.
>
> So on which MACHINE HOST do you run the above commands? I.e are you
> running that command x times on the master host to connect to all
> slaves, or are yuo running it once on each host for that one node.
>

Oops Sorry Scott/all, the above command is ran once on the each host, So
all hosts are running the slon Daemon.

Master Host (primary insert DB) (idb01.domain.com)

 slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
user=$SUPERUSER host=$MASTERHOST" 2>&1 > /data/logs/slon.log &

Slave Host (secondary Insert DB) idb02.domain.com

 slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &

Query Slave 1 (Read only query DB) qdb01.domain.com

 slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
user=$SUPERUSER host=$QUERYHOST1" 2>&1 > /data/logs/slon.log &

Query Slave 2 (Red only query DB) qdb02.domain.com

 slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
user=$SUPERUSER host=$QUERYHOST2" 2>&1 > /data/logs/slon.log &

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140103/e0e32525/attachment-0001.htm 

From nsammu at gmail.com  Fri Jan  3 20:33:13 2014
From: nsammu at gmail.com (ammu narasimham)
Date: Sat, 4 Jan 2014 10:03:13 +0530
Subject: [Slony1-general] query running slow in slony replication cluster
	environment
Message-ID: <CAL5YJY408+cr7ytV3kajEY=N4tX0gF2Tia6Xa0DwdXZAbuUDFw@mail.gmail.com>

Hello,

I have configured the replication cluster using slony on Postgres-9.2
successfully. It is 2-node cluster(Master-slave). When i tried to insert
100k rows in a table, the execution time is nearly doubled in the new
environment.

any insights are welcomed...

thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140104/401fba3e/attachment.htm 

From JanWieck at Yahoo.com  Sat Jan  4 05:22:14 2014
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat, 04 Jan 2014 08:22:14 -0500
Subject: [Slony1-general] query running slow in slony replication
 cluster environment
In-Reply-To: <CAL5YJY408+cr7ytV3kajEY=N4tX0gF2Tia6Xa0DwdXZAbuUDFw@mail.gmail.com>
References: <CAL5YJY408+cr7ytV3kajEY=N4tX0gF2Tia6Xa0DwdXZAbuUDFw@mail.gmail.com>
Message-ID: <52C80B06.30306@Yahoo.com>

On 01/03/14 23:33, ammu narasimham wrote:
> Hello,
> 
> I have configured the replication cluster using slony on Postgres-9.2
> successfully. It is 2-node cluster(Master-slave). When i tried to insert
> 100k rows in a table, the execution time is nearly doubled in the new
> environment.

Since each INSERT causes a second INSERT from the Slony log trigger into
the replication log, this is normal. Is this (inserting 100,000 rows at
once) something, your application does on a regular base, or is this
some artificial, unrealistic test, you thought would give you any useful
information?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cbbrowne at afilias.info  Sun Jan  5 09:14:01 2014
From: cbbrowne at afilias.info (Christopher Browne)
Date: Sun, 5 Jan 2014 12:14:01 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<CAOR=d=2zEEm+wUBCiMEzWBqDkmkcRwuQr2ua8mZ1kiWrGKgXJQ@mail.gmail.com>
	<CAEaSS0aCk_K6wjpEg8w0+4g7q3pu_2qHTs9hbM1yWtWBNhVRMw@mail.gmail.com>
Message-ID: <CANfbgbYoORvRcPx48fdGbB7fWkf+u0U_oYKrsKUOUzCuHX2VRQ@mail.gmail.com>

On Jan 3, 2014 8:26 PM, "Tory M Blue" <tmblue at gmail.com> wrote:
>
>
>
>
> On Fri, Jan 3, 2014 at 5:03 PM, Scott Marlowe <scott.marlowe at gmail.com>
wrote:
>>
>> On Thu, Jan 2, 2014 at 12:35 PM, Tory M Blue <tmblue at gmail.com> wrote:
>> >
>> > Wondering what settings I need to speed this up. To do a rebuild of a
db it
>> > takes a long time, 6 hours for a singe  table. No I/O issues, no load,
just
>> > slon postgres taking their sweet old time. I would like to use the
resources
>> > available to speed this up.
>>
>> 1: How fast can you scp a file between these two boxes?
>> 2: How far apart are they / is there a lot of latency between them?
>> 3: How fast can you create a good sized file on the slave? (dd
>> if=/etc/zero of=bigfile bs=1000000 count=1000)
>>
>> If they are far apart then make sure your slon daemon is running on
>> the slave not the master. This can make a huge difference.
>>
>> > #sync_max_rowsize=8192
>>
>> Try turning this up to 32768 as well.

Hmm.  Probably only matters if you have big tuples.  E.g. where some rows
can get large, such as where some columns sometimes contain big blobs of
data.  (We have replicated the RT ticket system; if someone attaches a huge
80MB spreadsheet to a ticket, that leads to a ~80MB tuple to replicate.)

If this matters, bumping the max improves streaming, but at the cost of
potentially increasing memory consumption a lot.  If it's an issue, then
WAY better to upgrade to Slony 2.2 where it uses COPY to stream in updates,
and the issue (and the parameter) goes away.

>
> 1) Very quick , Accessing an iscsi volume I can scp  the bigfile across
at roughly 45MB/s, so plenty.
> 2) Again same segment, nothing in the way
> 3) ISCSI volume (ISCSI device is running at roughly 6GB network speed)
>  [user at idb02 tmp]# time dd if=/dev/zero of=bigfile bs=1000000 count=1000
> 1000+0 records in
> 1000+0 records out
> 1000000000 bytes (1.0 GB) copied, 1.52658 s, 655 MB/s
>
> real 0m1.529s
> user 0m0.002s
> sys 0m1.526s
>
> "Slon daemon is running on the slave not the master", ummm I'm running
the slon daemon on all of my DB's (master/Slave, QuerySlave1, QuerySlave2)
>
>  slon $CLS_CLUSTER -f /data/pgsql/slon.conf  "dbname=$ADMCLSDB
user=$SUPERUSER host=$SLAVEHOST" 2>&1 > /data/logs/slon.log &

The point is that the slon that manages each node talks to its oo;;f[; a
lot, so keep the slon near its "own" database.

Best is for slon processes to do their initial connections to localhost, so
they are located on The Very Same Host.

Nearby is probably ok; we've had DB 'app servers,' one at each data centre,
and all the slons for the data centre run on one host.  Arguably slower,
but it means processes and logs are centralized to one place, which is
pretty convenient when managing the cluster.  (No need to ssh to a bunch of
places to start/stop the cluster.)

Bad is if there's a data centre in New York and one in San Francisco, and
the slons are all in NY, so that the ones managing SF nodes have
cross-the-country latency.

Less a big deal in Slony 2.2, where replication uses COPY streaming on
subscribers, but for 2.1 and lower, bad latency is Real Bad; keep slons
close to the nodes they manage :-).

> I'll give the sync_max_rowsize a check as well
>
> Thanks again
>
> Tory(((((((((((8((
>
> _________________________________((_____________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinf((((o/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140105/4bedfd8c/attachment.htm 

From ssinger at ca.afilias.info  Sun Jan  5 11:49:57 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Sun, 05 Jan 2014 14:49:57 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<00ea01cf07fe$d0272e30$70758a90$@com>
	<CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>
	<52C7107D.70709@Yahoo.com>
	<CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>
Message-ID: <52C9B765.2040505@ca.afilias.info>

On 01/03/2014 02:42 PM, Tory M Blue wrote:


> And since it appears that my index creation is the big time hole, I'm
> going to be testing work mem, by going from 100M to 2GB+ and i'll reduce
> my vacuum processes from 7 to maybe 2 or 3, so I don't blow anything out
> . But you have intrigued me :)
>

Have you actually confirmed that the index rebuild is what is taking 
most of the time?

The other thing you might want to consider is dropping the indexes on 
the replica before you start the subscribe set.  Then you can create new 
indexes outside of slon after the subscription is done.  If you do it 
this way you can create multiple indexes on the table in parallel and 
use multiple cores.




> Thanks
> Tory
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From tmblue at gmail.com  Sun Jan  5 12:05:27 2014
From: tmblue at gmail.com (Tory M Blue)
Date: Sun, 5 Jan 2014 12:05:27 -0800
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <52C9B765.2040505@ca.afilias.info>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<00ea01cf07fe$d0272e30$70758a90$@com>
	<CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>
	<52C7107D.70709@Yahoo.com>
	<CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>
	<52C9B765.2040505@ca.afilias.info>
Message-ID: <CAEaSS0b195KZuCtR_TUio8kyGPcCc1ESKx8zEB_ZHSuUgKEZYw@mail.gmail.com>

On Sun, Jan 5, 2014 at 11:49 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> On 01/03/2014 02:42 PM, Tory M Blue wrote:
>
>
>  And since it appears that my index creation is the big time hole, I'm
>> going to be testing work mem, by going from 100M to 2GB+ and i'll reduce
>> my vacuum processes from 7 to maybe 2 or 3, so I don't blow anything out
>> . But you have intrigued me :)
>>
>>
> Have you actually confirmed that the index rebuild is what is taking most
> of the time?
>
> The other thing you might want to consider is dropping the indexes on the
> replica before you start the subscribe set.  Then you can create new
> indexes outside of slon after the subscription is done.  If you do it this
> way you can create multiple indexes on the table in parallel and use
> multiple cores.
>
>
> I believe so, based on how fast I get the copy statement, meaning that it
copied the table over and the 5+hours has to be the index creation.

I did some tests today with my query DB's, which used to take 2-4 hours to
replicate (from a ground up (drop/add)). Took 25 minutes today. So I think
giving slony resources to create the indexes is vital, I had it set to
100MB, set it to 2000MB today and reduced the vacuum processes from 5 to 3
(just to be sure) and this really seems to help. I won't be able to test
this on my insert db's for a bit (have to wait for a scheduled maintenance).

Droppping indexes on a production node before replicating, that seems like
a really bad idea, it would cause my production system to drop to it's
knees or fall over while trying to replicate to the secondary. The indexes
are required and I really can't remove them. Now maybe you are meaning that
I move the index creation to the end of the addnode, vs doing it
immediately upon a table creation?

thanks again I appreciate the assistance this group has given!

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20140105/c5b7d102/attachment.htm 

From ssinger at ca.afilias.info  Sun Jan  5 13:03:31 2014
From: ssinger at ca.afilias.info (Steve Singer)
Date: Sun, 05 Jan 2014 16:03:31 -0500
Subject: [Slony1-general] 6 hours to replicate single table,
 12 hours to replicate DB
In-Reply-To: <CAEaSS0b195KZuCtR_TUio8kyGPcCc1ESKx8zEB_ZHSuUgKEZYw@mail.gmail.com>
References: <CAEaSS0a=GVoNJjre3_tpEEknzyk-2qzh6dYM1x+UQRr7ufTwmg@mail.gmail.com>
	<00ea01cf07fe$d0272e30$70758a90$@com>
	<CAEaSS0bZ50ntoeNcgkx=fpjHwZpPZm8X5NBJTCu5v8jiztG1Gg@mail.gmail.com>
	<52C7107D.70709@Yahoo.com>
	<CAEaSS0Y4Y42G-coghAooFwNvBuQ1UKX9Z+BTJ4LE2ZG1QCm5FQ@mail.gmail.com>
	<52C9B765.2040505@ca.afilias.info>
	<CAEaSS0b195KZuCtR_TUio8kyGPcCc1ESKx8zEB_ZHSuUgKEZYw@mail.gmail.com>
Message-ID: <52C9C8A3.8060103@ca.afilias.info>

On 01/05/2014 03:05 PM, Tory M Blue wrote:
>
>
> I believe so, based on how fast I get the copy statement, meaning that
> it copied the table over and the 5+hours has to be the index creation.
>
> I did some tests today with my query DB's, which used to take 2-4 hours
> to replicate (from a ground up (drop/add)). Took 25 minutes today. So I
> think giving slony resources to create the indexes is vital, I had it
> set to 100MB, set it to 2000MB today and reduced the vacuum processes
> from 5 to 3 (just to be sure) and this really seems to help. I won't be
> able to test this on my insert db's for a bit (have to wait for a
> scheduled maintenance).
>
> Droppping indexes on a production node before replicating, that seems
> like a really bad idea, it would cause my production system to drop to
> it's knees or fall over while trying to replicate to the secondary. The
> indexes are required and I really can't remove them. Now maybe you are
> meaning that I move the index creation to the end of the addnode, vs
> doing it immediately upon a table creation?
>

I am not saying you should drop the indexes from your master, just this 
replica.  Slony disables those indexes during the COPY anyway.  If you 
have any applications querying this table on the replica being rebuilt 
then I'd expect those queries to be blocked behind the copy anyway. 
Moving index-creation for this table to the end of the addnode would 
have the same effect, you could then create multiple indexes on the 
table in parallel (I'm assuming that the table has multiple indexes).

But it sounds like tuning the memory settings are enough for you.


> thanks again I appreciate the assistance this group has given!
>
> Tory


