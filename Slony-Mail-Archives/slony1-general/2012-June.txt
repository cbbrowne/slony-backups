From faber_pad at yahoo.it  Fri Jun  1 11:39:56 2012
From: faber_pad at yahoo.it (Fabio P.)
Date: Fri, 1 Jun 2012 19:39:56 +0100 (BST)
Subject: [Slony1-general] Question on failover on slony 1.23
Message-ID: <1338575996.81198.YahooMailNeo@web29704.mail.ird.yahoo.com>



?
Dear all,

I used the slony v1.23 to create a cluster on a postgres db v8.0.x, I am using this old version of slony since, due to constraints on software using the db I cannot upgrade the postgres version.
The database to be put in safety using slony is composed of 4 databases.
The main goal I need to reach is to have a master and a slave containing the same data and, in case of failure on the main host to switch on the slave and use them in read/write mode.
Following the instruction in slony documentation I was able to create the first part (i.e. the data mirroring on the slave), the second part is very far to be reached since the slave is locked. Is there any way to unlock the slave? It could be also accettable for me to destroy all the cluster information ?on the slave and to use them as the slony was never used and in case of newly availability of the master to dump/import the databases newly on the master and to recreate again the whole cluster, but I was not able to do this since it seems that there are a lot of "zombies" objects not identifieble in simply way, is there any chanche to use a sort of bigbang tool on the slave cleaning up all slony structures (but leaving the business data unchanged)?
I guess there is the possibility that this email address is not that I have to use, if this is the case, I would ask if there is any address(or forum) I can use.
Many thanks in advance.
Fabio.?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120601/2baa9e3d/attachment.htm 

From laurapassigni at yahoo.it  Mon Jun  4 02:07:04 2012
From: laurapassigni at yahoo.it (Laura Passigni)
Date: Mon, 4 Jun 2012 10:07:04 +0100 (BST)
Subject: [Slony1-general] FOR UPDATE query
Message-ID: <1338800824.92959.YahooMailNeo@web28706.mail.ir2.yahoo.com>

Hi, 
I found in my server the following query, waiting for more than 1 day.
?
SELECT * FROM "my_table" WHERE ("my_table"."id" = 2626968981)? FOR UPDATE;
?
It is not?coming form my client, in fact it is from the internal server.
My_table is a table involved in the slony, so I was wondering if this query is launched by slony process.
Can someone help me in understanding this? 
?
Thank you,
Laura Passigni
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120604/cf66c4e0/attachment.htm 

From ssinger at ca.afilias.info  Mon Jun  4 06:58:38 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 04 Jun 2012 09:58:38 -0400
Subject: [Slony1-general] Question on failover on slony 1.23
In-Reply-To: <1338575996.81198.YahooMailNeo@web29704.mail.ird.yahoo.com>
References: <1338575996.81198.YahooMailNeo@web29704.mail.ird.yahoo.com>
Message-ID: <4FCCBF0E.9080503@ca.afilias.info>

On 12-06-01 02:39 PM, Fabio P. wrote:
>
> Dear all,
>
> I used the slony v1.23 to create a cluster on a postgres db v8.0.x, I am
> using this old version of slony since, due to constraints on software
> using the db I cannot upgrade the postgres version.
> The database to be put in safety using slony is composed of 4 databases.
> The main goal I need to reach is to have a master and a slave containing
> the same data and, in case of failure on the main host to switch on the
> slave and use them in read/write mode.
> Following the instruction in slony documentation I was able to create
> the first part (i.e. the data mirroring on the slave), the second part
> is very far to be reached since the slave is locked. Is there any way to
> unlock the slave? It could be also accettable for me to destroy all the
> cluster information on the slave and to use them as the slony was never

1) The FAILOVER command will unlock the tables on the slave. See 
http://www.slony.info/documentation/1.2/stmtfailover.html

2) Use the UNINSTALL NODE 
http://www.slony.info/documentation/1.2/stmtuninstallnode.html command 
to uninstall slony on the slave

The UNINSTALL NODE command will remove all the slony information on the 
slave.


> used and in case of newly availability of the master to dump/import the
> databases newly on the master and to recreate again the whole cluster,
> but I was not able to do this since it seems that there are a lot of
> "zombies" objects not identifieble in simply way, is there any chanche
> to use a sort of bigbang tool on the slave cleaning up all slony
> structures (but leaving the business data unchanged)?
> I guess there is the possibility that this email address is not that I
> have to use, if this is the case, I would ask if there is any address(or
> forum) I can use.
> Many thanks in advance.
> Fabio.
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From cbbrowne at afilias.info  Mon Jun  4 07:58:04 2012
From: cbbrowne at afilias.info (Christopher Browne)
Date: Mon, 4 Jun 2012 10:58:04 -0400
Subject: [Slony1-general] FOR UPDATE query
In-Reply-To: <1338800824.92959.YahooMailNeo@web28706.mail.ir2.yahoo.com>
References: <1338800824.92959.YahooMailNeo@web28706.mail.ir2.yahoo.com>
Message-ID: <CANfbgbbvKKOysOvgSmFiHuJFqzdrrf1YtgdY+scyz1a6hyHKWA@mail.gmail.com>

On Mon, Jun 4, 2012 at 5:07 AM, Laura Passigni <laurapassigni at yahoo.it> wrote:
>
> Hi,
> I found in my server the following query, waiting for more than 1 day.
>
> SELECT * FROM "my_table" WHERE ("my_table"."id" = 2626968981)? FOR UPDATE;
>
> It is not?coming form my client, in fact it is from the internal server.
> My_table is a table involved in the slony, so I was wondering if this query is launched by slony process.
> Can someone help me in understanding this?

That doesn't seem like a query that Slony would generate.

If you have set up Slony to use its own user, then you could check
pg_stat_activity, and validate that that query isn't being run as that
user.

From riyav at hotmail.com  Tue Jun  5 11:27:21 2012
From: riyav at hotmail.com (Riya V)
Date: Tue, 5 Jun 2012 11:27:21 -0700
Subject: [Slony1-general] creating a new trigger for a table on a slave host
Message-ID: <BLU166-W3030AE8EF3E51FD36A32CABA0C0@phx.gbl>










I need to modify my database schema and create a new trigger function. I know that triggers are suppressed on the slave once Slony starts up. However since slony is now already running on the slave, if I create a new trigger there, it will not be suppressed. What is the best practice for creating a trigger on the slave? Should you take the table out of replication, create trigger , then add the table back in? Is there some other way to do it? 
Thanks!
RV 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120605/0e9a10d7/attachment.htm 

From ssinger at ca.afilias.info  Tue Jun  5 14:33:50 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 05 Jun 2012 17:33:50 -0400
Subject: [Slony1-general] slony.info website outage
Message-ID: <4FCE7B3E.308@ca.afilias.info>

The slony.info website will be taken offline for maintenance on:

Tuesday June 5 starting around 7pm PST.

* The website - www.slony.info
* The slony bugzilla instance
* All slony mailing lists

will be offline, this is expected to last a few hours.



Thanks

From glynastill at yahoo.co.uk  Wed Jun  6 04:19:02 2012
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed, 6 Jun 2012 12:19:02 +0100 (BST)
Subject: [Slony1-general] creating a new trigger for a table on a slave
	host
In-Reply-To: <BLU166-W3030AE8EF3E51FD36A32CABA0C0@phx.gbl>
References: <BLU166-W3030AE8EF3E51FD36A32CABA0C0@phx.gbl>
Message-ID: <1338981542.99081.YahooMailNeo@web171402.mail.ir2.yahoo.com>

What version of slony?? 


For 1.2 and earlier add the trigger via slonik and use store trigger if you want to enable it:

http://slony.info/documentation/1.2/stmtstoretrigger.html

For 2.0 and later use DISABLE/ENABLE [REPLICA|ALWAYS] TRIGGER in postgresql.




>________________________________
> From: Riya V <riyav at hotmail.com>
>To: slony1-general at lists.slony.info 
>Sent: Tuesday, 5 June 2012, 19:27
>Subject: [Slony1-general] creating a new trigger for a table on a slave host
> 
>
> 
>I need to modify my database schema and create a new trigger function. I know that triggers are suppressed on the slave once Slony starts up. However since slony is now already running on the slave, if I create a new trigger there, it will not be suppressed. What is the best practice for creating a trigger on the slave? Should you take the table out of replication, create trigger , then add the table back in? Is there some other way to do it??
>Thanks!
>
>
>RV
>_______________________________________________
>Slony1-general mailing list
>Slony1-general at lists.slony.info
>http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120606/15772264/attachment.html 

From glynastill at yahoo.co.uk  Wed Jun  6 14:53:13 2012
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed, 6 Jun 2012 22:53:13 +0100 (BST)
Subject: [Slony1-general] creating a new trigger for a table on a slave
	host
In-Reply-To: <BLU166-W6317BDD84E504C4730972CBA0D0@phx.gbl>
References: <BLU166-W3030AE8EF3E51FD36A32CABA0C0@phx.gbl>,
	<1338981542.99081.YahooMailNeo@web171402.mail.ir2.yahoo.com>
	<BLU166-W6317BDD84E504C4730972CBA0D0@phx.gbl>
Message-ID: <1339019593.43501.YahooMailNeo@web171402.mail.ir2.yahoo.com>

Great, try to keep the messages on the mailing list.




>________________________________
> From: Riya V <riyav at hotmail.com>
>To: glynastill at yahoo.co.uk 
>Sent: Wednesday, 6 June 2012, 18:12
>Subject: RE: [Slony1-general] creating a new trigger for a table on a slave host
> 
>
> 
>Thanks!
>We are on 1.2 now and planning to move to 2.X
>
>
>Riya
>
>
>
>________________________________
>Date: Wed, 6 Jun 2012 12:19:02 +0100
>From: glynastill at yahoo.co.uk
>Subject: Re: [Slony1-general] creating a new trigger for a table on a slave host
>To: riyav at hotmail.com; slony1-general at lists.slony.info
>
>
>What version of slony?? 
>
>
>
>For 1.2 and earlier add the trigger via slonik and use store trigger if you want to enable it:
>
>
>http://slony.info/documentation/1.2/stmtstoretrigger.html
>
>
>For 2.0 and later use DISABLE/ENABLE [REPLICA|ALWAYS] TRIGGER in postgresql.
>
>
>
>>________________________________
>> From: Riya V <riyav at hotmail.com>
>>To: slony1-general at lists.slony.info 
>>Sent: Tuesday, 5 June 2012, 19:27
>>Subject: [Slony1-general] creating a new trigger for a table on a slave host
>> 
>>
>> 
>>I need to modify my database schema and create a new trigger function. I know that triggers are suppressed on the slave once Slony starts up. However since slony is now already running on the slave, if I create a new trigger there, it will not be suppressed. What is the best practice for creating a trigger on the slave? Should you take the table out of replication, create trigger , then add the table back in? Is there some other way to do it??
>>Thanks!
>>
>>
>>RV
>>_______________________________________________
>>Slony1-general mailing list
>>Slony1-general at lists.slony.info
>>http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120606/0c163a2a/attachment.htm 

From zbentley at crsinc.com  Thu Jun  7 07:03:24 2012
From: zbentley at crsinc.com (Zac Bentley)
Date: Thu, 7 Jun 2012 14:03:24 +0000
Subject: [Slony1-general] Slony initial subscription is causing
 out-of-memory kernel panics. Why?
Message-ID: <9275DB4EA36C2542985A16A04243E85A591F9A@mbx025-e1-nj-8.exch025.domain.local>

Context:

We run Slony 2.0 with Postgres 8.4 on two CentOS 6 servers--one master, and one slave. Our database is about 30GB in size, which isn't unusual, but we do have a couple of tables that are more than 5GB each.

Recently, we needed to re-build our Slony cluster. I turned off Slony, restored identical database snapshots on the master and the slave, set up my slony.conf and slon_tools.conf, started the slons, ran slonik_init_cluster | slonik, then slonik_create_set 1 | slonik (we only have one replication set), and finally slonik_subscribe_set 1 2 | slonik. Everything looked good, and I was able to watch subscription progress in the logs.

Then the server stopped responding. I rebooted it, and saw "Kernel panic - not syncing: Out of memory and no killable processes" after it had killed everything it could.

It happens during the subscription process when our first "large" (3GB) table is encountered. The logs report "so and so bytes copied for table" for the table in question, then a few dozen queued SYNC events, and then they detect a child process crash and log a watchdog-initialized restart.



What I've tried:

First I blew away the database completely, re ran initdb, and then restored the identical snapshots again. Same kernel panic. Then I blew it away, uninstalled Postgres and Slony, and reinstalled them. I double-checked all of our memory-based settings in postgresql.conf, and they are all at stock/recommended levels (i.e. shared_buffers is at 1/4 of RAM etc etc). I ran a VACUUM ANALYZE FULL on the database before initializing the Slony cluster. As a last-ditch effort, I completely reinstalled the OS and all base software on the db servers and started from scratch. Same result every time: kernel panic, out of memory. It happens on the slave server, which runs both of the slons.



Question:

Why is this happening?

Our database has grown fairly linearly over the past few months (at the beginning of the year it was about 23GB, now it's 30), and every other time I have had to re-initialize the Slony cluster on these same servers, it has worked fine.


Zac Bentley
Systems Administrator
Corporate Reimbursement Services, Inc.
www.crsinc.com<http://www.crsinc.com/>
617-467-1949


This email message contains information that Corporate Reimbursement Services, Inc. considers confidential and/or proprietary, or may later designate as confidential and proprietary. It is intended only for use of the individual or entity named above and should not be forwarded to any other persons or entities without the express consent of Corporate Reimbursement Services, Inc., nor should it be used for any purpose other than in the course of any potential or actual business relationship with Corporate Reimbursement Services, Inc. If the reader of this message is not the intended recipient, or the employee or agent responsible to deliver it to the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication is strictly prohibited. If you have received this communication in error, please notify sender immediately and destroy the original message.

Internal Revenue Service regulations require that certain types of written advice include a disclaimer. To the extent the preceding message contains advice relating to a Federal tax issue, unless expressly stated otherwise the advice is not intended or written to be used, and it cannot be used by the recipient or any other taxpayer, for the purpose of avoiding Federal tax penalties, and was not written to support the promotion or marketing of any transaction or matter discussed herein.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120607/2a743884/attachment.htm 

From ssinger at ca.afilias.info  Thu Jun  7 07:11:10 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 07 Jun 2012 10:11:10 -0400
Subject: [Slony1-general] Slony initial subscription is causing
 out-of-memory kernel panics. Why?
In-Reply-To: <9275DB4EA36C2542985A16A04243E85A591F9A@mbx025-e1-nj-8.exch025.domain.local>
References: <9275DB4EA36C2542985A16A04243E85A591F9A@mbx025-e1-nj-8.exch025.domain.local>
Message-ID: <4FD0B67E.5090705@ca.afilias.info>

On 12-06-07 10:03 AM, Zac Bentley wrote:

1.  Exactly which version of slony is this 2.0.7 ?

2.  If you watch your server (ie with top) as this happens which 
processes grow in memory?  postgres or slon?  How big do they grow to 
before things start being killed?  Does the memory usage grow gradually 
over time or does it stay low and then spike right before the crash?

3. Does the SUBSCRIBE SET  actually finish before the crash?

4. If after the crash you restart postgresql and slon (but don't delete 
anything, perform an initdb or reinstall slony) what happens?




> *Context:*
>
> We run Slony 2.0 with Postgres 8.4 on two CentOS 6 servers--one master,
> and one slave. Our database is about 30GB in size, which isn't unusual,
> but we do have a couple of tables that are more than 5GB each.
>
> Recently, we needed to re-build our Slony cluster. I turned off Slony,
> restored identical database snapshots on the master and the slave, set
> up my slony.conf and slon_tools.conf, started the slons, ran
> |slonik_init_cluster | slonik|, then|slonik_create_set 1 | slonik|(we
> only have one replication set), and finally|slonik_subscribe_set 1 2 |
> slonik|. Everything looked good, and I was able to watch subscription
> progress in the logs.
>
> Then the server stopped responding. I rebooted it, and saw "Kernel panic
> - not syncing: Out of memory and no killable processes" after it had
> killed everything it could.
>
> It happens during the subscription process when our first ?large? (3GB)
> table is encountered. The logs report ?so and so bytes copied for table?
> for the table in question, then a few dozen queued SYNC events, and then
> they detect a child process crash and log a watchdog-initialized restart.
>
> *What I've tried:*
>
> First I blew away the database completely, re ran|initdb|, and then
> restored the identical snapshots again. Same kernel panic. Then I blew
> it away, uninstalled Postgres and Slony, and reinstalled them. I
> double-checked all of our memory-based settings in postgresql.conf, and
> they are all at stock/recommended levels (i.e.|shared_buffers|is at 1/4
> of RAM etc etc). I ran a|VACUUM ANALYZE FULL |on the database before
> initializing the Slony cluster. As a last-ditch effort, I completely
> reinstalled the OS and all base software on the db servers and started
> from scratch. Same result every time: kernel panic, out of memory. It
> happens on the slave server, which runs both of the slons.
>
> *Question:*
>
> Why is this happening?
>
> Our database has grown fairly linearly over the past few months (at the
> beginning of the year it was about 23GB, now it's 30), and every other
> time I have had to re-initialize the Slony cluster on these same
> servers, it has worked fine.
>
> Zac Bentley
>
> Systems Administrator
>
> Corporate Reimbursement Services, Inc.
>
> www.crsinc.com <http://www.crsinc.com/>
>
> 617-467-1949
>
> This email message contains information that Corporate Reimbursement
> Services, Inc. considers confidential and/or proprietary, or may later
> designate as confidential and proprietary. It is intended only for use
> of the individual or entity named above and should not be forwarded to
> any other persons or entities without the express consent of Corporate
> Reimbursement Services, Inc., nor should it be used for any purpose
> other than in the course of any potential or actual business
> relationship with Corporate Reimbursement Services, Inc. If the reader
> of this message is not the intended recipient, or the employee or agent
> responsible to deliver it to the intended recipient, you are hereby
> notified that any dissemination, distribution, or copying of this
> communication is strictly prohibited. If you have received this
> communication in error, please notify sender immediately and destroy the
> original message.
>
> Internal Revenue Service regulations require that certain types of
> written advice include a disclaimer. To the extent the preceding message
> contains advice relating to a Federal tax issue, unless expressly stated
> otherwise the advice is not intended or written to be used, and it
> cannot be used by the recipient or any other taxpayer, for the purpose
> of avoiding Federal tax penalties, and was not written to support the
> promotion or marketing of any transaction or matter discussed herein.
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From zbentley at crsinc.com  Thu Jun  7 07:22:14 2012
From: zbentley at crsinc.com (Zac Bentley)
Date: Thu, 7 Jun 2012 14:22:14 +0000
Subject: [Slony1-general] Slony initial subscription is causing
 out-of-memory kernel panics. Why?
In-Reply-To: <4FD0B67E.5090705@ca.afilias.info>
References: <9275DB4EA36C2542985A16A04243E85A591F9A@mbx025-e1-nj-8.exch025.domain.local>
	<4FD0B67E.5090705@ca.afilias.info>
Message-ID: <9275DB4EA36C2542985A16A04243E85A592005@mbx025-e1-nj-8.exch025.domain.local>

1. 2.0.7, correct.
2. I don't know for certain. I'm running another initial subscribe right now but it will take some time before it gets to the critical point. I would assume that the Slons are the ones eating memory, since I know from experience that the OOM killer will kill Postgres quite readily. Something that the OOM killer can't touch is leading to the system running out of memory, so I'd assume the Slons.
3. No, it doesn't. It gets part of the way through subscription of a large table and then the lots start showing watchdog-restarts, 4 or 5 times, and then the system kernel panics.
4. If I restart them after the crash without touching anything, the subscription process starts again from the very beginning.

Zac Bentley
Systems Administrator
Corporate Reimbursement Services, Inc.
www.crsinc.com
617-467-1949

This email message contains information that Corporate Reimbursement Services, Inc. considers confidential and/or proprietary, or may later designate as confidential and proprietary. It is intended only for use of the individual or entity named above and should not be forwarded to any other persons or entities without the express consent of Corporate Reimbursement Services, Inc., nor should it be used for any purpose other than in the course of any potential or actual business relationship with Corporate Reimbursement Services, Inc. If the reader of this message is not the intended recipient, or the employee or agent responsible to deliver it to the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication is strictly prohibited. If you have received this communication in error, please notify sender immediately and destroy the original message.

Internal Revenue Service regulations require that certain types of written advice include a disclaimer. To the extent the preceding message contains advice relating to a Federal tax issue, unless expressly stated otherwise the advice is not intended or written to be used, and it cannot be used by the recipient or any other taxpayer, for the purpose of avoiding Federal tax penalties, and was not written to support the promotion or marketing of any transaction or matter discussed herein.

From ssinger at ca.afilias.info  Thu Jun  7 07:28:53 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 07 Jun 2012 10:28:53 -0400
Subject: [Slony1-general] Slony initial subscription is causing
 out-of-memory kernel panics. Why?
In-Reply-To: <9275DB4EA36C2542985A16A04243E85A592005@mbx025-e1-nj-8.exch025.domain.local>
References: <9275DB4EA36C2542985A16A04243E85A591F9A@mbx025-e1-nj-8.exch025.domain.local>
	<4FD0B67E.5090705@ca.afilias.info>
	<9275DB4EA36C2542985A16A04243E85A592005@mbx025-e1-nj-8.exch025.domain.local>
Message-ID: <4FD0BAA5.3000902@ca.afilias.info>

On 12-06-07 10:22 AM, Zac Bentley wrote:
> 1. 2.0.7, correct.
> 2. I don't know for certain. I'm running another initial subscribe right now but it will take some time before it gets to the critical point. I would assume that the Slons are the ones eating memory, since I know from experience that the OOM killer will kill Postgres quite readily. Something that the OOM killer can't touch is leading to the system running out of memory, so I'd assume the Slons.

Keep an eye on it.  I can't think of a reason why the slon memory 
footprint should grow like that.

> 3. No, it doesn't. It gets part of the way through subscription of a large table and then the lots start showing watchdog-restarts, 4 or 5 times, and then the system kernel panics.

What is the reason/message given for the slon workers being killed? Is 
the OOM killer, killing them?

Off the top of my head I can't think of a reason why the OOM killer 
couldn't kill a slon.  (should the OOM killer be killing slons is a 
different question).



> 4. If I restart them after the crash without touching anything, the subscription process starts again from the very beginning.
>
> Zac Bentley
> Systems Administrator
> Corporate Reimbursement Services, Inc.
> www.crsinc.com
> 617-467-1949
>
> This email message contains information that Corporate Reimbursement Services, Inc. considers confidential and/or proprietary, or may later designate as confidential and proprietary. It is intended only for use of the individual or entity named above and should not be forwarded to any other persons or entities without the express consent of Corporate Reimbursement Services, Inc., nor should it be used for any purpose other than in the course of any potential or actual business relationship with Corporate Reimbursement Services, Inc. If the reader of this message is not the intended recipient, or the employee or agent responsible to deliver it to the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication is strictly prohibited. If you have received this communication in error, please notify sender immediately and destroy the original message.
>
> Internal Revenue Service regulations require that certain types of written advice include a disclaimer. To the extent the preceding message contains advice relating to a Federal tax issue, unless expressly stated otherwise the advice is not intended or written to be used, and it cannot be used by the recipient or any other taxpayer, for the purpose of avoiding Federal tax penalties, and was not written to support the promotion or marketing of any transaction or matter discussed herein.


From dineshkumar02 at gmail.com  Fri Jun  8 11:26:51 2012
From: dineshkumar02 at gmail.com (dinesh kumar)
Date: Fri, 8 Jun 2012 23:56:51 +0530
Subject: [Slony1-general] ::Requesting for suggestions on Slony Replication
	of 3 TB Of Data::
Message-ID: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>

Hi All,

Can i get any slony recommendations for the replication of 3 TB Data with
slony. I mean, slon configuration parameters ..

As of now, it's copied 145 GB of data from Primary to slave and seems from
here onwards it's running very very very slow  .. No Errors in slony logs
and pg_logs ... Slon process is running with it's default configuration
parameters ...

Your help is highly appreciated ..

Thank You,
Dinesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120608/1ef19ec8/attachment.htm 

From ssinger at ca.afilias.info  Fri Jun  8 11:42:08 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 08 Jun 2012 14:42:08 -0400
Subject: [Slony1-general] ::Requesting for suggestions on Slony
 Replication of 3 TB Of Data::
In-Reply-To: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>
References: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>
Message-ID: <4FD24780.7020208@ca.afilias.info>

On 12-06-08 02:26 PM, dinesh kumar wrote:
> Hi All,
>
> Can i get any slony recommendations for the replication of 3 TB Data
> with slony. I mean, slon configuration parameters ..
>
> As of now, it's copied 145 GB of data from Primary to slave and seems
> from here onwards it's running very very very slow  .. No Errors in
> slony logs and pg_logs ... Slon process is running with it's default
> configuration parameters ...

You don't tell us what version of slony your running. I would recommend 
you run at least 2.1.x.

1.2.x and 2.0.x have a performance bug that might mean that slony can 
fall so far behind on the initial sync that it might never catch up.

Has your initial copy_set finished?


>
> Your help is highly appreciated ..
>
> Thank You,
> Dinesh
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From dineshkumar02 at gmail.com  Fri Jun  8 11:50:00 2012
From: dineshkumar02 at gmail.com (dinesh kumar)
Date: Sat, 9 Jun 2012 00:20:00 +0530
Subject: [Slony1-general] ::Requesting for suggestions on Slony
 Replication of 3 TB Of Data::
In-Reply-To: <4FD24780.7020208@ca.afilias.info>
References: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>
	<4FD24780.7020208@ca.afilias.info>
Message-ID: <CALnrH7opmnFHiKUYAbov4AEqudCjsbYg3ENmiJKFUQzHjo4mKw@mail.gmail.com>

Thank you so much Steve ...

We are using Slony 2.1.x ... We are still at the initial stage only...
Slony is still trying to copy the data from source to destination host ..

>> Has your initial copy_set finished?

Here only, we are facing problem... Slony struggling to copy the data .. It
went well upto 140 GB. From 141GB onwards  the build started slow.. Now
it's very very very slow ...

Let me know if you need any configurations what we have used ...

Thanks again for your response ..

Best Regards,
Dinesh



On Sat, Jun 9, 2012 at 12:12 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> On 12-06-08 02:26 PM, dinesh kumar wrote:
>
>> Hi All,
>>
>> Can i get any slony recommendations for the replication of 3 TB Data
>> with slony. I mean, slon configuration parameters ..
>>
>> As of now, it's copied 145 GB of data from Primary to slave and seems
>> from here onwards it's running very very very slow  .. No Errors in
>> slony logs and pg_logs ... Slon process is running with it's default
>> configuration parameters ...
>>
>
> You don't tell us what version of slony your running. I would recommend
> you run at least 2.1.x.
>
> 1.2.x and 2.0.x have a performance bug that might mean that slony can fall
> so far behind on the initial sync that it might never catch up.
>
>
>
>
>> Your help is highly appreciated ..
>>
>> Thank You,
>> Dinesh
>>
>>
>>
>> ______________________________**_________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.**info <Slony1-general at lists.slony.info>
>> http://lists.slony.info/**mailman/listinfo/slony1-**general<http://lists.slony.info/mailman/listinfo/slony1-general>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120609/5aa69476/attachment.htm 

From ssinger at ca.afilias.info  Fri Jun  8 13:09:16 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 08 Jun 2012 16:09:16 -0400
Subject: [Slony1-general] ::Requesting for suggestions on Slony
 Replication of 3 TB Of Data::
In-Reply-To: <CALnrH7opmnFHiKUYAbov4AEqudCjsbYg3ENmiJKFUQzHjo4mKw@mail.gmail.com>
References: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>	<4FD24780.7020208@ca.afilias.info>
	<CALnrH7opmnFHiKUYAbov4AEqudCjsbYg3ENmiJKFUQzHjo4mKw@mail.gmail.com>
Message-ID: <4FD25BEC.30803@ca.afilias.info>

On 12-06-08 02:50 PM, dinesh kumar wrote:
> Thank you so much Steve ...
>
> We are using Slony 2.1.x ... We are still at the initial stage only...
> Slony is still trying to copy the data from source to destination host ..
>
>  >> Has your initial copy_set finished?
>
> Here only, we are facing problem... Slony struggling to copy the data ..
> It went well upto 140 GB. From 141GB onwards  the build started slow..
> Now it's very very very slow ...
>
> Let me know if you need any configurations what we have used ...
>

Are you doing this copy over a WAN?
How long is this taking? How long does it normally take you to load a 
pg_dump of your 3TB database on this hardware?

Can you tell what the bottleneck is? Are you IO bound on the slave? Are 
you CPU bound on the slave?

There isn't a lot of slony configuration that controls the intial sync 
which is done by COPY.  There are a bunch of parameters that control the 
behaviour to process normal SYNC events after your initial subscription 
but I don't think those would effect the intial copy.



> Thanks again for your response ..
>
> Best Regards,
> Dinesh
>
>
>
> On Sat, Jun 9, 2012 at 12:12 AM, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 12-06-08 02:26 PM, dinesh kumar wrote:
>
>         Hi All,
>
>         Can i get any slony recommendations for the replication of 3 TB Data
>         with slony. I mean, slon configuration parameters ..
>
>         As of now, it's copied 145 GB of data from Primary to slave and
>         seems
>         from here onwards it's running very very very slow  .. No Errors in
>         slony logs and pg_logs ... Slon process is running with it's default
>         configuration parameters ...
>
>
>     You don't tell us what version of slony your running. I would
>     recommend you run at least 2.1.x.
>
>     1.2.x and 2.0.x have a performance bug that might mean that slony
>     can fall so far behind on the initial sync that it might never catch up.
>
>
>
>
>         Your help is highly appreciated ..
>
>         Thank You,
>         Dinesh
>
>
>
>         _________________________________________________
>         Slony1-general mailing list
>         Slony1-general at lists.slony.__info
>         <mailto:Slony1-general at lists.slony.info>
>         http://lists.slony.info/__mailman/listinfo/slony1-__general
>         <http://lists.slony.info/mailman/listinfo/slony1-general>
>
>
>


From dineshkumar02 at gmail.com  Fri Jun  8 14:30:47 2012
From: dineshkumar02 at gmail.com (dinesh kumar)
Date: Sat, 9 Jun 2012 03:00:47 +0530
Subject: [Slony1-general] ::Requesting for suggestions on Slony
 Replication of 3 TB Of Data::
In-Reply-To: <4FD25BEC.30803@ca.afilias.info>
References: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>
	<4FD24780.7020208@ca.afilias.info>
	<CALnrH7opmnFHiKUYAbov4AEqudCjsbYg3ENmiJKFUQzHjo4mKw@mail.gmail.com>
	<4FD25BEC.30803@ca.afilias.info>
Message-ID: <CALnrH7oZgj8wUQSPoS6nVhEW+HAEX4bd8SCmYBbt6BYC_RQLZA@mail.gmail.com>

Hi Steve,

Thank you for the e-mail ..

On Sat, Jun 9, 2012 at 1:39 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> On 12-06-08 02:50 PM, dinesh kumar wrote:
>
>> Thank you so much Steve ...
>>
>> We are using Slony 2.1.x ... We are still at the initial stage only...
>> Slony is still trying to copy the data from source to destination host ..
>>
>>  >> Has your initial copy_set finished?
>>
>> Here only, we are facing problem... Slony struggling to copy the data ..
>> It went well upto 140 GB. From 141GB onwards  the build started slow..
>> Now it's very very very slow ...
>>
>> Let me know if you need any configurations what we have used ...
>>
>>
> Are you doing this copy over a WAN?
>

We are doing copy over LAN.. We got very good transfer rate for up to some
extent.. From after some time (i'm not sure the exact time), the build got
slow..


> How long is this taking? How long does it normally take you to load a
> pg_dump of your 3TB database on this hardware?
>
> Currently it's taking 1hour for 1 GB  Copy .. We haven't tested 3TB of
data using pg_dump and restore due to storage constraint ...



> Can you tell what the bottleneck is? Are you IO bound on the slave? Are
> you CPU bound on the slave?
>
> We don't face any performance issues. IO is really good on
production(Primary Cluster where it is 3 TB) and having normal CPU average
...

There isn't a lot of slony configuration that controls the intial sync
> which is done by COPY.  There are a bunch of parameters that control the
> behaviour to process normal SYNC events after your initial subscription but
> I don't think those would effect the intial copy.
>

We think, it's problem from the db itself.. We have around  400GB tables (5
to 8), those are having huge bloats also.. We think, slony is taking some
time to get those live rows bypassing the dead rows from DB. So we think,
the actual live rows might scatter to multiple files. I think this might be
the root cause ..

We just stopped the existing slony setup and will perform vacuum on DB once
and will do it again ...

Can we get the slony event configuration parameters what we need to tune.
So that, we will try those from our side with sample setup ...

If we configure the slony sets like (set 1 having 10 tables of size  400 GB
and set 2 having 10 tables of size 400 GB size .... ) is it good? Which is
the better option ?

Is Slony completes set1 and then only moves to set2 ... ? Or it switching
between the sets ..?

Kindly suggest us ...

Thank you,
Dinesh

>
>  Thanks again for your response ..
>>
>> Best Regards,
>> Dinesh
>>
>>
>>
>> On Sat, Jun 9, 2012 at 12:12 AM, Steve Singer <ssinger at ca.afilias.info
>> <mailto:ssinger at ca.afilias.**info <ssinger at ca.afilias.info>>> wrote:
>>
>>    On 12-06-08 02:26 PM, dinesh kumar wrote:
>>
>>        Hi All,
>>
>>        Can i get any slony recommendations for the replication of 3 TB
>> Data
>>        with slony. I mean, slon configuration parameters ..
>>
>>        As of now, it's copied 145 GB of data from Primary to slave and
>>        seems
>>        from here onwards it's running very very very slow  .. No Errors in
>>        slony logs and pg_logs ... Slon process is running with it's
>> default
>>        configuration parameters ...
>>
>>
>>    You don't tell us what version of slony your running. I would
>>    recommend you run at least 2.1.x.
>>
>>    1.2.x and 2.0.x have a performance bug that might mean that slony
>>    can fall so far behind on the initial sync that it might never catch
>> up.
>>
>>
>>
>>
>>        Your help is highly appreciated ..
>>
>>        Thank You,
>>        Dinesh
>>
>>
>>
>>        ______________________________**___________________
>>        Slony1-general mailing list
>>        Slony1-general at lists.slony.__**info
>>        <mailto:Slony1-general at lists.**slony.info<Slony1-general at lists.slony.info>
>> >
>>        http://lists.slony.info/__**mailman/listinfo/slony1-__**general<http://lists.slony.info/__mailman/listinfo/slony1-__general>
>>        <http://lists.slony.info/**mailman/listinfo/slony1-**general<http://lists.slony.info/mailman/listinfo/slony1-general>
>> >
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120609/52b239e6/attachment.htm 

From scott.marlowe at gmail.com  Fri Jun  8 16:10:41 2012
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri, 8 Jun 2012 17:10:41 -0600
Subject: [Slony1-general] ::Requesting for suggestions on Slony
 Replication of 3 TB Of Data::
In-Reply-To: <CALnrH7oZgj8wUQSPoS6nVhEW+HAEX4bd8SCmYBbt6BYC_RQLZA@mail.gmail.com>
References: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>
	<4FD24780.7020208@ca.afilias.info>
	<CALnrH7opmnFHiKUYAbov4AEqudCjsbYg3ENmiJKFUQzHjo4mKw@mail.gmail.com>
	<4FD25BEC.30803@ca.afilias.info>
	<CALnrH7oZgj8wUQSPoS6nVhEW+HAEX4bd8SCmYBbt6BYC_RQLZA@mail.gmail.com>
Message-ID: <CAOR=d=3wpsvUZ6Z4mCECdvFfwkaz8gRV3ADgiLBYY2Z2caBK5Q@mail.gmail.com>

On Fri, Jun 8, 2012 at 3:30 PM, dinesh kumar <dineshkumar02 at gmail.com> wrote:
> Hi Steve,
>
> Thank you for the e-mail ..
>
> On Sat, Jun 9, 2012 at 1:39 AM, Steve Singer <ssinger at ca.afilias.info>
> wrote:
>>
>> On 12-06-08 02:50 PM, dinesh kumar wrote:
>>>
>>> Thank you so much Steve ...
>>>
>>> We are using Slony 2.1.x ... We are still at the initial stage only...
>>> Slony is still trying to copy the data from source to destination host ..
>>>
>>> ?>> Has your initial copy_set finished?
>>>
>>> Here only, we are facing problem... Slony struggling to copy the data ..
>>> It went well upto 140 GB. From 141GB onwards ?the build started slow..
>>> Now it's very very very slow ...
>>>
>>> Let me know if you need any configurations what we have used ...
>>>
>>
>> Are you doing this copy over a WAN?
>
>
> We are doing copy over LAN.. We got very good transfer rate for up to some
> extent.. From after some time (i'm not sure the exact time), the build got
> slow..

We need to know why.

>> Can you tell what the bottleneck is? Are you IO bound on the slave? Are
>> you CPU bound on the slave?
>>
> We don't face any performance issues. IO is really good on
> production(Primary Cluster where it is 3 TB) and having normal CPU average

What does iostat -xd 10 or such have to say about the performance on
the new slony destination when it's slow?

> We think, it's problem from the db itself.. We have around ?400GB tables (5
> to 8), those are having huge bloats also.. We think, slony is taking some
> time to get those live rows bypassing the dead rows from DB. So we think,
> the actual live rows might scatter to multiple files. I think this might be
> the root cause ..

I doubt bloat in the source database would make things 1GB/hr slow.
That's only 16MB/minute.

> We just stopped the existing slony setup and will perform vacuum on DB once
> and will do it again ...

Yeah, this time try and figure out what the bottleneck is via iostat,
vmstat, iotop, nettop, select * from pg_stat_activity and so on.

From dineshkumar02 at gmail.com  Fri Jun  8 17:13:19 2012
From: dineshkumar02 at gmail.com (dinesh kumar)
Date: Sat, 9 Jun 2012 05:43:19 +0530
Subject: [Slony1-general] ::Requesting for suggestions on Slony
 Replication of 3 TB Of Data::
In-Reply-To: <CAOR=d=3wpsvUZ6Z4mCECdvFfwkaz8gRV3ADgiLBYY2Z2caBK5Q@mail.gmail.com>
References: <CALnrH7rNxP8a4ZKp6sLt2BuZm7R=ndMbKZLty8v4sKXfOK1=rw@mail.gmail.com>
	<4FD24780.7020208@ca.afilias.info>
	<CALnrH7opmnFHiKUYAbov4AEqudCjsbYg3ENmiJKFUQzHjo4mKw@mail.gmail.com>
	<4FD25BEC.30803@ca.afilias.info>
	<CALnrH7oZgj8wUQSPoS6nVhEW+HAEX4bd8SCmYBbt6BYC_RQLZA@mail.gmail.com>
	<CAOR=d=3wpsvUZ6Z4mCECdvFfwkaz8gRV3ADgiLBYY2Z2caBK5Q@mail.gmail.com>
Message-ID: <CALnrH7qy2Jj2KAKFLjpYDOQrFoDX_UH9bawegXiV2kp=8t4u3A@mail.gmail.com>

Thanks scott ...

This time we will track why it's been slow ..

Thanks,
Dinesh..

On Sat, Jun 9, 2012 at 4:40 AM, Scott Marlowe <scott.marlowe at gmail.com>wrote:

> On Fri, Jun 8, 2012 at 3:30 PM, dinesh kumar <dineshkumar02 at gmail.com>
> wrote:
> > Hi Steve,
> >
> > Thank you for the e-mail ..
> >
> > On Sat, Jun 9, 2012 at 1:39 AM, Steve Singer <ssinger at ca.afilias.info>
> > wrote:
> >>
> >> On 12-06-08 02:50 PM, dinesh kumar wrote:
> >>>
> >>> Thank you so much Steve ...
> >>>
> >>> We are using Slony 2.1.x ... We are still at the initial stage only...
> >>> Slony is still trying to copy the data from source to destination host
> ..
> >>>
> >>>  >> Has your initial copy_set finished?
> >>>
> >>> Here only, we are facing problem... Slony struggling to copy the data
> ..
> >>> It went well upto 140 GB. From 141GB onwards  the build started slow..
> >>> Now it's very very very slow ...
> >>>
> >>> Let me know if you need any configurations what we have used ...
> >>>
> >>
> >> Are you doing this copy over a WAN?
> >
> >
> > We are doing copy over LAN.. We got very good transfer rate for up to
> some
> > extent.. From after some time (i'm not sure the exact time), the build
> got
> > slow..
>
> We need to know why.
>
> >> Can you tell what the bottleneck is? Are you IO bound on the slave? Are
> >> you CPU bound on the slave?
> >>
> > We don't face any performance issues. IO is really good on
> > production(Primary Cluster where it is 3 TB) and having normal CPU
> average
>
> What does iostat -xd 10 or such have to say about the performance on
> the new slony destination when it's slow?
>
> > We think, it's problem from the db itself.. We have around  400GB tables
> (5
> > to 8), those are having huge bloats also.. We think, slony is taking some
> > time to get those live rows bypassing the dead rows from DB. So we think,
> > the actual live rows might scatter to multiple files. I think this might
> be
> > the root cause ..
>
> I doubt bloat in the source database would make things 1GB/hr slow.
> That's only 16MB/minute.
>
> > We just stopped the existing slony setup and will perform vacuum on DB
> once
> > and will do it again ...
>
> Yeah, this time try and figure out what the bottleneck is via iostat,
> vmstat, iotop, nettop, select * from pg_stat_activity and so on.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120609/bac13a70/attachment-0001.htm 

From ragavendra.dba at gmail.com  Tue Jun 12 03:30:22 2012
From: ragavendra.dba at gmail.com (Raghav)
Date: Tue, 12 Jun 2012 16:00:22 +0530
Subject: [Slony1-general] How to fast the Slave Sync with Primary on large
	tables ?
Message-ID: <CANwAqWhSvR7jiMSGCVBiXeERgYfF+5-23rfyDaBP+=dG=fS9pQ@mail.gmail.com>

Respected all,

Currently we are in process of upgrading PG 8.4 to PG 9.1 using Slony 2.1.
We have successfully configured Slony 2.1 on to PG 9.1 & 8.4 and its
running fine.
Since the database is 1.5+ TB with large tables, how could we fast the
Slave to catch up Sync with primary ?
What are the parameter's need to be tuned at Slony Level & PostgreSQL level
?

Current Setup :
OS: RHEL 6
DB : PG 9.1 & 8.4
Slony: 2.1

Please advice.

--
Regards
Raghav
Blog: htt://raghavt.blogspot.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120612/52901683/attachment.htm 

From laurapassigni at yahoo.it  Wed Jun 13 01:45:26 2012
From: laurapassigni at yahoo.it (Laura Passigni)
Date: Wed, 13 Jun 2012 09:45:26 +0100 (BST)
Subject: [Slony1-general] Slony configuration
Message-ID: <1339577126.63985.YahooMailNeo@web28701.mail.ir2.yahoo.com>

Hi,
I have 2 versions of Postgres on my test machine (8.4 and 9.0).
After installing Slony, it has been installed in the postgres 9.0, but I would like to have Slony?in Postgres 8.4.
?
I would like to know how to configure Slony for postgres 8.4. Is it feasible? 
?
Thank you,
Laura
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120613/0e58a415/attachment.htm 

