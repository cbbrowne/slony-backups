From ssinger at ca.afilias.info  Wed May  2 14:39:49 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 02 May 2012 17:39:49 -0400
Subject: [Slony1-general] Logship files printing incorrectly
In-Reply-To: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
References: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
Message-ID: <4FA1A9A5.1030503@ca.afilias.info>

On 12-04-11 09:24 AM, Richard Yen wrote:
> Hi,

Richard,

We've looked a bit at the files you sent me.

Jan was able to reproduce a very similar type of file where two 
processes were writing to the same file in "w' mode (like slony does).

Are any of the above possible:

1. You had multiple slon daemons writing to the same log archive 
directory (maybe for different clusters?)

2.  The mechanism you used for copying the .sql files could have caused 
processes to try to write to the same file on the destination machine

If the answer to both of those is no then maybe there is a bug in how 
archive file numbers are assigned in remote_worker.c:archive_open.
We don't YET see any obvious faults with this logic but if this logic 
somehow assigned 2 slon worker threads the same id then you could get a 
file like you sent us.


For others on the list the files looked like:

------------------------------------------------------------------
-- Slony-I log shipping archive
-- Node 5, Event 5000002252
------------------------------------------------------------------
set session_replication_role to replica;
start transaction;
select "_cluster".archiveTracking_offline('9750225', '2012-04-23 
10:43:10.921274');
-- end of log archiving header
------------------------------------------------------------------
-- start of Slony-I data
------------------------------------------------------------------

------------------------------------------------------------------
-- End Of Archive Log
------------------------------------------------------------------
commit;
vacuum analyze "_cluster".sl_archive_tracking;
@@@@@@@@@@@
...random SQL like text

where @@@ is really kilobytes of the NULL character (0x0).

We feel that one thread/process had written a longer version of the file 
while another process/thread had the same file open with a different 
file handle in "w" mode and it truncated the file, while the first 
process/thread still had the handle open pointing later on in the file.

Steve

>
> I've recently come across a situation where slony logshipping files were
> being written incorrectly.  Normally, the files are written in this order:
> - Begin transaction
> - Set archive tracking index
> - INSERTs/UPDATEs/DELETEs
> - Set sequences
> - Vacuum
> - Commit
>
> I noticed that in my particular instance, the files were being written
> as such:
> - Begin transaction
> - Set archive tracking index
> - Vacuum
> - Commit
> - INSERTs/UPDATEs/DELETEs
> - Set sequences
>
> Obviously, this sounds pretty dangerous, and I actually encountered the
> situation where my logship destination got corrupted, and needed to
> rebuild that node.
>
> Would anyone have any insight into why this happens?
>
> Using 2.0.6 on CentOS, with Postgres 8.4.5
>
> Thanks!
> --Richard
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From JanWieck at Yahoo.com  Fri May  4 05:10:09 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 04 May 2012 08:10:09 -0400
Subject: [Slony1-general] some questions which popped up while setting
 up...
In-Reply-To: <4F96BB48.3070003@nanl.de>
References: <4F96BB48.3070003@nanl.de>
Message-ID: <4FA3C721.5000506@Yahoo.com>

On 4/24/2012 10:40 AM, Mirko Vogt wrote:
> Okay, fine, obviously they try to connect to each other: so I purged out
> the respective node-definitions out of the slon_tools.conf file on the
> slaves (on node 11 I deleted the definition of node 12 and vice versa).

Removing the node information is definitely the wrong way. If you want 
to stop the replicas from talking to each other, remove the paths with 
the slonik command "DROP PATH". That way they will exchange their 
information via the central origin and everything will work.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From richyen at iparadigms.com  Fri May  4 14:46:26 2012
From: richyen at iparadigms.com (Richard Yen)
Date: Fri, 4 May 2012 14:46:26 -0700
Subject: [Slony1-general] Logship files printing incorrectly
In-Reply-To: <4FA1A9A5.1030503@ca.afilias.info>
References: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
	<4FA1A9A5.1030503@ca.afilias.info>
Message-ID: <CAKWMdJYLcS9=tMmdkAr1feQNxfg0zMB7Eak2Ln-YwY2iaBGoNw@mail.gmail.com>

On Wed, May 2, 2012 at 2:39 PM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Are any of the above possible:
>
> 1. You had multiple slon daemons writing to the same log archive directory
> (maybe for different clusters?)
>
We have several clusters writing to a directory, but there's a separate
directory for each cluster.  For example:

/home/log_ship
/home/log_ship/cluster1/new_logfiles
/home/log_ship/cluster2/new_logfiles
/home/log_ship/cluster3/new_logfiles
...etc...

We don't have two daemons writing to the same directory.


2.  The mechanism you used for copying the .sql files could have caused
> processes to try to write to the same file on the destination machine
>
I'm fairly certain this is not the case.  The files that I sent you were
directly from the origin machine, not from the destination machine.  Our
scheme is like this:

Node1 is origin
Node2 is subscriber, with -a mode, writing files to
/home/log_ship/cluster1/new_logfiles
Cronjob moves files from /home/log_ship/cluster1/new_logfiles to
/home/log_ship/cluster1/log_staging (we filter out the *.sql.tmp files so
that we can let them finish writing before we move them)
RemoteNode makes rsync connection to Node2 and copies the files from
Node2/home/log_ship/cluster1/log_staging to its local directory
Log files are replayed


> If the answer to both of those is no then maybe there is a bug in how
> archive file numbers are assigned in remote_worker.c:archive_open.
> We don't YET see any obvious faults with this logic but if this logic
> somehow assigned 2 slon worker threads the same id then you could get a
> file like you sent us.
>

As I look at the files you sent me, I only see differences between the
third (Node X, Event XXXXXX) and seventh
(archiveTracking_offline(xxx,'xxxx-xx-xx xx:xx:xx')) lines.  I noticed that
Node number can vary per file, but only one daemon has the -a option
enabled.  Not sure why the node number changes--shouldn't it always
correspond to the node number of the daemon with the -a option turned on?

Aside from that, I tried poking around the sl_* tables that I had dumped,
but didn't really find anything.  One thing is certain, though--a given DML
statement shows up in sl_log_x only once, even though it shows up several
times in the various logship files that are generated.  I can't seem to
find the corresponding sl_event row, so I'm not sure if there might be
anything in that direction, in terms of duplicated events.

--Richard
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120504/896a2a0c/attachment.htm 

From JanWieck at Yahoo.com  Sat May  5 07:14:16 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat, 05 May 2012 10:14:16 -0400
Subject: [Slony1-general] Logship files printing incorrectly
In-Reply-To: <4FA1A9A5.1030503@ca.afilias.info>
References: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
	<4FA1A9A5.1030503@ca.afilias.info>
Message-ID: <4FA535B8.5030408@Yahoo.com>

On 5/2/2012 5:39 PM, Steve Singer wrote:

> Jan was able to reproduce a very similar type of file where two
> processes were writing to the same file in "w' mode (like slony does).

Some more details about that:

The log file I have seen contains an OK but empty archive log at the 
beginning. All the usual content but just no data. Then follow ASCII NUL 
bytes up to exactly 32K, followed by some log data starting in the 
middle of a statement but not properly terminated by the final commit 
and vacuum statements.

The way I can create a file with a similar pattern is as follows:


proc1: FILE *fp = fopen("file", "w");
proc1: fprintf(fp, "data from proc1\n"); // continue doing that until
                                          // over 32K written

proc2: FILE *fp = fopen("file", "w");
proc2: fprintf(fp, "data from proc2\n");
proc2: fclose(fp);

proc1: fprintf(fp, "data from proc1\n");
proc1: fclose(fp);


What happens here is that both processes use fprintf(), like slon, and 
fprintf uses an internal buffer of 32K. When proc1 has written over 32K, 
the buffer had been flushed and the underlying file descriptor's lseek 
position is at 32K.

Now comes proc2 and opens the file also for writing. The underlying file 
descriptor is created with O_CREAT only, which means that the file gets 
truncated. proc2 writes its stuff and closes the file.

When proc1 finally closes the file it flushes its second buffer. Since 
the file was not opened in append mode, the truncate that happened in 
the meantime does not affect the lseek position in its file descriptor, 
so it writes that buffer at the 32K position and the OS fills in the 
void with NUL bytes.

I have attached a small patch against 2.0 that changes the opening of 
the temporary archive file to use O_CREAT|O_EXCL and then fdopen(3). 
This would cause proc2 above to fail with an error because the file 
already exists. This will tell you in the slon logs which slon is 
actually doing this.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: archive_fdopen.diff
Type: text/x-patch
Size: 1379 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20120505/abcbd91b/attachment.bin 

