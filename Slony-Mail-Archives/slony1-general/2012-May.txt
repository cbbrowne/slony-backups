From ssinger at ca.afilias.info  Wed May  2 14:39:49 2012
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 02 May 2012 17:39:49 -0400
Subject: [Slony1-general] Logship files printing incorrectly
In-Reply-To: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
References: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
Message-ID: <4FA1A9A5.1030503@ca.afilias.info>

On 12-04-11 09:24 AM, Richard Yen wrote:
> Hi,

Richard,

We've looked a bit at the files you sent me.

Jan was able to reproduce a very similar type of file where two 
processes were writing to the same file in "w' mode (like slony does).

Are any of the above possible:

1. You had multiple slon daemons writing to the same log archive 
directory (maybe for different clusters?)

2.  The mechanism you used for copying the .sql files could have caused 
processes to try to write to the same file on the destination machine

If the answer to both of those is no then maybe there is a bug in how 
archive file numbers are assigned in remote_worker.c:archive_open.
We don't YET see any obvious faults with this logic but if this logic 
somehow assigned 2 slon worker threads the same id then you could get a 
file like you sent us.


For others on the list the files looked like:

------------------------------------------------------------------
-- Slony-I log shipping archive
-- Node 5, Event 5000002252
------------------------------------------------------------------
set session_replication_role to replica;
start transaction;
select "_cluster".archiveTracking_offline('9750225', '2012-04-23 
10:43:10.921274');
-- end of log archiving header
------------------------------------------------------------------
-- start of Slony-I data
------------------------------------------------------------------

------------------------------------------------------------------
-- End Of Archive Log
------------------------------------------------------------------
commit;
vacuum analyze "_cluster".sl_archive_tracking;
@@@@@@@@@@@
...random SQL like text

where @@@ is really kilobytes of the NULL character (0x0).

We feel that one thread/process had written a longer version of the file 
while another process/thread had the same file open with a different 
file handle in "w" mode and it truncated the file, while the first 
process/thread still had the handle open pointing later on in the file.

Steve

>
> I've recently come across a situation where slony logshipping files were
> being written incorrectly.  Normally, the files are written in this order:
> - Begin transaction
> - Set archive tracking index
> - INSERTs/UPDATEs/DELETEs
> - Set sequences
> - Vacuum
> - Commit
>
> I noticed that in my particular instance, the files were being written
> as such:
> - Begin transaction
> - Set archive tracking index
> - Vacuum
> - Commit
> - INSERTs/UPDATEs/DELETEs
> - Set sequences
>
> Obviously, this sounds pretty dangerous, and I actually encountered the
> situation where my logship destination got corrupted, and needed to
> rebuild that node.
>
> Would anyone have any insight into why this happens?
>
> Using 2.0.6 on CentOS, with Postgres 8.4.5
>
> Thanks!
> --Richard
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From JanWieck at Yahoo.com  Fri May  4 05:10:09 2012
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 04 May 2012 08:10:09 -0400
Subject: [Slony1-general] some questions which popped up while setting
 up...
In-Reply-To: <4F96BB48.3070003@nanl.de>
References: <4F96BB48.3070003@nanl.de>
Message-ID: <4FA3C721.5000506@Yahoo.com>

On 4/24/2012 10:40 AM, Mirko Vogt wrote:
> Okay, fine, obviously they try to connect to each other: so I purged out
> the respective node-definitions out of the slon_tools.conf file on the
> slaves (on node 11 I deleted the definition of node 12 and vice versa).

Removing the node information is definitely the wrong way. If you want 
to stop the replicas from talking to each other, remove the paths with 
the slonik command "DROP PATH". That way they will exchange their 
information via the central origin and everything will work.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From richyen at iparadigms.com  Fri May  4 14:46:26 2012
From: richyen at iparadigms.com (Richard Yen)
Date: Fri, 4 May 2012 14:46:26 -0700
Subject: [Slony1-general] Logship files printing incorrectly
In-Reply-To: <4FA1A9A5.1030503@ca.afilias.info>
References: <CAKWMdJbsQYdQ_PoVoseQj=NcCf2Y0ZVro16Ba7yydFGoxz4Sjg@mail.gmail.com>
	<4FA1A9A5.1030503@ca.afilias.info>
Message-ID: <CAKWMdJYLcS9=tMmdkAr1feQNxfg0zMB7Eak2Ln-YwY2iaBGoNw@mail.gmail.com>

On Wed, May 2, 2012 at 2:39 PM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Are any of the above possible:
>
> 1. You had multiple slon daemons writing to the same log archive directory
> (maybe for different clusters?)
>
We have several clusters writing to a directory, but there's a separate
directory for each cluster.  For example:

/home/log_ship
/home/log_ship/cluster1/new_logfiles
/home/log_ship/cluster2/new_logfiles
/home/log_ship/cluster3/new_logfiles
...etc...

We don't have two daemons writing to the same directory.


2.  The mechanism you used for copying the .sql files could have caused
> processes to try to write to the same file on the destination machine
>
I'm fairly certain this is not the case.  The files that I sent you were
directly from the origin machine, not from the destination machine.  Our
scheme is like this:

Node1 is origin
Node2 is subscriber, with -a mode, writing files to
/home/log_ship/cluster1/new_logfiles
Cronjob moves files from /home/log_ship/cluster1/new_logfiles to
/home/log_ship/cluster1/log_staging (we filter out the *.sql.tmp files so
that we can let them finish writing before we move them)
RemoteNode makes rsync connection to Node2 and copies the files from
Node2/home/log_ship/cluster1/log_staging to its local directory
Log files are replayed


> If the answer to both of those is no then maybe there is a bug in how
> archive file numbers are assigned in remote_worker.c:archive_open.
> We don't YET see any obvious faults with this logic but if this logic
> somehow assigned 2 slon worker threads the same id then you could get a
> file like you sent us.
>

As I look at the files you sent me, I only see differences between the
third (Node X, Event XXXXXX) and seventh
(archiveTracking_offline(xxx,'xxxx-xx-xx xx:xx:xx')) lines.  I noticed that
Node number can vary per file, but only one daemon has the -a option
enabled.  Not sure why the node number changes--shouldn't it always
correspond to the node number of the daemon with the -a option turned on?

Aside from that, I tried poking around the sl_* tables that I had dumped,
but didn't really find anything.  One thing is certain, though--a given DML
statement shows up in sl_log_x only once, even though it shows up several
times in the various logship files that are generated.  I can't seem to
find the corresponding sl_event row, so I'm not sure if there might be
anything in that direction, in terms of duplicated events.

--Richard
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20120504/896a2a0c/attachment.htm 

