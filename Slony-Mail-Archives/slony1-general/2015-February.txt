From ssinger at ca.afilias.info  Mon Feb  2 06:41:36 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 02 Feb 2015 09:41:36 -0500
Subject: [Slony1-general] Slony 2.2.3 extended lag recovery
In-Reply-To: <685898092.2705713.1422550898490.JavaMail.yahoo@mail.yahoo.com>
References: <685898092.2705713.1422550898490.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <54CF8CA0.5090206@ca.afilias.info>

On 01/29/2015 12:01 PM, Glyn Astill wrote:


Why are you regularly running vacuum full (versus normal vaccum?)

Vacuum full will take an exclusive lock on the table for its duration. 
Also the cluster command tends to be faster than vacuum full but with 
similar results (re-writing your tables)

Are you running vacuum full on specific tables or on all tables in your 
cluster?

If you do a vacuum full on a table on your replica slony won't be able 
to insert any data into that table until the vacuum transaction is 
complete and the exclusive lock is released.





> Hi All,
>
> We're currently running slony 2.2.3 with 4 pg 9.0 nodes.  Occasionally since we upgraded from 2.1 I've been seeing some "humps" where subscribers are lagging and taking an extended period of time to recover.
>
> I can't ever reproduce it and I've come to a dead end.  I'm going to waffle a bit below, but I'm hoping someone can see something I'm missing.
>
> These humps appear to not really correlate with increased activity on the origin, and I've been struggling to put my finger on anything aggravating the issue.  Today however I've seen the same symptoms, and the start times of the lag align with an exclusive lock on a subscribers replicated table whilst vaccum full was run.
>
> Whilst I'd expect that to cause some lag and a bit of a backlog, the vacuum full took only 2 minutes and the lag builds up gradually afterwards.  Eventually after a long time replication will catch up, but it's out of proportion to our transaction rate, and a restart of the slon on the subscriber causes it to catch up very swiftly.  I've attached a graph of sl_status from the origin showing the time and event lag buildup and a pretty swift slice on the end of the humps where I restart the slons.
>
> The graph (attached) shows nodes 4 & 5 starting to lag first, as they were the first to have the vacuum full run, then node 7 starts to lag when it has the same vacuum full run (at this point the lag on the two other nodes hadn't been noticed).  This excerpt from one of the subscribers shows the copy being blocked:
>
> 2015-01-29 10:09:54 GMT [13246]: [39-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
> 2015-01-29 10:09:54 GMT [13246]: [40-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  SQL statement "SELECT "_main_replication".logswitch_start()"
> 2015-01-29 10:12:04 GMT [13243]: [9-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  duration: 5089.684 ms  statement: COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:19:05 GMT [13243]: [10-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  process 13243 still waiting for RowExclusiveLock on relation 279233 of database 274556 after 1000.038 ms at character 13
> 2015-01-29 10:19:05 GMT [13243]: [11-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost QUERY:  UPDATE ONLY "myschema"."table_being_full_vacuumed" SET "text" = $1 WHERE "address" = $2;
> 2015-01-29 10:19:05 GMT [13243]: [12-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost CONTEXT:  COPY sl_log_1, line 37: "8    1108084090    2    1219750937    myschema    table_being_full_vacuumed    U    1    {text,"",address,some_address_data}"
> 2015-01-29 10:19:05 GMT [13243]: [13-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost STATEMENT:  COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:20:43 GMT [13243]: [14-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  process 13243 acquired RowExclusiveLock on relation 279233 of database 274556 after 98754.902 ms at character 13
> 2015-01-29 10:20:43 GMT [13243]: [15-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost QUERY:  UPDATE ONLY "myschema"."table_being_full_vacuumed" SET "text" = $1 WHERE "address" = $2;
> 2015-01-29 10:20:43 GMT [13243]: [16-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost CONTEXT:  COPY sl_log_1, line 37: "8    1108084090    2    1219750937    myschema    table_being_full_vacuumed    U    1    {text,"",address,some_address_data}"
> 2015-01-29 10:20:43 GMT [13243]: [17-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost STATEMENT:  COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:20:43 GMT [13243]: [18-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  duration: 98915.154 ms  statement: COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:22:00 GMT [13246]: [41-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: log switch to sl_log_1 complete - truncate sl_log_2
> 2015-01-29 10:22:00 GMT [13246]: [42-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  PL/pgSQL function "cleanupevent" line 94 at assignment
> 2015-01-29 10:34:01 GMT [13246]: [43-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
> 2015-01-29 10:34:01 GMT [13246]: [44-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  SQL statement "SELECT "_main_replication".logswitch_start()"
> 2015-01-29 10:46:08 GMT [13246]: [45-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: could not lock sl_log_1 - sl_log_1 not truncated
> 2015-01-29 10:46:08 GMT [13246]: [46-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  PL/pgSQL function "cleanupevent" line 94 at assignment
>
> After this the copies go through cycles of increasing and decreacing duration, which I'm guessing is something normal (perhaps syncs being grouped?), and I'm seeing messages stating "could not lock sl_log_1 - sl_log_1 not truncated" a couple of times before the switch completes, and again I'm guessing this is just blocking because of inserts capturing changes and is normal? Autovacuum hasn't hit sl_log at all during this period.
>
> Does anyone have any ideas?  I've debug logs from the slons, and postgres logs I can send off list if anyone has any ideas.
>
> Thanks
> Glyn
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From glynastill at yahoo.co.uk  Mon Feb  2 09:59:54 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 2 Feb 2015 17:59:54 +0000 (UTC)
Subject: [Slony1-general] Slony 2.2.3 extended lag recovery
In-Reply-To: <54CF8CA0.5090206@ca.afilias.info>
References: <54CF8CA0.5090206@ca.afilias.info>
Message-ID: <725041738.1147357.1422899994574.JavaMail.yahoo@mail.yahoo.com>

----- Original Message -----

> From: Steve Singer <ssinger at ca.afilias.info>
> To: slony1-general at lists.slony.info
> Cc: 
> Sent: Monday, 2 February 2015, 14:41
> Subject: Re: [Slony1-general] Slony 2.2.3 extended lag recovery
> 
> On 01/29/2015 12:01 PM, Glyn Astill wrote:
> 
> 
> Why are you regularly running vacuum full (versus normal vaccum?)

> 

We're not regularly running vacuum full, I didn't state that.

> Vacuum full will take an exclusive lock on the table for its duration. 
> Also the cluster command tends to be faster than vacuum full but with 
> similar results (re-writing your tables)
> 
> Are you running vacuum full on specific tables or on all tables in your 
> cluster?

> 

Understood.

Just the one, just the once, I only mentioned it because it was the thing that caused the lag build up - and the lag was expected, no ignorance there.


However we've had similar long running backlogs since moving to 2.2 and we're definatey not ever running vacuum full at any other time.

> If you do a vacuum full on a table on your replica slony won't be able 
> to insert any data into that table until the vacuum transaction is 
> complete and the exclusive lock is released.
> 
> 


Understood.  My point is that we seem to be seeing lag recovery taking longer than expected since slony switched to using copy vs regular insert/update/delete.  



From tmblue at gmail.com  Tue Feb  3 14:27:37 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 3 Feb 2015 14:27:37 -0800
Subject: [Slony1-general] Slony monitoring, management
Message-ID: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>

Good afternoon,

I ran into an issue today where a node seemed to be lost, at 5am, a single
query node stopped replicating (although logs and slon process showed
things were healthy), but my primary DB kept storing entries until we hit
over 7 million rows, because my query db was not "doing something" that
something is a mystery. I finally dropped and re-added the node.   This has
been running stable since we went to 9.3.4 and 2.2.3 slony. Nothing
happened to the hardware, the query node was healthy, but seemingly not
replicating (replication check , showed that the queried table was not
 changing, when the other nodes saw the change).

So anyways, I didn't really know how to check to see what data was in
sl_logs (other than querying them), nor did I know of any way to verify
that data was moving out and being replicated (other than my repl check). I
wanted to know if there was something out there that peeled the covers back
on slon and sl_log to verify that things are replicating in a timely
period, to know where the data in the sl_log is destined to and if a single
host is holding up the show.

Just need more ways to check slons health, progress, backlog etc. Is there
a front end somewhere that will let you see into the inner workings and
states of Slony?

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150203/355c7b8d/attachment.htm 

From cbbrowne at afilias.info  Wed Feb  4 12:07:05 2015
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 4 Feb 2015 15:07:05 -0500
Subject: [Slony1-general] Slony monitoring, management
In-Reply-To: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
References: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
Message-ID: <CANfbgbZdqkZKQcATSn+ecjHSkTYcea4YT-UTM0u=9CcZYi62_A@mail.gmail.com>

There's a health check script that's kind of recommended...

http://slony.info/documentation/2.2/deploymentconcerns.html#TESTSLONYSTATE

I'd suggest the one, "test_slony_state.sh"; it looks at quite a few issues,
and I should think it would have noticed something going on.  I can't
predict offhand what it would have first complained about, but doubtless
something of some use.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150204/d8f1f79c/attachment.htm 

From tmblue at gmail.com  Wed Feb  4 12:54:35 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 4 Feb 2015 12:54:35 -0800
Subject: [Slony1-general] Slony monitoring, management
In-Reply-To: <CANfbgbZdqkZKQcATSn+ecjHSkTYcea4YT-UTM0u=9CcZYi62_A@mail.gmail.com>
References: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
	<CANfbgbZdqkZKQcATSn+ecjHSkTYcea4YT-UTM0u=9CcZYi62_A@mail.gmail.com>
Message-ID: <CAEaSS0ahsrdC=B2Qgpca8QR+TxcSYimv4-hVkGyKXS5GpD5V7g@mail.gmail.com>

On Wed, Feb 4, 2015 at 12:07 PM, Christopher Browne <cbbrowne at afilias.info>
wrote:

> There's a health check script that's kind of recommended...
>
> http://slony.info/documentation/2.2/deploymentconcerns.html#TESTSLONYSTATE
>
> I'd suggest the one, "test_slony_state.sh"; it looks at quite a few
> issues, and I should think it would have noticed something going on.  I
> can't predict offhand what it would have first complained about, but
> doubtless something of some use.
>

Thanks Christopher.

Anyone know more about this and why it seems to be looking for a node 0,
also why it's citing my truples are so many on my master node?

My existing graphs show the following for table sizes

count log1:  260.8K
size log 1:  99.81K
count log2:  8.54K
size log 2:  3.57K

I only have 4 nodes, node 1 through node 4..


Thanks some of these will definitely help, if they are reporting the right
data :)

Tory

Node: 0 threads seem stuck
================================================
Slony-I components have not reported into sl_components in interval 00:05:00

Perhaps slon is not running properly?

Query:
     select co_actor, co_pid, co_node, co_connection_pid, co_activity,
co_starttime, now() - co_starttime, co_event, co_eventtype
     from "_cls".sl_components
     where  (now() - co_starttime) > '00:05:00'::interval
     order by co_starttime;



Node: 1 sl_log_1 tuples = 241823 > 200000
================================================
Number of tuples in Slony-I table sl_log_1 is 241823 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150204/e88461da/attachment.htm 

From greg at endpoint.com  Wed Feb  4 16:13:12 2015
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 4 Feb 2015 19:13:12 -0500
Subject: [Slony1-general] Slony monitoring, management
In-Reply-To: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
References: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
Message-ID: <20150205001312.GS3022@broken.home>

On Tue, Feb 03, 2015 at 02:27:37PM -0800, Tory M Blue wrote:

> happened to the hardware, the query node was healthy, but seemingly not
> replicating (replication check , showed that the queried table was not
>  changing, when the other nodes saw the change).

Not quite what you are asking for, but the check_postgres script has a 
nice feature that simply adds a row and sees how long it takes to 
reach one or more destinations. It's a great canary-in-the-coalmine kind 
of check that Slony is up and running and doing what it is supposed to do.

https://bucardo.org/check_postgres/check_postgres.pl.html#replicate_row

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: Digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150204/5151c80f/attachment.pgp 

