From ssinger at ca.afilias.info  Mon Feb  2 06:41:36 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 02 Feb 2015 09:41:36 -0500
Subject: [Slony1-general] Slony 2.2.3 extended lag recovery
In-Reply-To: <685898092.2705713.1422550898490.JavaMail.yahoo@mail.yahoo.com>
References: <685898092.2705713.1422550898490.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <54CF8CA0.5090206@ca.afilias.info>

On 01/29/2015 12:01 PM, Glyn Astill wrote:


Why are you regularly running vacuum full (versus normal vaccum?)

Vacuum full will take an exclusive lock on the table for its duration. 
Also the cluster command tends to be faster than vacuum full but with 
similar results (re-writing your tables)

Are you running vacuum full on specific tables or on all tables in your 
cluster?

If you do a vacuum full on a table on your replica slony won't be able 
to insert any data into that table until the vacuum transaction is 
complete and the exclusive lock is released.





> Hi All,
>
> We're currently running slony 2.2.3 with 4 pg 9.0 nodes.  Occasionally since we upgraded from 2.1 I've been seeing some "humps" where subscribers are lagging and taking an extended period of time to recover.
>
> I can't ever reproduce it and I've come to a dead end.  I'm going to waffle a bit below, but I'm hoping someone can see something I'm missing.
>
> These humps appear to not really correlate with increased activity on the origin, and I've been struggling to put my finger on anything aggravating the issue.  Today however I've seen the same symptoms, and the start times of the lag align with an exclusive lock on a subscribers replicated table whilst vaccum full was run.
>
> Whilst I'd expect that to cause some lag and a bit of a backlog, the vacuum full took only 2 minutes and the lag builds up gradually afterwards.  Eventually after a long time replication will catch up, but it's out of proportion to our transaction rate, and a restart of the slon on the subscriber causes it to catch up very swiftly.  I've attached a graph of sl_status from the origin showing the time and event lag buildup and a pretty swift slice on the end of the humps where I restart the slons.
>
> The graph (attached) shows nodes 4 & 5 starting to lag first, as they were the first to have the vacuum full run, then node 7 starts to lag when it has the same vacuum full run (at this point the lag on the two other nodes hadn't been noticed).  This excerpt from one of the subscribers shows the copy being blocked:
>
> 2015-01-29 10:09:54 GMT [13246]: [39-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: Logswitch to sl_log_1 initiated
> 2015-01-29 10:09:54 GMT [13246]: [40-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  SQL statement "SELECT "_main_replication".logswitch_start()"
> 2015-01-29 10:12:04 GMT [13243]: [9-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  duration: 5089.684 ms  statement: COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:19:05 GMT [13243]: [10-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  process 13243 still waiting for RowExclusiveLock on relation 279233 of database 274556 after 1000.038 ms at character 13
> 2015-01-29 10:19:05 GMT [13243]: [11-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost QUERY:  UPDATE ONLY "myschema"."table_being_full_vacuumed" SET "text" = $1 WHERE "address" = $2;
> 2015-01-29 10:19:05 GMT [13243]: [12-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost CONTEXT:  COPY sl_log_1, line 37: "8    1108084090    2    1219750937    myschema    table_being_full_vacuumed    U    1    {text,"",address,some_address_data}"
> 2015-01-29 10:19:05 GMT [13243]: [13-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost STATEMENT:  COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:20:43 GMT [13243]: [14-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  process 13243 acquired RowExclusiveLock on relation 279233 of database 274556 after 98754.902 ms at character 13
> 2015-01-29 10:20:43 GMT [13243]: [15-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost QUERY:  UPDATE ONLY "myschema"."table_being_full_vacuumed" SET "text" = $1 WHERE "address" = $2;
> 2015-01-29 10:20:43 GMT [13243]: [16-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost CONTEXT:  COPY sl_log_1, line 37: "8    1108084090    2    1219750937    myschema    table_being_full_vacuumed    U    1    {text,"",address,some_address_data}"
> 2015-01-29 10:20:43 GMT [13243]: [17-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost STATEMENT:  COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:20:43 GMT [13243]: [18-1] app=slon.remoteWorkerThread_8,user=slony,db=X,host=somehost LOG:  duration: 98915.154 ms  statement: COPY "_main_replication"."sl_log_1" ( log_origin, log_txid,log_tableid,log_actionseq,log_tablenspname, log_tablerelname, log_cmdtype, log_cmdupdncols,log_cmdargs) FROM STDIN
> 2015-01-29 10:22:00 GMT [13246]: [41-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: log switch to sl_log_1 complete - truncate sl_log_2
> 2015-01-29 10:22:00 GMT [13246]: [42-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  PL/pgSQL function "cleanupevent" line 94 at assignment
> 2015-01-29 10:34:01 GMT [13246]: [43-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
> 2015-01-29 10:34:01 GMT [13246]: [44-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  SQL statement "SELECT "_main_replication".logswitch_start()"
> 2015-01-29 10:46:08 GMT [13246]: [45-1] app=slon.local_cleanup,user=slony,db=X,host=somehost NOTICE:  Slony-I: could not lock sl_log_1 - sl_log_1 not truncated
> 2015-01-29 10:46:08 GMT [13246]: [46-1] app=slon.local_cleanup,user=slony,db=X,host=somehost CONTEXT:  PL/pgSQL function "cleanupevent" line 94 at assignment
>
> After this the copies go through cycles of increasing and decreacing duration, which I'm guessing is something normal (perhaps syncs being grouped?), and I'm seeing messages stating "could not lock sl_log_1 - sl_log_1 not truncated" a couple of times before the switch completes, and again I'm guessing this is just blocking because of inserts capturing changes and is normal? Autovacuum hasn't hit sl_log at all during this period.
>
> Does anyone have any ideas?  I've debug logs from the slons, and postgres logs I can send off list if anyone has any ideas.
>
> Thanks
> Glyn
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From glynastill at yahoo.co.uk  Mon Feb  2 09:59:54 2015
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon, 2 Feb 2015 17:59:54 +0000 (UTC)
Subject: [Slony1-general] Slony 2.2.3 extended lag recovery
In-Reply-To: <54CF8CA0.5090206@ca.afilias.info>
References: <54CF8CA0.5090206@ca.afilias.info>
Message-ID: <725041738.1147357.1422899994574.JavaMail.yahoo@mail.yahoo.com>

----- Original Message -----

> From: Steve Singer <ssinger at ca.afilias.info>
> To: slony1-general at lists.slony.info
> Cc: 
> Sent: Monday, 2 February 2015, 14:41
> Subject: Re: [Slony1-general] Slony 2.2.3 extended lag recovery
> 
> On 01/29/2015 12:01 PM, Glyn Astill wrote:
> 
> 
> Why are you regularly running vacuum full (versus normal vaccum?)

> 

We're not regularly running vacuum full, I didn't state that.

> Vacuum full will take an exclusive lock on the table for its duration. 
> Also the cluster command tends to be faster than vacuum full but with 
> similar results (re-writing your tables)
> 
> Are you running vacuum full on specific tables or on all tables in your 
> cluster?

> 

Understood.

Just the one, just the once, I only mentioned it because it was the thing that caused the lag build up - and the lag was expected, no ignorance there.


However we've had similar long running backlogs since moving to 2.2 and we're definatey not ever running vacuum full at any other time.

> If you do a vacuum full on a table on your replica slony won't be able 
> to insert any data into that table until the vacuum transaction is 
> complete and the exclusive lock is released.
> 
> 


Understood.  My point is that we seem to be seeing lag recovery taking longer than expected since slony switched to using copy vs regular insert/update/delete.  



From tmblue at gmail.com  Tue Feb  3 14:27:37 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 3 Feb 2015 14:27:37 -0800
Subject: [Slony1-general] Slony monitoring, management
Message-ID: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>

Good afternoon,

I ran into an issue today where a node seemed to be lost, at 5am, a single
query node stopped replicating (although logs and slon process showed
things were healthy), but my primary DB kept storing entries until we hit
over 7 million rows, because my query db was not "doing something" that
something is a mystery. I finally dropped and re-added the node.   This has
been running stable since we went to 9.3.4 and 2.2.3 slony. Nothing
happened to the hardware, the query node was healthy, but seemingly not
replicating (replication check , showed that the queried table was not
 changing, when the other nodes saw the change).

So anyways, I didn't really know how to check to see what data was in
sl_logs (other than querying them), nor did I know of any way to verify
that data was moving out and being replicated (other than my repl check). I
wanted to know if there was something out there that peeled the covers back
on slon and sl_log to verify that things are replicating in a timely
period, to know where the data in the sl_log is destined to and if a single
host is holding up the show.

Just need more ways to check slons health, progress, backlog etc. Is there
a front end somewhere that will let you see into the inner workings and
states of Slony?

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150203/355c7b8d/attachment.htm 

From cbbrowne at afilias.info  Wed Feb  4 12:07:05 2015
From: cbbrowne at afilias.info (Christopher Browne)
Date: Wed, 4 Feb 2015 15:07:05 -0500
Subject: [Slony1-general] Slony monitoring, management
In-Reply-To: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
References: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
Message-ID: <CANfbgbZdqkZKQcATSn+ecjHSkTYcea4YT-UTM0u=9CcZYi62_A@mail.gmail.com>

There's a health check script that's kind of recommended...

http://slony.info/documentation/2.2/deploymentconcerns.html#TESTSLONYSTATE

I'd suggest the one, "test_slony_state.sh"; it looks at quite a few issues,
and I should think it would have noticed something going on.  I can't
predict offhand what it would have first complained about, but doubtless
something of some use.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150204/d8f1f79c/attachment.htm 

From tmblue at gmail.com  Wed Feb  4 12:54:35 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Wed, 4 Feb 2015 12:54:35 -0800
Subject: [Slony1-general] Slony monitoring, management
In-Reply-To: <CANfbgbZdqkZKQcATSn+ecjHSkTYcea4YT-UTM0u=9CcZYi62_A@mail.gmail.com>
References: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
	<CANfbgbZdqkZKQcATSn+ecjHSkTYcea4YT-UTM0u=9CcZYi62_A@mail.gmail.com>
Message-ID: <CAEaSS0ahsrdC=B2Qgpca8QR+TxcSYimv4-hVkGyKXS5GpD5V7g@mail.gmail.com>

On Wed, Feb 4, 2015 at 12:07 PM, Christopher Browne <cbbrowne at afilias.info>
wrote:

> There's a health check script that's kind of recommended...
>
> http://slony.info/documentation/2.2/deploymentconcerns.html#TESTSLONYSTATE
>
> I'd suggest the one, "test_slony_state.sh"; it looks at quite a few
> issues, and I should think it would have noticed something going on.  I
> can't predict offhand what it would have first complained about, but
> doubtless something of some use.
>

Thanks Christopher.

Anyone know more about this and why it seems to be looking for a node 0,
also why it's citing my truples are so many on my master node?

My existing graphs show the following for table sizes

count log1:  260.8K
size log 1:  99.81K
count log2:  8.54K
size log 2:  3.57K

I only have 4 nodes, node 1 through node 4..


Thanks some of these will definitely help, if they are reporting the right
data :)

Tory

Node: 0 threads seem stuck
================================================
Slony-I components have not reported into sl_components in interval 00:05:00

Perhaps slon is not running properly?

Query:
     select co_actor, co_pid, co_node, co_connection_pid, co_activity,
co_starttime, now() - co_starttime, co_event, co_eventtype
     from "_cls".sl_components
     where  (now() - co_starttime) > '00:05:00'::interval
     order by co_starttime;



Node: 1 sl_log_1 tuples = 241823 > 200000
================================================
Number of tuples in Slony-I table sl_log_1 is 241823 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150204/e88461da/attachment.htm 

From greg at endpoint.com  Wed Feb  4 16:13:12 2015
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 4 Feb 2015 19:13:12 -0500
Subject: [Slony1-general] Slony monitoring, management
In-Reply-To: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
References: <CAEaSS0a61LO64B_O8M_RihJdHp9Qxh3qSpFwni1LGwe6ViTWEA@mail.gmail.com>
Message-ID: <20150205001312.GS3022@broken.home>

On Tue, Feb 03, 2015 at 02:27:37PM -0800, Tory M Blue wrote:

> happened to the hardware, the query node was healthy, but seemingly not
> replicating (replication check , showed that the queried table was not
>  changing, when the other nodes saw the change).

Not quite what you are asking for, but the check_postgres script has a 
nice feature that simply adds a row and sees how long it takes to 
reach one or more destinations. It's a great canary-in-the-coalmine kind 
of check that Slony is up and running and doing what it is supposed to do.

https://bucardo.org/check_postgres/check_postgres.pl.html#replicate_row

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: Digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20150204/5151c80f/attachment.pgp 

From tmblue at gmail.com  Thu Feb  5 10:19:49 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 5 Feb 2015 10:19:49 -0800
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
Message-ID: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>

2015-02-05 09:53:29 PST clsdb postgres 10.13.200.232(54830) 51877
2015-02-05 09:53:29.976 PSTNOTICE:  Slony-I: log switch to sl_log_2
complete - truncate sl_log_1

2015-02-05 10:01:34 PST clsdb postgres 10.13.200.231(46083) 42459
2015-02-05 10:01:34.481 PSTNOTICE:  Slony-I: could not lock sl_log_1 -
sl_log_1 not truncated

Sooo I have 13 million rows in sl_log_1 and from my checks of various
tables things are replicated, but this table still has a lock and is not
being truncated, These errors have been happening since 12:08 AM..

my sl_log_2 table now has 8 million rows but I'm replicating and not adding
a bunch of data. We did some massive deletes last nights, 20million was the
last batch when things stopped seeming to switch and truncate.

Soooo, questions. How can I verify sl_log_1 can be truncated (everything in
it has been replicated) and how can I figure out what is locking, so that
slony can't truncate?

I'm not hurting, just stressing at this point

Thanks
tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150205/df81c31f/attachment.htm 

From ssinger at ca.afilias.info  Thu Feb  5 10:33:41 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 05 Feb 2015 13:33:41 -0500
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
In-Reply-To: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>
References: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>
Message-ID: <54D3B785.3010606@ca.afilias.info>

On 02/05/2015 01:19 PM, Tory M Blue wrote:
> 2015-02-05 09:53:29 PST clsdb postgres 10.13.200.232(54830) 51877
> 2015-02-05 09:53:29.976 PSTNOTICE:  Slony-I: log switch to sl_log_2
> complete - truncate sl_log_1
>
> 2015-02-05 10:01:34 PST clsdb postgres 10.13.200.231(46083) 42459
> 2015-02-05 10:01:34.481 PSTNOTICE:  Slony-I: could not lock sl_log_1 -
> sl_log_1 not truncated
>
> Sooo I have 13 million rows in sl_log_1 and from my checks of various
> tables things are replicated, but this table still has a lock and is not
> being truncated, These errors have been happening since 12:08 AM..
>
> my sl_log_2 table now has 8 million rows but I'm replicating and not
> adding a bunch of data. We did some massive deletes last nights,
> 20million was the last batch when things stopped seeming to switch and
> truncate.
>
> Soooo, questions. How can I verify sl_log_1 can be truncated (everything
> in it has been replicated) and how can I figure out what is locking, so
> that slony can't truncate?


Everything must be replicated the confirmation must make it back to the 
origin before a log can be truncated?

What does your sl_status on your origin show?

The most common reasons why log truncation hasn't happened are

1) Replication is behind
2) Confirmations aren't making it back to the origin
3) You have a long running transaction on the origin.  A log running 
transaction can prevent log switching from taking place even if that 
transaction doesn't actually change replicated tables.



>
> I'm not hurting, just stressing at this point
>
> Thanks
> tory
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From tmblue at gmail.com  Thu Feb  5 10:41:55 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 5 Feb 2015 10:41:55 -0800
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
In-Reply-To: <54D3B785.3010606@ca.afilias.info>
References: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>
	<54D3B785.3010606@ca.afilias.info>
Message-ID: <CAEaSS0bbWPSyZH15nEW11apgGp_psdqA-pyKEF_WE=Y5ZvqUHQ@mail.gmail.com>

On Thu, Feb 5, 2015 at 10:33 AM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 02/05/2015 01:19 PM, Tory M Blue wrote:
>
>> 2015-02-05 09:53:29 PST clsdb postgres 10.13.200.232(54830) 51877
>> 2015-02-05 09:53:29.976 PSTNOTICE:  Slony-I: log switch to sl_log_2
>> complete - truncate sl_log_1
>>
>> 2015-02-05 10:01:34 PST clsdb postgres 10.13.200.231(46083) 42459
>> 2015-02-05 10:01:34.481 PSTNOTICE:  Slony-I: could not lock sl_log_1 -
>> sl_log_1 not truncated
>>
>> Sooo I have 13 million rows in sl_log_1 and from my checks of various
>> tables things are replicated, but this table still has a lock and is not
>> being truncated, These errors have been happening since 12:08 AM..
>>
>> my sl_log_2 table now has 8 million rows but I'm replicating and not
>> adding a bunch of data. We did some massive deletes last nights,
>> 20million was the last batch when things stopped seeming to switch and
>> truncate.
>>
>> Soooo, questions. How can I verify sl_log_1 can be truncated (everything
>> in it has been replicated) and how can I figure out what is locking, so
>> that slony can't truncate?
>>
>
>
> Everything must be replicated the confirmation must make it back to the
> origin before a log can be truncated?
>
> What does your sl_status on your origin show?
>
> The most common reasons why log truncation hasn't happened are
>
> 1) Replication is behind
> 2) Confirmations aren't making it back to the origin
> 3) You have a long running transaction on the origin.  A log running
> transaction can prevent log switching from taking place even if that
> transaction doesn't actually change replicated tables.
>
>
>
>
>> I'm not hurting, just stressing at this point
>>
>> Thanks
>> tory
>>
>>
Thanks Steve

 st_origin | st_received | st_last_event |      st_last_event_ts       |
st_last
_received |      st_last_received_ts      |   st_last_received_event_ts   |
st_l
ag_num_events |   st_lag_time
-----------+-------------+---------------+-----------------------------+--------
----------+-------------------------------+-------------------------------+-----
--------------+-----------------
         1 |           3 |    5003919991 | 2015-02-05 10:39:51.6253-08 |
    5
003919976 | 2015-02-05 10:39:38.031286-08 | 2015-02-05 10:39:35.615581-08 |

           15 | 00:00:16.763937
         1 |           2 |    5003919991 | 2015-02-05 10:39:51.6253-08 |
    5
003919865 | 2015-02-05 10:37:37.127661-08 | 2015-02-05 10:37:34.596838-08 |

          126 | 00:02:17.78268
         1 |           4 |    5003919991 | 2015-02-05 10:39:51.6253-08 |
    5
003919982 | 2015-02-05 10:39:43.971439-08 | 2015-02-05 10:39:42.618684-08 |

            9 | 00:00:09.760834
(3 rows)

Not sure what this means exactly, but again if we are fully replicated,
that would tell me that we have syncs being replied to.  Obviously we have
new data coming in but it's just slowing growing sl_log_2, which can't
switch to sl_log_1, because _log_1 has not been able to truncate. And this
is after restarting slon (for grins!)

Thanks!
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150205/d8cb96f5/attachment-0001.htm 

From ssinger at ca.afilias.info  Thu Feb  5 10:45:32 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 05 Feb 2015 13:45:32 -0500
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
In-Reply-To: <CAEaSS0bbWPSyZH15nEW11apgGp_psdqA-pyKEF_WE=Y5ZvqUHQ@mail.gmail.com>
References: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>	<54D3B785.3010606@ca.afilias.info>
	<CAEaSS0bbWPSyZH15nEW11apgGp_psdqA-pyKEF_WE=Y5ZvqUHQ@mail.gmail.com>
Message-ID: <54D3BA4C.9070607@ca.afilias.info>

On 02/05/2015 01:41 PM, Tory M Blue wrote:
>

>
>
>     Everything must be replicated the confirmation must make it back to
>     the origin before a log can be truncated?
>
>     What does your sl_status on your origin show?
>
>     The most common reasons why log truncation hasn't happened are
>
>     1) Replication is behind
>     2) Confirmations aren't making it back to the origin
>     3) You have a long running transaction on the origin.  A log running
>     transaction can prevent log switching from taking place even if that
>     transaction doesn't actually change replicated tables.
>
>
>
>
>         I'm not hurting, just stressing at this point
>
>         Thanks
>         tory
>
>
> Thanks Steve
>
>   st_origin | st_received | st_last_event |      st_last_event_ts
> | st_last
> _received |      st_last_received_ts      |   st_last_received_event_ts
>    | st_l
> ag_num_events |   st_lag_time
> -----------+-------------+---------------+-----------------------------+--------
> ----------+-------------------------------+-------------------------------+-----
> --------------+-----------------
>           1 |           3 |    5003919991 | 2015-02-05 10:39:51.6253-08
> |       5
> 003919976 | 2015-02-05 10:39:38.031286-08 | 2015-02-05 10:39:35.615581-08 |
>             15 | 00:00:16.763937
>           1 |           2 |    5003919991 | 2015-02-05 10:39:51.6253-08
> |       5
> 003919865 | 2015-02-05 10:37:37.127661-08 | 2015-02-05 10:37:34.596838-08 |
>            126 | 00:02:17.78268
>           1 |           4 |    5003919991 | 2015-02-05 10:39:51.6253-08
> |       5
> 003919982 | 2015-02-05 10:39:43.971439-08 | 2015-02-05 10:39:42.618684-08 |
>              9 | 00:00:09.760834
> (3 rows)
>
> Not sure what this means exactly, but again if we are fully replicated,
> that would tell me that we have syncs being replied to.  Obviously we
> have new data coming in but it's just slowing growing sl_log_2, which
> can't switch to sl_log_1, because _log_1 has not been able to truncate.
> And this is after restarting slon (for grins!)


Based on that I'd say the most likely culprit is (3) since it doesn't 
appear to be case 1 or 2. Do you have any long running transactions on 
your origin?





>
> Thanks!
> Tory


From tmblue at gmail.com  Thu Feb  5 11:03:18 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 5 Feb 2015 11:03:18 -0800
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
In-Reply-To: <54D3BA4C.9070607@ca.afilias.info>
References: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>
	<54D3B785.3010606@ca.afilias.info>
	<CAEaSS0bbWPSyZH15nEW11apgGp_psdqA-pyKEF_WE=Y5ZvqUHQ@mail.gmail.com>
	<54D3BA4C.9070607@ca.afilias.info>
Message-ID: <CAEaSS0YOJgSLs9P4MbKzUCo26MUzd9F_89eV2VA9wB05LhjaJQ@mail.gmail.com>

On Thu, Feb 5, 2015 at 10:45 AM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 02/05/2015 01:41 PM, Tory M Blue wrote:
>
>>
>>
>
>>
>>     Everything must be replicated the confirmation must make it back to
>>     the origin before a log can be truncated?
>>
>>     What does your sl_status on your origin show?
>>
>>     The most common reasons why log truncation hasn't happened are
>>
>>     1) Replication is behind
>>     2) Confirmations aren't making it back to the origin
>>     3) You have a long running transaction on the origin.  A log running
>>     transaction can prevent log switching from taking place even if that
>>     transaction doesn't actually change replicated tables.
>>
>>
>>
>>
>>         I'm not hurting, just stressing at this point
>>
>>         Thanks
>>         tory
>>
>>
>> Thanks Steve
>>
>>   st_origin | st_received | st_last_event |      st_last_event_ts
>> | st_last
>> _received |      st_last_received_ts      |   st_last_received_event_ts
>>    | st_l
>> ag_num_events |   st_lag_time
>> -----------+-------------+---------------+------------------
>> -----------+--------
>> ----------+-------------------------------+-----------------
>> --------------+-----
>> --------------+-----------------
>>           1 |           3 |    5003919991 | 2015-02-05 10:39:51.6253-08
>> |       5
>> 003919976 | 2015-02-05 10:39:38.031286-08 | 2015-02-05 10:39:35.615581-08
>> |
>>             15 | 00:00:16.763937
>>           1 |           2 |    5003919991 | 2015-02-05 10:39:51.6253-08
>> |       5
>> 003919865 | 2015-02-05 10:37:37.127661-08 | 2015-02-05 10:37:34.596838-08
>> |
>>            126 | 00:02:17.78268
>>           1 |           4 |    5003919991 | 2015-02-05 10:39:51.6253-08
>> |       5
>> 003919982 | 2015-02-05 10:39:43.971439-08 | 2015-02-05 10:39:42.618684-08
>> |
>>              9 | 00:00:09.760834
>> (3 rows)
>>
>> Not sure what this means exactly, but again if we are fully replicated,
>> that would tell me that we have syncs being replied to.  Obviously we
>> have new data coming in but it's just slowing growing sl_log_2, which
>> can't switch to sl_log_1, because _log_1 has not been able to truncate.
>> And this is after restarting slon (for grins!)
>>
>
>
> Based on that I'd say the most likely culprit is (3) since it doesn't
> appear to be case 1 or 2. Do you have any long running transactions on your
> origin?
>
>
I'm looking and don't see any long running queries, obviously with such a
large sl_log table now, updates are in the 5-9 second range

2015-02-05 10:50:58 PST clsdb postgres 10.13.200.242(45605) 39989
2015-02-05 10:50:58.522 PSTLOG:  duration: 6350.246 ms  statement: COPY (
select log_origin, log_txid,LOTS OF STUFF DELETED.

Also the below tells me unless 2.2 is totally different that everything in
my log tables has been syncd and just needs to be deleted?

 ev_origin |  ev_seqno  | txid_snapshot_xmin
-----------+------------+--------------------
         1 | 5003918695 |          459837392
         2 | 5000984115 |           34125442
         3 | 5000016464 |           15773130
         4 | 5000878307 |           15901785
(4 rows)

 Thanks Steve!

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150205/ca86036c/attachment.htm 

From ssinger at ca.afilias.info  Thu Feb  5 11:41:43 2015
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 05 Feb 2015 14:41:43 -0500
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
In-Reply-To: <CAEaSS0YOJgSLs9P4MbKzUCo26MUzd9F_89eV2VA9wB05LhjaJQ@mail.gmail.com>
References: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>	<54D3B785.3010606@ca.afilias.info>	<CAEaSS0bbWPSyZH15nEW11apgGp_psdqA-pyKEF_WE=Y5ZvqUHQ@mail.gmail.com>	<54D3BA4C.9070607@ca.afilias.info>
	<CAEaSS0YOJgSLs9P4MbKzUCo26MUzd9F_89eV2VA9wB05LhjaJQ@mail.gmail.com>
Message-ID: <54D3C777.3070402@ca.afilias.info>

On 02/05/2015 02:03 PM, Tory M Blue wrote:
>
>
> On Thu, Feb 5, 2015 at 10:45 AM, Steve Singer <ssinger at ca.afilias.info
> <mailto:ssinger at ca.afilias.info>> wrote:
>
>     On 02/05/2015 01:41 PM, Tory M Blue wrote:
>
>
>
>
>
>              Everything must be replicated the confirmation must make it
>         back to
>              the origin before a log can be truncated?
>
>              What does your sl_status on your origin show?
>
>              The most common reasons why log truncation hasn't happened are
>
>              1) Replication is behind
>              2) Confirmations aren't making it back to the origin
>              3) You have a long running transaction on the origin.  A
>         log running
>              transaction can prevent log switching from taking place
>         even if that
>              transaction doesn't actually change replicated tables.
>
>
>
>
>                  I'm not hurting, just stressing at this point
>
>                  Thanks
>                  tory
>
>
>         Thanks Steve
>
>            st_origin | st_received | st_last_event |      st_last_event_ts
>         | st_last
>         _received |      st_last_received_ts      |
>           st_last_received_event_ts
>             | st_l
>         ag_num_events |   st_lag_time
>         -----------+-------------+----__-----------+------------------__-----------+--------
>         ----------+-------------------__------------+-----------------__--------------+-----
>         --------------+---------------__--
>                    1 |           3 | 5003919991 <tel:5003919991> |
>         2015-02-05 10:39:51.6253-08
>         |       5
>         003919976 | 2015-02-05 10:39:38.031286-08 | 2015-02-05
>         10:39:35.615581-08 |
>                      15 | 00:00:16.763937
>                    1 |           2 | 5003919991 <tel:5003919991> |
>         2015-02-05 10:39:51.6253-08
>         |       5
>         003919865 | 2015-02-05 10:37:37.127661-08 | 2015-02-05
>         10:37:34.596838-08 |
>                     126 | 00:02:17.78268
>                    1 |           4 | 5003919991 <tel:5003919991> |
>         2015-02-05 10:39:51.6253-08
>         |       5
>         003919982 | 2015-02-05 10:39:43.971439-08 | 2015-02-05
>         10:39:42.618684-08 |
>                       9 | 00:00:09.760834
>         (3 rows)
>
>         Not sure what this means exactly, but again if we are fully
>         replicated,
>         that would tell me that we have syncs being replied to.
>         Obviously we
>         have new data coming in but it's just slowing growing sl_log_2,
>         which
>         can't switch to sl_log_1, because _log_1 has not been able to
>         truncate.
>         And this is after restarting slon (for grins!)
>
>
>
>     Based on that I'd say the most likely culprit is (3) since it
>     doesn't appear to be case 1 or 2. Do you have any long running
>     transactions on your origin?
>
>
> I'm looking and don't see any long running queries, obviously with such
> a large sl_log table now, updates are in the 5-9 second range


Your looking for long running transactions not queries.

What does this give you?

select state,min(xact_start) from pg_stat_activity group by state ;

and what is
select txid_current_snapshot()

>
> 2015-02-05 10:50:58 PST clsdb postgres 10.13.200.242(45605) 39989
> 2015-02-05 10:50:58.522 PSTLOG:  duration: 6350.246 ms  statement: COPY
> ( select log_origin, log_txid,LOTS OF STUFF DELETED.
>
> Also the below tells me unless 2.2 is totally different that everything
> in my log tables has been syncd and just needs to be deleted?
>
>   ev_origin |  ev_seqno  | txid_snapshot_xmin
> -----------+------------+--------------------
>           1 | 5003918695 <tel:5003918695> |          459837392
>           2 | 5000984115 |           34125442
>           3 | 5000016464 |           15773130
>           4 | 5000878307 |           15901785
> (4 rows)
>
>   Thanks Steve!
>
> Tory


From tmblue at gmail.com  Thu Feb  5 12:25:35 2015
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 5 Feb 2015 12:25:35 -0800
Subject: [Slony1-general] sl_log_1 not truncated, could not lock
In-Reply-To: <54D3C777.3070402@ca.afilias.info>
References: <CAEaSS0bYKrArHg86iith=1orwa4SO2s9dhk9ga5BMFyXHF9eyA@mail.gmail.com>
	<54D3B785.3010606@ca.afilias.info>
	<CAEaSS0bbWPSyZH15nEW11apgGp_psdqA-pyKEF_WE=Y5ZvqUHQ@mail.gmail.com>
	<54D3BA4C.9070607@ca.afilias.info>
	<CAEaSS0YOJgSLs9P4MbKzUCo26MUzd9F_89eV2VA9wB05LhjaJQ@mail.gmail.com>
	<54D3C777.3070402@ca.afilias.info>
Message-ID: <CAEaSS0ait0dZHDO+HB-4_VGtAA=T7CHzOK+5p_YA4=7QmYXKqQ@mail.gmail.com>

On Thu, Feb 5, 2015 at 11:41 AM, Steve Singer <ssinger at ca.afilias.info>
wrote:

> On 02/05/2015 02:03 PM, Tory M Blue wrote:
>
>>
>>
>> On Thu, Feb 5, 2015 at 10:45 AM, Steve Singer <ssinger at ca.afilias.info
>> <mailto:ssinger at ca.afilias.info>> wrote:
>>
>>     On 02/05/2015 01:41 PM, Tory M Blue wrote:
>>
>>
>>
>>
>>
> Your looking for long running transactions not queries.
>
> What does this give you?
>
> select state,min(xact_start) from pg_stat_activity group by state ;
>

 state  |              min
--------+-------------------------------
 active | 2015-02-05 12:14:13.445753-08
 idle   |
(2 rows)


>
> and what is
> select txid_current_snapshot(


 txid_current_snapshot
-------------------------------
 460123733:460124165:460123733
(1 row)


I didn't see anything in the pg_stat table that was not a current job, or a
job that had been running for any length of time..

I went ahead and manually truncated sl_log_1 , the next log switch worked
just fine, and the following truncate of sl_log_2 worked automatically as
well. I think this has something to do with the size of the table, and the
fact that sync events were taking 5-9 seconds each. Maybe, the truncate was
being blocked based on those events? I did some checking and I felt secure
that all events in sl_log_1 were in fact sync'd/replicated. I can't tell
you what was locking that table however.

But as of right now I seem to be in a good state

I based the commands off this thread/site

http://venublog.com/2012/03/12/postgresql-slony-replication-lag-manual-log-tables-cleanup/

older version but since I was not doing a delete and following the truncate
that 2.2 uses, felt I was in okay shape to go behind the curtains and try
to shake slon loose.

Thanks Steve!
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150205/5a885f94/attachment.htm 

From mark.steben at drivedominion.com  Thu Feb 19 08:38:50 2015
From: mark.steben at drivedominion.com (Mark Steben)
Date: Thu, 19 Feb 2015 11:38:50 -0500
Subject: [Slony1-general] Fwd: The results of your email commands
In-Reply-To: <mailman.8.1424355163.4545.slony1-general@lists.slony.info>
References: <mailman.8.1424355163.4545.slony1-general@lists.slony.info>
Message-ID: <CADyzmyySKqmzrASG3+zX6PTe2tOHf-oqUjapbzvY9FY=hYtsaA@mail.gmail.com>

Greetings, I put in a question to slony_general_requests and got this back
almost immediately.
Does this mean I'm in the queue or have I been bounced out?
Thx, Mark

---------- Forwarded message ----------
From: <slony1-general-bounces at lists.slony.info>
Date: Thu, Feb 19, 2015 at 9:12 AM
Subject: The results of your email commands
To: mark.steben at drivedominion.com


The results of your email command are provided below. Attached is your
original message.

- Results:
    Ignoring non-text/plain MIME parts

- Unprocessed:
    We are running the following on both master and slave: (a simple 1
master
    to 1 slave configuration)
        postgresql 9.2.5
        slony1-2.2.2
         x86_64 GNU/Linux
    We currently run altperl scripts to kill / start slon daemons from the
    slave:
       cd ...bin folder
       ./slon_kill -c .../slon_tools.....conf
             and
       ./slon_start -c ../slon_tools...conf 1 (and 2)
     Because we need to run maintenance on the replicated db on the master
    without slony running I would like to run these commands on the master
    before and after the maintenance.  Since the daemons now run on the
slave
    when I attempt to run these commands on the master the daemons aren't
    found.  Is
    there a prescribed way to accomplish this?  I could continue to run
them on
    the
    slave and send a flag to the master when complete but I'd like to take a
    simpler approach if possible.
     Any insight appreciated.  Thank you.

- Ignored:


    --
    *Mark Steben*
     Database Administrator
    @utoRevenue <http://www.autorevenue.com/> | Autobase
    <http://www.autobase.net/>
      CRM division of Dominion Dealer Solutions
    95D Ashley Ave.
    West Springfield, MA 01089
    t: 413.327-3045
    f: 413.383-9567

    www.fb.com/DominionDealerSolutions
    www.twitter.com/DominionDealer
     www.drivedominion.com <http://www.autorevenue.com/>

    <http://autobasedigital.net/marketing/DD12_sig.jpg>

- Done.



---------- Forwarded message ----------
From: Mark Steben <mark.steben at drivedominion.com>
To: slony1-general-request at lists.slony.info
Cc:
Date: Thu, 19 Feb 2015 09:15:06 -0500
Subject: slon_kill, slon_start to run on master
Good morning,

We are running the following on both master and slave: (a simple 1 master
to 1 slave configuration)
    postgresql 9.2.5
    slony1-2.2.2
     x86_64 GNU/Linux

We currently run altperl scripts to kill / start slon daemons from the
slave:
   cd ...bin folder
   ./slon_kill -c .../slon_tools.....conf
         and
   ./slon_start -c ../slon_tools...conf 1 (and 2)

 Because we need to run maintenance on the replicated db on the master
without slony running I would like to run these commands on the master
before and after the maintenance.  Since the daemons now run on the slave
when I attempt to run these commands on the master the daemons aren't
found.  Is
there a prescribed way to accomplish this?  I could continue to run them on
the
slave and send a flag to the master when complete but I'd like to take a
simpler approach if possible.
 Any insight appreciated.  Thank you.


-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase
<http://www.autobase.net/>
  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>







-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase
<http://www.autobase.net/>
  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150219/3d97b7dc/attachment.htm 

From mark.steben at drivedominion.com  Thu Feb 19 08:59:22 2015
From: mark.steben at drivedominion.com (Mark Steben)
Date: Thu, 19 Feb 2015 11:59:22 -0500
Subject: [Slony1-general] Wish to run altperl scripts on master rather than
	slave
Message-ID: <CADyzmyxuY1Z5zCTuow_Ca4EA0f=UQmyrwLzennWsjKDXSC9yAw@mail.gmail.com>

Good morning,

We are running the following on both master and slave: (a simple 1 master
to 1 slave configuration)
    postgresql 9.2.5
    slony1-2.2.2
     x86_64 GNU/Linux

We currently run altperl scripts to kill / start slon daemons from the
slave:
   cd ...bin folder
   ./slon_kill -c .../slon_tools.....conf
         and
   ./slon_start -c ../slon_tools...conf 1 (and 2)

 Because we need to run maintenance on the replicated db on the master
without slony running I would like to run these commands on the master
before and after the maintenance.  Since the daemons now run on the slave
when I attempt to run these commands on the master the daemons aren't
found.  Is
there a prescribed way to accomplish this?  I could continue to run them on
the
slave and send a flag to the master when complete but I'd like to take a
simpler approach if possible.
 Any insight appreciated.  Thank you.



-- 
*Mark Steben*
 Database Administrator
@utoRevenue <http://www.autorevenue.com/> | Autobase
<http://www.autobase.net/>
  CRM division of Dominion Dealer Solutions
95D Ashley Ave.
West Springfield, MA 01089
t: 413.327-3045
f: 413.383-9567

www.fb.com/DominionDealerSolutions
www.twitter.com/DominionDealer
 www.drivedominion.com <http://www.autorevenue.com/>

<http://autobasedigital.net/marketing/DD12_sig.jpg>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150219/c5ae6ab0/attachment.htm 

From cbbrowne at afilias.info  Thu Feb 19 12:05:22 2015
From: cbbrowne at afilias.info (Christopher Browne)
Date: Thu, 19 Feb 2015 15:05:22 -0500
Subject: [Slony1-general] Wish to run altperl scripts on master rather
 than slave
In-Reply-To: <CADyzmyxuY1Z5zCTuow_Ca4EA0f=UQmyrwLzennWsjKDXSC9yAw@mail.gmail.com>
References: <CADyzmyxuY1Z5zCTuow_Ca4EA0f=UQmyrwLzennWsjKDXSC9yAw@mail.gmail.com>
Message-ID: <CANfbgbY1gJa6bGYFjbV=ukOKNzA3Zs28HkeQBCFWoChM7q2ChA@mail.gmail.com>

On Thu, Feb 19, 2015 at 11:59 AM, Mark Steben <mark.steben at drivedominion.com
> wrote:

> Good morning,
>
> We are running the following on both master and slave: (a simple 1 master
> to 1 slave configuration)
>     postgresql 9.2.5
>     slony1-2.2.2
>      x86_64 GNU/Linux
>
> We currently run altperl scripts to kill / start slon daemons from the
> slave:
>    cd ...bin folder
>    ./slon_kill -c .../slon_tools.....conf
>          and
>    ./slon_start -c ../slon_tools...conf 1 (and 2)
>
>  Because we need to run maintenance on the replicated db on the master
> without slony running I would like to run these commands on the master
> before and after the maintenance.  Since the daemons now run on the slave
> when I attempt to run these commands on the master the daemons aren't
> found.  Is
> there a prescribed way to accomplish this?  I could continue to run them
> on the
> slave and send a flag to the master when complete but I'd like to take a
> simpler approach if possible.
>  Any insight appreciated.  Thank you.
>

There are three things you are identifying here, and they are each quite
independent of each other:

a) Each database participating in replication is a "cluster node", and will
run whereever it happens to run

b) Each node requires a slon process that manages replicating data to that
node, as well as bookkeeping (e.g. - managing the flow of replication
events)

c) Slonik is the tool that manages configuration of the cluster; it must
have access to all of the nodes that it is to manage

The only one of those things that needs to run in a particular place is the
set of Postgres databases.  (And you get to pick where they run.)

There is no "prescribed way" to run the slon processes of b); you are free
to run those processes where ever you prefer.  We have found it useful to
run all the slon processes for the replicas within a given data centre on
the same host, as it is generally more convenient to manage logs and
restart processes if they are in one place.  You are apparently running
them on the same host that is also hosting one of the replicated databases;
nothing wrong with that.

If you want to run slonik on another host, that's fine, but, as observed,
if you need to also do management of slon processes (e.g. - need to restart
them), it tends to be convenient to run slonik on the same node so that
your shell scripts can both manage the slon processes and the slonik
scripts.

The approach we have tended to take has been to define a "database
administration" host which hosts both slons and slonik.  That seems to be
most convenient.  Commonly, we have added a host (real or virtual) that is
devoted to this sort of thing, separate from the hosts supporting Postgres
backends.

That adds an extra host, so I don't think it's entirely fair to call it a
"simpler approach"; by having a separate host, we don't need to think about
whether that host is hosting an origin or a subscriber, or to imagine we
need to shift database management tasks elsewhere supposing we reshape a
cluster.  We just assume "connect to the DB App Server and manage things
from there."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20150219/2a38fe64/attachment.htm 

